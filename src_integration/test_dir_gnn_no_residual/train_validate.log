2023-03-14 21:48:17,084 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-14 21:49:27,900 - INFO - data - average code tokens = 109.28999515442095
2023-03-14 21:49:27,900 - INFO - data - average ast tokens = 188.85505342888476
2023-03-14 21:49:27,900 - INFO - data - average text tokens = 15.993139680191783
2023-03-14 21:49:27,900 - INFO - data - average position tokens = 188.85505342888476
2023-03-14 21:49:27,900 - INFO - data - average ast edges = 375.7101068577695
2023-03-14 21:49:45,091 - INFO - data - code vocab length = 26684
2023-03-14 21:49:45,091 - INFO - data - text vocab length = 13207
2023-03-14 21:49:45,091 - INFO - data - position vocab length = 20587
2023-03-14 21:49:55,986 - INFO - model - Build Model...
2023-03-14 21:49:56,572 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (2): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (3): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (4): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (5): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-14 21:49:56,576 - INFO - model - Total parameters number: 91563520
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,577 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.weight               [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.bias                 [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.weight               [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.bias                 [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.weight               [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.bias                 [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.weight               [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.bias                 [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.bias [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,578 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,579 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-14 21:49:56,580 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,581 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-14 21:49:56,582 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-14 21:49:56,582 - INFO - model - The model is built.
2023-03-14 21:49:56,583 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-14 21:49:56,584 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-14 21:49:57,750 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-14 21:49:57,750 - INFO - __main__ - Scheduler = StepLR
2023-03-14 21:49:57,751 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-14 21:49:57,751 - INFO - __main__ - Epoch 1
2023-03-14 21:50:26,008 - INFO - __main__ - Epoch   1, Step:     100, Batch Loss:    90.191246, Lr: 0.000100, Tokens per sec:   1874
2023-03-14 21:50:52,159 - INFO - __main__ - Epoch   1, Step:     200, Batch Loss:    84.933388, Lr: 0.000100, Tokens per sec:   2031
2023-03-14 21:51:18,327 - INFO - __main__ - Epoch   1, Step:     300, Batch Loss:    74.271103, Lr: 0.000100, Tokens per sec:   2100
2023-03-14 21:51:44,149 - INFO - __main__ - Epoch   1, Step:     400, Batch Loss:    72.588348, Lr: 0.000100, Tokens per sec:   2103
2023-03-14 21:52:10,818 - INFO - __main__ - Epoch   1, Step:     500, Batch Loss:    68.287971, Lr: 0.000100, Tokens per sec:   2037
2023-03-14 21:52:37,175 - INFO - __main__ - Epoch   1, Step:     600, Batch Loss:    90.235916, Lr: 0.000100, Tokens per sec:   2044
2023-03-14 21:53:03,521 - INFO - __main__ - Epoch   1, Step:     700, Batch Loss:    78.997665, Lr: 0.000100, Tokens per sec:   2035
2023-03-14 21:53:30,105 - INFO - __main__ - Epoch   1, Step:     800, Batch Loss:    71.602997, Lr: 0.000100, Tokens per sec:   2027
2023-03-14 21:53:56,382 - INFO - __main__ - Epoch   1, Step:     900, Batch Loss:    74.175194, Lr: 0.000100, Tokens per sec:   2079
2023-03-14 21:54:22,784 - INFO - __main__ - Epoch   1, Step:    1000, Batch Loss:    82.072205, Lr: 0.000100, Tokens per sec:   2032
2023-03-14 21:54:49,119 - INFO - __main__ - Epoch   1, Step:    1100, Batch Loss:    81.505234, Lr: 0.000100, Tokens per sec:   2007
2023-03-14 21:55:15,448 - INFO - __main__ - Epoch   1, Step:    1200, Batch Loss:    69.212242, Lr: 0.000100, Tokens per sec:   2040
2023-03-14 21:55:41,880 - INFO - __main__ - Epoch   1, Step:    1300, Batch Loss:    74.317749, Lr: 0.000100, Tokens per sec:   2060
2023-03-14 21:56:08,286 - INFO - __main__ - Epoch   1, Step:    1400, Batch Loss:    76.439407, Lr: 0.000100, Tokens per sec:   2007
2023-03-14 21:56:35,263 - INFO - __main__ - Epoch   1, Step:    1500, Batch Loss:    88.694557, Lr: 0.000100, Tokens per sec:   1980
2023-03-14 21:57:01,409 - INFO - __main__ - Epoch   1, Step:    1600, Batch Loss:    96.787071, Lr: 0.000100, Tokens per sec:   2070
2023-03-14 21:57:27,813 - INFO - __main__ - Epoch   1, Step:    1700, Batch Loss:    87.422974, Lr: 0.000100, Tokens per sec:   2072
2023-03-14 21:57:54,673 - INFO - __main__ - Epoch   1, Step:    1800, Batch Loss:    85.552956, Lr: 0.000100, Tokens per sec:   2018
2023-03-14 21:58:21,378 - INFO - __main__ - Epoch   1, Step:    1900, Batch Loss:    90.601059, Lr: 0.000100, Tokens per sec:   2016
2023-03-14 21:58:47,716 - INFO - __main__ - Epoch   1, Step:    2000, Batch Loss:    71.145851, Lr: 0.000100, Tokens per sec:   2052
2023-03-14 21:59:14,402 - INFO - __main__ - Epoch   1, Step:    2100, Batch Loss:    71.969208, Lr: 0.000100, Tokens per sec:   1999
2023-03-14 21:59:35,363 - INFO - __main__ - Epoch   1: total training loss 171424.19
2023-03-14 21:59:35,364 - INFO - __main__ - Epoch 2
2023-03-14 21:59:41,130 - INFO - __main__ - Epoch   2, Step:    2200, Batch Loss:    70.735458, Lr: 0.000099, Tokens per sec:   1942
2023-03-14 22:00:07,596 - INFO - __main__ - Epoch   2, Step:    2300, Batch Loss:    63.963829, Lr: 0.000099, Tokens per sec:   2023
2023-03-14 22:00:34,385 - INFO - __main__ - Epoch   2, Step:    2400, Batch Loss:    61.227386, Lr: 0.000099, Tokens per sec:   1975
2023-03-14 22:01:01,731 - INFO - __main__ - Epoch   2, Step:    2500, Batch Loss:    49.753819, Lr: 0.000099, Tokens per sec:   1997
2023-03-14 22:01:28,616 - INFO - __main__ - Epoch   2, Step:    2600, Batch Loss:    62.672131, Lr: 0.000099, Tokens per sec:   2010
2023-03-14 22:01:55,279 - INFO - __main__ - Epoch   2, Step:    2700, Batch Loss:    58.627071, Lr: 0.000099, Tokens per sec:   2015
2023-03-14 22:02:22,017 - INFO - __main__ - Epoch   2, Step:    2800, Batch Loss:    72.058418, Lr: 0.000099, Tokens per sec:   2029
2023-03-14 22:02:49,473 - INFO - __main__ - Epoch   2, Step:    2900, Batch Loss:    71.529999, Lr: 0.000099, Tokens per sec:   1962
2023-03-14 22:03:16,481 - INFO - __main__ - Epoch   2, Step:    3000, Batch Loss:    67.965469, Lr: 0.000099, Tokens per sec:   1994
2023-03-14 22:03:43,376 - INFO - __main__ - Epoch   2, Step:    3100, Batch Loss:    57.913456, Lr: 0.000099, Tokens per sec:   2005
2023-03-14 22:04:09,867 - INFO - __main__ - Epoch   2, Step:    3200, Batch Loss:    53.893955, Lr: 0.000099, Tokens per sec:   2046
2023-03-14 22:04:36,539 - INFO - __main__ - Epoch   2, Step:    3300, Batch Loss:    60.386490, Lr: 0.000099, Tokens per sec:   1996
2023-03-14 22:05:04,224 - INFO - __main__ - Epoch   2, Step:    3400, Batch Loss:    65.585297, Lr: 0.000099, Tokens per sec:   1942
2023-03-14 22:05:31,225 - INFO - __main__ - Epoch   2, Step:    3500, Batch Loss:    62.007088, Lr: 0.000099, Tokens per sec:   1990
2023-03-14 22:05:57,820 - INFO - __main__ - Epoch   2, Step:    3600, Batch Loss:    50.330181, Lr: 0.000099, Tokens per sec:   2023
2023-03-14 22:06:24,531 - INFO - __main__ - Epoch   2, Step:    3700, Batch Loss:    57.777405, Lr: 0.000099, Tokens per sec:   2064
2023-03-14 22:06:51,611 - INFO - __main__ - Epoch   2, Step:    3800, Batch Loss:    78.725029, Lr: 0.000099, Tokens per sec:   1980
2023-03-14 22:07:18,749 - INFO - __main__ - Epoch   2, Step:    3900, Batch Loss:    56.483387, Lr: 0.000099, Tokens per sec:   2004
2023-03-14 22:07:45,582 - INFO - __main__ - Epoch   2, Step:    4000, Batch Loss:    54.096710, Lr: 0.000099, Tokens per sec:   2005
2023-03-14 22:08:12,176 - INFO - __main__ - Epoch   2, Step:    4100, Batch Loss:    52.453533, Lr: 0.000099, Tokens per sec:   1970
2023-03-14 22:08:38,938 - INFO - __main__ - Epoch   2, Step:    4200, Batch Loss:    68.165047, Lr: 0.000099, Tokens per sec:   1991
2023-03-14 22:09:05,380 - INFO - __main__ - Epoch   2, Step:    4300, Batch Loss:    79.445320, Lr: 0.000099, Tokens per sec:   2058
2023-03-14 22:09:21,328 - INFO - __main__ - Epoch   2: total training loss 139907.86
2023-03-14 22:09:21,330 - INFO - __main__ - Epoch 3
2023-03-14 22:09:32,993 - INFO - __main__ - Epoch   3, Step:    4400, Batch Loss:    71.146217, Lr: 0.000098, Tokens per sec:   1958
2023-03-14 22:09:59,813 - INFO - __main__ - Epoch   3, Step:    4500, Batch Loss:    63.153416, Lr: 0.000098, Tokens per sec:   2006
2023-03-14 22:10:27,081 - INFO - __main__ - Epoch   3, Step:    4600, Batch Loss:    47.113510, Lr: 0.000098, Tokens per sec:   1962
2023-03-14 22:10:54,925 - INFO - __main__ - Epoch   3, Step:    4700, Batch Loss:    54.480026, Lr: 0.000098, Tokens per sec:   1960
2023-03-14 22:11:21,200 - INFO - __main__ - Epoch   3, Step:    4800, Batch Loss:    68.406593, Lr: 0.000098, Tokens per sec:   2076
2023-03-14 22:11:48,748 - INFO - __main__ - Epoch   3, Step:    4900, Batch Loss:    54.834473, Lr: 0.000098, Tokens per sec:   1941
2023-03-14 22:12:15,254 - INFO - __main__ - Epoch   3, Step:    5000, Batch Loss:    44.205601, Lr: 0.000098, Tokens per sec:   2036
2023-03-14 22:12:42,835 - INFO - __main__ - Epoch   3, Step:    5100, Batch Loss:    60.723278, Lr: 0.000098, Tokens per sec:   1953
2023-03-14 22:13:09,118 - INFO - __main__ - Epoch   3, Step:    5200, Batch Loss:    57.515755, Lr: 0.000098, Tokens per sec:   2066
2023-03-14 22:13:36,528 - INFO - __main__ - Epoch   3, Step:    5300, Batch Loss:    54.488453, Lr: 0.000098, Tokens per sec:   1937
2023-03-14 22:14:03,328 - INFO - __main__ - Epoch   3, Step:    5400, Batch Loss:    53.937889, Lr: 0.000098, Tokens per sec:   2008
2023-03-14 22:14:30,003 - INFO - __main__ - Epoch   3, Step:    5500, Batch Loss:    69.753082, Lr: 0.000098, Tokens per sec:   1984
2023-03-14 22:14:57,313 - INFO - __main__ - Epoch   3, Step:    5600, Batch Loss:    52.397106, Lr: 0.000098, Tokens per sec:   1973
2023-03-14 22:15:23,911 - INFO - __main__ - Epoch   3, Step:    5700, Batch Loss:    43.075367, Lr: 0.000098, Tokens per sec:   2037
2023-03-14 22:15:51,571 - INFO - __main__ - Epoch   3, Step:    5800, Batch Loss:    73.205544, Lr: 0.000098, Tokens per sec:   1930
2023-03-14 22:16:18,101 - INFO - __main__ - Epoch   3, Step:    5900, Batch Loss:    59.986656, Lr: 0.000098, Tokens per sec:   2027
2023-03-14 22:16:45,272 - INFO - __main__ - Epoch   3, Step:    6000, Batch Loss:    61.562363, Lr: 0.000098, Tokens per sec:   2001
2023-03-14 22:17:12,830 - INFO - __main__ - Epoch   3, Step:    6100, Batch Loss:    42.939495, Lr: 0.000098, Tokens per sec:   1938
2023-03-14 22:17:39,828 - INFO - __main__ - Epoch   3, Step:    6200, Batch Loss:    54.406143, Lr: 0.000098, Tokens per sec:   2008
2023-03-14 22:18:06,912 - INFO - __main__ - Epoch   3, Step:    6300, Batch Loss:    62.307377, Lr: 0.000098, Tokens per sec:   1983
2023-03-14 22:18:34,362 - INFO - __main__ - Epoch   3, Step:    6400, Batch Loss:    49.039978, Lr: 0.000098, Tokens per sec:   1982
2023-03-14 22:19:01,267 - INFO - __main__ - Epoch   3, Step:    6500, Batch Loss:    60.774506, Lr: 0.000098, Tokens per sec:   1990
2023-03-14 22:19:11,856 - INFO - __main__ - Epoch   3: total training loss 125262.12
2023-03-14 22:19:11,857 - INFO - __main__ - Epoch 4
2023-03-14 22:19:29,419 - INFO - __main__ - Epoch   4, Step:    6600, Batch Loss:    44.398041, Lr: 0.000097, Tokens per sec:   1961
2023-03-14 22:19:57,407 - INFO - __main__ - Epoch   4, Step:    6700, Batch Loss:    56.801685, Lr: 0.000097, Tokens per sec:   1915
2023-03-14 22:20:24,954 - INFO - __main__ - Epoch   4, Step:    6800, Batch Loss:    55.351917, Lr: 0.000097, Tokens per sec:   1970
2023-03-14 22:20:52,614 - INFO - __main__ - Epoch   4, Step:    6900, Batch Loss:    55.227280, Lr: 0.000097, Tokens per sec:   1955
2023-03-14 22:21:20,609 - INFO - __main__ - Epoch   4, Step:    7000, Batch Loss:    54.880989, Lr: 0.000097, Tokens per sec:   1897
2023-03-14 22:21:48,561 - INFO - __main__ - Epoch   4, Step:    7100, Batch Loss:    64.260330, Lr: 0.000097, Tokens per sec:   1964
2023-03-14 22:22:16,540 - INFO - __main__ - Epoch   4, Step:    7200, Batch Loss:    47.692020, Lr: 0.000097, Tokens per sec:   1893
2023-03-14 22:22:44,356 - INFO - __main__ - Epoch   4, Step:    7300, Batch Loss:    58.618557, Lr: 0.000097, Tokens per sec:   1959
2023-03-14 22:23:12,220 - INFO - __main__ - Epoch   4, Step:    7400, Batch Loss:    54.913792, Lr: 0.000097, Tokens per sec:   1923
2023-03-14 22:23:40,243 - INFO - __main__ - Epoch   4, Step:    7500, Batch Loss:    48.205772, Lr: 0.000097, Tokens per sec:   1958
2023-03-14 22:24:08,054 - INFO - __main__ - Epoch   4, Step:    7600, Batch Loss:    58.871944, Lr: 0.000097, Tokens per sec:   1953
2023-03-14 22:24:35,179 - INFO - __main__ - Epoch   4, Step:    7700, Batch Loss:    50.662460, Lr: 0.000097, Tokens per sec:   1948
2023-03-14 22:25:02,673 - INFO - __main__ - Epoch   4, Step:    7800, Batch Loss:    52.615688, Lr: 0.000097, Tokens per sec:   1944
2023-03-14 22:25:30,485 - INFO - __main__ - Epoch   4, Step:    7900, Batch Loss:    64.150002, Lr: 0.000097, Tokens per sec:   1918
2023-03-14 22:25:57,182 - INFO - __main__ - Epoch   4, Step:    8000, Batch Loss:    58.873043, Lr: 0.000097, Tokens per sec:   2015
2023-03-14 22:26:24,821 - INFO - __main__ - Epoch   4, Step:    8100, Batch Loss:    56.284225, Lr: 0.000097, Tokens per sec:   1959
2023-03-14 22:26:52,645 - INFO - __main__ - Epoch   4, Step:    8200, Batch Loss:    42.710846, Lr: 0.000097, Tokens per sec:   1918
2023-03-14 22:27:18,897 - INFO - __main__ - Epoch   4, Step:    8300, Batch Loss:    43.072968, Lr: 0.000097, Tokens per sec:   2041
2023-03-14 22:27:46,505 - INFO - __main__ - Epoch   4, Step:    8400, Batch Loss:    46.597881, Lr: 0.000097, Tokens per sec:   1943
2023-03-14 22:28:13,403 - INFO - __main__ - Epoch   4, Step:    8500, Batch Loss:    51.884884, Lr: 0.000097, Tokens per sec:   2033
2023-03-14 22:28:41,314 - INFO - __main__ - Epoch   4, Step:    8600, Batch Loss:    39.701088, Lr: 0.000097, Tokens per sec:   1901
2023-03-14 22:29:08,117 - INFO - __main__ - Epoch   4, Step:    8700, Batch Loss:    43.132133, Lr: 0.000097, Tokens per sec:   2015
2023-03-14 22:29:12,373 - INFO - __main__ - Epoch   4: total training loss 114498.82
2023-03-14 22:29:12,374 - INFO - __main__ - Epoch 5
2023-03-14 22:29:35,855 - INFO - __main__ - Epoch   5, Step:    8800, Batch Loss:    49.327785, Lr: 0.000096, Tokens per sec:   1911
2023-03-14 22:30:03,066 - INFO - __main__ - Epoch   5, Step:    8900, Batch Loss:    56.574810, Lr: 0.000096, Tokens per sec:   1996
2023-03-14 22:30:30,310 - INFO - __main__ - Epoch   5, Step:    9000, Batch Loss:    49.904037, Lr: 0.000096, Tokens per sec:   1981
2023-03-14 22:30:57,759 - INFO - __main__ - Epoch   5, Step:    9100, Batch Loss:    42.559887, Lr: 0.000096, Tokens per sec:   1926
2023-03-14 22:31:24,411 - INFO - __main__ - Epoch   5, Step:    9200, Batch Loss:    57.536827, Lr: 0.000096, Tokens per sec:   2043
2023-03-14 22:31:51,715 - INFO - __main__ - Epoch   5, Step:    9300, Batch Loss:    44.763451, Lr: 0.000096, Tokens per sec:   1945
2023-03-14 22:32:18,663 - INFO - __main__ - Epoch   5, Step:    9400, Batch Loss:    41.102463, Lr: 0.000096, Tokens per sec:   1994
2023-03-14 22:32:46,453 - INFO - __main__ - Epoch   5, Step:    9500, Batch Loss:    42.185501, Lr: 0.000096, Tokens per sec:   1917
2023-03-14 22:33:12,967 - INFO - __main__ - Epoch   5, Step:    9600, Batch Loss:    64.013153, Lr: 0.000096, Tokens per sec:   2011
2023-03-14 22:33:40,795 - INFO - __main__ - Epoch   5, Step:    9700, Batch Loss:    51.602951, Lr: 0.000096, Tokens per sec:   1970
2023-03-14 22:34:07,815 - INFO - __main__ - Epoch   5, Step:    9800, Batch Loss:    36.606823, Lr: 0.000096, Tokens per sec:   1991
2023-03-14 22:34:35,291 - INFO - __main__ - Epoch   5, Step:    9900, Batch Loss:    52.139103, Lr: 0.000096, Tokens per sec:   1926
2023-03-14 22:35:02,939 - INFO - __main__ - Epoch   5, Step:   10000, Batch Loss:    48.695572, Lr: 0.000096, Tokens per sec:   1957
2023-03-14 22:35:29,825 - INFO - __main__ - Epoch   5, Step:   10100, Batch Loss:    54.726711, Lr: 0.000096, Tokens per sec:   2029
2023-03-14 22:35:56,863 - INFO - __main__ - Epoch   5, Step:   10200, Batch Loss:    49.720982, Lr: 0.000096, Tokens per sec:   2005
2023-03-14 22:36:24,234 - INFO - __main__ - Epoch   5, Step:   10300, Batch Loss:    48.923485, Lr: 0.000096, Tokens per sec:   1981
2023-03-14 22:36:50,636 - INFO - __main__ - Epoch   5, Step:   10400, Batch Loss:    55.943222, Lr: 0.000096, Tokens per sec:   2043
2023-03-14 22:37:17,201 - INFO - __main__ - Epoch   5, Step:   10500, Batch Loss:    49.878498, Lr: 0.000096, Tokens per sec:   2047
2023-03-14 22:37:44,867 - INFO - __main__ - Epoch   5, Step:   10600, Batch Loss:    36.529572, Lr: 0.000096, Tokens per sec:   1957
2023-03-14 22:38:11,588 - INFO - __main__ - Epoch   5, Step:   10700, Batch Loss:    47.277908, Lr: 0.000096, Tokens per sec:   2001
2023-03-14 22:38:38,343 - INFO - __main__ - Epoch   5, Step:   10800, Batch Loss:    39.518890, Lr: 0.000096, Tokens per sec:   2019
2023-03-14 22:39:04,675 - INFO - __main__ - Epoch   5: total training loss 105653.23
2023-03-14 22:39:04,676 - INFO - __main__ - Epoch 6
2023-03-14 22:39:06,070 - INFO - __main__ - Epoch   6, Step:   10900, Batch Loss:    52.783356, Lr: 0.000095, Tokens per sec:   2173
2023-03-14 22:39:33,601 - INFO - __main__ - Epoch   6, Step:   11000, Batch Loss:    32.914410, Lr: 0.000095, Tokens per sec:   1989
2023-03-14 22:40:00,509 - INFO - __main__ - Epoch   6, Step:   11100, Batch Loss:    51.249336, Lr: 0.000095, Tokens per sec:   2020
2023-03-14 22:40:27,105 - INFO - __main__ - Epoch   6, Step:   11200, Batch Loss:    42.700191, Lr: 0.000095, Tokens per sec:   2025
2023-03-14 22:40:54,949 - INFO - __main__ - Epoch   6, Step:   11300, Batch Loss:    63.023781, Lr: 0.000095, Tokens per sec:   1930
2023-03-14 22:41:21,398 - INFO - __main__ - Epoch   6, Step:   11400, Batch Loss:    34.923931, Lr: 0.000095, Tokens per sec:   2042
2023-03-14 22:41:48,714 - INFO - __main__ - Epoch   6, Step:   11500, Batch Loss:    32.622513, Lr: 0.000095, Tokens per sec:   1963
2023-03-14 22:42:15,580 - INFO - __main__ - Epoch   6, Step:   11600, Batch Loss:    37.765629, Lr: 0.000095, Tokens per sec:   1978
2023-03-14 22:42:42,099 - INFO - __main__ - Epoch   6, Step:   11700, Batch Loss:    48.591454, Lr: 0.000095, Tokens per sec:   2023
2023-03-14 22:43:09,725 - INFO - __main__ - Epoch   6, Step:   11800, Batch Loss:    36.971584, Lr: 0.000095, Tokens per sec:   1935
2023-03-14 22:43:36,199 - INFO - __main__ - Epoch   6, Step:   11900, Batch Loss:    48.015602, Lr: 0.000095, Tokens per sec:   2020
2023-03-14 22:44:03,802 - INFO - __main__ - Epoch   6, Step:   12000, Batch Loss:    49.612801, Lr: 0.000095, Tokens per sec:   1947
2023-03-14 22:44:30,770 - INFO - __main__ - Epoch   6, Step:   12100, Batch Loss:    40.564491, Lr: 0.000095, Tokens per sec:   1987
2023-03-14 22:44:58,337 - INFO - __main__ - Epoch   6, Step:   12200, Batch Loss:    36.701488, Lr: 0.000095, Tokens per sec:   1965
2023-03-14 22:45:25,247 - INFO - __main__ - Epoch   6, Step:   12300, Batch Loss:    56.602299, Lr: 0.000095, Tokens per sec:   2037
2023-03-14 22:45:51,904 - INFO - __main__ - Epoch   6, Step:   12400, Batch Loss:    37.256325, Lr: 0.000095, Tokens per sec:   2017
2023-03-14 22:46:19,413 - INFO - __main__ - Epoch   6, Step:   12500, Batch Loss:    41.403263, Lr: 0.000095, Tokens per sec:   1936
2023-03-14 22:46:45,888 - INFO - __main__ - Epoch   6, Step:   12600, Batch Loss:    38.035686, Lr: 0.000095, Tokens per sec:   2044
2023-03-14 22:47:13,573 - INFO - __main__ - Epoch   6, Step:   12700, Batch Loss:    53.639034, Lr: 0.000095, Tokens per sec:   1943
2023-03-14 22:47:40,149 - INFO - __main__ - Epoch   6, Step:   12800, Batch Loss:    47.077419, Lr: 0.000095, Tokens per sec:   2011
2023-03-14 22:48:07,676 - INFO - __main__ - Epoch   6, Step:   12900, Batch Loss:    40.959637, Lr: 0.000095, Tokens per sec:   1939
2023-03-14 22:48:34,418 - INFO - __main__ - Epoch   6, Step:   13000, Batch Loss:    44.645390, Lr: 0.000095, Tokens per sec:   2025
2023-03-14 22:48:54,809 - INFO - __main__ - Epoch   6: total training loss 97935.68
2023-03-14 22:48:54,811 - INFO - __main__ - Epoch 7
2023-03-14 22:49:02,033 - INFO - __main__ - Epoch   7, Step:   13100, Batch Loss:    34.050423, Lr: 0.000094, Tokens per sec:   1999
2023-03-14 22:49:29,124 - INFO - __main__ - Epoch   7, Step:   13200, Batch Loss:    42.636681, Lr: 0.000094, Tokens per sec:   1982
2023-03-14 22:49:56,062 - INFO - __main__ - Epoch   7, Step:   13300, Batch Loss:    47.901321, Lr: 0.000094, Tokens per sec:   2021
2023-03-14 22:50:23,419 - INFO - __main__ - Epoch   7, Step:   13400, Batch Loss:    47.883251, Lr: 0.000094, Tokens per sec:   1953
2023-03-14 22:50:50,367 - INFO - __main__ - Epoch   7, Step:   13500, Batch Loss:    40.045864, Lr: 0.000094, Tokens per sec:   2002
2023-03-14 22:51:17,564 - INFO - __main__ - Epoch   7, Step:   13600, Batch Loss:    33.238876, Lr: 0.000094, Tokens per sec:   1964
2023-03-14 22:51:44,473 - INFO - __main__ - Epoch   7, Step:   13700, Batch Loss:    38.576916, Lr: 0.000094, Tokens per sec:   1978
2023-03-14 22:52:11,984 - INFO - __main__ - Epoch   7, Step:   13800, Batch Loss:    54.669155, Lr: 0.000094, Tokens per sec:   1925
2023-03-14 22:52:38,507 - INFO - __main__ - Epoch   7, Step:   13900, Batch Loss:    36.857563, Lr: 0.000094, Tokens per sec:   2053
2023-03-14 22:53:05,074 - INFO - __main__ - Epoch   7, Step:   14000, Batch Loss:    48.149509, Lr: 0.000094, Tokens per sec:   2044
2023-03-14 22:53:31,422 - INFO - __main__ - Epoch   7, Step:   14100, Batch Loss:    40.668217, Lr: 0.000094, Tokens per sec:   2040
2023-03-14 22:53:58,266 - INFO - __main__ - Epoch   7, Step:   14200, Batch Loss:    30.048725, Lr: 0.000094, Tokens per sec:   2027
2023-03-14 22:54:24,708 - INFO - __main__ - Epoch   7, Step:   14300, Batch Loss:    39.542068, Lr: 0.000094, Tokens per sec:   2034
2023-03-14 22:54:51,504 - INFO - __main__ - Epoch   7, Step:   14400, Batch Loss:    47.314026, Lr: 0.000094, Tokens per sec:   2029
2023-03-14 22:55:18,186 - INFO - __main__ - Epoch   7, Step:   14500, Batch Loss:    50.116810, Lr: 0.000094, Tokens per sec:   2050
2023-03-14 22:55:44,858 - INFO - __main__ - Epoch   7, Step:   14600, Batch Loss:    46.843163, Lr: 0.000094, Tokens per sec:   2019
2023-03-14 22:56:11,349 - INFO - __main__ - Epoch   7, Step:   14700, Batch Loss:    40.510422, Lr: 0.000094, Tokens per sec:   2021
2023-03-14 22:56:38,131 - INFO - __main__ - Epoch   7, Step:   14800, Batch Loss:    43.952675, Lr: 0.000094, Tokens per sec:   2003
2023-03-14 22:57:05,155 - INFO - __main__ - Epoch   7, Step:   14900, Batch Loss:    41.200783, Lr: 0.000094, Tokens per sec:   1984
2023-03-14 22:57:31,984 - INFO - __main__ - Epoch   7, Step:   15000, Batch Loss:    36.663570, Lr: 0.000094, Tokens per sec:   1990
2023-03-14 22:57:59,430 - INFO - __main__ - Epoch   7, Step:   15100, Batch Loss:    30.252613, Lr: 0.000094, Tokens per sec:   1944
2023-03-14 22:58:26,308 - INFO - __main__ - Epoch   7, Step:   15200, Batch Loss:    41.405148, Lr: 0.000094, Tokens per sec:   1983
2023-03-14 22:58:40,975 - INFO - __main__ - Epoch   7: total training loss 91142.78
2023-03-14 22:58:40,976 - INFO - __main__ - Epoch 8
2023-03-14 22:58:54,539 - INFO - __main__ - Epoch   8, Step:   15300, Batch Loss:    44.493061, Lr: 0.000093, Tokens per sec:   1828
2023-03-14 22:59:21,700 - INFO - __main__ - Epoch   8, Step:   15400, Batch Loss:    42.035671, Lr: 0.000093, Tokens per sec:   2013
2023-03-14 22:59:48,977 - INFO - __main__ - Epoch   8, Step:   15500, Batch Loss:    55.239922, Lr: 0.000093, Tokens per sec:   1974
2023-03-14 23:00:16,473 - INFO - __main__ - Epoch   8, Step:   15600, Batch Loss:    42.330326, Lr: 0.000093, Tokens per sec:   1979
2023-03-14 23:00:43,083 - INFO - __main__ - Epoch   8, Step:   15700, Batch Loss:    41.785198, Lr: 0.000093, Tokens per sec:   2045
2023-03-14 23:01:10,990 - INFO - __main__ - Epoch   8, Step:   15800, Batch Loss:    31.491468, Lr: 0.000093, Tokens per sec:   1923
2023-03-14 23:01:37,485 - INFO - __main__ - Epoch   8, Step:   15900, Batch Loss:    55.993744, Lr: 0.000093, Tokens per sec:   2030
2023-03-14 23:02:05,300 - INFO - __main__ - Epoch   8, Step:   16000, Batch Loss:    37.460480, Lr: 0.000093, Tokens per sec:   1913
2023-03-14 23:02:32,987 - INFO - __main__ - Epoch   8, Step:   16100, Batch Loss:    42.236507, Lr: 0.000093, Tokens per sec:   1954
2023-03-14 23:02:55,925 - INFO - __main__ - Epoch   8, Step:   16200, Batch Loss:    41.236290, Lr: 0.000093, Tokens per sec:   2365
2023-03-14 23:03:16,050 - INFO - __main__ - Epoch   8, Step:   16300, Batch Loss:    36.263676, Lr: 0.000093, Tokens per sec:   2626
2023-03-14 23:03:35,774 - INFO - __main__ - Epoch   8, Step:   16400, Batch Loss:    31.779020, Lr: 0.000093, Tokens per sec:   2753
2023-03-14 23:03:55,837 - INFO - __main__ - Epoch   8, Step:   16500, Batch Loss:    44.407833, Lr: 0.000093, Tokens per sec:   2693
2023-03-14 23:04:15,755 - INFO - __main__ - Epoch   8, Step:   16600, Batch Loss:    43.839436, Lr: 0.000093, Tokens per sec:   2691
2023-03-14 23:04:35,675 - INFO - __main__ - Epoch   8, Step:   16700, Batch Loss:    46.594467, Lr: 0.000093, Tokens per sec:   2660
2023-03-14 23:04:55,386 - INFO - __main__ - Epoch   8, Step:   16800, Batch Loss:    41.139549, Lr: 0.000093, Tokens per sec:   2739
2023-03-14 23:05:15,583 - INFO - __main__ - Epoch   8, Step:   16900, Batch Loss:    32.204708, Lr: 0.000093, Tokens per sec:   2670
2023-03-14 23:05:35,518 - INFO - __main__ - Epoch   8, Step:   17000, Batch Loss:    45.326996, Lr: 0.000093, Tokens per sec:   2724
2023-03-14 23:05:55,751 - INFO - __main__ - Epoch   8, Step:   17100, Batch Loss:    51.166931, Lr: 0.000093, Tokens per sec:   2641
2023-03-14 23:06:15,917 - INFO - __main__ - Epoch   8, Step:   17200, Batch Loss:    35.675537, Lr: 0.000093, Tokens per sec:   2678
2023-03-14 23:06:36,091 - INFO - __main__ - Epoch   8, Step:   17300, Batch Loss:    44.717884, Lr: 0.000093, Tokens per sec:   2642
2023-03-14 23:06:56,341 - INFO - __main__ - Epoch   8, Step:   17400, Batch Loss:    46.513973, Lr: 0.000093, Tokens per sec:   2680
2023-03-14 23:07:02,869 - INFO - __main__ - Epoch   8: total training loss 84947.85
2023-03-14 23:07:02,870 - INFO - __main__ - Epoch 9
2023-03-14 23:07:16,694 - INFO - __main__ - Epoch   9, Step:   17500, Batch Loss:    37.085217, Lr: 0.000092, Tokens per sec:   2633
2023-03-14 23:07:36,837 - INFO - __main__ - Epoch   9, Step:   17600, Batch Loss:    31.086008, Lr: 0.000092, Tokens per sec:   2669
2023-03-14 23:07:57,045 - INFO - __main__ - Epoch   9, Step:   17700, Batch Loss:    31.426527, Lr: 0.000092, Tokens per sec:   2700
2023-03-14 23:08:17,384 - INFO - __main__ - Epoch   9, Step:   17800, Batch Loss:    25.638971, Lr: 0.000092, Tokens per sec:   2668
2023-03-14 23:08:37,511 - INFO - __main__ - Epoch   9, Step:   17900, Batch Loss:    34.110905, Lr: 0.000092, Tokens per sec:   2653
2023-03-14 23:08:57,724 - INFO - __main__ - Epoch   9, Step:   18000, Batch Loss:    34.906460, Lr: 0.000092, Tokens per sec:   2658
2023-03-14 23:09:17,918 - INFO - __main__ - Epoch   9, Step:   18100, Batch Loss:    42.653538, Lr: 0.000092, Tokens per sec:   2679
2023-03-14 23:09:37,939 - INFO - __main__ - Epoch   9, Step:   18200, Batch Loss:    26.377890, Lr: 0.000092, Tokens per sec:   2759
2023-03-14 23:09:58,047 - INFO - __main__ - Epoch   9, Step:   18300, Batch Loss:    22.363792, Lr: 0.000092, Tokens per sec:   2677
2023-03-14 23:10:18,194 - INFO - __main__ - Epoch   9, Step:   18400, Batch Loss:    41.569202, Lr: 0.000092, Tokens per sec:   2651
2023-03-14 23:10:38,229 - INFO - __main__ - Epoch   9, Step:   18500, Batch Loss:    41.869301, Lr: 0.000092, Tokens per sec:   2703
2023-03-14 23:10:58,183 - INFO - __main__ - Epoch   9, Step:   18600, Batch Loss:    53.368500, Lr: 0.000092, Tokens per sec:   2694
2023-03-14 23:11:18,179 - INFO - __main__ - Epoch   9, Step:   18700, Batch Loss:    21.615761, Lr: 0.000092, Tokens per sec:   2708
2023-03-14 23:11:38,033 - INFO - __main__ - Epoch   9, Step:   18800, Batch Loss:    40.921894, Lr: 0.000092, Tokens per sec:   2698
2023-03-14 23:11:58,265 - INFO - __main__ - Epoch   9, Step:   18900, Batch Loss:    37.715027, Lr: 0.000092, Tokens per sec:   2650
2023-03-14 23:12:18,367 - INFO - __main__ - Epoch   9, Step:   19000, Batch Loss:    42.835011, Lr: 0.000092, Tokens per sec:   2671
2023-03-14 23:12:38,272 - INFO - __main__ - Epoch   9, Step:   19100, Batch Loss:    40.877254, Lr: 0.000092, Tokens per sec:   2730
2023-03-14 23:12:58,317 - INFO - __main__ - Epoch   9, Step:   19200, Batch Loss:    42.051521, Lr: 0.000092, Tokens per sec:   2666
2023-03-14 23:13:18,483 - INFO - __main__ - Epoch   9, Step:   19300, Batch Loss:    28.207367, Lr: 0.000092, Tokens per sec:   2634
2023-03-14 23:13:38,574 - INFO - __main__ - Epoch   9, Step:   19400, Batch Loss:    35.097317, Lr: 0.000092, Tokens per sec:   2695
2023-03-14 23:13:58,376 - INFO - __main__ - Epoch   9, Step:   19500, Batch Loss:    32.411095, Lr: 0.000092, Tokens per sec:   2686
2023-03-14 23:14:18,362 - INFO - __main__ - Epoch   9, Step:   19600, Batch Loss:    34.239830, Lr: 0.000092, Tokens per sec:   2675
2023-03-14 23:14:20,616 - INFO - __main__ - Epoch   9: total training loss 79289.12
2023-03-14 23:14:20,617 - INFO - __main__ - Epoch 10
2023-03-14 23:14:38,775 - INFO - __main__ - Epoch  10, Step:   19700, Batch Loss:    33.491501, Lr: 0.000091, Tokens per sec:   2634
2023-03-14 23:14:58,777 - INFO - __main__ - Epoch  10, Step:   19800, Batch Loss:    37.811047, Lr: 0.000091, Tokens per sec:   2713
2023-03-14 23:15:19,070 - INFO - __main__ - Epoch  10, Step:   19900, Batch Loss:    30.654106, Lr: 0.000091, Tokens per sec:   2702
2023-03-14 23:15:39,214 - INFO - __main__ - Epoch  10, Step:   20000, Batch Loss:    30.117147, Lr: 0.000091, Tokens per sec:   2640
2023-03-14 23:15:59,492 - INFO - __main__ - Epoch  10, Step:   20100, Batch Loss:    35.583786, Lr: 0.000091, Tokens per sec:   2688
2023-03-14 23:16:19,647 - INFO - __main__ - Epoch  10, Step:   20200, Batch Loss:    36.632519, Lr: 0.000091, Tokens per sec:   2690
2023-03-14 23:16:39,945 - INFO - __main__ - Epoch  10, Step:   20300, Batch Loss:    38.497673, Lr: 0.000091, Tokens per sec:   2674
2023-03-14 23:17:00,182 - INFO - __main__ - Epoch  10, Step:   20400, Batch Loss:    33.225380, Lr: 0.000091, Tokens per sec:   2608
2023-03-14 23:17:20,399 - INFO - __main__ - Epoch  10, Step:   20500, Batch Loss:    23.555706, Lr: 0.000091, Tokens per sec:   2685
2023-03-14 23:17:40,389 - INFO - __main__ - Epoch  10, Step:   20600, Batch Loss:    42.217953, Lr: 0.000091, Tokens per sec:   2721
2023-03-14 23:18:00,312 - INFO - __main__ - Epoch  10, Step:   20700, Batch Loss:    41.421211, Lr: 0.000091, Tokens per sec:   2683
2023-03-14 23:18:20,267 - INFO - __main__ - Epoch  10, Step:   20800, Batch Loss:    43.484581, Lr: 0.000091, Tokens per sec:   2675
2023-03-14 23:18:40,363 - INFO - __main__ - Epoch  10, Step:   20900, Batch Loss:    31.069426, Lr: 0.000091, Tokens per sec:   2665
2023-03-14 23:19:00,397 - INFO - __main__ - Epoch  10, Step:   21000, Batch Loss:    30.337154, Lr: 0.000091, Tokens per sec:   2710
2023-03-14 23:19:20,328 - INFO - __main__ - Epoch  10, Step:   21100, Batch Loss:    39.403137, Lr: 0.000091, Tokens per sec:   2661
2023-03-14 23:19:40,164 - INFO - __main__ - Epoch  10, Step:   21200, Batch Loss:    23.358419, Lr: 0.000091, Tokens per sec:   2694
2023-03-14 23:20:00,338 - INFO - __main__ - Epoch  10, Step:   21300, Batch Loss:    34.317425, Lr: 0.000091, Tokens per sec:   2649
2023-03-14 23:20:20,468 - INFO - __main__ - Epoch  10, Step:   21400, Batch Loss:    41.425888, Lr: 0.000091, Tokens per sec:   2688
2023-03-14 23:20:40,590 - INFO - __main__ - Epoch  10, Step:   21500, Batch Loss:    36.586288, Lr: 0.000091, Tokens per sec:   2669
2023-03-14 23:21:00,807 - INFO - __main__ - Epoch  10, Step:   21600, Batch Loss:    16.422409, Lr: 0.000091, Tokens per sec:   2634
2023-03-14 23:21:21,102 - INFO - __main__ - Epoch  10, Step:   21700, Batch Loss:    37.956116, Lr: 0.000091, Tokens per sec:   2708
2023-03-14 23:21:39,399 - INFO - __main__ - Epoch  10: total training loss 74109.90
2023-03-14 23:21:39,401 - INFO - __main__ - Epoch 11
2023-03-14 23:21:41,806 - INFO - __main__ - Epoch  11, Step:   21800, Batch Loss:    26.578968, Lr: 0.000090, Tokens per sec:   2153
2023-03-14 23:22:01,599 - INFO - __main__ - Epoch  11, Step:   21900, Batch Loss:    27.845070, Lr: 0.000090, Tokens per sec:   2745
2023-03-14 23:22:21,699 - INFO - __main__ - Epoch  11, Step:   22000, Batch Loss:    33.249298, Lr: 0.000090, Tokens per sec:   2616
2023-03-14 23:22:41,886 - INFO - __main__ - Epoch  11, Step:   22100, Batch Loss:    34.992069, Lr: 0.000090, Tokens per sec:   2669
2023-03-14 23:23:02,125 - INFO - __main__ - Epoch  11, Step:   22200, Batch Loss:    24.811970, Lr: 0.000090, Tokens per sec:   2652
2023-03-14 23:23:22,409 - INFO - __main__ - Epoch  11, Step:   22300, Batch Loss:    26.411865, Lr: 0.000090, Tokens per sec:   2658
2023-03-14 23:23:42,243 - INFO - __main__ - Epoch  11, Step:   22400, Batch Loss:    29.349701, Lr: 0.000090, Tokens per sec:   2713
2023-03-14 23:24:02,113 - INFO - __main__ - Epoch  11, Step:   22500, Batch Loss:    28.220121, Lr: 0.000090, Tokens per sec:   2736
2023-03-14 23:24:21,919 - INFO - __main__ - Epoch  11, Step:   22600, Batch Loss:    27.062874, Lr: 0.000090, Tokens per sec:   2700
2023-03-14 23:24:41,245 - INFO - __main__ - Epoch  11, Step:   22700, Batch Loss:    34.345261, Lr: 0.000090, Tokens per sec:   2827
2023-03-14 23:25:01,162 - INFO - __main__ - Epoch  11, Step:   22800, Batch Loss:    30.062788, Lr: 0.000090, Tokens per sec:   2754
2023-03-14 23:25:20,819 - INFO - __main__ - Epoch  11, Step:   22900, Batch Loss:    38.540504, Lr: 0.000090, Tokens per sec:   2698
2023-03-14 23:25:40,411 - INFO - __main__ - Epoch  11, Step:   23000, Batch Loss:    22.785767, Lr: 0.000090, Tokens per sec:   2769
2023-03-14 23:26:00,190 - INFO - __main__ - Epoch  11, Step:   23100, Batch Loss:    23.564051, Lr: 0.000090, Tokens per sec:   2716
2023-03-14 23:26:19,983 - INFO - __main__ - Epoch  11, Step:   23200, Batch Loss:    35.680962, Lr: 0.000090, Tokens per sec:   2736
2023-03-14 23:26:39,467 - INFO - __main__ - Epoch  11, Step:   23300, Batch Loss:    30.843897, Lr: 0.000090, Tokens per sec:   2732
2023-03-14 23:26:59,558 - INFO - __main__ - Epoch  11, Step:   23400, Batch Loss:    30.814823, Lr: 0.000090, Tokens per sec:   2661
2023-03-14 23:27:19,526 - INFO - __main__ - Epoch  11, Step:   23500, Batch Loss:    35.320610, Lr: 0.000090, Tokens per sec:   2707
2023-03-14 23:27:39,560 - INFO - __main__ - Epoch  11, Step:   23600, Batch Loss:    30.288120, Lr: 0.000090, Tokens per sec:   2714
2023-03-14 23:27:59,353 - INFO - __main__ - Epoch  11, Step:   23700, Batch Loss:    27.343697, Lr: 0.000090, Tokens per sec:   2661
2023-03-14 23:28:19,110 - INFO - __main__ - Epoch  11, Step:   23800, Batch Loss:    33.101311, Lr: 0.000090, Tokens per sec:   2712
2023-03-14 23:28:39,148 - INFO - __main__ - Epoch  11, Step:   23900, Batch Loss:    32.786518, Lr: 0.000090, Tokens per sec:   2725
2023-03-14 23:28:53,063 - INFO - __main__ - Epoch  11: total training loss 69346.89
2023-03-14 23:28:53,064 - INFO - __main__ - Epoch 12
2023-03-14 23:28:59,729 - INFO - __main__ - Epoch  12, Step:   24000, Batch Loss:    23.707323, Lr: 0.000090, Tokens per sec:   2488
2023-03-14 23:29:19,745 - INFO - __main__ - Epoch  12, Step:   24100, Batch Loss:    30.265440, Lr: 0.000090, Tokens per sec:   2673
2023-03-14 23:29:39,678 - INFO - __main__ - Epoch  12, Step:   24200, Batch Loss:    30.225702, Lr: 0.000090, Tokens per sec:   2662
2023-03-14 23:29:59,840 - INFO - __main__ - Epoch  12, Step:   24300, Batch Loss:    32.221977, Lr: 0.000090, Tokens per sec:   2665
2023-03-14 23:30:20,248 - INFO - __main__ - Epoch  12, Step:   24400, Batch Loss:    30.129305, Lr: 0.000090, Tokens per sec:   2633
2023-03-14 23:30:40,575 - INFO - __main__ - Epoch  12, Step:   24500, Batch Loss:    28.282131, Lr: 0.000090, Tokens per sec:   2666
2023-03-14 23:31:00,743 - INFO - __main__ - Epoch  12, Step:   24600, Batch Loss:    22.364254, Lr: 0.000090, Tokens per sec:   2641
2023-03-14 23:31:20,956 - INFO - __main__ - Epoch  12, Step:   24700, Batch Loss:    23.421535, Lr: 0.000090, Tokens per sec:   2670
2023-03-14 23:31:41,048 - INFO - __main__ - Epoch  12, Step:   24800, Batch Loss:    30.455067, Lr: 0.000090, Tokens per sec:   2689
2023-03-14 23:32:01,091 - INFO - __main__ - Epoch  12, Step:   24900, Batch Loss:    34.248539, Lr: 0.000090, Tokens per sec:   2709
2023-03-14 23:32:20,508 - INFO - __main__ - Epoch  12, Step:   25000, Batch Loss:    16.380894, Lr: 0.000090, Tokens per sec:   2751
2023-03-14 23:32:40,140 - INFO - __main__ - Epoch  12, Step:   25100, Batch Loss:    33.796131, Lr: 0.000090, Tokens per sec:   2737
2023-03-14 23:33:00,325 - INFO - __main__ - Epoch  12, Step:   25200, Batch Loss:    31.643969, Lr: 0.000090, Tokens per sec:   2701
2023-03-14 23:33:20,609 - INFO - __main__ - Epoch  12, Step:   25300, Batch Loss:    23.682766, Lr: 0.000090, Tokens per sec:   2618
2023-03-14 23:33:41,049 - INFO - __main__ - Epoch  12, Step:   25400, Batch Loss:    24.801079, Lr: 0.000090, Tokens per sec:   2670
2023-03-14 23:34:01,634 - INFO - __main__ - Epoch  12, Step:   25500, Batch Loss:    23.472357, Lr: 0.000090, Tokens per sec:   2592
2023-03-14 23:34:21,681 - INFO - __main__ - Epoch  12, Step:   25600, Batch Loss:    29.407490, Lr: 0.000090, Tokens per sec:   2660
2023-03-14 23:34:42,100 - INFO - __main__ - Epoch  12, Step:   25700, Batch Loss:    42.000431, Lr: 0.000090, Tokens per sec:   2652
2023-03-14 23:35:02,432 - INFO - __main__ - Epoch  12, Step:   25800, Batch Loss:    32.211773, Lr: 0.000090, Tokens per sec:   2666
2023-03-14 23:35:22,710 - INFO - __main__ - Epoch  12, Step:   25900, Batch Loss:    37.326126, Lr: 0.000090, Tokens per sec:   2661
2023-03-14 23:35:42,875 - INFO - __main__ - Epoch  12, Step:   26000, Batch Loss:    26.202860, Lr: 0.000090, Tokens per sec:   2656
2023-03-14 23:36:03,133 - INFO - __main__ - Epoch  12, Step:   26100, Batch Loss:    22.826862, Lr: 0.000090, Tokens per sec:   2710
2023-03-14 23:36:12,914 - INFO - __main__ - Epoch  12: total training loss 64933.44
2023-03-14 23:36:12,915 - INFO - __main__ - Epoch 13
2023-03-14 23:36:23,865 - INFO - __main__ - Epoch  13, Step:   26200, Batch Loss:    19.180748, Lr: 0.000089, Tokens per sec:   2565
2023-03-14 23:36:43,869 - INFO - __main__ - Epoch  13, Step:   26300, Batch Loss:    21.567442, Lr: 0.000089, Tokens per sec:   2680
2023-03-14 23:37:03,603 - INFO - __main__ - Epoch  13, Step:   26400, Batch Loss:    28.726509, Lr: 0.000089, Tokens per sec:   2728
2023-03-14 23:37:23,407 - INFO - __main__ - Epoch  13, Step:   26500, Batch Loss:    21.116934, Lr: 0.000089, Tokens per sec:   2704
2023-03-14 23:37:43,294 - INFO - __main__ - Epoch  13, Step:   26600, Batch Loss:    29.950745, Lr: 0.000089, Tokens per sec:   2710
2023-03-14 23:38:02,903 - INFO - __main__ - Epoch  13, Step:   26700, Batch Loss:    20.878086, Lr: 0.000089, Tokens per sec:   2768
2023-03-14 23:38:22,661 - INFO - __main__ - Epoch  13, Step:   26800, Batch Loss:    31.924576, Lr: 0.000089, Tokens per sec:   2753
2023-03-14 23:38:42,468 - INFO - __main__ - Epoch  13, Step:   26900, Batch Loss:    28.927647, Lr: 0.000089, Tokens per sec:   2711
2023-03-14 23:39:02,202 - INFO - __main__ - Epoch  13, Step:   27000, Batch Loss:    27.578516, Lr: 0.000089, Tokens per sec:   2747
2023-03-14 23:39:21,848 - INFO - __main__ - Epoch  13, Step:   27100, Batch Loss:    21.564962, Lr: 0.000089, Tokens per sec:   2765
2023-03-14 23:39:41,728 - INFO - __main__ - Epoch  13, Step:   27200, Batch Loss:    27.640860, Lr: 0.000089, Tokens per sec:   2729
2023-03-14 23:40:01,621 - INFO - __main__ - Epoch  13, Step:   27300, Batch Loss:    35.875580, Lr: 0.000089, Tokens per sec:   2701
2023-03-14 23:40:21,414 - INFO - __main__ - Epoch  13, Step:   27400, Batch Loss:    32.182659, Lr: 0.000089, Tokens per sec:   2693
2023-03-14 23:40:41,038 - INFO - __main__ - Epoch  13, Step:   27500, Batch Loss:    23.898275, Lr: 0.000089, Tokens per sec:   2751
2023-03-14 23:41:00,730 - INFO - __main__ - Epoch  13, Step:   27600, Batch Loss:    29.726963, Lr: 0.000089, Tokens per sec:   2735
2023-03-14 23:41:20,295 - INFO - __main__ - Epoch  13, Step:   27700, Batch Loss:    25.146425, Lr: 0.000089, Tokens per sec:   2770
2023-03-14 23:41:39,927 - INFO - __main__ - Epoch  13, Step:   27800, Batch Loss:    29.695053, Lr: 0.000089, Tokens per sec:   2766
2023-03-14 23:42:00,246 - INFO - __main__ - Epoch  13, Step:   27900, Batch Loss:    22.625355, Lr: 0.000089, Tokens per sec:   2653
2023-03-14 23:42:20,048 - INFO - __main__ - Epoch  13, Step:   28000, Batch Loss:    22.903498, Lr: 0.000089, Tokens per sec:   2667
2023-03-14 23:42:40,135 - INFO - __main__ - Epoch  13, Step:   28100, Batch Loss:    26.010508, Lr: 0.000089, Tokens per sec:   2615
2023-03-14 23:43:00,510 - INFO - __main__ - Epoch  13, Step:   28200, Batch Loss:    32.448254, Lr: 0.000089, Tokens per sec:   2631
2023-03-14 23:43:21,527 - INFO - __main__ - Epoch  13, Step:   28300, Batch Loss:    23.525496, Lr: 0.000089, Tokens per sec:   2564
2023-03-14 23:43:27,273 - INFO - __main__ - Epoch  13: total training loss 60792.22
2023-03-14 23:43:27,274 - INFO - __main__ - Epoch 14
2023-03-14 23:43:42,542 - INFO - __main__ - Epoch  14, Step:   28400, Batch Loss:    24.236526, Lr: 0.000088, Tokens per sec:   2594
2023-03-14 23:44:02,783 - INFO - __main__ - Epoch  14, Step:   28500, Batch Loss:    29.951466, Lr: 0.000088, Tokens per sec:   2676
2023-03-14 23:44:22,472 - INFO - __main__ - Epoch  14, Step:   28600, Batch Loss:    28.955648, Lr: 0.000088, Tokens per sec:   2730
2023-03-14 23:44:42,874 - INFO - __main__ - Epoch  14, Step:   28700, Batch Loss:    24.674206, Lr: 0.000088, Tokens per sec:   2627
2023-03-14 23:45:03,283 - INFO - __main__ - Epoch  14, Step:   28800, Batch Loss:    27.526966, Lr: 0.000088, Tokens per sec:   2630
2023-03-14 23:45:23,457 - INFO - __main__ - Epoch  14, Step:   28900, Batch Loss:    30.190393, Lr: 0.000088, Tokens per sec:   2642
2023-03-14 23:45:43,562 - INFO - __main__ - Epoch  14, Step:   29000, Batch Loss:    28.983196, Lr: 0.000088, Tokens per sec:   2704
2023-03-14 23:46:03,815 - INFO - __main__ - Epoch  14, Step:   29100, Batch Loss:    20.002214, Lr: 0.000088, Tokens per sec:   2652
2023-03-14 23:46:23,829 - INFO - __main__ - Epoch  14, Step:   29200, Batch Loss:    23.982031, Lr: 0.000088, Tokens per sec:   2702
2023-03-14 23:46:43,867 - INFO - __main__ - Epoch  14, Step:   29300, Batch Loss:    18.734056, Lr: 0.000088, Tokens per sec:   2662
2023-03-14 23:47:03,604 - INFO - __main__ - Epoch  14, Step:   29400, Batch Loss:    32.278759, Lr: 0.000088, Tokens per sec:   2723
2023-03-14 23:47:24,034 - INFO - __main__ - Epoch  14, Step:   29500, Batch Loss:    28.252010, Lr: 0.000088, Tokens per sec:   2646
2023-03-14 23:47:44,130 - INFO - __main__ - Epoch  14, Step:   29600, Batch Loss:    25.547920, Lr: 0.000088, Tokens per sec:   2688
2023-03-14 23:48:04,421 - INFO - __main__ - Epoch  14, Step:   29700, Batch Loss:    29.300020, Lr: 0.000088, Tokens per sec:   2629
2023-03-14 23:48:24,192 - INFO - __main__ - Epoch  14, Step:   29800, Batch Loss:    26.947615, Lr: 0.000088, Tokens per sec:   2702
2023-03-14 23:48:44,317 - INFO - __main__ - Epoch  14, Step:   29900, Batch Loss:    32.218304, Lr: 0.000088, Tokens per sec:   2717
2023-03-14 23:49:04,165 - INFO - __main__ - Epoch  14, Step:   30000, Batch Loss:    23.754080, Lr: 0.000088, Tokens per sec:   2673
2023-03-14 23:49:24,000 - INFO - __main__ - Epoch  14, Step:   30100, Batch Loss:    27.179995, Lr: 0.000088, Tokens per sec:   2749
2023-03-14 23:49:44,282 - INFO - __main__ - Epoch  14, Step:   30200, Batch Loss:    25.777500, Lr: 0.000088, Tokens per sec:   2686
2023-03-14 23:50:04,598 - INFO - __main__ - Epoch  14, Step:   30300, Batch Loss:    23.734350, Lr: 0.000088, Tokens per sec:   2650
2023-03-14 23:50:24,809 - INFO - __main__ - Epoch  14, Step:   30400, Batch Loss:    29.467602, Lr: 0.000088, Tokens per sec:   2648
2023-03-14 23:50:44,650 - INFO - __main__ - Epoch  14, Step:   30500, Batch Loss:    33.318932, Lr: 0.000088, Tokens per sec:   2696
2023-03-14 23:50:46,005 - INFO - __main__ - Epoch  14: total training loss 56979.86
2023-03-14 23:50:46,005 - INFO - __main__ - Epoch 15
2023-03-14 23:51:05,156 - INFO - __main__ - Epoch  15, Step:   30600, Batch Loss:    20.333294, Lr: 0.000087, Tokens per sec:   2626
2023-03-14 23:51:24,995 - INFO - __main__ - Epoch  15, Step:   30700, Batch Loss:    22.240051, Lr: 0.000087, Tokens per sec:   2716
2023-03-14 23:51:45,590 - INFO - __main__ - Epoch  15, Step:   30800, Batch Loss:    12.245677, Lr: 0.000087, Tokens per sec:   2562
2023-03-14 23:52:06,250 - INFO - __main__ - Epoch  15, Step:   30900, Batch Loss:    23.323940, Lr: 0.000087, Tokens per sec:   2557
2023-03-14 23:52:26,858 - INFO - __main__ - Epoch  15, Step:   31000, Batch Loss:    26.558659, Lr: 0.000087, Tokens per sec:   2637
2023-03-14 23:52:47,750 - INFO - __main__ - Epoch  15, Step:   31100, Batch Loss:    23.317488, Lr: 0.000087, Tokens per sec:   2598
2023-03-14 23:53:08,456 - INFO - __main__ - Epoch  15, Step:   31200, Batch Loss:    22.715302, Lr: 0.000087, Tokens per sec:   2622
2023-03-14 23:53:28,771 - INFO - __main__ - Epoch  15, Step:   31300, Batch Loss:    36.585426, Lr: 0.000087, Tokens per sec:   2649
2023-03-14 23:53:49,013 - INFO - __main__ - Epoch  15, Step:   31400, Batch Loss:    26.453861, Lr: 0.000087, Tokens per sec:   2670
2023-03-14 23:54:09,620 - INFO - __main__ - Epoch  15, Step:   31500, Batch Loss:    24.832165, Lr: 0.000087, Tokens per sec:   2621
2023-03-14 23:54:30,114 - INFO - __main__ - Epoch  15, Step:   31600, Batch Loss:    30.998667, Lr: 0.000087, Tokens per sec:   2601
2023-03-14 23:54:50,317 - INFO - __main__ - Epoch  15, Step:   31700, Batch Loss:    35.435520, Lr: 0.000087, Tokens per sec:   2670
2023-03-14 23:55:11,104 - INFO - __main__ - Epoch  15, Step:   31800, Batch Loss:    27.299849, Lr: 0.000087, Tokens per sec:   2579
2023-03-14 23:55:31,778 - INFO - __main__ - Epoch  15, Step:   31900, Batch Loss:    26.808899, Lr: 0.000087, Tokens per sec:   2610
2023-03-14 23:55:52,465 - INFO - __main__ - Epoch  15, Step:   32000, Batch Loss:    18.301241, Lr: 0.000087, Tokens per sec:   2623
2023-03-14 23:56:12,774 - INFO - __main__ - Epoch  15, Step:   32100, Batch Loss:    19.582735, Lr: 0.000087, Tokens per sec:   2642
2023-03-14 23:56:33,167 - INFO - __main__ - Epoch  15, Step:   32200, Batch Loss:    28.489494, Lr: 0.000087, Tokens per sec:   2661
2023-03-14 23:56:53,677 - INFO - __main__ - Epoch  15, Step:   32300, Batch Loss:    20.323792, Lr: 0.000087, Tokens per sec:   2619
2023-03-14 23:57:14,243 - INFO - __main__ - Epoch  15, Step:   32400, Batch Loss:    23.648607, Lr: 0.000087, Tokens per sec:   2640
2023-03-14 23:57:34,331 - INFO - __main__ - Epoch  15, Step:   32500, Batch Loss:    23.054724, Lr: 0.000087, Tokens per sec:   2701
2023-03-14 23:57:54,548 - INFO - __main__ - Epoch  15, Step:   32600, Batch Loss:    28.132746, Lr: 0.000087, Tokens per sec:   2674
2023-03-14 23:58:11,863 - INFO - __main__ - Epoch  15: total training loss 53514.16
2023-03-14 23:58:11,863 - INFO - __main__ - Epoch 16
2023-03-14 23:58:15,406 - INFO - __main__ - Epoch  16, Step:   32700, Batch Loss:    23.243418, Lr: 0.000086, Tokens per sec:   2339
2023-03-14 23:58:36,247 - INFO - __main__ - Epoch  16, Step:   32800, Batch Loss:    23.252872, Lr: 0.000086, Tokens per sec:   2576
2023-03-14 23:58:56,733 - INFO - __main__ - Epoch  16, Step:   32900, Batch Loss:    20.895458, Lr: 0.000086, Tokens per sec:   2632
2023-03-14 23:59:16,962 - INFO - __main__ - Epoch  16, Step:   33000, Batch Loss:    33.942341, Lr: 0.000086, Tokens per sec:   2676
2023-03-14 23:59:37,200 - INFO - __main__ - Epoch  16, Step:   33100, Batch Loss:    25.105680, Lr: 0.000086, Tokens per sec:   2646
2023-03-14 23:59:57,421 - INFO - __main__ - Epoch  16, Step:   33200, Batch Loss:    19.808460, Lr: 0.000086, Tokens per sec:   2688
2023-03-15 00:00:17,963 - INFO - __main__ - Epoch  16, Step:   33300, Batch Loss:    22.413857, Lr: 0.000086, Tokens per sec:   2577
2023-03-15 00:00:38,622 - INFO - __main__ - Epoch  16, Step:   33400, Batch Loss:    21.459787, Lr: 0.000086, Tokens per sec:   2605
2023-03-15 00:00:59,438 - INFO - __main__ - Epoch  16, Step:   33500, Batch Loss:    20.998812, Lr: 0.000086, Tokens per sec:   2584
2023-03-15 00:01:19,928 - INFO - __main__ - Epoch  16, Step:   33600, Batch Loss:    25.276522, Lr: 0.000086, Tokens per sec:   2619
2023-03-15 00:01:40,739 - INFO - __main__ - Epoch  16, Step:   33700, Batch Loss:    20.370859, Lr: 0.000086, Tokens per sec:   2574
2023-03-15 00:02:01,756 - INFO - __main__ - Epoch  16, Step:   33800, Batch Loss:    21.097879, Lr: 0.000086, Tokens per sec:   2566
2023-03-15 00:02:22,541 - INFO - __main__ - Epoch  16, Step:   33900, Batch Loss:    30.556135, Lr: 0.000086, Tokens per sec:   2624
2023-03-15 00:02:43,131 - INFO - __main__ - Epoch  16, Step:   34000, Batch Loss:    15.471834, Lr: 0.000086, Tokens per sec:   2546
2023-03-15 00:03:03,445 - INFO - __main__ - Epoch  16, Step:   34100, Batch Loss:    24.616575, Lr: 0.000086, Tokens per sec:   2669
2023-03-15 00:03:24,040 - INFO - __main__ - Epoch  16, Step:   34200, Batch Loss:    24.213129, Lr: 0.000086, Tokens per sec:   2636
2023-03-15 00:03:44,307 - INFO - __main__ - Epoch  16, Step:   34300, Batch Loss:    31.990578, Lr: 0.000086, Tokens per sec:   2620
2023-03-15 00:04:05,031 - INFO - __main__ - Epoch  16, Step:   34400, Batch Loss:    19.838957, Lr: 0.000086, Tokens per sec:   2594
2023-03-15 00:04:25,511 - INFO - __main__ - Epoch  16, Step:   34500, Batch Loss:    19.769249, Lr: 0.000086, Tokens per sec:   2684
2023-03-15 00:04:45,903 - INFO - __main__ - Epoch  16, Step:   34600, Batch Loss:    24.785093, Lr: 0.000086, Tokens per sec:   2655
2023-03-15 00:05:06,605 - INFO - __main__ - Epoch  16, Step:   34700, Batch Loss:    20.025183, Lr: 0.000086, Tokens per sec:   2612
2023-03-15 00:05:27,422 - INFO - __main__ - Epoch  16, Step:   34800, Batch Loss:    15.813687, Lr: 0.000086, Tokens per sec:   2583
2023-03-15 00:05:41,016 - INFO - __main__ - Epoch  16: total training loss 50216.76
2023-03-15 00:05:41,017 - INFO - __main__ - Epoch 17
2023-03-15 00:05:48,968 - INFO - __main__ - Epoch  17, Step:   34900, Batch Loss:    15.245955, Lr: 0.000085, Tokens per sec:   2435
2023-03-15 00:06:09,654 - INFO - __main__ - Epoch  17, Step:   35000, Batch Loss:    15.348135, Lr: 0.000085, Tokens per sec:   2575
2023-03-15 00:06:30,696 - INFO - __main__ - Epoch  17, Step:   35100, Batch Loss:    13.872154, Lr: 0.000085, Tokens per sec:   2518
2023-03-15 00:06:50,896 - INFO - __main__ - Epoch  17, Step:   35200, Batch Loss:    16.081203, Lr: 0.000085, Tokens per sec:   2677
2023-03-15 00:07:11,584 - INFO - __main__ - Epoch  17, Step:   35300, Batch Loss:    24.372213, Lr: 0.000085, Tokens per sec:   2602
2023-03-15 00:07:31,977 - INFO - __main__ - Epoch  17, Step:   35400, Batch Loss:    22.618959, Lr: 0.000085, Tokens per sec:   2613
2023-03-15 00:07:52,657 - INFO - __main__ - Epoch  17, Step:   35500, Batch Loss:    15.623335, Lr: 0.000085, Tokens per sec:   2597
2023-03-15 00:08:13,540 - INFO - __main__ - Epoch  17, Step:   35600, Batch Loss:    22.148563, Lr: 0.000085, Tokens per sec:   2621
2023-03-15 00:08:34,107 - INFO - __main__ - Epoch  17, Step:   35700, Batch Loss:    21.488026, Lr: 0.000085, Tokens per sec:   2696
2023-03-15 00:08:54,782 - INFO - __main__ - Epoch  17, Step:   35800, Batch Loss:    32.795856, Lr: 0.000085, Tokens per sec:   2638
2023-03-15 00:09:15,305 - INFO - __main__ - Epoch  17, Step:   35900, Batch Loss:    20.425356, Lr: 0.000085, Tokens per sec:   2630
2023-03-15 00:09:35,715 - INFO - __main__ - Epoch  17, Step:   36000, Batch Loss:    14.140249, Lr: 0.000085, Tokens per sec:   2611
2023-03-15 00:09:56,400 - INFO - __main__ - Epoch  17, Step:   36100, Batch Loss:    21.982618, Lr: 0.000085, Tokens per sec:   2604
2023-03-15 00:10:16,953 - INFO - __main__ - Epoch  17, Step:   36200, Batch Loss:    25.236443, Lr: 0.000085, Tokens per sec:   2643
2023-03-15 00:10:37,570 - INFO - __main__ - Epoch  17, Step:   36300, Batch Loss:    18.690788, Lr: 0.000085, Tokens per sec:   2639
2023-03-15 00:10:58,083 - INFO - __main__ - Epoch  17, Step:   36400, Batch Loss:    21.640467, Lr: 0.000085, Tokens per sec:   2589
2023-03-15 00:11:18,539 - INFO - __main__ - Epoch  17, Step:   36500, Batch Loss:    32.216766, Lr: 0.000085, Tokens per sec:   2610
2023-03-15 00:11:38,513 - INFO - __main__ - Epoch  17, Step:   36600, Batch Loss:    24.534458, Lr: 0.000085, Tokens per sec:   2711
2023-03-15 00:11:58,998 - INFO - __main__ - Epoch  17, Step:   36700, Batch Loss:    20.713659, Lr: 0.000085, Tokens per sec:   2603
2023-03-15 00:12:19,650 - INFO - __main__ - Epoch  17, Step:   36800, Batch Loss:    15.847611, Lr: 0.000085, Tokens per sec:   2570
2023-03-15 00:12:40,184 - INFO - __main__ - Epoch  17, Step:   36900, Batch Loss:    23.340771, Lr: 0.000085, Tokens per sec:   2655
2023-03-15 00:13:00,897 - INFO - __main__ - Epoch  17, Step:   37000, Batch Loss:    22.786489, Lr: 0.000085, Tokens per sec:   2587
2023-03-15 00:13:10,005 - INFO - __main__ - Epoch  17: total training loss 47114.44
2023-03-15 00:13:10,006 - INFO - __main__ - Epoch 18
2023-03-15 00:13:22,092 - INFO - __main__ - Epoch  18, Step:   37100, Batch Loss:    13.265884, Lr: 0.000084, Tokens per sec:   2501
2023-03-15 00:13:43,069 - INFO - __main__ - Epoch  18, Step:   37200, Batch Loss:    15.922132, Lr: 0.000084, Tokens per sec:   2581
2023-03-15 00:14:03,206 - INFO - __main__ - Epoch  18, Step:   37300, Batch Loss:    27.177420, Lr: 0.000084, Tokens per sec:   2666
2023-03-15 00:14:23,850 - INFO - __main__ - Epoch  18, Step:   37400, Batch Loss:    25.159191, Lr: 0.000084, Tokens per sec:   2633
2023-03-15 00:14:44,505 - INFO - __main__ - Epoch  18, Step:   37500, Batch Loss:    23.566545, Lr: 0.000084, Tokens per sec:   2606
2023-03-15 00:15:04,960 - INFO - __main__ - Epoch  18, Step:   37600, Batch Loss:    17.906811, Lr: 0.000084, Tokens per sec:   2602
2023-03-15 00:15:25,860 - INFO - __main__ - Epoch  18, Step:   37700, Batch Loss:    28.388929, Lr: 0.000084, Tokens per sec:   2594
2023-03-15 00:15:46,343 - INFO - __main__ - Epoch  18, Step:   37800, Batch Loss:    16.307875, Lr: 0.000084, Tokens per sec:   2605
2023-03-15 00:16:07,133 - INFO - __main__ - Epoch  18, Step:   37900, Batch Loss:    27.389910, Lr: 0.000084, Tokens per sec:   2609
2023-03-15 00:16:27,852 - INFO - __main__ - Epoch  18, Step:   38000, Batch Loss:    15.560476, Lr: 0.000084, Tokens per sec:   2591
2023-03-15 00:16:48,143 - INFO - __main__ - Epoch  18, Step:   38100, Batch Loss:    13.487127, Lr: 0.000084, Tokens per sec:   2639
2023-03-15 00:17:08,453 - INFO - __main__ - Epoch  18, Step:   38200, Batch Loss:    18.819012, Lr: 0.000084, Tokens per sec:   2647
2023-03-15 00:17:28,576 - INFO - __main__ - Epoch  18, Step:   38300, Batch Loss:    22.882160, Lr: 0.000084, Tokens per sec:   2657
2023-03-15 00:17:48,974 - INFO - __main__ - Epoch  18, Step:   38400, Batch Loss:    19.264301, Lr: 0.000084, Tokens per sec:   2638
2023-03-15 00:18:09,643 - INFO - __main__ - Epoch  18, Step:   38500, Batch Loss:    18.945442, Lr: 0.000084, Tokens per sec:   2609
2023-03-15 00:18:30,237 - INFO - __main__ - Epoch  18, Step:   38600, Batch Loss:    24.680172, Lr: 0.000084, Tokens per sec:   2626
2023-03-15 00:18:50,790 - INFO - __main__ - Epoch  18, Step:   38700, Batch Loss:    25.187656, Lr: 0.000084, Tokens per sec:   2610
2023-03-15 00:19:11,346 - INFO - __main__ - Epoch  18, Step:   38800, Batch Loss:    19.501139, Lr: 0.000084, Tokens per sec:   2615
2023-03-15 00:19:32,111 - INFO - __main__ - Epoch  18, Step:   38900, Batch Loss:    16.229282, Lr: 0.000084, Tokens per sec:   2562
2023-03-15 00:19:52,888 - INFO - __main__ - Epoch  18, Step:   39000, Batch Loss:    15.888430, Lr: 0.000084, Tokens per sec:   2609
2023-03-15 00:20:13,776 - INFO - __main__ - Epoch  18, Step:   39100, Batch Loss:    17.726679, Lr: 0.000084, Tokens per sec:   2639
2023-03-15 00:20:34,521 - INFO - __main__ - Epoch  18, Step:   39200, Batch Loss:    24.595795, Lr: 0.000084, Tokens per sec:   2576
2023-03-15 00:20:39,110 - INFO - __main__ - Epoch  18: total training loss 44281.89
2023-03-15 00:20:39,111 - INFO - __main__ - Epoch 19
2023-03-15 00:20:55,632 - INFO - __main__ - Epoch  19, Step:   39300, Batch Loss:    16.950256, Lr: 0.000083, Tokens per sec:   2541
2023-03-15 00:21:15,675 - INFO - __main__ - Epoch  19, Step:   39400, Batch Loss:    20.656279, Lr: 0.000083, Tokens per sec:   2667
2023-03-15 00:21:35,964 - INFO - __main__ - Epoch  19, Step:   39500, Batch Loss:    11.610756, Lr: 0.000083, Tokens per sec:   2653
2023-03-15 00:21:56,643 - INFO - __main__ - Epoch  19, Step:   39600, Batch Loss:    24.516695, Lr: 0.000083, Tokens per sec:   2638
2023-03-15 00:22:17,335 - INFO - __main__ - Epoch  19, Step:   39700, Batch Loss:    18.076696, Lr: 0.000083, Tokens per sec:   2582
2023-03-15 00:22:38,344 - INFO - __main__ - Epoch  19, Step:   39800, Batch Loss:    18.698927, Lr: 0.000083, Tokens per sec:   2577
2023-03-15 00:22:59,308 - INFO - __main__ - Epoch  19, Step:   39900, Batch Loss:    16.357674, Lr: 0.000083, Tokens per sec:   2553
2023-03-15 00:23:20,454 - INFO - __main__ - Epoch  19, Step:   40000, Batch Loss:    19.333681, Lr: 0.000083, Tokens per sec:   2538
2023-03-15 00:23:41,276 - INFO - __main__ - Epoch  19, Step:   40100, Batch Loss:    20.643785, Lr: 0.000083, Tokens per sec:   2635
2023-03-15 00:24:02,167 - INFO - __main__ - Epoch  19, Step:   40200, Batch Loss:    16.207573, Lr: 0.000083, Tokens per sec:   2604
2023-03-15 00:24:23,041 - INFO - __main__ - Epoch  19, Step:   40300, Batch Loss:    21.737877, Lr: 0.000083, Tokens per sec:   2551
2023-03-15 00:24:43,730 - INFO - __main__ - Epoch  19, Step:   40400, Batch Loss:    24.899647, Lr: 0.000083, Tokens per sec:   2580
2023-03-15 00:25:04,300 - INFO - __main__ - Epoch  19, Step:   40500, Batch Loss:    18.723204, Lr: 0.000083, Tokens per sec:   2616
2023-03-15 00:25:25,124 - INFO - __main__ - Epoch  19, Step:   40600, Batch Loss:    20.797432, Lr: 0.000083, Tokens per sec:   2576
2023-03-15 00:25:46,071 - INFO - __main__ - Epoch  19, Step:   40700, Batch Loss:    24.174980, Lr: 0.000083, Tokens per sec:   2583
2023-03-15 00:26:06,921 - INFO - __main__ - Epoch  19, Step:   40800, Batch Loss:    22.806414, Lr: 0.000083, Tokens per sec:   2571
2023-03-15 00:26:27,832 - INFO - __main__ - Epoch  19, Step:   40900, Batch Loss:    20.861147, Lr: 0.000083, Tokens per sec:   2565
2023-03-15 00:26:48,605 - INFO - __main__ - Epoch  19, Step:   41000, Batch Loss:    20.322910, Lr: 0.000083, Tokens per sec:   2547
2023-03-15 00:27:09,648 - INFO - __main__ - Epoch  19, Step:   41100, Batch Loss:    19.497299, Lr: 0.000083, Tokens per sec:   2529
2023-03-15 00:27:30,552 - INFO - __main__ - Epoch  19, Step:   41200, Batch Loss:    13.274479, Lr: 0.000083, Tokens per sec:   2627
2023-03-15 00:27:51,468 - INFO - __main__ - Epoch  19, Step:   41300, Batch Loss:    17.749081, Lr: 0.000083, Tokens per sec:   2606
2023-03-15 00:28:12,747 - INFO - __main__ - Epoch  19, Step:   41400, Batch Loss:    22.935284, Lr: 0.000083, Tokens per sec:   2530
2023-03-15 00:28:13,042 - INFO - __main__ - Epoch  19: total training loss 41556.50
2023-03-15 00:28:13,042 - INFO - __main__ - Epoch 20
2023-03-15 00:28:33,771 - INFO - __main__ - Epoch  20, Step:   41500, Batch Loss:    13.570100, Lr: 0.000083, Tokens per sec:   2554
2023-03-15 00:28:54,959 - INFO - __main__ - Epoch  20, Step:   41600, Batch Loss:    13.669320, Lr: 0.000083, Tokens per sec:   2572
2023-03-15 00:29:15,652 - INFO - __main__ - Epoch  20, Step:   41700, Batch Loss:    17.891209, Lr: 0.000083, Tokens per sec:   2593
2023-03-15 00:29:36,370 - INFO - __main__ - Epoch  20, Step:   41800, Batch Loss:    20.364658, Lr: 0.000083, Tokens per sec:   2621
2023-03-15 00:29:57,104 - INFO - __main__ - Epoch  20, Step:   41900, Batch Loss:    15.301924, Lr: 0.000083, Tokens per sec:   2587
2023-03-15 00:30:18,181 - INFO - __main__ - Epoch  20, Step:   42000, Batch Loss:    18.430803, Lr: 0.000083, Tokens per sec:   2570
2023-03-15 00:30:39,096 - INFO - __main__ - Epoch  20, Step:   42100, Batch Loss:    19.248152, Lr: 0.000083, Tokens per sec:   2611
2023-03-15 00:30:59,659 - INFO - __main__ - Epoch  20, Step:   42200, Batch Loss:    14.560504, Lr: 0.000083, Tokens per sec:   2603
2023-03-15 00:31:20,136 - INFO - __main__ - Epoch  20, Step:   42300, Batch Loss:    21.026983, Lr: 0.000083, Tokens per sec:   2604
2023-03-15 00:31:40,834 - INFO - __main__ - Epoch  20, Step:   42400, Batch Loss:    13.758594, Lr: 0.000083, Tokens per sec:   2605
2023-03-15 00:32:01,430 - INFO - __main__ - Epoch  20, Step:   42500, Batch Loss:    21.539385, Lr: 0.000083, Tokens per sec:   2590
2023-03-15 00:32:21,958 - INFO - __main__ - Epoch  20, Step:   42600, Batch Loss:    15.496810, Lr: 0.000083, Tokens per sec:   2607
2023-03-15 00:32:42,839 - INFO - __main__ - Epoch  20, Step:   42700, Batch Loss:    17.035202, Lr: 0.000083, Tokens per sec:   2577
2023-03-15 00:33:03,687 - INFO - __main__ - Epoch  20, Step:   42800, Batch Loss:    17.308107, Lr: 0.000083, Tokens per sec:   2632
2023-03-15 00:33:24,340 - INFO - __main__ - Epoch  20, Step:   42900, Batch Loss:    17.399254, Lr: 0.000083, Tokens per sec:   2637
2023-03-15 00:33:45,075 - INFO - __main__ - Epoch  20, Step:   43000, Batch Loss:    15.557714, Lr: 0.000083, Tokens per sec:   2563
2023-03-15 00:34:06,052 - INFO - __main__ - Epoch  20, Step:   43100, Batch Loss:    18.082846, Lr: 0.000083, Tokens per sec:   2585
2023-03-15 00:34:26,866 - INFO - __main__ - Epoch  20, Step:   43200, Batch Loss:    20.569368, Lr: 0.000083, Tokens per sec:   2568
2023-03-15 00:34:47,623 - INFO - __main__ - Epoch  20, Step:   43300, Batch Loss:    20.019764, Lr: 0.000083, Tokens per sec:   2583
2023-03-15 00:35:08,203 - INFO - __main__ - Epoch  20, Step:   43400, Batch Loss:    17.036194, Lr: 0.000083, Tokens per sec:   2586
2023-03-15 00:35:28,744 - INFO - __main__ - Epoch  20, Step:   43500, Batch Loss:    26.519003, Lr: 0.000083, Tokens per sec:   2604
2023-03-15 00:35:45,489 - INFO - __main__ - Epoch  20: total training loss 39179.00
2023-03-15 00:35:45,490 - INFO - __main__ - Epoch 21
2023-03-15 00:35:50,140 - INFO - __main__ - Epoch  21, Step:   43600, Batch Loss:    13.343100, Lr: 0.000082, Tokens per sec:   2261
2023-03-15 00:36:10,611 - INFO - __main__ - Epoch  21, Step:   43700, Batch Loss:    14.934163, Lr: 0.000082, Tokens per sec:   2671
2023-03-15 00:36:31,510 - INFO - __main__ - Epoch  21, Step:   43800, Batch Loss:    14.208611, Lr: 0.000082, Tokens per sec:   2576
2023-03-15 00:36:52,259 - INFO - __main__ - Epoch  21, Step:   43900, Batch Loss:    12.681591, Lr: 0.000082, Tokens per sec:   2572
2023-03-15 00:37:12,787 - INFO - __main__ - Epoch  21, Step:   44000, Batch Loss:    14.836447, Lr: 0.000082, Tokens per sec:   2629
2023-03-15 00:37:33,916 - INFO - __main__ - Epoch  21, Step:   44100, Batch Loss:    14.735812, Lr: 0.000082, Tokens per sec:   2551
2023-03-15 00:37:54,744 - INFO - __main__ - Epoch  21, Step:   44200, Batch Loss:    13.777976, Lr: 0.000082, Tokens per sec:   2541
2023-03-15 00:38:15,632 - INFO - __main__ - Epoch  21, Step:   44300, Batch Loss:    15.214648, Lr: 0.000082, Tokens per sec:   2538
2023-03-15 00:38:36,645 - INFO - __main__ - Epoch  21, Step:   44400, Batch Loss:    15.483821, Lr: 0.000082, Tokens per sec:   2578
2023-03-15 00:38:57,095 - INFO - __main__ - Epoch  21, Step:   44500, Batch Loss:    16.264536, Lr: 0.000082, Tokens per sec:   2640
2023-03-15 00:39:17,407 - INFO - __main__ - Epoch  21, Step:   44600, Batch Loss:    21.774261, Lr: 0.000082, Tokens per sec:   2657
2023-03-15 00:39:37,781 - INFO - __main__ - Epoch  21, Step:   44700, Batch Loss:    15.773824, Lr: 0.000082, Tokens per sec:   2669
2023-03-15 00:39:58,078 - INFO - __main__ - Epoch  21, Step:   44800, Batch Loss:    23.555826, Lr: 0.000082, Tokens per sec:   2617
2023-03-15 00:40:18,610 - INFO - __main__ - Epoch  21, Step:   44900, Batch Loss:    21.644308, Lr: 0.000082, Tokens per sec:   2655
2023-03-15 00:40:39,234 - INFO - __main__ - Epoch  21, Step:   45000, Batch Loss:    20.904938, Lr: 0.000082, Tokens per sec:   2627
2023-03-15 00:40:59,714 - INFO - __main__ - Epoch  21, Step:   45100, Batch Loss:    15.951686, Lr: 0.000082, Tokens per sec:   2631
2023-03-15 00:41:20,247 - INFO - __main__ - Epoch  21, Step:   45200, Batch Loss:    19.348249, Lr: 0.000082, Tokens per sec:   2659
2023-03-15 00:41:40,648 - INFO - __main__ - Epoch  21, Step:   45300, Batch Loss:    16.949427, Lr: 0.000082, Tokens per sec:   2645
2023-03-15 00:42:01,402 - INFO - __main__ - Epoch  21, Step:   45400, Batch Loss:    21.045347, Lr: 0.000082, Tokens per sec:   2552
2023-03-15 00:42:22,357 - INFO - __main__ - Epoch  21, Step:   45500, Batch Loss:    21.657843, Lr: 0.000082, Tokens per sec:   2566
2023-03-15 00:42:43,374 - INFO - __main__ - Epoch  21, Step:   45600, Batch Loss:    15.301934, Lr: 0.000082, Tokens per sec:   2550
2023-03-15 00:43:04,853 - INFO - __main__ - Epoch  21, Step:   45700, Batch Loss:    19.857010, Lr: 0.000082, Tokens per sec:   2511
2023-03-15 00:43:17,347 - INFO - __main__ - Epoch  21: total training loss 36773.16
2023-03-15 00:43:17,347 - INFO - __main__ - Epoch 22
2023-03-15 00:43:26,484 - INFO - __main__ - Epoch  22, Step:   45800, Batch Loss:    13.539829, Lr: 0.000081, Tokens per sec:   2441
2023-03-15 00:43:47,061 - INFO - __main__ - Epoch  22, Step:   45900, Batch Loss:    14.084809, Lr: 0.000081, Tokens per sec:   2600
2023-03-15 00:44:07,808 - INFO - __main__ - Epoch  22, Step:   46000, Batch Loss:    17.833260, Lr: 0.000081, Tokens per sec:   2598
2023-03-15 00:44:28,939 - INFO - __main__ - Epoch  22, Step:   46100, Batch Loss:    13.392446, Lr: 0.000081, Tokens per sec:   2548
2023-03-15 00:44:49,861 - INFO - __main__ - Epoch  22, Step:   46200, Batch Loss:    14.458375, Lr: 0.000081, Tokens per sec:   2624
2023-03-15 00:45:10,721 - INFO - __main__ - Epoch  22, Step:   46300, Batch Loss:    18.800528, Lr: 0.000081, Tokens per sec:   2619
2023-03-15 00:45:32,083 - INFO - __main__ - Epoch  22, Step:   46400, Batch Loss:    16.306831, Lr: 0.000081, Tokens per sec:   2523
2023-03-15 00:45:53,182 - INFO - __main__ - Epoch  22, Step:   46500, Batch Loss:    18.963903, Lr: 0.000081, Tokens per sec:   2543
2023-03-15 00:46:14,054 - INFO - __main__ - Epoch  22, Step:   46600, Batch Loss:    19.986835, Lr: 0.000081, Tokens per sec:   2539
2023-03-15 00:46:34,974 - INFO - __main__ - Epoch  22, Step:   46700, Batch Loss:    17.244858, Lr: 0.000081, Tokens per sec:   2569
2023-03-15 00:46:55,868 - INFO - __main__ - Epoch  22, Step:   46800, Batch Loss:    14.710113, Lr: 0.000081, Tokens per sec:   2535
2023-03-15 00:47:17,044 - INFO - __main__ - Epoch  22, Step:   46900, Batch Loss:    16.286154, Lr: 0.000081, Tokens per sec:   2538
2023-03-15 00:47:37,540 - INFO - __main__ - Epoch  22, Step:   47000, Batch Loss:    21.233896, Lr: 0.000081, Tokens per sec:   2642
2023-03-15 00:47:58,222 - INFO - __main__ - Epoch  22, Step:   47100, Batch Loss:    14.877635, Lr: 0.000081, Tokens per sec:   2582
2023-03-15 00:48:19,105 - INFO - __main__ - Epoch  22, Step:   47200, Batch Loss:    11.500352, Lr: 0.000081, Tokens per sec:   2577
2023-03-15 00:48:39,752 - INFO - __main__ - Epoch  22, Step:   47300, Batch Loss:    10.072569, Lr: 0.000081, Tokens per sec:   2588
2023-03-15 00:49:00,479 - INFO - __main__ - Epoch  22, Step:   47400, Batch Loss:    15.342212, Lr: 0.000081, Tokens per sec:   2572
2023-03-15 00:49:21,072 - INFO - __main__ - Epoch  22, Step:   47500, Batch Loss:    22.352272, Lr: 0.000081, Tokens per sec:   2638
2023-03-15 00:49:41,887 - INFO - __main__ - Epoch  22, Step:   47600, Batch Loss:    16.410927, Lr: 0.000081, Tokens per sec:   2600
2023-03-15 00:50:03,161 - INFO - __main__ - Epoch  22, Step:   47700, Batch Loss:    18.837929, Lr: 0.000081, Tokens per sec:   2554
2023-03-15 00:50:24,137 - INFO - __main__ - Epoch  22, Step:   47800, Batch Loss:    18.114370, Lr: 0.000081, Tokens per sec:   2565
2023-03-15 00:50:44,887 - INFO - __main__ - Epoch  22, Step:   47900, Batch Loss:    13.365369, Lr: 0.000081, Tokens per sec:   2587
2023-03-15 00:50:52,655 - INFO - __main__ - Epoch  22: total training loss 34610.21
2023-03-15 00:50:52,656 - INFO - __main__ - Epoch 23
2023-03-15 00:51:05,779 - INFO - __main__ - Epoch  23, Step:   48000, Batch Loss:    15.804022, Lr: 0.000080, Tokens per sec:   2542
2023-03-15 00:51:26,297 - INFO - __main__ - Epoch  23, Step:   48100, Batch Loss:    12.398493, Lr: 0.000080, Tokens per sec:   2628
2023-03-15 00:51:46,480 - INFO - __main__ - Epoch  23, Step:   48200, Batch Loss:    14.036804, Lr: 0.000080, Tokens per sec:   2708
2023-03-15 00:52:07,287 - INFO - __main__ - Epoch  23, Step:   48300, Batch Loss:    12.439754, Lr: 0.000080, Tokens per sec:   2567
2023-03-15 00:52:27,825 - INFO - __main__ - Epoch  23, Step:   48400, Batch Loss:    11.361297, Lr: 0.000080, Tokens per sec:   2659
2023-03-15 00:52:47,971 - INFO - __main__ - Epoch  23, Step:   48500, Batch Loss:    15.779369, Lr: 0.000080, Tokens per sec:   2686
2023-03-15 00:53:09,016 - INFO - __main__ - Epoch  23, Step:   48600, Batch Loss:    16.733271, Lr: 0.000080, Tokens per sec:   2571
2023-03-15 00:53:29,736 - INFO - __main__ - Epoch  23, Step:   48700, Batch Loss:    15.500898, Lr: 0.000080, Tokens per sec:   2604
2023-03-15 00:53:50,429 - INFO - __main__ - Epoch  23, Step:   48800, Batch Loss:    11.053870, Lr: 0.000080, Tokens per sec:   2587
2023-03-15 00:54:11,233 - INFO - __main__ - Epoch  23, Step:   48900, Batch Loss:    13.205541, Lr: 0.000080, Tokens per sec:   2593
2023-03-15 00:54:31,792 - INFO - __main__ - Epoch  23, Step:   49000, Batch Loss:    15.187936, Lr: 0.000080, Tokens per sec:   2623
2023-03-15 00:54:52,265 - INFO - __main__ - Epoch  23, Step:   49100, Batch Loss:    16.036327, Lr: 0.000080, Tokens per sec:   2617
2023-03-15 00:55:13,174 - INFO - __main__ - Epoch  23, Step:   49200, Batch Loss:     8.200421, Lr: 0.000080, Tokens per sec:   2579
2023-03-15 00:55:33,819 - INFO - __main__ - Epoch  23, Step:   49300, Batch Loss:    16.968153, Lr: 0.000080, Tokens per sec:   2610
2023-03-15 00:55:54,621 - INFO - __main__ - Epoch  23, Step:   49400, Batch Loss:    17.973927, Lr: 0.000080, Tokens per sec:   2625
2023-03-15 00:56:15,182 - INFO - __main__ - Epoch  23, Step:   49500, Batch Loss:    17.037634, Lr: 0.000080, Tokens per sec:   2617
2023-03-15 00:56:35,927 - INFO - __main__ - Epoch  23, Step:   49600, Batch Loss:    11.489246, Lr: 0.000080, Tokens per sec:   2614
2023-03-15 00:56:56,424 - INFO - __main__ - Epoch  23, Step:   49700, Batch Loss:    16.049635, Lr: 0.000080, Tokens per sec:   2600
2023-03-15 00:57:16,922 - INFO - __main__ - Epoch  23, Step:   49800, Batch Loss:    15.714032, Lr: 0.000080, Tokens per sec:   2641
2023-03-15 00:57:37,615 - INFO - __main__ - Epoch  23, Step:   49900, Batch Loss:    15.614988, Lr: 0.000080, Tokens per sec:   2570
2023-03-15 00:57:58,388 - INFO - __main__ - Epoch  23, Step:   50000, Batch Loss:    13.813500, Lr: 0.000080, Tokens per sec:   2546
2023-03-15 00:58:19,479 - INFO - __main__ - Epoch  23, Step:   50100, Batch Loss:    14.000272, Lr: 0.000080, Tokens per sec:   2509
2023-03-15 00:58:23,166 - INFO - __main__ - Epoch  23: total training loss 32597.39
2023-03-15 00:58:23,166 - INFO - __main__ - Epoch 24
2023-03-15 00:58:40,624 - INFO - __main__ - Epoch  24, Step:   50200, Batch Loss:    10.804190, Lr: 0.000079, Tokens per sec:   2583
2023-03-15 00:59:01,128 - INFO - __main__ - Epoch  24, Step:   50300, Batch Loss:    18.083269, Lr: 0.000079, Tokens per sec:   2616
2023-03-15 00:59:21,728 - INFO - __main__ - Epoch  24, Step:   50400, Batch Loss:    16.650908, Lr: 0.000079, Tokens per sec:   2644
2023-03-15 00:59:42,918 - INFO - __main__ - Epoch  24, Step:   50500, Batch Loss:    10.166763, Lr: 0.000079, Tokens per sec:   2519
2023-03-15 01:00:03,292 - INFO - __main__ - Epoch  24, Step:   50600, Batch Loss:    13.042850, Lr: 0.000079, Tokens per sec:   2657
2023-03-15 01:00:23,711 - INFO - __main__ - Epoch  24, Step:   50700, Batch Loss:    12.008767, Lr: 0.000079, Tokens per sec:   2626
2023-03-15 01:00:44,879 - INFO - __main__ - Epoch  24, Step:   50800, Batch Loss:    13.943724, Lr: 0.000079, Tokens per sec:   2565
2023-03-15 01:01:05,775 - INFO - __main__ - Epoch  24, Step:   50900, Batch Loss:    12.953888, Lr: 0.000079, Tokens per sec:   2528
2023-03-15 01:01:26,457 - INFO - __main__ - Epoch  24, Step:   51000, Batch Loss:    12.288034, Lr: 0.000079, Tokens per sec:   2571
2023-03-15 01:01:46,977 - INFO - __main__ - Epoch  24, Step:   51100, Batch Loss:    12.920530, Lr: 0.000079, Tokens per sec:   2630
2023-03-15 01:02:07,750 - INFO - __main__ - Epoch  24, Step:   51200, Batch Loss:    14.928273, Lr: 0.000079, Tokens per sec:   2628
2023-03-15 01:02:28,660 - INFO - __main__ - Epoch  24, Step:   51300, Batch Loss:    10.826147, Lr: 0.000079, Tokens per sec:   2590
2023-03-15 01:02:49,331 - INFO - __main__ - Epoch  24, Step:   51400, Batch Loss:     9.494744, Lr: 0.000079, Tokens per sec:   2620
2023-03-15 01:03:09,751 - INFO - __main__ - Epoch  24, Step:   51500, Batch Loss:    12.932133, Lr: 0.000079, Tokens per sec:   2648
2023-03-15 01:03:30,770 - INFO - __main__ - Epoch  24, Step:   51600, Batch Loss:    17.944992, Lr: 0.000079, Tokens per sec:   2561
2023-03-15 01:03:51,296 - INFO - __main__ - Epoch  24, Step:   51700, Batch Loss:    15.792328, Lr: 0.000079, Tokens per sec:   2588
2023-03-15 01:04:12,349 - INFO - __main__ - Epoch  24, Step:   51800, Batch Loss:    13.050022, Lr: 0.000079, Tokens per sec:   2524
2023-03-15 01:04:33,107 - INFO - __main__ - Epoch  24, Step:   51900, Batch Loss:    13.597193, Lr: 0.000079, Tokens per sec:   2637
2023-03-15 01:04:53,976 - INFO - __main__ - Epoch  24, Step:   52000, Batch Loss:    16.134050, Lr: 0.000079, Tokens per sec:   2568
2023-03-15 01:05:14,802 - INFO - __main__ - Epoch  24, Step:   52100, Batch Loss:    10.581470, Lr: 0.000079, Tokens per sec:   2587
2023-03-15 01:05:35,644 - INFO - __main__ - Epoch  24, Step:   52200, Batch Loss:    10.809917, Lr: 0.000079, Tokens per sec:   2579
2023-03-15 01:05:55,818 - INFO - __main__ - Epoch  24: total training loss 30663.04
2023-03-15 01:05:55,819 - INFO - __main__ - Epoch 25
2023-03-15 01:05:57,071 - INFO - __main__ - Epoch  25, Step:   52300, Batch Loss:    10.993428, Lr: 0.000079, Tokens per sec:   1733
2023-03-15 01:06:17,927 - INFO - __main__ - Epoch  25, Step:   52400, Batch Loss:    13.305436, Lr: 0.000079, Tokens per sec:   2583
2023-03-15 01:06:38,426 - INFO - __main__ - Epoch  25, Step:   52500, Batch Loss:    10.879877, Lr: 0.000079, Tokens per sec:   2628
2023-03-15 01:06:58,856 - INFO - __main__ - Epoch  25, Step:   52600, Batch Loss:    13.125227, Lr: 0.000079, Tokens per sec:   2587
2023-03-15 01:07:19,201 - INFO - __main__ - Epoch  25, Step:   52700, Batch Loss:    15.437382, Lr: 0.000079, Tokens per sec:   2643
2023-03-15 01:07:40,195 - INFO - __main__ - Epoch  25, Step:   52800, Batch Loss:    14.185122, Lr: 0.000079, Tokens per sec:   2557
2023-03-15 01:08:01,027 - INFO - __main__ - Epoch  25, Step:   52900, Batch Loss:    18.930365, Lr: 0.000079, Tokens per sec:   2569
2023-03-15 01:08:22,170 - INFO - __main__ - Epoch  25, Step:   53000, Batch Loss:    12.609379, Lr: 0.000079, Tokens per sec:   2504
2023-03-15 01:08:42,980 - INFO - __main__ - Epoch  25, Step:   53100, Batch Loss:    20.460030, Lr: 0.000079, Tokens per sec:   2641
2023-03-15 01:09:03,389 - INFO - __main__ - Epoch  25, Step:   53200, Batch Loss:    12.477195, Lr: 0.000079, Tokens per sec:   2628
2023-03-15 01:09:23,786 - INFO - __main__ - Epoch  25, Step:   53300, Batch Loss:    12.196368, Lr: 0.000079, Tokens per sec:   2615
2023-03-15 01:09:44,343 - INFO - __main__ - Epoch  25, Step:   53400, Batch Loss:    18.855225, Lr: 0.000079, Tokens per sec:   2661
2023-03-15 01:10:04,966 - INFO - __main__ - Epoch  25, Step:   53500, Batch Loss:    16.786470, Lr: 0.000079, Tokens per sec:   2538
2023-03-15 01:10:25,745 - INFO - __main__ - Epoch  25, Step:   53600, Batch Loss:     7.295403, Lr: 0.000079, Tokens per sec:   2537
2023-03-15 01:10:45,780 - INFO - __main__ - Epoch  25, Step:   53700, Batch Loss:    13.016294, Lr: 0.000079, Tokens per sec:   2718
2023-03-15 01:11:06,110 - INFO - __main__ - Epoch  25, Step:   53800, Batch Loss:    16.550468, Lr: 0.000079, Tokens per sec:   2631
2023-03-15 01:11:26,275 - INFO - __main__ - Epoch  25, Step:   53900, Batch Loss:    14.448631, Lr: 0.000079, Tokens per sec:   2707
2023-03-15 01:11:46,472 - INFO - __main__ - Epoch  25, Step:   54000, Batch Loss:    14.973550, Lr: 0.000079, Tokens per sec:   2731
2023-03-15 01:12:06,770 - INFO - __main__ - Epoch  25, Step:   54100, Batch Loss:    13.187741, Lr: 0.000079, Tokens per sec:   2660
2023-03-15 01:12:27,185 - INFO - __main__ - Epoch  25, Step:   54200, Batch Loss:    13.584163, Lr: 0.000079, Tokens per sec:   2670
2023-03-15 01:12:47,524 - INFO - __main__ - Epoch  25, Step:   54300, Batch Loss:    21.382788, Lr: 0.000079, Tokens per sec:   2653
2023-03-15 01:13:07,652 - INFO - __main__ - Epoch  25, Step:   54400, Batch Loss:    15.310583, Lr: 0.000079, Tokens per sec:   2688
2023-03-15 01:13:22,723 - INFO - __main__ - Epoch  25: total training loss 28937.26
2023-03-15 01:13:22,724 - INFO - __main__ - Epoch 26
2023-03-15 01:13:28,162 - INFO - __main__ - Epoch  26, Step:   54500, Batch Loss:    10.789404, Lr: 0.000078, Tokens per sec:   2527
2023-03-15 01:13:48,392 - INFO - __main__ - Epoch  26, Step:   54600, Batch Loss:     6.877358, Lr: 0.000078, Tokens per sec:   2614
2023-03-15 01:14:08,881 - INFO - __main__ - Epoch  26, Step:   54700, Batch Loss:     9.878770, Lr: 0.000078, Tokens per sec:   2632
2023-03-15 01:14:28,919 - INFO - __main__ - Epoch  26, Step:   54800, Batch Loss:    12.224477, Lr: 0.000078, Tokens per sec:   2727
2023-03-15 01:14:48,388 - INFO - __main__ - Epoch  26, Step:   54900, Batch Loss:    12.790658, Lr: 0.000078, Tokens per sec:   2760
2023-03-15 01:15:07,902 - INFO - __main__ - Epoch  26, Step:   55000, Batch Loss:    13.282035, Lr: 0.000078, Tokens per sec:   2735
2023-03-15 01:15:28,051 - INFO - __main__ - Epoch  26, Step:   55100, Batch Loss:     8.401335, Lr: 0.000078, Tokens per sec:   2647
2023-03-15 01:15:48,204 - INFO - __main__ - Epoch  26, Step:   55200, Batch Loss:    13.854874, Lr: 0.000078, Tokens per sec:   2662
2023-03-15 01:16:08,476 - INFO - __main__ - Epoch  26, Step:   55300, Batch Loss:    12.230087, Lr: 0.000078, Tokens per sec:   2657
2023-03-15 01:16:28,598 - INFO - __main__ - Epoch  26, Step:   55400, Batch Loss:    11.365277, Lr: 0.000078, Tokens per sec:   2654
2023-03-15 01:16:48,688 - INFO - __main__ - Epoch  26, Step:   55500, Batch Loss:    11.430148, Lr: 0.000078, Tokens per sec:   2709
2023-03-15 01:17:08,572 - INFO - __main__ - Epoch  26, Step:   55600, Batch Loss:    10.779041, Lr: 0.000078, Tokens per sec:   2702
2023-03-15 01:17:28,708 - INFO - __main__ - Epoch  26, Step:   55700, Batch Loss:     9.901884, Lr: 0.000078, Tokens per sec:   2682
2023-03-15 01:17:48,773 - INFO - __main__ - Epoch  26, Step:   55800, Batch Loss:    12.511455, Lr: 0.000078, Tokens per sec:   2676
2023-03-15 01:18:08,783 - INFO - __main__ - Epoch  26, Step:   55900, Batch Loss:    16.437689, Lr: 0.000078, Tokens per sec:   2681
2023-03-15 01:18:29,005 - INFO - __main__ - Epoch  26, Step:   56000, Batch Loss:    17.511938, Lr: 0.000078, Tokens per sec:   2638
2023-03-15 01:18:49,037 - INFO - __main__ - Epoch  26, Step:   56100, Batch Loss:    10.750048, Lr: 0.000078, Tokens per sec:   2720
2023-03-15 01:19:09,220 - INFO - __main__ - Epoch  26, Step:   56200, Batch Loss:    14.010376, Lr: 0.000078, Tokens per sec:   2699
2023-03-15 01:19:28,828 - INFO - __main__ - Epoch  26, Step:   56300, Batch Loss:    11.876445, Lr: 0.000078, Tokens per sec:   2758
2023-03-15 01:19:49,004 - INFO - __main__ - Epoch  26, Step:   56400, Batch Loss:    18.877602, Lr: 0.000078, Tokens per sec:   2678
2023-03-15 01:20:09,257 - INFO - __main__ - Epoch  26, Step:   56500, Batch Loss:    10.332678, Lr: 0.000078, Tokens per sec:   2648
2023-03-15 01:20:29,569 - INFO - __main__ - Epoch  26, Step:   56600, Batch Loss:    12.360248, Lr: 0.000078, Tokens per sec:   2659
2023-03-15 01:20:40,527 - INFO - __main__ - Epoch  26: total training loss 27291.73
2023-03-15 01:20:40,528 - INFO - __main__ - Epoch 27
2023-03-15 01:20:50,141 - INFO - __main__ - Epoch  27, Step:   56700, Batch Loss:    12.507766, Lr: 0.000077, Tokens per sec:   2610
2023-03-15 01:21:09,874 - INFO - __main__ - Epoch  27, Step:   56800, Batch Loss:    11.086518, Lr: 0.000077, Tokens per sec:   2738
2023-03-15 01:21:30,087 - INFO - __main__ - Epoch  27, Step:   56900, Batch Loss:    10.536671, Lr: 0.000077, Tokens per sec:   2636
2023-03-15 01:21:50,172 - INFO - __main__ - Epoch  27, Step:   57000, Batch Loss:    13.747036, Lr: 0.000077, Tokens per sec:   2674
2023-03-15 01:22:10,408 - INFO - __main__ - Epoch  27, Step:   57100, Batch Loss:     9.453385, Lr: 0.000077, Tokens per sec:   2681
2023-03-15 01:22:30,572 - INFO - __main__ - Epoch  27, Step:   57200, Batch Loss:    14.646438, Lr: 0.000077, Tokens per sec:   2646
2023-03-15 01:22:50,808 - INFO - __main__ - Epoch  27, Step:   57300, Batch Loss:    15.881605, Lr: 0.000077, Tokens per sec:   2681
2023-03-15 01:23:11,102 - INFO - __main__ - Epoch  27, Step:   57400, Batch Loss:    16.199665, Lr: 0.000077, Tokens per sec:   2722
2023-03-15 01:23:31,151 - INFO - __main__ - Epoch  27, Step:   57500, Batch Loss:    10.071372, Lr: 0.000077, Tokens per sec:   2654
2023-03-15 01:23:51,043 - INFO - __main__ - Epoch  27, Step:   57600, Batch Loss:     7.934856, Lr: 0.000077, Tokens per sec:   2729
2023-03-15 01:24:11,223 - INFO - __main__ - Epoch  27, Step:   57700, Batch Loss:    12.639211, Lr: 0.000077, Tokens per sec:   2735
2023-03-15 01:24:31,066 - INFO - __main__ - Epoch  27, Step:   57800, Batch Loss:    10.026258, Lr: 0.000077, Tokens per sec:   2697
2023-03-15 01:24:51,321 - INFO - __main__ - Epoch  27, Step:   57900, Batch Loss:     9.200739, Lr: 0.000077, Tokens per sec:   2655
2023-03-15 01:25:11,504 - INFO - __main__ - Epoch  27, Step:   58000, Batch Loss:    10.522226, Lr: 0.000077, Tokens per sec:   2666
2023-03-15 01:25:31,770 - INFO - __main__ - Epoch  27, Step:   58100, Batch Loss:    14.882156, Lr: 0.000077, Tokens per sec:   2655
2023-03-15 01:25:51,985 - INFO - __main__ - Epoch  27, Step:   58200, Batch Loss:    11.025837, Lr: 0.000077, Tokens per sec:   2650
2023-03-15 01:26:12,050 - INFO - __main__ - Epoch  27, Step:   58300, Batch Loss:    14.569924, Lr: 0.000077, Tokens per sec:   2713
2023-03-15 01:26:31,912 - INFO - __main__ - Epoch  27, Step:   58400, Batch Loss:    10.391720, Lr: 0.000077, Tokens per sec:   2695
2023-03-15 01:26:52,004 - INFO - __main__ - Epoch  27, Step:   58500, Batch Loss:    10.637926, Lr: 0.000077, Tokens per sec:   2624
2023-03-15 01:27:12,060 - INFO - __main__ - Epoch  27, Step:   58600, Batch Loss:     9.856885, Lr: 0.000077, Tokens per sec:   2655
2023-03-15 01:27:32,109 - INFO - __main__ - Epoch  27, Step:   58700, Batch Loss:    15.563687, Lr: 0.000077, Tokens per sec:   2656
2023-03-15 01:27:51,994 - INFO - __main__ - Epoch  27, Step:   58800, Batch Loss:    12.424381, Lr: 0.000077, Tokens per sec:   2697
2023-03-15 01:27:58,522 - INFO - __main__ - Epoch  27: total training loss 25788.05
2023-03-15 01:27:58,523 - INFO - __main__ - Epoch 28
2023-03-15 01:28:12,394 - INFO - __main__ - Epoch  28, Step:   58900, Batch Loss:    13.748144, Lr: 0.000076, Tokens per sec:   2578
2023-03-15 01:28:32,351 - INFO - __main__ - Epoch  28, Step:   59000, Batch Loss:     8.883279, Lr: 0.000076, Tokens per sec:   2717
2023-03-15 01:28:52,455 - INFO - __main__ - Epoch  28, Step:   59100, Batch Loss:    13.709866, Lr: 0.000076, Tokens per sec:   2678
2023-03-15 01:29:12,605 - INFO - __main__ - Epoch  28, Step:   59200, Batch Loss:    12.844093, Lr: 0.000076, Tokens per sec:   2705
2023-03-15 01:29:32,673 - INFO - __main__ - Epoch  28, Step:   59300, Batch Loss:     8.982834, Lr: 0.000076, Tokens per sec:   2681
2023-03-15 01:29:52,767 - INFO - __main__ - Epoch  28, Step:   59400, Batch Loss:    11.528252, Lr: 0.000076, Tokens per sec:   2676
2023-03-15 01:30:12,999 - INFO - __main__ - Epoch  28, Step:   59500, Batch Loss:     8.838115, Lr: 0.000076, Tokens per sec:   2670
2023-03-15 01:30:33,136 - INFO - __main__ - Epoch  28, Step:   59600, Batch Loss:    11.893293, Lr: 0.000076, Tokens per sec:   2708
2023-03-15 01:30:52,858 - INFO - __main__ - Epoch  28, Step:   59700, Batch Loss:    11.941249, Lr: 0.000076, Tokens per sec:   2720
2023-03-15 01:31:13,020 - INFO - __main__ - Epoch  28, Step:   59800, Batch Loss:     8.533624, Lr: 0.000076, Tokens per sec:   2671
2023-03-15 01:31:33,159 - INFO - __main__ - Epoch  28, Step:   59900, Batch Loss:    13.132080, Lr: 0.000076, Tokens per sec:   2608
2023-03-15 01:31:53,404 - INFO - __main__ - Epoch  28, Step:   60000, Batch Loss:    11.105353, Lr: 0.000076, Tokens per sec:   2672
2023-03-15 01:32:13,296 - INFO - __main__ - Epoch  28, Step:   60100, Batch Loss:    13.483959, Lr: 0.000076, Tokens per sec:   2753
2023-03-15 01:32:33,249 - INFO - __main__ - Epoch  28, Step:   60200, Batch Loss:    13.355101, Lr: 0.000076, Tokens per sec:   2709
2023-03-15 01:32:53,479 - INFO - __main__ - Epoch  28, Step:   60300, Batch Loss:    11.811651, Lr: 0.000076, Tokens per sec:   2586
2023-03-15 01:33:13,663 - INFO - __main__ - Epoch  28, Step:   60400, Batch Loss:    11.068357, Lr: 0.000076, Tokens per sec:   2681
2023-03-15 01:33:33,630 - INFO - __main__ - Epoch  28, Step:   60500, Batch Loss:    12.169646, Lr: 0.000076, Tokens per sec:   2682
2023-03-15 01:33:53,444 - INFO - __main__ - Epoch  28, Step:   60600, Batch Loss:    10.811091, Lr: 0.000076, Tokens per sec:   2733
2023-03-15 01:34:13,502 - INFO - __main__ - Epoch  28, Step:   60700, Batch Loss:    12.522743, Lr: 0.000076, Tokens per sec:   2692
2023-03-15 01:34:33,424 - INFO - __main__ - Epoch  28, Step:   60800, Batch Loss:    14.958552, Lr: 0.000076, Tokens per sec:   2738
2023-03-15 01:34:53,560 - INFO - __main__ - Epoch  28, Step:   60900, Batch Loss:    10.764935, Lr: 0.000076, Tokens per sec:   2654
2023-03-15 01:35:13,310 - INFO - __main__ - Epoch  28, Step:   61000, Batch Loss:    14.046494, Lr: 0.000076, Tokens per sec:   2704
2023-03-15 01:35:15,743 - INFO - __main__ - Epoch  28: total training loss 24386.00
2023-03-15 01:35:15,744 - INFO - __main__ - Epoch 29
2023-03-15 01:35:33,838 - INFO - __main__ - Epoch  29, Step:   61100, Batch Loss:     7.894293, Lr: 0.000075, Tokens per sec:   2615
2023-03-15 01:35:53,848 - INFO - __main__ - Epoch  29, Step:   61200, Batch Loss:    10.487643, Lr: 0.000075, Tokens per sec:   2712
2023-03-15 01:36:13,853 - INFO - __main__ - Epoch  29, Step:   61300, Batch Loss:    10.414018, Lr: 0.000075, Tokens per sec:   2623
2023-03-15 01:36:33,682 - INFO - __main__ - Epoch  29, Step:   61400, Batch Loss:    13.236313, Lr: 0.000075, Tokens per sec:   2707
2023-03-15 01:36:53,938 - INFO - __main__ - Epoch  29, Step:   61500, Batch Loss:     7.442798, Lr: 0.000075, Tokens per sec:   2702
2023-03-15 01:37:14,149 - INFO - __main__ - Epoch  29, Step:   61600, Batch Loss:    12.870531, Lr: 0.000075, Tokens per sec:   2704
2023-03-15 01:37:34,113 - INFO - __main__ - Epoch  29, Step:   61700, Batch Loss:    11.000113, Lr: 0.000075, Tokens per sec:   2721
2023-03-15 01:37:54,219 - INFO - __main__ - Epoch  29, Step:   61800, Batch Loss:    11.508977, Lr: 0.000075, Tokens per sec:   2710
2023-03-15 01:38:14,511 - INFO - __main__ - Epoch  29, Step:   61900, Batch Loss:    13.339085, Lr: 0.000075, Tokens per sec:   2694
2023-03-15 01:38:34,546 - INFO - __main__ - Epoch  29, Step:   62000, Batch Loss:    10.474899, Lr: 0.000075, Tokens per sec:   2632
2023-03-15 01:38:54,339 - INFO - __main__ - Epoch  29, Step:   62100, Batch Loss:    10.446774, Lr: 0.000075, Tokens per sec:   2682
2023-03-15 01:39:14,551 - INFO - __main__ - Epoch  29, Step:   62200, Batch Loss:     9.301349, Lr: 0.000075, Tokens per sec:   2688
2023-03-15 01:39:34,597 - INFO - __main__ - Epoch  29, Step:   62300, Batch Loss:     6.534027, Lr: 0.000075, Tokens per sec:   2671
2023-03-15 01:39:54,610 - INFO - __main__ - Epoch  29, Step:   62400, Batch Loss:    11.991225, Lr: 0.000075, Tokens per sec:   2708
2023-03-15 01:40:14,398 - INFO - __main__ - Epoch  29, Step:   62500, Batch Loss:    11.641911, Lr: 0.000075, Tokens per sec:   2715
2023-03-15 01:40:34,345 - INFO - __main__ - Epoch  29, Step:   62600, Batch Loss:    12.097052, Lr: 0.000075, Tokens per sec:   2682
2023-03-15 01:40:54,406 - INFO - __main__ - Epoch  29, Step:   62700, Batch Loss:    11.065001, Lr: 0.000075, Tokens per sec:   2693
2023-03-15 01:41:14,412 - INFO - __main__ - Epoch  29, Step:   62800, Batch Loss:    11.261075, Lr: 0.000075, Tokens per sec:   2654
2023-03-15 01:41:34,333 - INFO - __main__ - Epoch  29, Step:   62900, Batch Loss:    12.858699, Lr: 0.000075, Tokens per sec:   2688
2023-03-15 01:41:54,560 - INFO - __main__ - Epoch  29, Step:   63000, Batch Loss:    11.448021, Lr: 0.000075, Tokens per sec:   2638
2023-03-15 01:42:14,632 - INFO - __main__ - Epoch  29, Step:   63100, Batch Loss:    12.479527, Lr: 0.000075, Tokens per sec:   2656
2023-03-15 01:42:32,917 - INFO - __main__ - Epoch  29: total training loss 23086.08
2023-03-15 01:42:32,918 - INFO - __main__ - Epoch 30
2023-03-15 01:42:34,986 - INFO - __main__ - Epoch  30, Step:   63200, Batch Loss:     8.124805, Lr: 0.000075, Tokens per sec:   2167
2023-03-15 01:42:54,905 - INFO - __main__ - Epoch  30, Step:   63300, Batch Loss:     7.529731, Lr: 0.000075, Tokens per sec:   2751
2023-03-15 01:43:15,085 - INFO - __main__ - Epoch  30, Step:   63400, Batch Loss:     7.727286, Lr: 0.000075, Tokens per sec:   2653
2023-03-15 01:43:34,843 - INFO - __main__ - Epoch  30, Step:   63500, Batch Loss:    12.510201, Lr: 0.000075, Tokens per sec:   2722
2023-03-15 01:43:54,974 - INFO - __main__ - Epoch  30, Step:   63600, Batch Loss:     9.228363, Lr: 0.000075, Tokens per sec:   2674
2023-03-15 01:44:14,746 - INFO - __main__ - Epoch  30, Step:   63700, Batch Loss:    10.898745, Lr: 0.000075, Tokens per sec:   2755
2023-03-15 01:44:34,870 - INFO - __main__ - Epoch  30, Step:   63800, Batch Loss:    12.168377, Lr: 0.000075, Tokens per sec:   2711
2023-03-15 01:44:55,086 - INFO - __main__ - Epoch  30, Step:   63900, Batch Loss:     9.939847, Lr: 0.000075, Tokens per sec:   2665
2023-03-15 01:45:15,024 - INFO - __main__ - Epoch  30, Step:   64000, Batch Loss:     8.428020, Lr: 0.000075, Tokens per sec:   2700
2023-03-15 01:45:34,663 - INFO - __main__ - Epoch  30, Step:   64100, Batch Loss:     6.960480, Lr: 0.000075, Tokens per sec:   2763
2023-03-15 01:45:54,801 - INFO - __main__ - Epoch  30, Step:   64200, Batch Loss:    11.700871, Lr: 0.000075, Tokens per sec:   2664
2023-03-15 01:46:14,974 - INFO - __main__ - Epoch  30, Step:   64300, Batch Loss:    12.693665, Lr: 0.000075, Tokens per sec:   2627
2023-03-15 01:46:34,977 - INFO - __main__ - Epoch  30, Step:   64400, Batch Loss:    11.796802, Lr: 0.000075, Tokens per sec:   2677
2023-03-15 01:46:55,154 - INFO - __main__ - Epoch  30, Step:   64500, Batch Loss:    14.455347, Lr: 0.000075, Tokens per sec:   2680
2023-03-15 01:47:15,365 - INFO - __main__ - Epoch  30, Step:   64600, Batch Loss:     9.087412, Lr: 0.000075, Tokens per sec:   2659
2023-03-15 01:47:35,551 - INFO - __main__ - Epoch  30, Step:   64700, Batch Loss:    10.379188, Lr: 0.000075, Tokens per sec:   2600
2023-03-15 01:47:55,726 - INFO - __main__ - Epoch  30, Step:   64800, Batch Loss:     5.816170, Lr: 0.000075, Tokens per sec:   2662
2023-03-15 01:48:15,893 - INFO - __main__ - Epoch  30, Step:   64900, Batch Loss:    13.362186, Lr: 0.000075, Tokens per sec:   2727
2023-03-15 01:48:35,966 - INFO - __main__ - Epoch  30, Step:   65000, Batch Loss:    11.386728, Lr: 0.000075, Tokens per sec:   2658
2023-03-15 01:48:56,013 - INFO - __main__ - Epoch  30, Step:   65100, Batch Loss:     9.656037, Lr: 0.000075, Tokens per sec:   2693
2023-03-15 01:49:16,150 - INFO - __main__ - Epoch  30, Step:   65200, Batch Loss:    11.128330, Lr: 0.000075, Tokens per sec:   2636
2023-03-15 01:49:36,341 - INFO - __main__ - Epoch  30, Step:   65300, Batch Loss:    13.897692, Lr: 0.000075, Tokens per sec:   2701
2023-03-15 01:49:50,275 - INFO - __main__ - Epoch  30: total training loss 21838.26
2023-03-15 01:49:50,277 - INFO - __main__ - Epoch 31
2023-03-15 01:49:56,722 - INFO - __main__ - Epoch  31, Step:   65400, Batch Loss:    10.640017, Lr: 0.000074, Tokens per sec:   2493
2023-03-15 01:50:16,489 - INFO - __main__ - Epoch  31, Step:   65500, Batch Loss:     6.735075, Lr: 0.000074, Tokens per sec:   2729
2023-03-15 01:50:36,607 - INFO - __main__ - Epoch  31, Step:   65600, Batch Loss:     9.154962, Lr: 0.000074, Tokens per sec:   2684
2023-03-15 01:50:56,732 - INFO - __main__ - Epoch  31, Step:   65700, Batch Loss:     7.619112, Lr: 0.000074, Tokens per sec:   2680
2023-03-15 01:51:16,616 - INFO - __main__ - Epoch  31, Step:   65800, Batch Loss:     7.793766, Lr: 0.000074, Tokens per sec:   2663
2023-03-15 01:51:36,681 - INFO - __main__ - Epoch  31, Step:   65900, Batch Loss:     7.490688, Lr: 0.000074, Tokens per sec:   2655
2023-03-15 01:51:56,789 - INFO - __main__ - Epoch  31, Step:   66000, Batch Loss:     9.940174, Lr: 0.000074, Tokens per sec:   2656
2023-03-15 01:52:16,891 - INFO - __main__ - Epoch  31, Step:   66100, Batch Loss:    10.363597, Lr: 0.000074, Tokens per sec:   2686
2023-03-15 01:52:37,001 - INFO - __main__ - Epoch  31, Step:   66200, Batch Loss:     6.129655, Lr: 0.000074, Tokens per sec:   2687
2023-03-15 01:52:57,184 - INFO - __main__ - Epoch  31, Step:   66300, Batch Loss:    10.281592, Lr: 0.000074, Tokens per sec:   2618
2023-03-15 01:53:17,357 - INFO - __main__ - Epoch  31, Step:   66400, Batch Loss:    10.975616, Lr: 0.000074, Tokens per sec:   2723
2023-03-15 01:53:37,685 - INFO - __main__ - Epoch  31, Step:   66500, Batch Loss:     7.331362, Lr: 0.000074, Tokens per sec:   2662
2023-03-15 01:53:57,861 - INFO - __main__ - Epoch  31, Step:   66600, Batch Loss:    11.593138, Lr: 0.000074, Tokens per sec:   2646
2023-03-15 01:54:18,027 - INFO - __main__ - Epoch  31, Step:   66700, Batch Loss:     8.163512, Lr: 0.000074, Tokens per sec:   2662
2023-03-15 01:54:38,076 - INFO - __main__ - Epoch  31, Step:   66800, Batch Loss:     7.909204, Lr: 0.000074, Tokens per sec:   2693
2023-03-15 01:54:57,965 - INFO - __main__ - Epoch  31, Step:   66900, Batch Loss:     9.227271, Lr: 0.000074, Tokens per sec:   2700
2023-03-15 01:55:18,127 - INFO - __main__ - Epoch  31, Step:   67000, Batch Loss:    10.602730, Lr: 0.000074, Tokens per sec:   2737
2023-03-15 01:55:38,083 - INFO - __main__ - Epoch  31, Step:   67100, Batch Loss:    10.367771, Lr: 0.000074, Tokens per sec:   2700
2023-03-15 01:55:58,267 - INFO - __main__ - Epoch  31, Step:   67200, Batch Loss:    10.282998, Lr: 0.000074, Tokens per sec:   2654
2023-03-15 01:56:17,909 - INFO - __main__ - Epoch  31, Step:   67300, Batch Loss:    14.729430, Lr: 0.000074, Tokens per sec:   2761
2023-03-15 01:56:38,117 - INFO - __main__ - Epoch  31, Step:   67400, Batch Loss:     8.526731, Lr: 0.000074, Tokens per sec:   2651
2023-03-15 01:56:58,226 - INFO - __main__ - Epoch  31, Step:   67500, Batch Loss:     7.563526, Lr: 0.000074, Tokens per sec:   2654
2023-03-15 01:57:08,202 - INFO - __main__ - Epoch  31: total training loss 20716.01
2023-03-15 01:57:08,203 - INFO - __main__ - Epoch 32
2023-03-15 01:57:18,819 - INFO - __main__ - Epoch  32, Step:   67600, Batch Loss:     6.841953, Lr: 0.000073, Tokens per sec:   2569
2023-03-15 01:57:38,919 - INFO - __main__ - Epoch  32, Step:   67700, Batch Loss:     7.958895, Lr: 0.000073, Tokens per sec:   2702
2023-03-15 01:57:58,900 - INFO - __main__ - Epoch  32, Step:   67800, Batch Loss:     7.520569, Lr: 0.000073, Tokens per sec:   2725
2023-03-15 01:58:19,156 - INFO - __main__ - Epoch  32, Step:   67900, Batch Loss:     8.504411, Lr: 0.000073, Tokens per sec:   2661
2023-03-15 01:58:38,989 - INFO - __main__ - Epoch  32, Step:   68000, Batch Loss:     6.697134, Lr: 0.000073, Tokens per sec:   2634
2023-03-15 01:58:59,289 - INFO - __main__ - Epoch  32, Step:   68100, Batch Loss:     8.282273, Lr: 0.000073, Tokens per sec:   2673
2023-03-15 01:59:19,356 - INFO - __main__ - Epoch  32, Step:   68200, Batch Loss:     8.180067, Lr: 0.000073, Tokens per sec:   2682
2023-03-15 01:59:39,120 - INFO - __main__ - Epoch  32, Step:   68300, Batch Loss:    11.816940, Lr: 0.000073, Tokens per sec:   2698
2023-03-15 01:59:59,294 - INFO - __main__ - Epoch  32, Step:   68400, Batch Loss:     8.418033, Lr: 0.000073, Tokens per sec:   2649
2023-03-15 02:00:19,513 - INFO - __main__ - Epoch  32, Step:   68500, Batch Loss:     8.959300, Lr: 0.000073, Tokens per sec:   2686
2023-03-15 02:00:39,656 - INFO - __main__ - Epoch  32, Step:   68600, Batch Loss:     8.857428, Lr: 0.000073, Tokens per sec:   2687
2023-03-15 02:00:59,863 - INFO - __main__ - Epoch  32, Step:   68700, Batch Loss:    11.913258, Lr: 0.000073, Tokens per sec:   2599
2023-03-15 02:01:20,037 - INFO - __main__ - Epoch  32, Step:   68800, Batch Loss:    11.035921, Lr: 0.000073, Tokens per sec:   2700
2023-03-15 02:01:40,214 - INFO - __main__ - Epoch  32, Step:   68900, Batch Loss:     8.469583, Lr: 0.000073, Tokens per sec:   2681
2023-03-15 02:02:00,474 - INFO - __main__ - Epoch  32, Step:   69000, Batch Loss:    10.735622, Lr: 0.000073, Tokens per sec:   2661
2023-03-15 02:02:20,596 - INFO - __main__ - Epoch  32, Step:   69100, Batch Loss:     9.088383, Lr: 0.000073, Tokens per sec:   2675
2023-03-15 02:02:40,128 - INFO - __main__ - Epoch  32, Step:   69200, Batch Loss:     7.315936, Lr: 0.000073, Tokens per sec:   2822
2023-03-15 02:03:00,122 - INFO - __main__ - Epoch  32, Step:   69300, Batch Loss:    11.105585, Lr: 0.000073, Tokens per sec:   2701
2023-03-15 02:03:20,211 - INFO - __main__ - Epoch  32, Step:   69400, Batch Loss:     8.356616, Lr: 0.000073, Tokens per sec:   2686
2023-03-15 02:03:40,337 - INFO - __main__ - Epoch  32, Step:   69500, Batch Loss:     6.244795, Lr: 0.000073, Tokens per sec:   2675
2023-03-15 02:04:00,002 - INFO - __main__ - Epoch  32, Step:   69600, Batch Loss:    10.200928, Lr: 0.000073, Tokens per sec:   2762
2023-03-15 02:04:20,028 - INFO - __main__ - Epoch  32, Step:   69700, Batch Loss:     8.384077, Lr: 0.000073, Tokens per sec:   2612
2023-03-15 02:04:25,768 - INFO - __main__ - Epoch  32: total training loss 19720.25
2023-03-15 02:04:25,769 - INFO - __main__ - Epoch 33
2023-03-15 02:04:40,647 - INFO - __main__ - Epoch  33, Step:   69800, Batch Loss:     7.302055, Lr: 0.000072, Tokens per sec:   2620
2023-03-15 02:05:00,867 - INFO - __main__ - Epoch  33, Step:   69900, Batch Loss:     8.541676, Lr: 0.000072, Tokens per sec:   2646
2023-03-15 02:05:21,262 - INFO - __main__ - Epoch  33, Step:   70000, Batch Loss:     6.429674, Lr: 0.000072, Tokens per sec:   2654
2023-03-15 02:05:41,283 - INFO - __main__ - Epoch  33, Step:   70100, Batch Loss:     6.396585, Lr: 0.000072, Tokens per sec:   2626
2023-03-15 02:06:01,476 - INFO - __main__ - Epoch  33, Step:   70200, Batch Loss:     9.073312, Lr: 0.000072, Tokens per sec:   2696
2023-03-15 02:06:21,489 - INFO - __main__ - Epoch  33, Step:   70300, Batch Loss:     7.193463, Lr: 0.000072, Tokens per sec:   2683
2023-03-15 02:06:41,670 - INFO - __main__ - Epoch  33, Step:   70400, Batch Loss:     6.045494, Lr: 0.000072, Tokens per sec:   2657
2023-03-15 02:07:01,754 - INFO - __main__ - Epoch  33, Step:   70500, Batch Loss:     7.268191, Lr: 0.000072, Tokens per sec:   2671
2023-03-15 02:07:21,856 - INFO - __main__ - Epoch  33, Step:   70600, Batch Loss:     7.055671, Lr: 0.000072, Tokens per sec:   2720
2023-03-15 02:07:41,843 - INFO - __main__ - Epoch  33, Step:   70700, Batch Loss:    10.953270, Lr: 0.000072, Tokens per sec:   2686
2023-03-15 02:08:01,958 - INFO - __main__ - Epoch  33, Step:   70800, Batch Loss:    12.121325, Lr: 0.000072, Tokens per sec:   2661
2023-03-15 02:08:21,962 - INFO - __main__ - Epoch  33, Step:   70900, Batch Loss:    10.687447, Lr: 0.000072, Tokens per sec:   2671
2023-03-15 02:08:41,921 - INFO - __main__ - Epoch  33, Step:   71000, Batch Loss:     6.437364, Lr: 0.000072, Tokens per sec:   2671
2023-03-15 02:09:01,206 - INFO - __main__ - Epoch  33, Step:   71100, Batch Loss:     7.137245, Lr: 0.000072, Tokens per sec:   2771
2023-03-15 02:09:21,059 - INFO - __main__ - Epoch  33, Step:   71200, Batch Loss:     7.229190, Lr: 0.000072, Tokens per sec:   2700
2023-03-15 02:09:41,127 - INFO - __main__ - Epoch  33, Step:   71300, Batch Loss:     8.768392, Lr: 0.000072, Tokens per sec:   2700
2023-03-15 02:10:01,460 - INFO - __main__ - Epoch  33, Step:   71400, Batch Loss:     9.042936, Lr: 0.000072, Tokens per sec:   2631
2023-03-15 02:10:21,274 - INFO - __main__ - Epoch  33, Step:   71500, Batch Loss:    10.792610, Lr: 0.000072, Tokens per sec:   2728
2023-03-15 02:10:41,161 - INFO - __main__ - Epoch  33, Step:   71600, Batch Loss:     7.688093, Lr: 0.000072, Tokens per sec:   2739
2023-03-15 02:11:01,168 - INFO - __main__ - Epoch  33, Step:   71700, Batch Loss:    10.041963, Lr: 0.000072, Tokens per sec:   2690
2023-03-15 02:11:21,360 - INFO - __main__ - Epoch  33, Step:   71800, Batch Loss:    10.511731, Lr: 0.000072, Tokens per sec:   2761
2023-03-15 02:11:41,552 - INFO - __main__ - Epoch  33, Step:   71900, Batch Loss:     7.942400, Lr: 0.000072, Tokens per sec:   2650
2023-03-15 02:11:43,059 - INFO - __main__ - Epoch  33: total training loss 18692.85
2023-03-15 02:11:43,060 - INFO - __main__ - Epoch 34
2023-03-15 02:12:02,089 - INFO - __main__ - Epoch  34, Step:   72000, Batch Loss:    11.569242, Lr: 0.000072, Tokens per sec:   2622
2023-03-15 02:12:22,313 - INFO - __main__ - Epoch  34, Step:   72100, Batch Loss:     6.990130, Lr: 0.000072, Tokens per sec:   2668
2023-03-15 02:12:42,489 - INFO - __main__ - Epoch  34, Step:   72200, Batch Loss:     5.928802, Lr: 0.000072, Tokens per sec:   2672
2023-03-15 02:13:02,098 - INFO - __main__ - Epoch  34, Step:   72300, Batch Loss:     6.096198, Lr: 0.000072, Tokens per sec:   2752
2023-03-15 02:13:22,178 - INFO - __main__ - Epoch  34, Step:   72400, Batch Loss:    10.600403, Lr: 0.000072, Tokens per sec:   2708
2023-03-15 02:13:42,284 - INFO - __main__ - Epoch  34, Step:   72500, Batch Loss:     7.239406, Lr: 0.000072, Tokens per sec:   2610
2023-03-15 02:14:02,258 - INFO - __main__ - Epoch  34, Step:   72600, Batch Loss:     9.330314, Lr: 0.000072, Tokens per sec:   2774
2023-03-15 02:14:22,403 - INFO - __main__ - Epoch  34, Step:   72700, Batch Loss:     6.371952, Lr: 0.000072, Tokens per sec:   2615
2023-03-15 02:14:42,505 - INFO - __main__ - Epoch  34, Step:   72800, Batch Loss:     5.990392, Lr: 0.000072, Tokens per sec:   2677
2023-03-15 02:15:02,428 - INFO - __main__ - Epoch  34, Step:   72900, Batch Loss:     7.442381, Lr: 0.000072, Tokens per sec:   2676
2023-03-15 02:15:22,291 - INFO - __main__ - Epoch  34, Step:   73000, Batch Loss:     5.659297, Lr: 0.000072, Tokens per sec:   2695
2023-03-15 02:15:42,586 - INFO - __main__ - Epoch  34, Step:   73100, Batch Loss:     8.509432, Lr: 0.000072, Tokens per sec:   2634
2023-03-15 02:16:02,848 - INFO - __main__ - Epoch  34, Step:   73200, Batch Loss:     7.478819, Lr: 0.000072, Tokens per sec:   2681
2023-03-15 02:16:23,158 - INFO - __main__ - Epoch  34, Step:   73300, Batch Loss:     7.269070, Lr: 0.000072, Tokens per sec:   2692
2023-03-15 02:16:42,931 - INFO - __main__ - Epoch  34, Step:   73400, Batch Loss:     7.143378, Lr: 0.000072, Tokens per sec:   2713
2023-03-15 02:17:03,203 - INFO - __main__ - Epoch  34, Step:   73500, Batch Loss:     9.290320, Lr: 0.000072, Tokens per sec:   2706
2023-03-15 02:17:23,284 - INFO - __main__ - Epoch  34, Step:   73600, Batch Loss:     6.679868, Lr: 0.000072, Tokens per sec:   2636
2023-03-15 02:17:43,427 - INFO - __main__ - Epoch  34, Step:   73700, Batch Loss:     6.960049, Lr: 0.000072, Tokens per sec:   2678
2023-03-15 02:18:03,128 - INFO - __main__ - Epoch  34, Step:   73800, Batch Loss:     7.756524, Lr: 0.000072, Tokens per sec:   2732
2023-03-15 02:18:23,284 - INFO - __main__ - Epoch  34, Step:   73900, Batch Loss:    11.926702, Lr: 0.000072, Tokens per sec:   2673
2023-03-15 02:18:43,320 - INFO - __main__ - Epoch  34, Step:   74000, Batch Loss:    10.671447, Lr: 0.000072, Tokens per sec:   2701
2023-03-15 02:19:00,805 - INFO - __main__ - Epoch  34: total training loss 17801.89
2023-03-15 02:19:00,806 - INFO - __main__ - Epoch 35
2023-03-15 02:19:04,088 - INFO - __main__ - Epoch  35, Step:   74100, Batch Loss:     8.774226, Lr: 0.000071, Tokens per sec:   2338
2023-03-15 02:19:24,325 - INFO - __main__ - Epoch  35, Step:   74200, Batch Loss:     7.459119, Lr: 0.000071, Tokens per sec:   2707
2023-03-15 02:19:44,448 - INFO - __main__ - Epoch  35, Step:   74300, Batch Loss:     5.563937, Lr: 0.000071, Tokens per sec:   2690
2023-03-15 02:20:04,345 - INFO - __main__ - Epoch  35, Step:   74400, Batch Loss:     6.628905, Lr: 0.000071, Tokens per sec:   2658
2023-03-15 02:20:24,548 - INFO - __main__ - Epoch  35, Step:   74500, Batch Loss:     9.106837, Lr: 0.000071, Tokens per sec:   2685
2023-03-15 02:20:44,661 - INFO - __main__ - Epoch  35, Step:   74600, Batch Loss:     5.905849, Lr: 0.000071, Tokens per sec:   2691
2023-03-15 02:21:04,600 - INFO - __main__ - Epoch  35, Step:   74700, Batch Loss:     7.752311, Lr: 0.000071, Tokens per sec:   2678
2023-03-15 02:21:24,471 - INFO - __main__ - Epoch  35, Step:   74800, Batch Loss:     6.097422, Lr: 0.000071, Tokens per sec:   2685
2023-03-15 02:21:44,626 - INFO - __main__ - Epoch  35, Step:   74900, Batch Loss:     6.835781, Lr: 0.000071, Tokens per sec:   2713
2023-03-15 02:22:04,717 - INFO - __main__ - Epoch  35, Step:   75000, Batch Loss:     7.604886, Lr: 0.000071, Tokens per sec:   2669
2023-03-15 02:22:24,766 - INFO - __main__ - Epoch  35, Step:   75100, Batch Loss:     7.433135, Lr: 0.000071, Tokens per sec:   2714
2023-03-15 02:22:44,969 - INFO - __main__ - Epoch  35, Step:   75200, Batch Loss:     6.709877, Lr: 0.000071, Tokens per sec:   2651
2023-03-15 02:23:05,061 - INFO - __main__ - Epoch  35, Step:   75300, Batch Loss:     8.536179, Lr: 0.000071, Tokens per sec:   2688
2023-03-15 02:23:25,164 - INFO - __main__ - Epoch  35, Step:   75400, Batch Loss:     9.486135, Lr: 0.000071, Tokens per sec:   2689
2023-03-15 02:23:45,427 - INFO - __main__ - Epoch  35, Step:   75500, Batch Loss:     8.458046, Lr: 0.000071, Tokens per sec:   2651
2023-03-15 02:24:05,578 - INFO - __main__ - Epoch  35, Step:   75600, Batch Loss:    10.300318, Lr: 0.000071, Tokens per sec:   2684
2023-03-15 02:24:25,647 - INFO - __main__ - Epoch  35, Step:   75700, Batch Loss:    10.401212, Lr: 0.000071, Tokens per sec:   2688
2023-03-15 02:24:45,762 - INFO - __main__ - Epoch  35, Step:   75800, Batch Loss:    10.323639, Lr: 0.000071, Tokens per sec:   2657
2023-03-15 02:25:06,088 - INFO - __main__ - Epoch  35, Step:   75900, Batch Loss:    12.267388, Lr: 0.000071, Tokens per sec:   2611
2023-03-15 02:25:26,311 - INFO - __main__ - Epoch  35, Step:   76000, Batch Loss:     8.312465, Lr: 0.000071, Tokens per sec:   2658
2023-03-15 02:25:46,471 - INFO - __main__ - Epoch  35, Step:   76100, Batch Loss:     8.542218, Lr: 0.000071, Tokens per sec:   2651
2023-03-15 02:26:06,541 - INFO - __main__ - Epoch  35, Step:   76200, Batch Loss:     9.980687, Lr: 0.000071, Tokens per sec:   2660
2023-03-15 02:26:19,635 - INFO - __main__ - Epoch  35: total training loss 17006.51
2023-03-15 02:26:19,636 - INFO - __main__ - Epoch 36
2023-03-15 02:26:27,035 - INFO - __main__ - Epoch  36, Step:   76300, Batch Loss:     7.016505, Lr: 0.000070, Tokens per sec:   2647
2023-03-15 02:26:47,221 - INFO - __main__ - Epoch  36, Step:   76400, Batch Loss:     7.342273, Lr: 0.000070, Tokens per sec:   2652
2023-03-15 02:27:07,402 - INFO - __main__ - Epoch  36, Step:   76500, Batch Loss:     9.905078, Lr: 0.000070, Tokens per sec:   2708
2023-03-15 02:27:27,602 - INFO - __main__ - Epoch  36, Step:   76600, Batch Loss:     7.642121, Lr: 0.000070, Tokens per sec:   2692
2023-03-15 02:27:47,536 - INFO - __main__ - Epoch  36, Step:   76700, Batch Loss:     5.651489, Lr: 0.000070, Tokens per sec:   2681
2023-03-15 02:28:07,366 - INFO - __main__ - Epoch  36, Step:   76800, Batch Loss:     9.562751, Lr: 0.000070, Tokens per sec:   2751
2023-03-15 02:28:27,434 - INFO - __main__ - Epoch  36, Step:   76900, Batch Loss:     5.359700, Lr: 0.000070, Tokens per sec:   2655
2023-03-15 02:28:47,214 - INFO - __main__ - Epoch  36, Step:   77000, Batch Loss:     5.964620, Lr: 0.000070, Tokens per sec:   2759
2023-03-15 02:29:06,999 - INFO - __main__ - Epoch  36, Step:   77100, Batch Loss:     5.443376, Lr: 0.000070, Tokens per sec:   2737
2023-03-15 02:29:26,818 - INFO - __main__ - Epoch  36, Step:   77200, Batch Loss:     8.273881, Lr: 0.000070, Tokens per sec:   2679
2023-03-15 02:29:46,561 - INFO - __main__ - Epoch  36, Step:   77300, Batch Loss:     7.685239, Lr: 0.000070, Tokens per sec:   2728
2023-03-15 02:30:06,316 - INFO - __main__ - Epoch  36, Step:   77400, Batch Loss:    10.634237, Lr: 0.000070, Tokens per sec:   2720
2023-03-15 02:30:26,336 - INFO - __main__ - Epoch  36, Step:   77500, Batch Loss:     8.664875, Lr: 0.000070, Tokens per sec:   2639
2023-03-15 02:30:46,227 - INFO - __main__ - Epoch  36, Step:   77600, Batch Loss:     6.171454, Lr: 0.000070, Tokens per sec:   2707
2023-03-15 02:31:05,741 - INFO - __main__ - Epoch  36, Step:   77700, Batch Loss:    11.133018, Lr: 0.000070, Tokens per sec:   2737
2023-03-15 02:31:25,346 - INFO - __main__ - Epoch  36, Step:   77800, Batch Loss:     5.282672, Lr: 0.000070, Tokens per sec:   2740
2023-03-15 02:31:45,219 - INFO - __main__ - Epoch  36, Step:   77900, Batch Loss:     7.272833, Lr: 0.000070, Tokens per sec:   2704
2023-03-15 02:32:04,983 - INFO - __main__ - Epoch  36, Step:   78000, Batch Loss:     7.954423, Lr: 0.000070, Tokens per sec:   2725
2023-03-15 02:32:24,543 - INFO - __main__ - Epoch  36, Step:   78100, Batch Loss:     8.534050, Lr: 0.000070, Tokens per sec:   2764
2023-03-15 02:32:43,383 - INFO - __main__ - Epoch  36, Step:   78200, Batch Loss:     6.682991, Lr: 0.000070, Tokens per sec:   2825
2023-03-15 02:33:03,266 - INFO - __main__ - Epoch  36, Step:   78300, Batch Loss:     8.979951, Lr: 0.000070, Tokens per sec:   2719
2023-03-15 02:33:23,339 - INFO - __main__ - Epoch  36, Step:   78400, Batch Loss:     8.330857, Lr: 0.000070, Tokens per sec:   2685
2023-03-15 02:33:32,184 - INFO - __main__ - Epoch  36: total training loss 16222.96
2023-03-15 02:33:32,186 - INFO - __main__ - Epoch 37
2023-03-15 02:33:43,549 - INFO - __main__ - Epoch  37, Step:   78500, Batch Loss:     9.261913, Lr: 0.000070, Tokens per sec:   2696
2023-03-15 02:34:03,370 - INFO - __main__ - Epoch  37, Step:   78600, Batch Loss:     7.484154, Lr: 0.000070, Tokens per sec:   2671
2023-03-15 02:34:22,756 - INFO - __main__ - Epoch  37, Step:   78700, Batch Loss:     6.671059, Lr: 0.000070, Tokens per sec:   2759
2023-03-15 02:34:42,384 - INFO - __main__ - Epoch  37, Step:   78800, Batch Loss:     6.428343, Lr: 0.000070, Tokens per sec:   2743
2023-03-15 02:35:02,053 - INFO - __main__ - Epoch  37, Step:   78900, Batch Loss:     5.761544, Lr: 0.000070, Tokens per sec:   2719
2023-03-15 02:35:21,022 - INFO - __main__ - Epoch  37, Step:   79000, Batch Loss:     7.827535, Lr: 0.000070, Tokens per sec:   2866
2023-03-15 02:35:40,803 - INFO - __main__ - Epoch  37, Step:   79100, Batch Loss:     5.608765, Lr: 0.000070, Tokens per sec:   2677
2023-03-15 02:36:00,263 - INFO - __main__ - Epoch  37, Step:   79200, Batch Loss:     4.660430, Lr: 0.000070, Tokens per sec:   2765
2023-03-15 02:36:19,733 - INFO - __main__ - Epoch  37, Step:   79300, Batch Loss:     9.000445, Lr: 0.000070, Tokens per sec:   2765
2023-03-15 02:36:38,114 - INFO - __main__ - Epoch  37, Step:   79400, Batch Loss:     6.944699, Lr: 0.000070, Tokens per sec:   2949
2023-03-15 02:36:56,930 - INFO - __main__ - Epoch  37, Step:   79500, Batch Loss:     9.264166, Lr: 0.000070, Tokens per sec:   2821
2023-03-15 02:37:16,466 - INFO - __main__ - Epoch  37, Step:   79600, Batch Loss:     9.537465, Lr: 0.000070, Tokens per sec:   2747
2023-03-15 02:37:35,462 - INFO - __main__ - Epoch  37, Step:   79700, Batch Loss:     5.520785, Lr: 0.000070, Tokens per sec:   2850
2023-03-15 02:37:54,857 - INFO - __main__ - Epoch  37, Step:   79800, Batch Loss:     6.908143, Lr: 0.000070, Tokens per sec:   2774
2023-03-15 02:38:14,525 - INFO - __main__ - Epoch  37, Step:   79900, Batch Loss:     8.083329, Lr: 0.000070, Tokens per sec:   2713
2023-03-15 02:38:34,046 - INFO - __main__ - Epoch  37, Step:   80000, Batch Loss:     5.648771, Lr: 0.000070, Tokens per sec:   2771
2023-03-15 02:38:53,358 - INFO - __main__ - Epoch  37, Step:   80100, Batch Loss:     7.216590, Lr: 0.000070, Tokens per sec:   2846
2023-03-15 02:39:12,800 - INFO - __main__ - Epoch  37, Step:   80200, Batch Loss:     5.917430, Lr: 0.000070, Tokens per sec:   2797
2023-03-15 02:39:32,265 - INFO - __main__ - Epoch  37, Step:   80300, Batch Loss:     7.273132, Lr: 0.000070, Tokens per sec:   2809
2023-03-15 02:39:51,500 - INFO - __main__ - Epoch  37, Step:   80400, Batch Loss:     6.529400, Lr: 0.000070, Tokens per sec:   2772
2023-03-15 02:40:10,544 - INFO - __main__ - Epoch  37, Step:   80500, Batch Loss:     8.032377, Lr: 0.000070, Tokens per sec:   2852
2023-03-15 02:40:30,267 - INFO - __main__ - Epoch  37, Step:   80600, Batch Loss:     7.581821, Lr: 0.000070, Tokens per sec:   2741
2023-03-15 02:40:34,771 - INFO - __main__ - Epoch  37: total training loss 15605.25
2023-03-15 02:40:34,773 - INFO - __main__ - Epoch 38
2023-03-15 02:40:50,465 - INFO - __main__ - Epoch  38, Step:   80700, Batch Loss:     5.862012, Lr: 0.000069, Tokens per sec:   2638
2023-03-15 02:41:10,362 - INFO - __main__ - Epoch  38, Step:   80800, Batch Loss:     5.746113, Lr: 0.000069, Tokens per sec:   2692
2023-03-15 02:41:29,862 - INFO - __main__ - Epoch  38, Step:   80900, Batch Loss:     5.138771, Lr: 0.000069, Tokens per sec:   2765
2023-03-15 02:41:48,827 - INFO - __main__ - Epoch  38, Step:   81000, Batch Loss:     6.027940, Lr: 0.000069, Tokens per sec:   2822
2023-03-15 02:42:08,109 - INFO - __main__ - Epoch  38, Step:   81100, Batch Loss:     6.689293, Lr: 0.000069, Tokens per sec:   2782
2023-03-15 02:42:28,125 - INFO - __main__ - Epoch  38, Step:   81200, Batch Loss:     4.944590, Lr: 0.000069, Tokens per sec:   2712
2023-03-15 02:42:47,835 - INFO - __main__ - Epoch  38, Step:   81300, Batch Loss:     8.774641, Lr: 0.000069, Tokens per sec:   2728
2023-03-15 02:43:06,886 - INFO - __main__ - Epoch  38, Step:   81400, Batch Loss:     6.248463, Lr: 0.000069, Tokens per sec:   2862
2023-03-15 02:43:26,191 - INFO - __main__ - Epoch  38, Step:   81500, Batch Loss:     5.199249, Lr: 0.000069, Tokens per sec:   2801
2023-03-15 02:43:45,576 - INFO - __main__ - Epoch  38, Step:   81600, Batch Loss:     7.820332, Lr: 0.000069, Tokens per sec:   2788
2023-03-15 02:44:04,912 - INFO - __main__ - Epoch  38, Step:   81700, Batch Loss:     5.958229, Lr: 0.000069, Tokens per sec:   2738
2023-03-15 02:44:24,723 - INFO - __main__ - Epoch  38, Step:   81800, Batch Loss:     7.724631, Lr: 0.000069, Tokens per sec:   2723
2023-03-15 02:44:44,160 - INFO - __main__ - Epoch  38, Step:   81900, Batch Loss:     6.329170, Lr: 0.000069, Tokens per sec:   2735
2023-03-15 02:45:03,762 - INFO - __main__ - Epoch  38, Step:   82000, Batch Loss:     5.386222, Lr: 0.000069, Tokens per sec:   2708
2023-03-15 02:45:23,420 - INFO - __main__ - Epoch  38, Step:   82100, Batch Loss:     7.771857, Lr: 0.000069, Tokens per sec:   2745
2023-03-15 02:45:43,282 - INFO - __main__ - Epoch  38, Step:   82200, Batch Loss:     8.394107, Lr: 0.000069, Tokens per sec:   2719
2023-03-15 02:46:02,834 - INFO - __main__ - Epoch  38, Step:   82300, Batch Loss:     4.828790, Lr: 0.000069, Tokens per sec:   2789
2023-03-15 02:46:22,606 - INFO - __main__ - Epoch  38, Step:   82400, Batch Loss:     8.528813, Lr: 0.000069, Tokens per sec:   2735
2023-03-15 02:46:42,182 - INFO - __main__ - Epoch  38, Step:   82500, Batch Loss:     6.656679, Lr: 0.000069, Tokens per sec:   2768
2023-03-15 02:47:02,059 - INFO - __main__ - Epoch  38, Step:   82600, Batch Loss:     9.433825, Lr: 0.000069, Tokens per sec:   2669
2023-03-15 02:47:21,850 - INFO - __main__ - Epoch  38, Step:   82700, Batch Loss:     8.386561, Lr: 0.000069, Tokens per sec:   2723
2023-03-15 02:47:41,407 - INFO - __main__ - Epoch  38, Step:   82800, Batch Loss:     6.785324, Lr: 0.000069, Tokens per sec:   2782
2023-03-15 02:47:41,857 - INFO - __main__ - Epoch  38: total training loss 14856.83
2023-03-15 02:47:41,858 - INFO - __main__ - Epoch 39
2023-03-15 02:48:01,789 - INFO - __main__ - Epoch  39, Step:   82900, Batch Loss:     6.061774, Lr: 0.000068, Tokens per sec:   2612
2023-03-15 02:48:21,750 - INFO - __main__ - Epoch  39, Step:   83000, Batch Loss:     5.790831, Lr: 0.000068, Tokens per sec:   2686
2023-03-15 02:48:41,829 - INFO - __main__ - Epoch  39, Step:   83100, Batch Loss:     4.913874, Lr: 0.000068, Tokens per sec:   2719
2023-03-15 02:49:01,835 - INFO - __main__ - Epoch  39, Step:   83200, Batch Loss:     4.656396, Lr: 0.000068, Tokens per sec:   2686
2023-03-15 02:49:22,072 - INFO - __main__ - Epoch  39, Step:   83300, Batch Loss:     6.466692, Lr: 0.000068, Tokens per sec:   2696
2023-03-15 02:49:42,400 - INFO - __main__ - Epoch  39, Step:   83400, Batch Loss:     6.581024, Lr: 0.000068, Tokens per sec:   2684
2023-03-15 02:50:02,616 - INFO - __main__ - Epoch  39, Step:   83500, Batch Loss:     6.260139, Lr: 0.000068, Tokens per sec:   2631
2023-03-15 02:50:22,714 - INFO - __main__ - Epoch  39, Step:   83600, Batch Loss:     6.287818, Lr: 0.000068, Tokens per sec:   2687
2023-03-15 02:50:42,984 - INFO - __main__ - Epoch  39, Step:   83700, Batch Loss:     7.674989, Lr: 0.000068, Tokens per sec:   2655
2023-03-15 02:51:03,270 - INFO - __main__ - Epoch  39, Step:   83800, Batch Loss:     6.712175, Lr: 0.000068, Tokens per sec:   2655
2023-03-15 02:51:23,474 - INFO - __main__ - Epoch  39, Step:   83900, Batch Loss:     7.975582, Lr: 0.000068, Tokens per sec:   2693
2023-03-15 02:51:43,771 - INFO - __main__ - Epoch  39, Step:   84000, Batch Loss:     7.335318, Lr: 0.000068, Tokens per sec:   2676
2023-03-15 02:52:03,690 - INFO - __main__ - Epoch  39, Step:   84100, Batch Loss:     6.832204, Lr: 0.000068, Tokens per sec:   2703
2023-03-15 02:52:23,930 - INFO - __main__ - Epoch  39, Step:   84200, Batch Loss:     5.077740, Lr: 0.000068, Tokens per sec:   2647
2023-03-15 02:52:44,131 - INFO - __main__ - Epoch  39, Step:   84300, Batch Loss:     8.272733, Lr: 0.000068, Tokens per sec:   2619
2023-03-15 02:53:04,196 - INFO - __main__ - Epoch  39, Step:   84400, Batch Loss:    10.268240, Lr: 0.000068, Tokens per sec:   2695
2023-03-15 02:53:24,132 - INFO - __main__ - Epoch  39, Step:   84500, Batch Loss:     5.610312, Lr: 0.000068, Tokens per sec:   2686
2023-03-15 02:53:44,476 - INFO - __main__ - Epoch  39, Step:   84600, Batch Loss:     6.726038, Lr: 0.000068, Tokens per sec:   2619
2023-03-15 02:54:04,623 - INFO - __main__ - Epoch  39, Step:   84700, Batch Loss:     6.566070, Lr: 0.000068, Tokens per sec:   2690
2023-03-15 02:54:24,900 - INFO - __main__ - Epoch  39, Step:   84800, Batch Loss:     8.974977, Lr: 0.000068, Tokens per sec:   2686
2023-03-15 02:54:44,832 - INFO - __main__ - Epoch  39, Step:   84900, Batch Loss:     7.322415, Lr: 0.000068, Tokens per sec:   2663
2023-03-15 02:55:01,313 - INFO - __main__ - Epoch  39: total training loss 14249.17
2023-03-15 02:55:01,314 - INFO - __main__ - Epoch 40
2023-03-15 02:55:05,286 - INFO - __main__ - Epoch  40, Step:   85000, Batch Loss:     3.965932, Lr: 0.000068, Tokens per sec:   2533
2023-03-15 02:55:25,382 - INFO - __main__ - Epoch  40, Step:   85100, Batch Loss:     4.468069, Lr: 0.000068, Tokens per sec:   2649
2023-03-15 02:55:45,626 - INFO - __main__ - Epoch  40, Step:   85200, Batch Loss:     5.850861, Lr: 0.000068, Tokens per sec:   2644
2023-03-15 02:56:05,757 - INFO - __main__ - Epoch  40, Step:   85300, Batch Loss:     6.057292, Lr: 0.000068, Tokens per sec:   2657
2023-03-15 02:56:25,734 - INFO - __main__ - Epoch  40, Step:   85400, Batch Loss:     5.490174, Lr: 0.000068, Tokens per sec:   2690
2023-03-15 02:56:46,038 - INFO - __main__ - Epoch  40, Step:   85500, Batch Loss:     5.059676, Lr: 0.000068, Tokens per sec:   2640
2023-03-15 02:57:05,741 - INFO - __main__ - Epoch  40, Step:   85600, Batch Loss:     5.362611, Lr: 0.000068, Tokens per sec:   2716
2023-03-15 02:57:26,001 - INFO - __main__ - Epoch  40, Step:   85700, Batch Loss:     5.120919, Lr: 0.000068, Tokens per sec:   2675
2023-03-15 02:57:46,228 - INFO - __main__ - Epoch  40, Step:   85800, Batch Loss:    11.943884, Lr: 0.000068, Tokens per sec:   2697
2023-03-15 02:58:06,552 - INFO - __main__ - Epoch  40, Step:   85900, Batch Loss:     4.425363, Lr: 0.000068, Tokens per sec:   2632
2023-03-15 02:58:26,878 - INFO - __main__ - Epoch  40, Step:   86000, Batch Loss:     5.906193, Lr: 0.000068, Tokens per sec:   2656
2023-03-15 02:58:47,053 - INFO - __main__ - Epoch  40, Step:   86100, Batch Loss:     6.912659, Lr: 0.000068, Tokens per sec:   2714
2023-03-15 02:59:07,068 - INFO - __main__ - Epoch  40, Step:   86200, Batch Loss:     6.704725, Lr: 0.000068, Tokens per sec:   2691
2023-03-15 02:59:27,330 - INFO - __main__ - Epoch  40, Step:   86300, Batch Loss:     5.068424, Lr: 0.000068, Tokens per sec:   2672
2023-03-15 02:59:47,508 - INFO - __main__ - Epoch  40, Step:   86400, Batch Loss:     6.937997, Lr: 0.000068, Tokens per sec:   2641
2023-03-15 03:00:07,647 - INFO - __main__ - Epoch  40, Step:   86500, Batch Loss:     7.197714, Lr: 0.000068, Tokens per sec:   2669
2023-03-15 03:00:27,735 - INFO - __main__ - Epoch  40, Step:   86600, Batch Loss:     6.931461, Lr: 0.000068, Tokens per sec:   2659
2023-03-15 03:00:47,458 - INFO - __main__ - Epoch  40, Step:   86700, Batch Loss:     5.883137, Lr: 0.000068, Tokens per sec:   2718
2023-03-15 03:01:07,629 - INFO - __main__ - Epoch  40, Step:   86800, Batch Loss:     6.499477, Lr: 0.000068, Tokens per sec:   2736
2023-03-15 03:01:27,279 - INFO - __main__ - Epoch  40, Step:   86900, Batch Loss:     7.427826, Lr: 0.000068, Tokens per sec:   2757
2023-03-15 03:01:47,503 - INFO - __main__ - Epoch  40, Step:   87000, Batch Loss:     6.177774, Lr: 0.000068, Tokens per sec:   2640
2023-03-15 03:02:07,717 - INFO - __main__ - Epoch  40, Step:   87100, Batch Loss:     4.684545, Lr: 0.000068, Tokens per sec:   2633
2023-03-15 03:02:19,948 - INFO - __main__ - Epoch  40: total training loss 13670.71
2023-03-15 03:02:19,949 - INFO - __main__ - Epoch 41
2023-03-15 03:02:28,493 - INFO - __main__ - Epoch  41, Step:   87200, Batch Loss:     5.686204, Lr: 0.000067, Tokens per sec:   2500
2023-03-15 03:02:48,469 - INFO - __main__ - Epoch  41, Step:   87300, Batch Loss:     5.899484, Lr: 0.000067, Tokens per sec:   2706
2023-03-15 03:03:08,751 - INFO - __main__ - Epoch  41, Step:   87400, Batch Loss:     5.193087, Lr: 0.000067, Tokens per sec:   2677
2023-03-15 03:03:29,015 - INFO - __main__ - Epoch  41, Step:   87500, Batch Loss:     5.160457, Lr: 0.000067, Tokens per sec:   2634
2023-03-15 03:03:49,090 - INFO - __main__ - Epoch  41, Step:   87600, Batch Loss:     5.190673, Lr: 0.000067, Tokens per sec:   2727
2023-03-15 03:04:09,170 - INFO - __main__ - Epoch  41, Step:   87700, Batch Loss:     5.542514, Lr: 0.000067, Tokens per sec:   2636
2023-03-15 03:04:29,020 - INFO - __main__ - Epoch  41, Step:   87800, Batch Loss:     3.421614, Lr: 0.000067, Tokens per sec:   2763
2023-03-15 03:04:49,094 - INFO - __main__ - Epoch  41, Step:   87900, Batch Loss:     4.475152, Lr: 0.000067, Tokens per sec:   2681
2023-03-15 03:05:09,255 - INFO - __main__ - Epoch  41, Step:   88000, Batch Loss:     5.843138, Lr: 0.000067, Tokens per sec:   2664
2023-03-15 03:05:29,257 - INFO - __main__ - Epoch  41, Step:   88100, Batch Loss:     5.758341, Lr: 0.000067, Tokens per sec:   2651
2023-03-15 03:05:49,352 - INFO - __main__ - Epoch  41, Step:   88200, Batch Loss:     5.846291, Lr: 0.000067, Tokens per sec:   2674
2023-03-15 03:06:09,445 - INFO - __main__ - Epoch  41, Step:   88300, Batch Loss:     4.064715, Lr: 0.000067, Tokens per sec:   2624
2023-03-15 03:06:29,476 - INFO - __main__ - Epoch  41, Step:   88400, Batch Loss:     6.452142, Lr: 0.000067, Tokens per sec:   2704
2023-03-15 03:06:49,675 - INFO - __main__ - Epoch  41, Step:   88500, Batch Loss:     7.356174, Lr: 0.000067, Tokens per sec:   2643
2023-03-15 03:07:09,550 - INFO - __main__ - Epoch  41, Step:   88600, Batch Loss:     6.513301, Lr: 0.000067, Tokens per sec:   2705
2023-03-15 03:07:29,264 - INFO - __main__ - Epoch  41, Step:   88700, Batch Loss:     7.222054, Lr: 0.000067, Tokens per sec:   2753
2023-03-15 03:07:49,125 - INFO - __main__ - Epoch  41, Step:   88800, Batch Loss:     5.964975, Lr: 0.000067, Tokens per sec:   2685
2023-03-15 03:08:09,264 - INFO - __main__ - Epoch  41, Step:   88900, Batch Loss:     4.883979, Lr: 0.000067, Tokens per sec:   2727
2023-03-15 03:08:29,416 - INFO - __main__ - Epoch  41, Step:   89000, Batch Loss:     4.040223, Lr: 0.000067, Tokens per sec:   2670
2023-03-15 03:08:49,616 - INFO - __main__ - Epoch  41, Step:   89100, Batch Loss:     4.561285, Lr: 0.000067, Tokens per sec:   2700
2023-03-15 03:09:09,421 - INFO - __main__ - Epoch  41, Step:   89200, Batch Loss:     6.561116, Lr: 0.000067, Tokens per sec:   2710
2023-03-15 03:09:29,627 - INFO - __main__ - Epoch  41, Step:   89300, Batch Loss:     7.192366, Lr: 0.000067, Tokens per sec:   2658
2023-03-15 03:09:37,644 - INFO - __main__ - Epoch  41: total training loss 13127.03
2023-03-15 03:09:37,645 - INFO - __main__ - Epoch 42
2023-03-15 03:09:50,291 - INFO - __main__ - Epoch  42, Step:   89400, Batch Loss:     3.670002, Lr: 0.000066, Tokens per sec:   2554
2023-03-15 03:10:10,566 - INFO - __main__ - Epoch  42, Step:   89500, Batch Loss:     2.444534, Lr: 0.000066, Tokens per sec:   2629
2023-03-15 03:10:30,583 - INFO - __main__ - Epoch  42, Step:   89600, Batch Loss:     6.194628, Lr: 0.000066, Tokens per sec:   2658
2023-03-15 03:10:50,827 - INFO - __main__ - Epoch  42, Step:   89700, Batch Loss:     4.828398, Lr: 0.000066, Tokens per sec:   2662
2023-03-15 03:11:10,823 - INFO - __main__ - Epoch  42, Step:   89800, Batch Loss:     6.721214, Lr: 0.000066, Tokens per sec:   2682
2023-03-15 03:11:30,990 - INFO - __main__ - Epoch  42, Step:   89900, Batch Loss:     5.485660, Lr: 0.000066, Tokens per sec:   2715
2023-03-15 03:11:50,797 - INFO - __main__ - Epoch  42, Step:   90000, Batch Loss:     5.119078, Lr: 0.000066, Tokens per sec:   2747
2023-03-15 03:12:10,897 - INFO - __main__ - Epoch  42, Step:   90100, Batch Loss:     5.457413, Lr: 0.000066, Tokens per sec:   2672
2023-03-15 03:12:30,918 - INFO - __main__ - Epoch  42, Step:   90200, Batch Loss:     5.473412, Lr: 0.000066, Tokens per sec:   2646
2023-03-15 03:12:50,681 - INFO - __main__ - Epoch  42, Step:   90300, Batch Loss:     3.922507, Lr: 0.000066, Tokens per sec:   2786
2023-03-15 03:13:10,473 - INFO - __main__ - Epoch  42, Step:   90400, Batch Loss:     4.093857, Lr: 0.000066, Tokens per sec:   2684
2023-03-15 03:13:29,747 - INFO - __main__ - Epoch  42, Step:   90500, Batch Loss:     6.144612, Lr: 0.000066, Tokens per sec:   2789
2023-03-15 03:13:49,725 - INFO - __main__ - Epoch  42, Step:   90600, Batch Loss:     6.472996, Lr: 0.000066, Tokens per sec:   2670
2023-03-15 03:14:08,523 - INFO - __main__ - Epoch  42, Step:   90700, Batch Loss:     4.301430, Lr: 0.000066, Tokens per sec:   2866
2023-03-15 03:14:27,491 - INFO - __main__ - Epoch  42, Step:   90800, Batch Loss:     5.497525, Lr: 0.000066, Tokens per sec:   2846
2023-03-15 03:14:46,901 - INFO - __main__ - Epoch  42, Step:   90900, Batch Loss:     5.649996, Lr: 0.000066, Tokens per sec:   2762
2023-03-15 03:15:06,377 - INFO - __main__ - Epoch  42, Step:   91000, Batch Loss:     5.052887, Lr: 0.000066, Tokens per sec:   2755
2023-03-15 03:15:24,934 - INFO - __main__ - Epoch  42, Step:   91100, Batch Loss:     6.736064, Lr: 0.000066, Tokens per sec:   2947
2023-03-15 03:15:43,892 - INFO - __main__ - Epoch  42, Step:   91200, Batch Loss:     5.949858, Lr: 0.000066, Tokens per sec:   2825
2023-03-15 03:16:02,271 - INFO - __main__ - Epoch  42, Step:   91300, Batch Loss:     5.370564, Lr: 0.000066, Tokens per sec:   2903
2023-03-15 03:16:21,400 - INFO - __main__ - Epoch  42, Step:   91400, Batch Loss:     6.903347, Lr: 0.000066, Tokens per sec:   2824
2023-03-15 03:16:41,447 - INFO - __main__ - Epoch  42, Step:   91500, Batch Loss:     6.511597, Lr: 0.000066, Tokens per sec:   2739
2023-03-15 03:16:45,074 - INFO - __main__ - Epoch  42: total training loss 12637.97
2023-03-15 03:16:45,075 - INFO - __main__ - Epoch 43
2023-03-15 03:17:01,789 - INFO - __main__ - Epoch  43, Step:   91600, Batch Loss:     6.086617, Lr: 0.000066, Tokens per sec:   2650
2023-03-15 03:17:20,915 - INFO - __main__ - Epoch  43, Step:   91700, Batch Loss:     4.828642, Lr: 0.000066, Tokens per sec:   2807
2023-03-15 03:17:40,132 - INFO - __main__ - Epoch  43, Step:   91800, Batch Loss:     3.259260, Lr: 0.000066, Tokens per sec:   2770
2023-03-15 03:17:59,867 - INFO - __main__ - Epoch  43, Step:   91900, Batch Loss:     4.606310, Lr: 0.000066, Tokens per sec:   2740
2023-03-15 03:18:18,301 - INFO - __main__ - Epoch  43, Step:   92000, Batch Loss:     5.568692, Lr: 0.000066, Tokens per sec:   2975
2023-03-15 03:18:37,187 - INFO - __main__ - Epoch  43, Step:   92100, Batch Loss:     2.817269, Lr: 0.000066, Tokens per sec:   2871
2023-03-15 03:18:55,744 - INFO - __main__ - Epoch  43, Step:   92200, Batch Loss:     6.434837, Lr: 0.000066, Tokens per sec:   2912
2023-03-15 03:19:14,584 - INFO - __main__ - Epoch  43, Step:   92300, Batch Loss:     6.808936, Lr: 0.000066, Tokens per sec:   2841
2023-03-15 03:19:33,535 - INFO - __main__ - Epoch  43, Step:   92400, Batch Loss:     5.184729, Lr: 0.000066, Tokens per sec:   2822
2023-03-15 03:19:53,530 - INFO - __main__ - Epoch  43, Step:   92500, Batch Loss:     7.424826, Lr: 0.000066, Tokens per sec:   2724
2023-03-15 03:20:13,015 - INFO - __main__ - Epoch  43, Step:   92600, Batch Loss:     6.125330, Lr: 0.000066, Tokens per sec:   2747
2023-03-15 03:20:32,375 - INFO - __main__ - Epoch  43, Step:   92700, Batch Loss:     5.229467, Lr: 0.000066, Tokens per sec:   2772
2023-03-15 03:20:51,114 - INFO - __main__ - Epoch  43, Step:   92800, Batch Loss:     4.951940, Lr: 0.000066, Tokens per sec:   2873
2023-03-15 03:21:09,923 - INFO - __main__ - Epoch  43, Step:   92900, Batch Loss:     3.701694, Lr: 0.000066, Tokens per sec:   2914
2023-03-15 03:21:29,139 - INFO - __main__ - Epoch  43, Step:   93000, Batch Loss:     5.201331, Lr: 0.000066, Tokens per sec:   2795
2023-03-15 03:21:47,719 - INFO - __main__ - Epoch  43, Step:   93100, Batch Loss:     7.926407, Lr: 0.000066, Tokens per sec:   2899
2023-03-15 03:22:07,041 - INFO - __main__ - Epoch  43, Step:   93200, Batch Loss:     6.869068, Lr: 0.000066, Tokens per sec:   2762
2023-03-15 03:22:25,791 - INFO - __main__ - Epoch  43, Step:   93300, Batch Loss:     5.416198, Lr: 0.000066, Tokens per sec:   2842
2023-03-15 03:22:44,098 - INFO - __main__ - Epoch  43, Step:   93400, Batch Loss:     5.194765, Lr: 0.000066, Tokens per sec:   2900
2023-03-15 03:23:03,375 - INFO - __main__ - Epoch  43, Step:   93500, Batch Loss:     8.331737, Lr: 0.000066, Tokens per sec:   2812
2023-03-15 03:23:22,466 - INFO - __main__ - Epoch  43, Step:   93600, Batch Loss:     3.899587, Lr: 0.000066, Tokens per sec:   2746
2023-03-15 03:23:41,595 - INFO - __main__ - Epoch  43: total training loss 12153.67
2023-03-15 03:23:41,596 - INFO - __main__ - Epoch 44
2023-03-15 03:23:42,557 - INFO - __main__ - Epoch  44, Step:   93700, Batch Loss:     4.319179, Lr: 0.000065, Tokens per sec:   1531
2023-03-15 03:24:01,996 - INFO - __main__ - Epoch  44, Step:   93800, Batch Loss:     5.243881, Lr: 0.000065, Tokens per sec:   2793
2023-03-15 03:24:21,257 - INFO - __main__ - Epoch  44, Step:   93900, Batch Loss:     3.158253, Lr: 0.000065, Tokens per sec:   2819
2023-03-15 03:24:40,118 - INFO - __main__ - Epoch  44, Step:   94000, Batch Loss:     4.277739, Lr: 0.000065, Tokens per sec:   2797
2023-03-15 03:24:59,255 - INFO - __main__ - Epoch  44, Step:   94100, Batch Loss:     4.866568, Lr: 0.000065, Tokens per sec:   2780
2023-03-15 03:25:18,307 - INFO - __main__ - Epoch  44, Step:   94200, Batch Loss:     4.765553, Lr: 0.000065, Tokens per sec:   2779
2023-03-15 03:25:37,544 - INFO - __main__ - Epoch  44, Step:   94300, Batch Loss:     7.042113, Lr: 0.000065, Tokens per sec:   2812
2023-03-15 03:25:56,759 - INFO - __main__ - Epoch  44, Step:   94400, Batch Loss:     5.412701, Lr: 0.000065, Tokens per sec:   2778
2023-03-15 03:26:16,238 - INFO - __main__ - Epoch  44, Step:   94500, Batch Loss:     8.435601, Lr: 0.000065, Tokens per sec:   2761
2023-03-15 03:26:35,103 - INFO - __main__ - Epoch  44, Step:   94600, Batch Loss:     5.357738, Lr: 0.000065, Tokens per sec:   2849
2023-03-15 03:26:53,999 - INFO - __main__ - Epoch  44, Step:   94700, Batch Loss:     3.410123, Lr: 0.000065, Tokens per sec:   2856
2023-03-15 03:27:12,718 - INFO - __main__ - Epoch  44, Step:   94800, Batch Loss:     5.541241, Lr: 0.000065, Tokens per sec:   2900
2023-03-15 03:27:31,348 - INFO - __main__ - Epoch  44, Step:   94900, Batch Loss:     8.605819, Lr: 0.000065, Tokens per sec:   2948
2023-03-15 03:27:50,664 - INFO - __main__ - Epoch  44, Step:   95000, Batch Loss:     6.009403, Lr: 0.000065, Tokens per sec:   2816
2023-03-15 03:28:08,916 - INFO - __main__ - Epoch  44, Step:   95100, Batch Loss:     4.570803, Lr: 0.000065, Tokens per sec:   2993
2023-03-15 03:28:28,177 - INFO - __main__ - Epoch  44, Step:   95200, Batch Loss:     4.528017, Lr: 0.000065, Tokens per sec:   2796
2023-03-15 03:28:47,389 - INFO - __main__ - Epoch  44, Step:   95300, Batch Loss:     6.709329, Lr: 0.000065, Tokens per sec:   2815
2023-03-15 03:29:05,728 - INFO - __main__ - Epoch  44, Step:   95400, Batch Loss:     4.921557, Lr: 0.000065, Tokens per sec:   2939
2023-03-15 03:29:24,437 - INFO - __main__ - Epoch  44, Step:   95500, Batch Loss:     6.767951, Lr: 0.000065, Tokens per sec:   2840
2023-03-15 03:29:43,448 - INFO - __main__ - Epoch  44, Step:   95600, Batch Loss:     7.178701, Lr: 0.000065, Tokens per sec:   2800
2023-03-15 03:30:02,011 - INFO - __main__ - Epoch  44, Step:   95700, Batch Loss:     6.506001, Lr: 0.000065, Tokens per sec:   2909
2023-03-15 03:30:21,692 - INFO - __main__ - Epoch  44, Step:   95800, Batch Loss:     5.799917, Lr: 0.000065, Tokens per sec:   2693
2023-03-15 03:30:35,994 - INFO - __main__ - Epoch  44: total training loss 11729.77
2023-03-15 03:30:35,995 - INFO - __main__ - Epoch 45
2023-03-15 03:30:40,740 - INFO - __main__ - Epoch  45, Step:   95900, Batch Loss:     4.295371, Lr: 0.000064, Tokens per sec:   2646
2023-03-15 03:31:00,201 - INFO - __main__ - Epoch  45, Step:   96000, Batch Loss:     4.901032, Lr: 0.000064, Tokens per sec:   2750
2023-03-15 03:31:19,433 - INFO - __main__ - Epoch  45, Step:   96100, Batch Loss:     3.797547, Lr: 0.000064, Tokens per sec:   2783
2023-03-15 03:31:38,325 - INFO - __main__ - Epoch  45, Step:   96200, Batch Loss:     5.122077, Lr: 0.000064, Tokens per sec:   2867
2023-03-15 03:31:57,187 - INFO - __main__ - Epoch  45, Step:   96300, Batch Loss:     4.320078, Lr: 0.000064, Tokens per sec:   2838
2023-03-15 03:32:15,435 - INFO - __main__ - Epoch  45, Step:   96400, Batch Loss:     4.760274, Lr: 0.000064, Tokens per sec:   2961
2023-03-15 03:32:33,937 - INFO - __main__ - Epoch  45, Step:   96500, Batch Loss:     4.415219, Lr: 0.000064, Tokens per sec:   2868
2023-03-15 03:32:53,470 - INFO - __main__ - Epoch  45, Step:   96600, Batch Loss:     4.115539, Lr: 0.000064, Tokens per sec:   2748
2023-03-15 03:33:12,473 - INFO - __main__ - Epoch  45, Step:   96700, Batch Loss:     5.637581, Lr: 0.000064, Tokens per sec:   2835
2023-03-15 03:33:31,738 - INFO - __main__ - Epoch  45, Step:   96800, Batch Loss:     5.528923, Lr: 0.000064, Tokens per sec:   2828
2023-03-15 03:33:50,665 - INFO - __main__ - Epoch  45, Step:   96900, Batch Loss:     4.517896, Lr: 0.000064, Tokens per sec:   2875
2023-03-15 03:34:09,900 - INFO - __main__ - Epoch  45, Step:   97000, Batch Loss:     4.710728, Lr: 0.000064, Tokens per sec:   2753
2023-03-15 03:34:28,771 - INFO - __main__ - Epoch  45, Step:   97100, Batch Loss:     4.713979, Lr: 0.000064, Tokens per sec:   2845
2023-03-15 03:34:47,605 - INFO - __main__ - Epoch  45, Step:   97200, Batch Loss:     5.262248, Lr: 0.000064, Tokens per sec:   2929
2023-03-15 03:35:06,681 - INFO - __main__ - Epoch  45, Step:   97300, Batch Loss:     4.120724, Lr: 0.000064, Tokens per sec:   2818
2023-03-15 03:35:25,384 - INFO - __main__ - Epoch  45, Step:   97400, Batch Loss:     5.873076, Lr: 0.000064, Tokens per sec:   2876
2023-03-15 03:35:43,830 - INFO - __main__ - Epoch  45, Step:   97500, Batch Loss:     3.873016, Lr: 0.000064, Tokens per sec:   2895
2023-03-15 03:36:02,683 - INFO - __main__ - Epoch  45, Step:   97600, Batch Loss:     5.463449, Lr: 0.000064, Tokens per sec:   2817
2023-03-15 03:36:21,685 - INFO - __main__ - Epoch  45, Step:   97700, Batch Loss:     6.019048, Lr: 0.000064, Tokens per sec:   2830
2023-03-15 03:36:40,736 - INFO - __main__ - Epoch  45, Step:   97800, Batch Loss:     6.707635, Lr: 0.000064, Tokens per sec:   2811
2023-03-15 03:36:59,382 - INFO - __main__ - Epoch  45, Step:   97900, Batch Loss:     9.105098, Lr: 0.000064, Tokens per sec:   2959
2023-03-15 03:37:18,050 - INFO - __main__ - Epoch  45, Step:   98000, Batch Loss:     5.523400, Lr: 0.000064, Tokens per sec:   2919
2023-03-15 03:37:28,606 - INFO - __main__ - Epoch  45: total training loss 11313.15
2023-03-15 03:37:28,607 - INFO - __main__ - Epoch 46
2023-03-15 03:37:37,370 - INFO - __main__ - Epoch  46, Step:   98100, Batch Loss:     4.591633, Lr: 0.000064, Tokens per sec:   2756
2023-03-15 03:37:56,580 - INFO - __main__ - Epoch  46, Step:   98200, Batch Loss:     3.827626, Lr: 0.000064, Tokens per sec:   2810
2023-03-15 03:38:16,040 - INFO - __main__ - Epoch  46, Step:   98300, Batch Loss:     4.227488, Lr: 0.000064, Tokens per sec:   2784
2023-03-15 03:38:35,109 - INFO - __main__ - Epoch  46, Step:   98400, Batch Loss:     6.267611, Lr: 0.000064, Tokens per sec:   2821
2023-03-15 03:38:54,439 - INFO - __main__ - Epoch  46, Step:   98500, Batch Loss:     4.623670, Lr: 0.000064, Tokens per sec:   2813
2023-03-15 03:39:13,523 - INFO - __main__ - Epoch  46, Step:   98600, Batch Loss:     3.846712, Lr: 0.000064, Tokens per sec:   2812
2023-03-15 03:39:32,964 - INFO - __main__ - Epoch  46, Step:   98700, Batch Loss:     5.909456, Lr: 0.000064, Tokens per sec:   2774
2023-03-15 03:39:51,931 - INFO - __main__ - Epoch  46, Step:   98800, Batch Loss:     4.232938, Lr: 0.000064, Tokens per sec:   2885
2023-03-15 03:40:10,754 - INFO - __main__ - Epoch  46, Step:   98900, Batch Loss:     5.671534, Lr: 0.000064, Tokens per sec:   2804
2023-03-15 03:40:29,238 - INFO - __main__ - Epoch  46, Step:   99000, Batch Loss:     5.620559, Lr: 0.000064, Tokens per sec:   2879
2023-03-15 03:40:47,519 - INFO - __main__ - Epoch  46, Step:   99100, Batch Loss:     4.085855, Lr: 0.000064, Tokens per sec:   2944
2023-03-15 03:41:07,087 - INFO - __main__ - Epoch  46, Step:   99200, Batch Loss:     4.807502, Lr: 0.000064, Tokens per sec:   2814
2023-03-15 03:41:25,805 - INFO - __main__ - Epoch  46, Step:   99300, Batch Loss:     4.050212, Lr: 0.000064, Tokens per sec:   2842
2023-03-15 03:41:44,822 - INFO - __main__ - Epoch  46, Step:   99400, Batch Loss:     6.983733, Lr: 0.000064, Tokens per sec:   2862
2023-03-15 03:42:03,886 - INFO - __main__ - Epoch  46, Step:   99500, Batch Loss:     5.626998, Lr: 0.000064, Tokens per sec:   2806
2023-03-15 03:42:22,500 - INFO - __main__ - Epoch  46, Step:   99600, Batch Loss:     5.967957, Lr: 0.000064, Tokens per sec:   2898
2023-03-15 03:42:41,955 - INFO - __main__ - Epoch  46, Step:   99700, Batch Loss:     5.136725, Lr: 0.000064, Tokens per sec:   2787
2023-03-15 03:43:00,695 - INFO - __main__ - Epoch  46, Step:   99800, Batch Loss:     5.764469, Lr: 0.000064, Tokens per sec:   2861
2023-03-15 03:43:19,065 - INFO - __main__ - Epoch  46, Step:   99900, Batch Loss:     5.104737, Lr: 0.000064, Tokens per sec:   2924
2023-03-15 03:43:37,782 - INFO - __main__ - Epoch  46, Step:  100000, Batch Loss:     5.517552, Lr: 0.000064, Tokens per sec:   2865
2023-03-15 03:43:56,393 - INFO - __main__ - Epoch  46, Step:  100100, Batch Loss:     4.875542, Lr: 0.000064, Tokens per sec:   2837
2023-03-15 03:44:14,989 - INFO - __main__ - Epoch  46, Step:  100200, Batch Loss:     4.070583, Lr: 0.000064, Tokens per sec:   2925
2023-03-15 03:44:21,405 - INFO - __main__ - Epoch  46: total training loss 10922.36
2023-03-15 03:44:21,406 - INFO - __main__ - Epoch 47
2023-03-15 03:44:34,311 - INFO - __main__ - Epoch  47, Step:  100300, Batch Loss:     6.678133, Lr: 0.000063, Tokens per sec:   2731
2023-03-15 03:44:53,497 - INFO - __main__ - Epoch  47, Step:  100400, Batch Loss:     4.591937, Lr: 0.000063, Tokens per sec:   2795
2023-03-15 03:45:12,025 - INFO - __main__ - Epoch  47, Step:  100500, Batch Loss:     3.598257, Lr: 0.000063, Tokens per sec:   2938
2023-03-15 03:45:30,647 - INFO - __main__ - Epoch  47, Step:  100600, Batch Loss:     3.014520, Lr: 0.000063, Tokens per sec:   2900
2023-03-15 03:45:49,237 - INFO - __main__ - Epoch  47, Step:  100700, Batch Loss:     7.174499, Lr: 0.000063, Tokens per sec:   2903
2023-03-15 03:46:08,100 - INFO - __main__ - Epoch  47, Step:  100800, Batch Loss:     4.233743, Lr: 0.000063, Tokens per sec:   2867
2023-03-15 03:46:26,872 - INFO - __main__ - Epoch  47, Step:  100900, Batch Loss:     6.073276, Lr: 0.000063, Tokens per sec:   2875
2023-03-15 03:46:46,366 - INFO - __main__ - Epoch  47, Step:  101000, Batch Loss:     4.612222, Lr: 0.000063, Tokens per sec:   2719
2023-03-15 03:47:04,886 - INFO - __main__ - Epoch  47, Step:  101100, Batch Loss:     3.850795, Lr: 0.000063, Tokens per sec:   2920
2023-03-15 03:47:23,388 - INFO - __main__ - Epoch  47, Step:  101200, Batch Loss:     3.565937, Lr: 0.000063, Tokens per sec:   2953
2023-03-15 03:47:42,392 - INFO - __main__ - Epoch  47, Step:  101300, Batch Loss:     5.582129, Lr: 0.000063, Tokens per sec:   2794
2023-03-15 03:48:01,612 - INFO - __main__ - Epoch  47, Step:  101400, Batch Loss:     6.258807, Lr: 0.000063, Tokens per sec:   2809
2023-03-15 03:48:21,593 - INFO - __main__ - Epoch  47, Step:  101500, Batch Loss:     3.717439, Lr: 0.000063, Tokens per sec:   2708
2023-03-15 03:48:40,035 - INFO - __main__ - Epoch  47, Step:  101600, Batch Loss:     4.137847, Lr: 0.000063, Tokens per sec:   2907
2023-03-15 03:48:58,223 - INFO - __main__ - Epoch  47, Step:  101700, Batch Loss:     6.483263, Lr: 0.000063, Tokens per sec:   2913
2023-03-15 03:49:17,532 - INFO - __main__ - Epoch  47, Step:  101800, Batch Loss:     3.988601, Lr: 0.000063, Tokens per sec:   2812
2023-03-15 03:49:35,938 - INFO - __main__ - Epoch  47, Step:  101900, Batch Loss:     4.394159, Lr: 0.000063, Tokens per sec:   2883
2023-03-15 03:49:54,201 - INFO - __main__ - Epoch  47, Step:  102000, Batch Loss:     5.745452, Lr: 0.000063, Tokens per sec:   2926
2023-03-15 03:50:12,533 - INFO - __main__ - Epoch  47, Step:  102100, Batch Loss:     6.576386, Lr: 0.000063, Tokens per sec:   2973
2023-03-15 03:50:32,176 - INFO - __main__ - Epoch  47, Step:  102200, Batch Loss:     5.695084, Lr: 0.000063, Tokens per sec:   2703
2023-03-15 03:50:50,892 - INFO - __main__ - Epoch  47, Step:  102300, Batch Loss:     3.409366, Lr: 0.000063, Tokens per sec:   2887
2023-03-15 03:51:10,217 - INFO - __main__ - Epoch  47, Step:  102400, Batch Loss:     5.790680, Lr: 0.000063, Tokens per sec:   2824
2023-03-15 03:51:12,692 - INFO - __main__ - Epoch  47: total training loss 10583.17
2023-03-15 03:51:12,693 - INFO - __main__ - Epoch 48
2023-03-15 03:51:29,404 - INFO - __main__ - Epoch  48, Step:  102500, Batch Loss:     4.652509, Lr: 0.000062, Tokens per sec:   2789
2023-03-15 03:51:48,305 - INFO - __main__ - Epoch  48, Step:  102600, Batch Loss:     5.570108, Lr: 0.000062, Tokens per sec:   2852
2023-03-15 03:52:07,079 - INFO - __main__ - Epoch  48, Step:  102700, Batch Loss:     3.284150, Lr: 0.000062, Tokens per sec:   2867
2023-03-15 03:52:25,437 - INFO - __main__ - Epoch  48, Step:  102800, Batch Loss:     5.399407, Lr: 0.000062, Tokens per sec:   2927
2023-03-15 03:52:44,085 - INFO - __main__ - Epoch  48, Step:  102900, Batch Loss:     5.174832, Lr: 0.000062, Tokens per sec:   2894
2023-03-15 03:53:03,865 - INFO - __main__ - Epoch  48, Step:  103000, Batch Loss:     4.189388, Lr: 0.000062, Tokens per sec:   2738
2023-03-15 03:53:23,203 - INFO - __main__ - Epoch  48, Step:  103100, Batch Loss:     3.576807, Lr: 0.000062, Tokens per sec:   2755
2023-03-15 03:53:42,180 - INFO - __main__ - Epoch  48, Step:  103200, Batch Loss:     2.658450, Lr: 0.000062, Tokens per sec:   2836
2023-03-15 03:54:00,750 - INFO - __main__ - Epoch  48, Step:  103300, Batch Loss:     5.406669, Lr: 0.000062, Tokens per sec:   2886
2023-03-15 03:54:19,517 - INFO - __main__ - Epoch  48, Step:  103400, Batch Loss:     4.942454, Lr: 0.000062, Tokens per sec:   2870
2023-03-15 03:54:38,073 - INFO - __main__ - Epoch  48, Step:  103500, Batch Loss:     5.033368, Lr: 0.000062, Tokens per sec:   2890
2023-03-15 03:54:56,888 - INFO - __main__ - Epoch  48, Step:  103600, Batch Loss:     5.622789, Lr: 0.000062, Tokens per sec:   2834
2023-03-15 03:55:15,593 - INFO - __main__ - Epoch  48, Step:  103700, Batch Loss:     5.683809, Lr: 0.000062, Tokens per sec:   2929
2023-03-15 03:55:35,171 - INFO - __main__ - Epoch  48, Step:  103800, Batch Loss:     6.156719, Lr: 0.000062, Tokens per sec:   2757
2023-03-15 03:55:54,398 - INFO - __main__ - Epoch  48, Step:  103900, Batch Loss:     5.244991, Lr: 0.000062, Tokens per sec:   2767
2023-03-15 03:56:13,715 - INFO - __main__ - Epoch  48, Step:  104000, Batch Loss:     4.484676, Lr: 0.000062, Tokens per sec:   2768
2023-03-15 03:56:32,553 - INFO - __main__ - Epoch  48, Step:  104100, Batch Loss:     5.056877, Lr: 0.000062, Tokens per sec:   2862
2023-03-15 03:56:50,947 - INFO - __main__ - Epoch  48, Step:  104200, Batch Loss:     3.883457, Lr: 0.000062, Tokens per sec:   2954
2023-03-15 03:57:10,378 - INFO - __main__ - Epoch  48, Step:  104300, Batch Loss:     3.574708, Lr: 0.000062, Tokens per sec:   2738
2023-03-15 03:57:29,645 - INFO - __main__ - Epoch  48, Step:  104400, Batch Loss:     5.795133, Lr: 0.000062, Tokens per sec:   2815
2023-03-15 03:57:49,522 - INFO - __main__ - Epoch  48, Step:  104500, Batch Loss:     6.566749, Lr: 0.000062, Tokens per sec:   2756
2023-03-15 03:58:07,674 - INFO - __main__ - Epoch  48: total training loss 10181.36
2023-03-15 03:58:07,675 - INFO - __main__ - Epoch 49
2023-03-15 03:58:09,618 - INFO - __main__ - Epoch  49, Step:  104600, Batch Loss:     4.882397, Lr: 0.000062, Tokens per sec:   2188
2023-03-15 03:58:28,663 - INFO - __main__ - Epoch  49, Step:  104700, Batch Loss:     3.993246, Lr: 0.000062, Tokens per sec:   2809
2023-03-15 03:58:47,744 - INFO - __main__ - Epoch  49, Step:  104800, Batch Loss:     3.938034, Lr: 0.000062, Tokens per sec:   2832
2023-03-15 03:59:06,807 - INFO - __main__ - Epoch  49, Step:  104900, Batch Loss:     5.204147, Lr: 0.000062, Tokens per sec:   2842
2023-03-15 03:59:25,862 - INFO - __main__ - Epoch  49, Step:  105000, Batch Loss:     4.699979, Lr: 0.000062, Tokens per sec:   2848
2023-03-15 03:59:44,885 - INFO - __main__ - Epoch  49, Step:  105100, Batch Loss:     4.387405, Lr: 0.000062, Tokens per sec:   2812
2023-03-15 04:00:03,582 - INFO - __main__ - Epoch  49, Step:  105200, Batch Loss:     4.395610, Lr: 0.000062, Tokens per sec:   2844
2023-03-15 04:00:22,211 - INFO - __main__ - Epoch  49, Step:  105300, Batch Loss:     3.764565, Lr: 0.000062, Tokens per sec:   2905
2023-03-15 04:00:41,084 - INFO - __main__ - Epoch  49, Step:  105400, Batch Loss:     3.362101, Lr: 0.000062, Tokens per sec:   2884
2023-03-15 04:00:59,405 - INFO - __main__ - Epoch  49, Step:  105500, Batch Loss:     3.220050, Lr: 0.000062, Tokens per sec:   2929
2023-03-15 04:01:18,057 - INFO - __main__ - Epoch  49, Step:  105600, Batch Loss:     3.344511, Lr: 0.000062, Tokens per sec:   2888
2023-03-15 04:01:36,310 - INFO - __main__ - Epoch  49, Step:  105700, Batch Loss:     6.012207, Lr: 0.000062, Tokens per sec:   2971
2023-03-15 04:01:55,164 - INFO - __main__ - Epoch  49, Step:  105800, Batch Loss:     4.066118, Lr: 0.000062, Tokens per sec:   2876
2023-03-15 04:02:14,487 - INFO - __main__ - Epoch  49, Step:  105900, Batch Loss:     4.628665, Lr: 0.000062, Tokens per sec:   2819
2023-03-15 04:02:34,319 - INFO - __main__ - Epoch  49, Step:  106000, Batch Loss:     4.910205, Lr: 0.000062, Tokens per sec:   2681
2023-03-15 04:02:53,127 - INFO - __main__ - Epoch  49, Step:  106100, Batch Loss:     3.807730, Lr: 0.000062, Tokens per sec:   2865
2023-03-15 04:03:12,283 - INFO - __main__ - Epoch  49, Step:  106200, Batch Loss:     4.668792, Lr: 0.000062, Tokens per sec:   2819
2023-03-15 04:03:30,978 - INFO - __main__ - Epoch  49, Step:  106300, Batch Loss:     3.317101, Lr: 0.000062, Tokens per sec:   2883
2023-03-15 04:03:49,370 - INFO - __main__ - Epoch  49, Step:  106400, Batch Loss:     4.869633, Lr: 0.000062, Tokens per sec:   2921
2023-03-15 04:04:07,888 - INFO - __main__ - Epoch  49, Step:  106500, Batch Loss:     6.046887, Lr: 0.000062, Tokens per sec:   2883
2023-03-15 04:04:27,352 - INFO - __main__ - Epoch  49, Step:  106600, Batch Loss:     5.797163, Lr: 0.000062, Tokens per sec:   2808
2023-03-15 04:04:46,508 - INFO - __main__ - Epoch  49, Step:  106700, Batch Loss:     5.166329, Lr: 0.000062, Tokens per sec:   2723
2023-03-15 04:04:59,712 - INFO - __main__ - Epoch  49: total training loss 9871.01
2023-03-15 04:04:59,713 - INFO - __main__ - Epoch 50
2023-03-15 04:05:05,496 - INFO - __main__ - Epoch  50, Step:  106800, Batch Loss:     3.803584, Lr: 0.000061, Tokens per sec:   2740
2023-03-15 04:05:25,640 - INFO - __main__ - Epoch  50, Step:  106900, Batch Loss:     4.377302, Lr: 0.000061, Tokens per sec:   2676
2023-03-15 04:05:44,468 - INFO - __main__ - Epoch  50, Step:  107000, Batch Loss:     4.047133, Lr: 0.000061, Tokens per sec:   2933
2023-03-15 04:06:02,873 - INFO - __main__ - Epoch  50, Step:  107100, Batch Loss:     3.601098, Lr: 0.000061, Tokens per sec:   2931
2023-03-15 04:06:21,937 - INFO - __main__ - Epoch  50, Step:  107200, Batch Loss:     3.078821, Lr: 0.000061, Tokens per sec:   2803
2023-03-15 04:06:41,594 - INFO - __main__ - Epoch  50, Step:  107300, Batch Loss:     3.061186, Lr: 0.000061, Tokens per sec:   2722
2023-03-15 04:07:00,008 - INFO - __main__ - Epoch  50, Step:  107400, Batch Loss:     4.105067, Lr: 0.000061, Tokens per sec:   2901
2023-03-15 04:07:18,452 - INFO - __main__ - Epoch  50, Step:  107500, Batch Loss:     5.444683, Lr: 0.000061, Tokens per sec:   2947
2023-03-15 04:07:37,341 - INFO - __main__ - Epoch  50, Step:  107600, Batch Loss:     4.553983, Lr: 0.000061, Tokens per sec:   2859
2023-03-15 04:07:56,469 - INFO - __main__ - Epoch  50, Step:  107700, Batch Loss:     5.917323, Lr: 0.000061, Tokens per sec:   2833
2023-03-15 04:08:15,460 - INFO - __main__ - Epoch  50, Step:  107800, Batch Loss:     2.813296, Lr: 0.000061, Tokens per sec:   2785
2023-03-15 04:08:34,039 - INFO - __main__ - Epoch  50, Step:  107900, Batch Loss:     4.332633, Lr: 0.000061, Tokens per sec:   2881
2023-03-15 04:08:53,312 - INFO - __main__ - Epoch  50, Step:  108000, Batch Loss:     5.127172, Lr: 0.000061, Tokens per sec:   2829
2023-03-15 04:09:12,882 - INFO - __main__ - Epoch  50, Step:  108100, Batch Loss:     3.874634, Lr: 0.000061, Tokens per sec:   2766
2023-03-15 04:09:31,754 - INFO - __main__ - Epoch  50, Step:  108200, Batch Loss:     3.277688, Lr: 0.000061, Tokens per sec:   2823
2023-03-15 04:09:50,775 - INFO - __main__ - Epoch  50, Step:  108300, Batch Loss:     3.423677, Lr: 0.000061, Tokens per sec:   2854
2023-03-15 04:10:10,083 - INFO - __main__ - Epoch  50, Step:  108400, Batch Loss:     5.904328, Lr: 0.000061, Tokens per sec:   2769
2023-03-15 04:10:29,956 - INFO - __main__ - Epoch  50, Step:  108500, Batch Loss:     4.771228, Lr: 0.000061, Tokens per sec:   2720
2023-03-15 04:10:48,856 - INFO - __main__ - Epoch  50, Step:  108600, Batch Loss:     3.351564, Lr: 0.000061, Tokens per sec:   2830
2023-03-15 04:11:07,209 - INFO - __main__ - Epoch  50, Step:  108700, Batch Loss:     3.933347, Lr: 0.000061, Tokens per sec:   2921
2023-03-15 04:11:27,184 - INFO - __main__ - Epoch  50, Step:  108800, Batch Loss:     6.432263, Lr: 0.000061, Tokens per sec:   2691
2023-03-15 04:11:46,201 - INFO - __main__ - Epoch  50, Step:  108900, Batch Loss:     2.450478, Lr: 0.000061, Tokens per sec:   2839
2023-03-15 04:11:55,975 - INFO - __main__ - Epoch  50: total training loss 9536.66
2023-03-15 04:11:55,976 - INFO - __main__ - Epoch 51
2023-03-15 04:12:06,022 - INFO - __main__ - Epoch  51, Step:  109000, Batch Loss:     5.279549, Lr: 0.000061, Tokens per sec:   2668
2023-03-15 04:12:25,382 - INFO - __main__ - Epoch  51, Step:  109100, Batch Loss:     3.234617, Lr: 0.000061, Tokens per sec:   2737
2023-03-15 04:12:45,040 - INFO - __main__ - Epoch  51, Step:  109200, Batch Loss:     4.549517, Lr: 0.000061, Tokens per sec:   2746
2023-03-15 04:13:04,490 - INFO - __main__ - Epoch  51, Step:  109300, Batch Loss:     4.205509, Lr: 0.000061, Tokens per sec:   2800
2023-03-15 04:13:23,527 - INFO - __main__ - Epoch  51, Step:  109400, Batch Loss:     5.398918, Lr: 0.000061, Tokens per sec:   2841
2023-03-15 04:13:42,606 - INFO - __main__ - Epoch  51, Step:  109500, Batch Loss:     5.027506, Lr: 0.000061, Tokens per sec:   2834
2023-03-15 04:14:01,876 - INFO - __main__ - Epoch  51, Step:  109600, Batch Loss:     3.866552, Lr: 0.000061, Tokens per sec:   2768
2023-03-15 04:14:21,060 - INFO - __main__ - Epoch  51, Step:  109700, Batch Loss:     4.134898, Lr: 0.000061, Tokens per sec:   2783
2023-03-15 04:14:39,947 - INFO - __main__ - Epoch  51, Step:  109800, Batch Loss:     3.104523, Lr: 0.000061, Tokens per sec:   2859
2023-03-15 04:14:58,544 - INFO - __main__ - Epoch  51, Step:  109900, Batch Loss:     3.291484, Lr: 0.000061, Tokens per sec:   2908
2023-03-15 04:15:17,065 - INFO - __main__ - Epoch  51, Step:  110000, Batch Loss:     4.404835, Lr: 0.000061, Tokens per sec:   2918
2023-03-15 04:15:36,306 - INFO - __main__ - Epoch  51, Step:  110100, Batch Loss:     4.161678, Lr: 0.000061, Tokens per sec:   2816
2023-03-15 04:15:55,388 - INFO - __main__ - Epoch  51, Step:  110200, Batch Loss:     2.971005, Lr: 0.000061, Tokens per sec:   2822
2023-03-15 04:16:14,862 - INFO - __main__ - Epoch  51, Step:  110300, Batch Loss:     3.833468, Lr: 0.000061, Tokens per sec:   2836
2023-03-15 04:16:33,524 - INFO - __main__ - Epoch  51, Step:  110400, Batch Loss:     4.608712, Lr: 0.000061, Tokens per sec:   2828
2023-03-15 04:16:52,906 - INFO - __main__ - Epoch  51, Step:  110500, Batch Loss:     5.518887, Lr: 0.000061, Tokens per sec:   2722
2023-03-15 04:17:11,849 - INFO - __main__ - Epoch  51, Step:  110600, Batch Loss:     5.054005, Lr: 0.000061, Tokens per sec:   2890
2023-03-15 04:17:30,822 - INFO - __main__ - Epoch  51, Step:  110700, Batch Loss:     4.308609, Lr: 0.000061, Tokens per sec:   2840
2023-03-15 04:17:50,085 - INFO - __main__ - Epoch  51, Step:  110800, Batch Loss:     5.155958, Lr: 0.000061, Tokens per sec:   2794
2023-03-15 04:18:09,005 - INFO - __main__ - Epoch  51, Step:  110900, Batch Loss:     3.687391, Lr: 0.000061, Tokens per sec:   2822
2023-03-15 04:18:27,550 - INFO - __main__ - Epoch  51, Step:  111000, Batch Loss:     5.320630, Lr: 0.000061, Tokens per sec:   2897
2023-03-15 04:18:45,913 - INFO - __main__ - Epoch  51, Step:  111100, Batch Loss:     3.888517, Lr: 0.000061, Tokens per sec:   2938
2023-03-15 04:18:51,265 - INFO - __main__ - Epoch  51: total training loss 9245.83
2023-03-15 04:18:51,266 - INFO - __main__ - Epoch 52
2023-03-15 04:19:04,784 - INFO - __main__ - Epoch  52, Step:  111200, Batch Loss:     2.952104, Lr: 0.000060, Tokens per sec:   2887
2023-03-15 04:19:23,822 - INFO - __main__ - Epoch  52, Step:  111300, Batch Loss:     2.993616, Lr: 0.000060, Tokens per sec:   2780
2023-03-15 04:19:43,874 - INFO - __main__ - Epoch  52, Step:  111400, Batch Loss:     4.010605, Lr: 0.000060, Tokens per sec:   2684
2023-03-15 04:20:03,135 - INFO - __main__ - Epoch  52, Step:  111500, Batch Loss:     4.144946, Lr: 0.000060, Tokens per sec:   2816
2023-03-15 04:20:22,652 - INFO - __main__ - Epoch  52, Step:  111600, Batch Loss:     3.075385, Lr: 0.000060, Tokens per sec:   2716
2023-03-15 04:20:41,979 - INFO - __main__ - Epoch  52, Step:  111700, Batch Loss:     6.129759, Lr: 0.000060, Tokens per sec:   2807
2023-03-15 04:21:00,922 - INFO - __main__ - Epoch  52, Step:  111800, Batch Loss:     2.992066, Lr: 0.000060, Tokens per sec:   2915
2023-03-15 04:21:20,204 - INFO - __main__ - Epoch  52, Step:  111900, Batch Loss:     2.540629, Lr: 0.000060, Tokens per sec:   2814
2023-03-15 04:21:39,029 - INFO - __main__ - Epoch  52, Step:  112000, Batch Loss:     4.719334, Lr: 0.000060, Tokens per sec:   2846
2023-03-15 04:21:57,420 - INFO - __main__ - Epoch  52, Step:  112100, Batch Loss:     4.266353, Lr: 0.000060, Tokens per sec:   2936
2023-03-15 04:22:17,092 - INFO - __main__ - Epoch  52, Step:  112200, Batch Loss:     4.175993, Lr: 0.000060, Tokens per sec:   2691
2023-03-15 04:22:36,144 - INFO - __main__ - Epoch  52, Step:  112300, Batch Loss:     4.390261, Lr: 0.000060, Tokens per sec:   2827
2023-03-15 04:22:55,423 - INFO - __main__ - Epoch  52, Step:  112400, Batch Loss:     3.953871, Lr: 0.000060, Tokens per sec:   2771
2023-03-15 04:23:13,861 - INFO - __main__ - Epoch  52, Step:  112500, Batch Loss:     3.837563, Lr: 0.000060, Tokens per sec:   3001
2023-03-15 04:23:33,028 - INFO - __main__ - Epoch  52, Step:  112600, Batch Loss:     5.154171, Lr: 0.000060, Tokens per sec:   2797
2023-03-15 04:23:52,303 - INFO - __main__ - Epoch  52, Step:  112700, Batch Loss:     3.422253, Lr: 0.000060, Tokens per sec:   2794
2023-03-15 04:24:11,056 - INFO - __main__ - Epoch  52, Step:  112800, Batch Loss:     3.070305, Lr: 0.000060, Tokens per sec:   2812
2023-03-15 04:24:29,681 - INFO - __main__ - Epoch  52, Step:  112900, Batch Loss:     3.801973, Lr: 0.000060, Tokens per sec:   2845
2023-03-15 04:24:48,665 - INFO - __main__ - Epoch  52, Step:  113000, Batch Loss:     5.315413, Lr: 0.000060, Tokens per sec:   2885
2023-03-15 04:25:07,339 - INFO - __main__ - Epoch  52, Step:  113100, Batch Loss:     4.091010, Lr: 0.000060, Tokens per sec:   2866
2023-03-15 04:25:25,559 - INFO - __main__ - Epoch  52, Step:  113200, Batch Loss:     3.695522, Lr: 0.000060, Tokens per sec:   2955
2023-03-15 04:25:44,079 - INFO - __main__ - Epoch  52, Step:  113300, Batch Loss:     5.213174, Lr: 0.000060, Tokens per sec:   2904
2023-03-15 04:25:45,785 - INFO - __main__ - Epoch  52: total training loss 8982.18
2023-03-15 04:25:45,786 - INFO - __main__ - Epoch 53
2023-03-15 04:26:04,338 - INFO - __main__ - Epoch  53, Step:  113400, Batch Loss:     2.675174, Lr: 0.000059, Tokens per sec:   2665
2023-03-15 04:26:23,306 - INFO - __main__ - Epoch  53, Step:  113500, Batch Loss:     3.342322, Lr: 0.000059, Tokens per sec:   2797
2023-03-15 04:26:42,712 - INFO - __main__ - Epoch  53, Step:  113600, Batch Loss:     4.651446, Lr: 0.000059, Tokens per sec:   2808
2023-03-15 04:27:02,300 - INFO - __main__ - Epoch  53, Step:  113700, Batch Loss:     2.682610, Lr: 0.000059, Tokens per sec:   2731
2023-03-15 04:27:21,276 - INFO - __main__ - Epoch  53, Step:  113800, Batch Loss:     3.355026, Lr: 0.000059, Tokens per sec:   2828
2023-03-15 04:27:40,410 - INFO - __main__ - Epoch  53, Step:  113900, Batch Loss:     4.693268, Lr: 0.000059, Tokens per sec:   2830
2023-03-15 04:27:58,763 - INFO - __main__ - Epoch  53, Step:  114000, Batch Loss:     3.655625, Lr: 0.000059, Tokens per sec:   2943
2023-03-15 04:28:18,217 - INFO - __main__ - Epoch  53, Step:  114100, Batch Loss:     4.112072, Lr: 0.000059, Tokens per sec:   2802
2023-03-15 04:28:37,437 - INFO - __main__ - Epoch  53, Step:  114200, Batch Loss:     3.256784, Lr: 0.000059, Tokens per sec:   2771
2023-03-15 04:28:56,427 - INFO - __main__ - Epoch  53, Step:  114300, Batch Loss:     4.038064, Lr: 0.000059, Tokens per sec:   2814
2023-03-15 04:29:14,601 - INFO - __main__ - Epoch  53, Step:  114400, Batch Loss:     3.445593, Lr: 0.000059, Tokens per sec:   2992
2023-03-15 04:29:33,089 - INFO - __main__ - Epoch  53, Step:  114500, Batch Loss:     3.558339, Lr: 0.000059, Tokens per sec:   2947
2023-03-15 04:29:51,708 - INFO - __main__ - Epoch  53, Step:  114600, Batch Loss:     3.375634, Lr: 0.000059, Tokens per sec:   2856
2023-03-15 04:30:10,319 - INFO - __main__ - Epoch  53, Step:  114700, Batch Loss:     3.225882, Lr: 0.000059, Tokens per sec:   2899
2023-03-15 04:30:28,704 - INFO - __main__ - Epoch  53, Step:  114800, Batch Loss:     3.306195, Lr: 0.000059, Tokens per sec:   2909
2023-03-15 04:30:48,051 - INFO - __main__ - Epoch  53, Step:  114900, Batch Loss:     4.044423, Lr: 0.000059, Tokens per sec:   2801
2023-03-15 04:31:06,999 - INFO - __main__ - Epoch  53, Step:  115000, Batch Loss:     4.170127, Lr: 0.000059, Tokens per sec:   2879
2023-03-15 04:31:26,100 - INFO - __main__ - Epoch  53, Step:  115100, Batch Loss:     4.920187, Lr: 0.000059, Tokens per sec:   2851
2023-03-15 04:31:45,353 - INFO - __main__ - Epoch  53, Step:  115200, Batch Loss:     4.072909, Lr: 0.000059, Tokens per sec:   2777
2023-03-15 04:32:04,243 - INFO - __main__ - Epoch  53, Step:  115300, Batch Loss:     3.905571, Lr: 0.000059, Tokens per sec:   2831
2023-03-15 04:32:23,189 - INFO - __main__ - Epoch  53, Step:  115400, Batch Loss:     3.000762, Lr: 0.000059, Tokens per sec:   2814
2023-03-15 04:32:39,579 - INFO - __main__ - Epoch  53: total training loss 8745.71
2023-03-15 04:32:39,580 - INFO - __main__ - Epoch 54
2023-03-15 04:32:42,491 - INFO - __main__ - Epoch  54, Step:  115500, Batch Loss:     3.658026, Lr: 0.000059, Tokens per sec:   2443
2023-03-15 04:33:01,453 - INFO - __main__ - Epoch  54, Step:  115600, Batch Loss:     2.919624, Lr: 0.000059, Tokens per sec:   2861
2023-03-15 04:33:19,797 - INFO - __main__ - Epoch  54, Step:  115700, Batch Loss:     2.955917, Lr: 0.000059, Tokens per sec:   2932
2023-03-15 04:33:38,790 - INFO - __main__ - Epoch  54, Step:  115800, Batch Loss:     3.379531, Lr: 0.000059, Tokens per sec:   2837
2023-03-15 04:33:58,068 - INFO - __main__ - Epoch  54, Step:  115900, Batch Loss:     2.619837, Lr: 0.000059, Tokens per sec:   2731
2023-03-15 04:34:16,312 - INFO - __main__ - Epoch  54, Step:  116000, Batch Loss:     3.842679, Lr: 0.000059, Tokens per sec:   2912
2023-03-15 04:34:35,717 - INFO - __main__ - Epoch  54, Step:  116100, Batch Loss:     4.005328, Lr: 0.000059, Tokens per sec:   2751
2023-03-15 04:34:54,871 - INFO - __main__ - Epoch  54, Step:  116200, Batch Loss:     3.136598, Lr: 0.000059, Tokens per sec:   2796
2023-03-15 04:35:13,086 - INFO - __main__ - Epoch  54, Step:  116300, Batch Loss:     4.573673, Lr: 0.000059, Tokens per sec:   2932
2023-03-15 04:35:32,088 - INFO - __main__ - Epoch  54, Step:  116400, Batch Loss:     4.471044, Lr: 0.000059, Tokens per sec:   2915
2023-03-15 04:35:50,931 - INFO - __main__ - Epoch  54, Step:  116500, Batch Loss:     5.577226, Lr: 0.000059, Tokens per sec:   2857
2023-03-15 04:36:09,610 - INFO - __main__ - Epoch  54, Step:  116600, Batch Loss:     3.331924, Lr: 0.000059, Tokens per sec:   2851
2023-03-15 04:36:28,154 - INFO - __main__ - Epoch  54, Step:  116700, Batch Loss:     3.126587, Lr: 0.000059, Tokens per sec:   2935
2023-03-15 04:36:46,856 - INFO - __main__ - Epoch  54, Step:  116800, Batch Loss:     2.560772, Lr: 0.000059, Tokens per sec:   2891
2023-03-15 04:37:05,474 - INFO - __main__ - Epoch  54, Step:  116900, Batch Loss:     4.868462, Lr: 0.000059, Tokens per sec:   2926
2023-03-15 04:37:25,177 - INFO - __main__ - Epoch  54, Step:  117000, Batch Loss:     3.796132, Lr: 0.000059, Tokens per sec:   2758
2023-03-15 04:37:44,916 - INFO - __main__ - Epoch  54, Step:  117100, Batch Loss:     3.684788, Lr: 0.000059, Tokens per sec:   2698
2023-03-15 04:38:03,914 - INFO - __main__ - Epoch  54, Step:  117200, Batch Loss:     4.351938, Lr: 0.000059, Tokens per sec:   2825
2023-03-15 04:38:23,296 - INFO - __main__ - Epoch  54, Step:  117300, Batch Loss:     4.576194, Lr: 0.000059, Tokens per sec:   2757
2023-03-15 04:38:42,340 - INFO - __main__ - Epoch  54, Step:  117400, Batch Loss:     3.837814, Lr: 0.000059, Tokens per sec:   2845
2023-03-15 04:39:01,309 - INFO - __main__ - Epoch  54, Step:  117500, Batch Loss:     4.649739, Lr: 0.000059, Tokens per sec:   2861
2023-03-15 04:39:20,566 - INFO - __main__ - Epoch  54, Step:  117600, Batch Loss:     5.250247, Lr: 0.000059, Tokens per sec:   2792
2023-03-15 04:39:32,895 - INFO - __main__ - Epoch  54: total training loss 8492.23
2023-03-15 04:39:32,896 - INFO - __main__ - Epoch 55
2023-03-15 04:39:39,582 - INFO - __main__ - Epoch  55, Step:  117700, Batch Loss:     4.490435, Lr: 0.000058, Tokens per sec:   2779
2023-03-15 04:39:58,574 - INFO - __main__ - Epoch  55, Step:  117800, Batch Loss:     3.815124, Lr: 0.000058, Tokens per sec:   2807
2023-03-15 04:40:17,910 - INFO - __main__ - Epoch  55, Step:  117900, Batch Loss:     2.422605, Lr: 0.000058, Tokens per sec:   2777
2023-03-15 04:40:36,889 - INFO - __main__ - Epoch  55, Step:  118000, Batch Loss:     3.478527, Lr: 0.000058, Tokens per sec:   2820
2023-03-15 04:40:55,453 - INFO - __main__ - Epoch  55, Step:  118100, Batch Loss:     2.888867, Lr: 0.000058, Tokens per sec:   2929
2023-03-15 04:41:14,887 - INFO - __main__ - Epoch  55, Step:  118200, Batch Loss:     3.740085, Lr: 0.000058, Tokens per sec:   2746
2023-03-15 04:41:34,626 - INFO - __main__ - Epoch  55, Step:  118300, Batch Loss:     3.598697, Lr: 0.000058, Tokens per sec:   2739
2023-03-15 04:41:53,600 - INFO - __main__ - Epoch  55, Step:  118400, Batch Loss:     2.873066, Lr: 0.000058, Tokens per sec:   2841
2023-03-15 04:42:12,947 - INFO - __main__ - Epoch  55, Step:  118500, Batch Loss:     4.371475, Lr: 0.000058, Tokens per sec:   2770
2023-03-15 04:42:32,464 - INFO - __main__ - Epoch  55, Step:  118600, Batch Loss:     3.098351, Lr: 0.000058, Tokens per sec:   2793
2023-03-15 04:42:51,927 - INFO - __main__ - Epoch  55, Step:  118700, Batch Loss:     2.290262, Lr: 0.000058, Tokens per sec:   2815
2023-03-15 04:43:10,665 - INFO - __main__ - Epoch  55, Step:  118800, Batch Loss:     4.239297, Lr: 0.000058, Tokens per sec:   2880
2023-03-15 04:43:29,161 - INFO - __main__ - Epoch  55, Step:  118900, Batch Loss:     2.892929, Lr: 0.000058, Tokens per sec:   2930
2023-03-15 04:43:48,685 - INFO - __main__ - Epoch  55, Step:  119000, Batch Loss:     3.149728, Lr: 0.000058, Tokens per sec:   2758
2023-03-15 04:44:07,375 - INFO - __main__ - Epoch  55, Step:  119100, Batch Loss:     4.453574, Lr: 0.000058, Tokens per sec:   2880
2023-03-15 04:44:26,813 - INFO - __main__ - Epoch  55, Step:  119200, Batch Loss:     4.126864, Lr: 0.000058, Tokens per sec:   2747
2023-03-15 04:44:46,275 - INFO - __main__ - Epoch  55, Step:  119300, Batch Loss:     2.729552, Lr: 0.000058, Tokens per sec:   2763
2023-03-15 04:45:04,904 - INFO - __main__ - Epoch  55, Step:  119400, Batch Loss:     4.372588, Lr: 0.000058, Tokens per sec:   2855
2023-03-15 04:45:23,636 - INFO - __main__ - Epoch  55, Step:  119500, Batch Loss:     2.869745, Lr: 0.000058, Tokens per sec:   2863
2023-03-15 04:45:42,410 - INFO - __main__ - Epoch  55, Step:  119600, Batch Loss:     3.824392, Lr: 0.000058, Tokens per sec:   2851
2023-03-15 04:46:01,777 - INFO - __main__ - Epoch  55, Step:  119700, Batch Loss:     3.332174, Lr: 0.000058, Tokens per sec:   2789
2023-03-15 04:46:21,869 - INFO - __main__ - Epoch  55, Step:  119800, Batch Loss:     4.750058, Lr: 0.000058, Tokens per sec:   2671
2023-03-15 04:46:30,894 - INFO - __main__ - Epoch  55: total training loss 8226.13
2023-03-15 04:46:30,895 - INFO - __main__ - Epoch 56
2023-03-15 04:46:41,721 - INFO - __main__ - Epoch  56, Step:  119900, Batch Loss:     3.419707, Lr: 0.000058, Tokens per sec:   2757
2023-03-15 04:47:00,473 - INFO - __main__ - Epoch  56, Step:  120000, Batch Loss:     3.700487, Lr: 0.000058, Tokens per sec:   2874
2023-03-15 04:47:19,827 - INFO - __main__ - Epoch  56, Step:  120100, Batch Loss:     2.651239, Lr: 0.000058, Tokens per sec:   2816
2023-03-15 04:47:38,888 - INFO - __main__ - Epoch  56, Step:  120200, Batch Loss:     4.647465, Lr: 0.000058, Tokens per sec:   2841
2023-03-15 04:47:58,234 - INFO - __main__ - Epoch  56, Step:  120300, Batch Loss:     4.026423, Lr: 0.000058, Tokens per sec:   2779
2023-03-15 04:48:17,009 - INFO - __main__ - Epoch  56, Step:  120400, Batch Loss:     3.872794, Lr: 0.000058, Tokens per sec:   2843
2023-03-15 04:48:36,109 - INFO - __main__ - Epoch  56, Step:  120500, Batch Loss:     3.026233, Lr: 0.000058, Tokens per sec:   2842
2023-03-15 04:48:54,842 - INFO - __main__ - Epoch  56, Step:  120600, Batch Loss:     3.695725, Lr: 0.000058, Tokens per sec:   2868
2023-03-15 04:49:13,883 - INFO - __main__ - Epoch  56, Step:  120700, Batch Loss:     3.153045, Lr: 0.000058, Tokens per sec:   2795
2023-03-15 04:49:32,587 - INFO - __main__ - Epoch  56, Step:  120800, Batch Loss:     2.330643, Lr: 0.000058, Tokens per sec:   2893
2023-03-15 04:49:50,897 - INFO - __main__ - Epoch  56, Step:  120900, Batch Loss:     3.198142, Lr: 0.000058, Tokens per sec:   2977
2023-03-15 04:50:09,948 - INFO - __main__ - Epoch  56, Step:  121000, Batch Loss:     2.540155, Lr: 0.000058, Tokens per sec:   2838
2023-03-15 04:50:28,691 - INFO - __main__ - Epoch  56, Step:  121100, Batch Loss:     2.882815, Lr: 0.000058, Tokens per sec:   2910
2023-03-15 04:50:47,122 - INFO - __main__ - Epoch  56, Step:  121200, Batch Loss:     3.661762, Lr: 0.000058, Tokens per sec:   2857
2023-03-15 04:51:06,096 - INFO - __main__ - Epoch  56, Step:  121300, Batch Loss:     4.321190, Lr: 0.000058, Tokens per sec:   2843
2023-03-15 04:51:25,473 - INFO - __main__ - Epoch  56, Step:  121400, Batch Loss:     3.694692, Lr: 0.000058, Tokens per sec:   2758
2023-03-15 04:51:44,608 - INFO - __main__ - Epoch  56, Step:  121500, Batch Loss:     4.015083, Lr: 0.000058, Tokens per sec:   2793
2023-03-15 04:52:04,260 - INFO - __main__ - Epoch  56, Step:  121600, Batch Loss:     3.360744, Lr: 0.000058, Tokens per sec:   2707
2023-03-15 04:52:23,875 - INFO - __main__ - Epoch  56, Step:  121700, Batch Loss:     4.515478, Lr: 0.000058, Tokens per sec:   2722
2023-03-15 04:52:42,862 - INFO - __main__ - Epoch  56, Step:  121800, Batch Loss:     3.079810, Lr: 0.000058, Tokens per sec:   2856
2023-03-15 04:53:02,459 - INFO - __main__ - Epoch  56, Step:  121900, Batch Loss:     2.326369, Lr: 0.000058, Tokens per sec:   2731
2023-03-15 04:53:22,206 - INFO - __main__ - Epoch  56, Step:  122000, Batch Loss:     4.459246, Lr: 0.000058, Tokens per sec:   2736
2023-03-15 04:53:27,079 - INFO - __main__ - Epoch  56: total training loss 7980.01
2023-03-15 04:53:27,080 - INFO - __main__ - Epoch 57
2023-03-15 04:53:42,213 - INFO - __main__ - Epoch  57, Step:  122100, Batch Loss:     3.589318, Lr: 0.000057, Tokens per sec:   2722
2023-03-15 04:54:01,580 - INFO - __main__ - Epoch  57, Step:  122200, Batch Loss:     3.707453, Lr: 0.000057, Tokens per sec:   2752
2023-03-15 04:54:20,479 - INFO - __main__ - Epoch  57, Step:  122300, Batch Loss:     3.966210, Lr: 0.000057, Tokens per sec:   2791
2023-03-15 04:54:39,983 - INFO - __main__ - Epoch  57, Step:  122400, Batch Loss:     3.852560, Lr: 0.000057, Tokens per sec:   2805
2023-03-15 04:54:58,909 - INFO - __main__ - Epoch  57, Step:  122500, Batch Loss:     4.059682, Lr: 0.000057, Tokens per sec:   2855
2023-03-15 04:55:17,656 - INFO - __main__ - Epoch  57, Step:  122600, Batch Loss:     3.956423, Lr: 0.000057, Tokens per sec:   2850
2023-03-15 04:55:36,172 - INFO - __main__ - Epoch  57, Step:  122700, Batch Loss:     3.455207, Lr: 0.000057, Tokens per sec:   2881
2023-03-15 04:55:55,074 - INFO - __main__ - Epoch  57, Step:  122800, Batch Loss:     2.685880, Lr: 0.000057, Tokens per sec:   2833
2023-03-15 04:56:14,361 - INFO - __main__ - Epoch  57, Step:  122900, Batch Loss:     3.074296, Lr: 0.000057, Tokens per sec:   2824
2023-03-15 04:56:33,636 - INFO - __main__ - Epoch  57, Step:  123000, Batch Loss:     3.930096, Lr: 0.000057, Tokens per sec:   2781
2023-03-15 04:56:52,481 - INFO - __main__ - Epoch  57, Step:  123100, Batch Loss:     4.146634, Lr: 0.000057, Tokens per sec:   2810
2023-03-15 04:57:11,858 - INFO - __main__ - Epoch  57, Step:  123200, Batch Loss:     2.733035, Lr: 0.000057, Tokens per sec:   2793
2023-03-15 04:57:30,451 - INFO - __main__ - Epoch  57, Step:  123300, Batch Loss:     3.233487, Lr: 0.000057, Tokens per sec:   2835
2023-03-15 04:57:49,768 - INFO - __main__ - Epoch  57, Step:  123400, Batch Loss:     3.619582, Lr: 0.000057, Tokens per sec:   2821
2023-03-15 04:58:08,720 - INFO - __main__ - Epoch  57, Step:  123500, Batch Loss:     3.767421, Lr: 0.000057, Tokens per sec:   2789
2023-03-15 04:58:27,852 - INFO - __main__ - Epoch  57, Step:  123600, Batch Loss:     3.697259, Lr: 0.000057, Tokens per sec:   2894
2023-03-15 04:58:46,697 - INFO - __main__ - Epoch  57, Step:  123700, Batch Loss:     4.588999, Lr: 0.000057, Tokens per sec:   2876
2023-03-15 04:59:04,921 - INFO - __main__ - Epoch  57, Step:  123800, Batch Loss:     4.447000, Lr: 0.000057, Tokens per sec:   2958
2023-03-15 04:59:23,969 - INFO - __main__ - Epoch  57, Step:  123900, Batch Loss:     4.119161, Lr: 0.000057, Tokens per sec:   2823
2023-03-15 04:59:43,306 - INFO - __main__ - Epoch  57, Step:  124000, Batch Loss:     4.716414, Lr: 0.000057, Tokens per sec:   2742
2023-03-15 05:00:02,420 - INFO - __main__ - Epoch  57, Step:  124100, Batch Loss:     2.975398, Lr: 0.000057, Tokens per sec:   2910
2023-03-15 05:00:20,900 - INFO - __main__ - Epoch  57, Step:  124200, Batch Loss:     4.845628, Lr: 0.000057, Tokens per sec:   2918
2023-03-15 05:00:21,525 - INFO - __main__ - Epoch  57: total training loss 7765.21
2023-03-15 05:00:21,526 - INFO - __main__ - Epoch 58
2023-03-15 05:00:41,295 - INFO - __main__ - Epoch  58, Step:  124300, Batch Loss:     3.191209, Lr: 0.000056, Tokens per sec:   2620
2023-03-15 05:00:59,795 - INFO - __main__ - Epoch  58, Step:  124400, Batch Loss:     2.813035, Lr: 0.000056, Tokens per sec:   2878
2023-03-15 05:01:18,689 - INFO - __main__ - Epoch  58, Step:  124500, Batch Loss:     2.860214, Lr: 0.000056, Tokens per sec:   2817
2023-03-15 05:01:37,812 - INFO - __main__ - Epoch  58, Step:  124600, Batch Loss:     2.932466, Lr: 0.000056, Tokens per sec:   2844
2023-03-15 05:01:56,594 - INFO - __main__ - Epoch  58, Step:  124700, Batch Loss:     4.534100, Lr: 0.000056, Tokens per sec:   2819
2023-03-15 05:02:15,095 - INFO - __main__ - Epoch  58, Step:  124800, Batch Loss:     4.506269, Lr: 0.000056, Tokens per sec:   2919
2023-03-15 05:02:34,036 - INFO - __main__ - Epoch  58, Step:  124900, Batch Loss:     1.500575, Lr: 0.000056, Tokens per sec:   2812
2023-03-15 05:02:52,845 - INFO - __main__ - Epoch  58, Step:  125000, Batch Loss:     3.045668, Lr: 0.000056, Tokens per sec:   2855
2023-03-15 05:03:11,348 - INFO - __main__ - Epoch  58, Step:  125100, Batch Loss:     4.481538, Lr: 0.000056, Tokens per sec:   2932
2023-03-15 05:03:30,426 - INFO - __main__ - Epoch  58, Step:  125200, Batch Loss:     3.318801, Lr: 0.000056, Tokens per sec:   2837
2023-03-15 05:03:48,877 - INFO - __main__ - Epoch  58, Step:  125300, Batch Loss:     4.750123, Lr: 0.000056, Tokens per sec:   2934
2023-03-15 05:04:07,586 - INFO - __main__ - Epoch  58, Step:  125400, Batch Loss:     3.206272, Lr: 0.000056, Tokens per sec:   2856
2023-03-15 05:04:26,678 - INFO - __main__ - Epoch  58, Step:  125500, Batch Loss:     3.596519, Lr: 0.000056, Tokens per sec:   2857
2023-03-15 05:04:45,696 - INFO - __main__ - Epoch  58, Step:  125600, Batch Loss:     3.232704, Lr: 0.000056, Tokens per sec:   2884
2023-03-15 05:05:05,543 - INFO - __main__ - Epoch  58, Step:  125700, Batch Loss:     4.363770, Lr: 0.000056, Tokens per sec:   2754
2023-03-15 05:05:24,837 - INFO - __main__ - Epoch  58, Step:  125800, Batch Loss:     4.197684, Lr: 0.000056, Tokens per sec:   2813
2023-03-15 05:05:43,738 - INFO - __main__ - Epoch  58, Step:  125900, Batch Loss:     3.277151, Lr: 0.000056, Tokens per sec:   2863
2023-03-15 05:06:03,095 - INFO - __main__ - Epoch  58, Step:  126000, Batch Loss:     3.295448, Lr: 0.000056, Tokens per sec:   2762
2023-03-15 05:06:22,066 - INFO - __main__ - Epoch  58, Step:  126100, Batch Loss:     4.563799, Lr: 0.000056, Tokens per sec:   2791
2023-03-15 05:06:40,787 - INFO - __main__ - Epoch  58, Step:  126200, Batch Loss:     3.969879, Lr: 0.000056, Tokens per sec:   2836
2023-03-15 05:06:59,584 - INFO - __main__ - Epoch  58, Step:  126300, Batch Loss:     3.779851, Lr: 0.000056, Tokens per sec:   2853
2023-03-15 05:07:14,463 - INFO - __main__ - Epoch  58: total training loss 7528.67
2023-03-15 05:07:14,464 - INFO - __main__ - Epoch 59
2023-03-15 05:07:18,315 - INFO - __main__ - Epoch  59, Step:  126400, Batch Loss:     4.326367, Lr: 0.000056, Tokens per sec:   2393
2023-03-15 05:07:36,764 - INFO - __main__ - Epoch  59, Step:  126500, Batch Loss:     3.970722, Lr: 0.000056, Tokens per sec:   2893
2023-03-15 05:07:55,727 - INFO - __main__ - Epoch  59, Step:  126600, Batch Loss:     2.502452, Lr: 0.000056, Tokens per sec:   2843
2023-03-15 05:08:14,728 - INFO - __main__ - Epoch  59, Step:  126700, Batch Loss:     2.257578, Lr: 0.000056, Tokens per sec:   2835
2023-03-15 05:08:33,118 - INFO - __main__ - Epoch  59, Step:  126800, Batch Loss:     3.473178, Lr: 0.000056, Tokens per sec:   2991
2023-03-15 05:08:51,920 - INFO - __main__ - Epoch  59, Step:  126900, Batch Loss:     2.545234, Lr: 0.000056, Tokens per sec:   2807
2023-03-15 05:09:10,743 - INFO - __main__ - Epoch  59, Step:  127000, Batch Loss:     3.622318, Lr: 0.000056, Tokens per sec:   2852
2023-03-15 05:09:29,398 - INFO - __main__ - Epoch  59, Step:  127100, Batch Loss:     4.703774, Lr: 0.000056, Tokens per sec:   2871
2023-03-15 05:09:48,237 - INFO - __main__ - Epoch  59, Step:  127200, Batch Loss:     3.887188, Lr: 0.000056, Tokens per sec:   2905
2023-03-15 05:10:07,251 - INFO - __main__ - Epoch  59, Step:  127300, Batch Loss:     3.384229, Lr: 0.000056, Tokens per sec:   2826
2023-03-15 05:10:26,701 - INFO - __main__ - Epoch  59, Step:  127400, Batch Loss:     2.816510, Lr: 0.000056, Tokens per sec:   2757
2023-03-15 05:10:46,091 - INFO - __main__ - Epoch  59, Step:  127500, Batch Loss:     2.759757, Lr: 0.000056, Tokens per sec:   2742
2023-03-15 05:11:04,473 - INFO - __main__ - Epoch  59, Step:  127600, Batch Loss:     2.991397, Lr: 0.000056, Tokens per sec:   2918
2023-03-15 05:11:23,546 - INFO - __main__ - Epoch  59, Step:  127700, Batch Loss:     2.792856, Lr: 0.000056, Tokens per sec:   2848
2023-03-15 05:11:41,717 - INFO - __main__ - Epoch  59, Step:  127800, Batch Loss:     3.292462, Lr: 0.000056, Tokens per sec:   2975
2023-03-15 05:12:00,694 - INFO - __main__ - Epoch  59, Step:  127900, Batch Loss:     3.611514, Lr: 0.000056, Tokens per sec:   2861
2023-03-15 05:12:19,267 - INFO - __main__ - Epoch  59, Step:  128000, Batch Loss:     4.386099, Lr: 0.000056, Tokens per sec:   2941
2023-03-15 05:12:38,060 - INFO - __main__ - Epoch  59, Step:  128100, Batch Loss:     3.681848, Lr: 0.000056, Tokens per sec:   2826
2023-03-15 05:12:56,496 - INFO - __main__ - Epoch  59, Step:  128200, Batch Loss:     4.819606, Lr: 0.000056, Tokens per sec:   2977
2023-03-15 05:13:14,855 - INFO - __main__ - Epoch  59, Step:  128300, Batch Loss:     3.706231, Lr: 0.000056, Tokens per sec:   2953
2023-03-15 05:13:34,523 - INFO - __main__ - Epoch  59, Step:  128400, Batch Loss:     2.207718, Lr: 0.000056, Tokens per sec:   2701
2023-03-15 05:13:53,088 - INFO - __main__ - Epoch  59, Step:  128500, Batch Loss:     3.947085, Lr: 0.000056, Tokens per sec:   2886
2023-03-15 05:14:04,508 - INFO - __main__ - Epoch  59: total training loss 7334.87
2023-03-15 05:14:04,509 - INFO - __main__ - Epoch 60
2023-03-15 05:14:12,451 - INFO - __main__ - Epoch  60, Step:  128600, Batch Loss:     3.215869, Lr: 0.000055, Tokens per sec:   2577
2023-03-15 05:14:30,923 - INFO - __main__ - Epoch  60, Step:  128700, Batch Loss:     4.477465, Lr: 0.000055, Tokens per sec:   2934
2023-03-15 05:14:49,945 - INFO - __main__ - Epoch  60, Step:  128800, Batch Loss:     3.103690, Lr: 0.000055, Tokens per sec:   2861
2023-03-15 05:15:09,455 - INFO - __main__ - Epoch  60, Step:  128900, Batch Loss:     2.595052, Lr: 0.000055, Tokens per sec:   2760
2023-03-15 05:15:27,902 - INFO - __main__ - Epoch  60, Step:  129000, Batch Loss:     3.238269, Lr: 0.000055, Tokens per sec:   2830
2023-03-15 05:15:47,605 - INFO - __main__ - Epoch  60, Step:  129100, Batch Loss:     3.658197, Lr: 0.000055, Tokens per sec:   2785
2023-03-15 05:16:06,711 - INFO - __main__ - Epoch  60, Step:  129200, Batch Loss:     2.780773, Lr: 0.000055, Tokens per sec:   2774
2023-03-15 05:16:25,452 - INFO - __main__ - Epoch  60, Step:  129300, Batch Loss:     4.466289, Lr: 0.000055, Tokens per sec:   2884
2023-03-15 05:16:45,342 - INFO - __main__ - Epoch  60, Step:  129400, Batch Loss:     3.492125, Lr: 0.000055, Tokens per sec:   2716
2023-03-15 05:17:04,541 - INFO - __main__ - Epoch  60, Step:  129500, Batch Loss:     4.146789, Lr: 0.000055, Tokens per sec:   2821
2023-03-15 05:17:23,894 - INFO - __main__ - Epoch  60, Step:  129600, Batch Loss:     3.922759, Lr: 0.000055, Tokens per sec:   2821
2023-03-15 05:17:42,779 - INFO - __main__ - Epoch  60, Step:  129700, Batch Loss:     2.515722, Lr: 0.000055, Tokens per sec:   2830
2023-03-15 05:18:02,127 - INFO - __main__ - Epoch  60, Step:  129800, Batch Loss:     3.504558, Lr: 0.000055, Tokens per sec:   2785
2023-03-15 05:18:21,453 - INFO - __main__ - Epoch  60, Step:  129900, Batch Loss:     2.817590, Lr: 0.000055, Tokens per sec:   2783
2023-03-15 05:18:39,735 - INFO - __main__ - Epoch  60, Step:  130000, Batch Loss:     4.970121, Lr: 0.000055, Tokens per sec:   2946
2023-03-15 05:18:58,329 - INFO - __main__ - Epoch  60, Step:  130100, Batch Loss:     3.377438, Lr: 0.000055, Tokens per sec:   2913
2023-03-15 05:19:17,442 - INFO - __main__ - Epoch  60, Step:  130200, Batch Loss:     3.359807, Lr: 0.000055, Tokens per sec:   2838
2023-03-15 05:19:36,670 - INFO - __main__ - Epoch  60, Step:  130300, Batch Loss:     3.293269, Lr: 0.000055, Tokens per sec:   2768
2023-03-15 05:19:55,723 - INFO - __main__ - Epoch  60, Step:  130400, Batch Loss:     2.349333, Lr: 0.000055, Tokens per sec:   2778
2023-03-15 05:20:15,017 - INFO - __main__ - Epoch  60, Step:  130500, Batch Loss:     3.705307, Lr: 0.000055, Tokens per sec:   2815
2023-03-15 05:20:34,813 - INFO - __main__ - Epoch  60, Step:  130600, Batch Loss:     3.571829, Lr: 0.000055, Tokens per sec:   2734
2023-03-15 05:20:54,041 - INFO - __main__ - Epoch  60, Step:  130700, Batch Loss:     3.900006, Lr: 0.000055, Tokens per sec:   2814
2023-03-15 05:21:01,824 - INFO - __main__ - Epoch  60: total training loss 7156.63
2023-03-15 05:21:01,825 - INFO - __main__ - Epoch 61
2023-03-15 05:21:14,156 - INFO - __main__ - Epoch  61, Step:  130800, Batch Loss:     1.678161, Lr: 0.000055, Tokens per sec:   2625
2023-03-15 05:21:32,992 - INFO - __main__ - Epoch  61, Step:  130900, Batch Loss:     2.400436, Lr: 0.000055, Tokens per sec:   2846
2023-03-15 05:21:51,384 - INFO - __main__ - Epoch  61, Step:  131000, Batch Loss:     2.862876, Lr: 0.000055, Tokens per sec:   2914
2023-03-15 05:22:10,227 - INFO - __main__ - Epoch  61, Step:  131100, Batch Loss:     1.895342, Lr: 0.000055, Tokens per sec:   2838
2023-03-15 05:22:28,892 - INFO - __main__ - Epoch  61, Step:  131200, Batch Loss:     2.824902, Lr: 0.000055, Tokens per sec:   2926
2023-03-15 05:22:47,342 - INFO - __main__ - Epoch  61, Step:  131300, Batch Loss:     3.188034, Lr: 0.000055, Tokens per sec:   2944
2023-03-15 05:23:06,739 - INFO - __main__ - Epoch  61, Step:  131400, Batch Loss:     3.534109, Lr: 0.000055, Tokens per sec:   2822
2023-03-15 05:23:25,090 - INFO - __main__ - Epoch  61, Step:  131500, Batch Loss:     3.318303, Lr: 0.000055, Tokens per sec:   2980
2023-03-15 05:23:45,027 - INFO - __main__ - Epoch  61, Step:  131600, Batch Loss:     4.060012, Lr: 0.000055, Tokens per sec:   2736
2023-03-15 05:24:03,814 - INFO - __main__ - Epoch  61, Step:  131700, Batch Loss:     4.479029, Lr: 0.000055, Tokens per sec:   2845
2023-03-15 05:24:22,456 - INFO - __main__ - Epoch  61, Step:  131800, Batch Loss:     4.530703, Lr: 0.000055, Tokens per sec:   2871
2023-03-15 05:24:41,714 - INFO - __main__ - Epoch  61, Step:  131900, Batch Loss:     2.949262, Lr: 0.000055, Tokens per sec:   2788
2023-03-15 05:25:00,863 - INFO - __main__ - Epoch  61, Step:  132000, Batch Loss:     2.938971, Lr: 0.000055, Tokens per sec:   2757
2023-03-15 05:25:20,426 - INFO - __main__ - Epoch  61, Step:  132100, Batch Loss:     3.617392, Lr: 0.000055, Tokens per sec:   2733
2023-03-15 05:25:39,166 - INFO - __main__ - Epoch  61, Step:  132200, Batch Loss:     3.714188, Lr: 0.000055, Tokens per sec:   2858
2023-03-15 05:25:58,389 - INFO - __main__ - Epoch  61, Step:  132300, Batch Loss:     2.943238, Lr: 0.000055, Tokens per sec:   2826
2023-03-15 05:26:16,780 - INFO - __main__ - Epoch  61, Step:  132400, Batch Loss:     2.781517, Lr: 0.000055, Tokens per sec:   2949
2023-03-15 05:26:35,417 - INFO - __main__ - Epoch  61, Step:  132500, Batch Loss:     4.245179, Lr: 0.000055, Tokens per sec:   2885
2023-03-15 05:26:55,010 - INFO - __main__ - Epoch  61, Step:  132600, Batch Loss:     3.029039, Lr: 0.000055, Tokens per sec:   2747
2023-03-15 05:27:14,462 - INFO - __main__ - Epoch  61, Step:  132700, Batch Loss:     3.617137, Lr: 0.000055, Tokens per sec:   2771
2023-03-15 05:27:32,743 - INFO - __main__ - Epoch  61, Step:  132800, Batch Loss:     4.475307, Lr: 0.000055, Tokens per sec:   2913
2023-03-15 05:27:51,649 - INFO - __main__ - Epoch  61, Step:  132900, Batch Loss:     3.168425, Lr: 0.000055, Tokens per sec:   2816
2023-03-15 05:27:55,463 - INFO - __main__ - Epoch  61: total training loss 6967.03
2023-03-15 05:27:55,464 - INFO - __main__ - Epoch 62
2023-03-15 05:28:11,193 - INFO - __main__ - Epoch  62, Step:  133000, Batch Loss:     3.496909, Lr: 0.000054, Tokens per sec:   2852
2023-03-15 05:28:30,360 - INFO - __main__ - Epoch  62, Step:  133100, Batch Loss:     3.183769, Lr: 0.000054, Tokens per sec:   2822
2023-03-15 05:28:49,391 - INFO - __main__ - Epoch  62, Step:  133200, Batch Loss:     2.006573, Lr: 0.000054, Tokens per sec:   2808
2023-03-15 05:29:07,974 - INFO - __main__ - Epoch  62, Step:  133300, Batch Loss:     3.630339, Lr: 0.000054, Tokens per sec:   2937
2023-03-15 05:29:26,342 - INFO - __main__ - Epoch  62, Step:  133400, Batch Loss:     2.526310, Lr: 0.000054, Tokens per sec:   2937
2023-03-15 05:29:45,085 - INFO - __main__ - Epoch  62, Step:  133500, Batch Loss:     2.582004, Lr: 0.000054, Tokens per sec:   2841
2023-03-15 05:30:04,167 - INFO - __main__ - Epoch  62, Step:  133600, Batch Loss:     4.082519, Lr: 0.000054, Tokens per sec:   2802
2023-03-15 05:30:23,638 - INFO - __main__ - Epoch  62, Step:  133700, Batch Loss:     2.189626, Lr: 0.000054, Tokens per sec:   2727
2023-03-15 05:30:42,359 - INFO - __main__ - Epoch  62, Step:  133800, Batch Loss:     3.765745, Lr: 0.000054, Tokens per sec:   2848
2023-03-15 05:31:00,842 - INFO - __main__ - Epoch  62, Step:  133900, Batch Loss:     2.703285, Lr: 0.000054, Tokens per sec:   2923
2023-03-15 05:31:19,264 - INFO - __main__ - Epoch  62, Step:  134000, Batch Loss:     2.913869, Lr: 0.000054, Tokens per sec:   2921
2023-03-15 05:31:38,491 - INFO - __main__ - Epoch  62, Step:  134100, Batch Loss:     2.585835, Lr: 0.000054, Tokens per sec:   2781
2023-03-15 05:31:57,755 - INFO - __main__ - Epoch  62, Step:  134200, Batch Loss:     2.731173, Lr: 0.000054, Tokens per sec:   2777
2023-03-15 05:32:15,926 - INFO - __main__ - Epoch  62, Step:  134300, Batch Loss:     2.964823, Lr: 0.000054, Tokens per sec:   2969
2023-03-15 05:32:34,640 - INFO - __main__ - Epoch  62, Step:  134400, Batch Loss:     3.094161, Lr: 0.000054, Tokens per sec:   2867
2023-03-15 05:32:53,678 - INFO - __main__ - Epoch  62, Step:  134500, Batch Loss:     3.710924, Lr: 0.000054, Tokens per sec:   2820
2023-03-15 05:33:11,899 - INFO - __main__ - Epoch  62, Step:  134600, Batch Loss:     4.364595, Lr: 0.000054, Tokens per sec:   2994
2023-03-15 05:33:31,160 - INFO - __main__ - Epoch  62, Step:  134700, Batch Loss:     3.190426, Lr: 0.000054, Tokens per sec:   2819
2023-03-15 05:33:50,482 - INFO - __main__ - Epoch  62, Step:  134800, Batch Loss:     4.387720, Lr: 0.000054, Tokens per sec:   2771
2023-03-15 05:34:09,949 - INFO - __main__ - Epoch  62, Step:  134900, Batch Loss:     3.279797, Lr: 0.000054, Tokens per sec:   2776
2023-03-15 05:34:28,544 - INFO - __main__ - Epoch  62, Step:  135000, Batch Loss:     3.367616, Lr: 0.000054, Tokens per sec:   2878
2023-03-15 05:34:47,930 - INFO - __main__ - Epoch  62: total training loss 6812.36
2023-03-15 05:34:47,931 - INFO - __main__ - Epoch 63
2023-03-15 05:34:48,664 - INFO - __main__ - Epoch  63, Step:  135100, Batch Loss:     2.145144, Lr: 0.000054, Tokens per sec:   1485
2023-03-15 05:35:08,431 - INFO - __main__ - Epoch  63, Step:  135200, Batch Loss:     2.302691, Lr: 0.000054, Tokens per sec:   2713
2023-03-15 05:35:27,765 - INFO - __main__ - Epoch  63, Step:  135300, Batch Loss:     4.118157, Lr: 0.000054, Tokens per sec:   2807
2023-03-15 05:35:47,100 - INFO - __main__ - Epoch  63, Step:  135400, Batch Loss:     2.665353, Lr: 0.000054, Tokens per sec:   2811
2023-03-15 05:36:06,769 - INFO - __main__ - Epoch  63, Step:  135500, Batch Loss:     3.475578, Lr: 0.000054, Tokens per sec:   2712
2023-03-15 05:36:26,390 - INFO - __main__ - Epoch  63, Step:  135600, Batch Loss:     2.489576, Lr: 0.000054, Tokens per sec:   2755
2023-03-15 05:36:45,565 - INFO - __main__ - Epoch  63, Step:  135700, Batch Loss:     2.365865, Lr: 0.000054, Tokens per sec:   2769
2023-03-15 05:37:04,346 - INFO - __main__ - Epoch  63, Step:  135800, Batch Loss:     2.615834, Lr: 0.000054, Tokens per sec:   2832
2023-03-15 05:37:24,121 - INFO - __main__ - Epoch  63, Step:  135900, Batch Loss:     2.172324, Lr: 0.000054, Tokens per sec:   2739
2023-03-15 05:37:42,947 - INFO - __main__ - Epoch  63, Step:  136000, Batch Loss:     2.560789, Lr: 0.000054, Tokens per sec:   2861
2023-03-15 05:38:02,611 - INFO - __main__ - Epoch  63, Step:  136100, Batch Loss:     3.426632, Lr: 0.000054, Tokens per sec:   2727
2023-03-15 05:38:21,372 - INFO - __main__ - Epoch  63, Step:  136200, Batch Loss:     2.647435, Lr: 0.000054, Tokens per sec:   2887
2023-03-15 05:38:39,942 - INFO - __main__ - Epoch  63, Step:  136300, Batch Loss:     3.631369, Lr: 0.000054, Tokens per sec:   2883
2023-03-15 05:38:58,280 - INFO - __main__ - Epoch  63, Step:  136400, Batch Loss:     3.069211, Lr: 0.000054, Tokens per sec:   2986
2023-03-15 05:39:17,355 - INFO - __main__ - Epoch  63, Step:  136500, Batch Loss:     2.657577, Lr: 0.000054, Tokens per sec:   2838
2023-03-15 05:39:36,534 - INFO - __main__ - Epoch  63, Step:  136600, Batch Loss:     3.532442, Lr: 0.000054, Tokens per sec:   2797
2023-03-15 05:39:55,240 - INFO - __main__ - Epoch  63, Step:  136700, Batch Loss:     2.743905, Lr: 0.000054, Tokens per sec:   2858
2023-03-15 05:40:13,720 - INFO - __main__ - Epoch  63, Step:  136800, Batch Loss:     2.140071, Lr: 0.000054, Tokens per sec:   2901
2023-03-15 05:40:32,454 - INFO - __main__ - Epoch  63, Step:  136900, Batch Loss:     2.571634, Lr: 0.000054, Tokens per sec:   2888
2023-03-15 05:40:51,279 - INFO - __main__ - Epoch  63, Step:  137000, Batch Loss:     3.806815, Lr: 0.000054, Tokens per sec:   2881
2023-03-15 05:41:10,375 - INFO - __main__ - Epoch  63, Step:  137100, Batch Loss:     2.940343, Lr: 0.000054, Tokens per sec:   2819
2023-03-15 05:41:28,815 - INFO - __main__ - Epoch  63, Step:  137200, Batch Loss:     4.402474, Lr: 0.000054, Tokens per sec:   2920
2023-03-15 05:41:43,428 - INFO - __main__ - Epoch  63: total training loss 6637.57
2023-03-15 05:41:43,429 - INFO - __main__ - Epoch 64
2023-03-15 05:41:48,139 - INFO - __main__ - Epoch  64, Step:  137300, Batch Loss:     2.842122, Lr: 0.000053, Tokens per sec:   2579
2023-03-15 05:42:06,632 - INFO - __main__ - Epoch  64, Step:  137400, Batch Loss:     1.737486, Lr: 0.000053, Tokens per sec:   2931
2023-03-15 05:42:25,881 - INFO - __main__ - Epoch  64, Step:  137500, Batch Loss:     2.102348, Lr: 0.000053, Tokens per sec:   2803
2023-03-15 05:42:44,238 - INFO - __main__ - Epoch  64, Step:  137600, Batch Loss:     3.090722, Lr: 0.000053, Tokens per sec:   2938
2023-03-15 05:43:02,888 - INFO - __main__ - Epoch  64, Step:  137700, Batch Loss:     2.765310, Lr: 0.000053, Tokens per sec:   2857
2023-03-15 05:43:22,712 - INFO - __main__ - Epoch  64, Step:  137800, Batch Loss:     2.763114, Lr: 0.000053, Tokens per sec:   2705
2023-03-15 05:43:41,592 - INFO - __main__ - Epoch  64, Step:  137900, Batch Loss:     2.754370, Lr: 0.000053, Tokens per sec:   2934
2023-03-15 05:44:00,723 - INFO - __main__ - Epoch  64, Step:  138000, Batch Loss:     2.732296, Lr: 0.000053, Tokens per sec:   2806
2023-03-15 05:44:20,707 - INFO - __main__ - Epoch  64, Step:  138100, Batch Loss:     4.113183, Lr: 0.000053, Tokens per sec:   2666
2023-03-15 05:44:39,479 - INFO - __main__ - Epoch  64, Step:  138200, Batch Loss:     2.631323, Lr: 0.000053, Tokens per sec:   2862
2023-03-15 05:44:57,792 - INFO - __main__ - Epoch  64, Step:  138300, Batch Loss:     2.771078, Lr: 0.000053, Tokens per sec:   2919
2023-03-15 05:45:16,660 - INFO - __main__ - Epoch  64, Step:  138400, Batch Loss:     2.899909, Lr: 0.000053, Tokens per sec:   2860
2023-03-15 05:45:34,938 - INFO - __main__ - Epoch  64, Step:  138500, Batch Loss:     2.131011, Lr: 0.000053, Tokens per sec:   2983
2023-03-15 05:45:53,922 - INFO - __main__ - Epoch  64, Step:  138600, Batch Loss:     5.165958, Lr: 0.000053, Tokens per sec:   2845
2023-03-15 05:46:12,583 - INFO - __main__ - Epoch  64, Step:  138700, Batch Loss:     2.480428, Lr: 0.000053, Tokens per sec:   2805
2023-03-15 05:46:31,537 - INFO - __main__ - Epoch  64, Step:  138800, Batch Loss:     2.855911, Lr: 0.000053, Tokens per sec:   2825
2023-03-15 05:46:50,038 - INFO - __main__ - Epoch  64, Step:  138900, Batch Loss:     2.681924, Lr: 0.000053, Tokens per sec:   2959
2023-03-15 05:47:08,704 - INFO - __main__ - Epoch  64, Step:  139000, Batch Loss:     3.517118, Lr: 0.000053, Tokens per sec:   2904
2023-03-15 05:47:26,913 - INFO - __main__ - Epoch  64, Step:  139100, Batch Loss:     2.010119, Lr: 0.000053, Tokens per sec:   2978
2023-03-15 05:47:45,982 - INFO - __main__ - Epoch  64, Step:  139200, Batch Loss:     3.702686, Lr: 0.000053, Tokens per sec:   2835
2023-03-15 05:48:04,197 - INFO - __main__ - Epoch  64, Step:  139300, Batch Loss:     3.255021, Lr: 0.000053, Tokens per sec:   2912
2023-03-15 05:48:23,098 - INFO - __main__ - Epoch  64, Step:  139400, Batch Loss:     3.991695, Lr: 0.000053, Tokens per sec:   2880
2023-03-15 05:48:34,328 - INFO - __main__ - Epoch  64: total training loss 6446.44
2023-03-15 05:48:34,328 - INFO - __main__ - Epoch 65
2023-03-15 05:48:43,463 - INFO - __main__ - Epoch  65, Step:  139500, Batch Loss:     2.021051, Lr: 0.000053, Tokens per sec:   2606
2023-03-15 05:49:02,185 - INFO - __main__ - Epoch  65, Step:  139600, Batch Loss:     3.379544, Lr: 0.000053, Tokens per sec:   2854
2023-03-15 05:49:20,840 - INFO - __main__ - Epoch  65, Step:  139700, Batch Loss:     2.115891, Lr: 0.000053, Tokens per sec:   2866
2023-03-15 05:49:40,085 - INFO - __main__ - Epoch  65, Step:  139800, Batch Loss:     2.952809, Lr: 0.000053, Tokens per sec:   2802
2023-03-15 05:49:58,976 - INFO - __main__ - Epoch  65, Step:  139900, Batch Loss:     3.440741, Lr: 0.000053, Tokens per sec:   2833
2023-03-15 05:50:17,703 - INFO - __main__ - Epoch  65, Step:  140000, Batch Loss:     2.384295, Lr: 0.000053, Tokens per sec:   2856
2023-03-15 05:50:36,360 - INFO - __main__ - Epoch  65, Step:  140100, Batch Loss:     2.714224, Lr: 0.000053, Tokens per sec:   2911
2023-03-15 05:50:55,529 - INFO - __main__ - Epoch  65, Step:  140200, Batch Loss:     2.098055, Lr: 0.000053, Tokens per sec:   2780
2023-03-15 05:51:14,586 - INFO - __main__ - Epoch  65, Step:  140300, Batch Loss:     2.951047, Lr: 0.000053, Tokens per sec:   2809
2023-03-15 05:51:33,222 - INFO - __main__ - Epoch  65, Step:  140400, Batch Loss:     3.134573, Lr: 0.000053, Tokens per sec:   2871
2023-03-15 05:51:52,486 - INFO - __main__ - Epoch  65, Step:  140500, Batch Loss:     2.627670, Lr: 0.000053, Tokens per sec:   2789
2023-03-15 05:52:12,133 - INFO - __main__ - Epoch  65, Step:  140600, Batch Loss:     2.128223, Lr: 0.000053, Tokens per sec:   2749
2023-03-15 05:52:31,682 - INFO - __main__ - Epoch  65, Step:  140700, Batch Loss:     3.081041, Lr: 0.000053, Tokens per sec:   2794
2023-03-15 05:52:50,467 - INFO - __main__ - Epoch  65, Step:  140800, Batch Loss:     2.967482, Lr: 0.000053, Tokens per sec:   2799
2023-03-15 05:53:08,771 - INFO - __main__ - Epoch  65, Step:  140900, Batch Loss:     2.911500, Lr: 0.000053, Tokens per sec:   2965
2023-03-15 05:53:27,365 - INFO - __main__ - Epoch  65, Step:  141000, Batch Loss:     3.999795, Lr: 0.000053, Tokens per sec:   2940
2023-03-15 05:53:45,968 - INFO - __main__ - Epoch  65, Step:  141100, Batch Loss:     3.439867, Lr: 0.000053, Tokens per sec:   2908
2023-03-15 05:54:04,931 - INFO - __main__ - Epoch  65, Step:  141200, Batch Loss:     3.313792, Lr: 0.000053, Tokens per sec:   2819
2023-03-15 05:54:23,822 - INFO - __main__ - Epoch  65, Step:  141300, Batch Loss:     2.505265, Lr: 0.000053, Tokens per sec:   2905
2023-03-15 05:54:43,281 - INFO - __main__ - Epoch  65, Step:  141400, Batch Loss:     3.195192, Lr: 0.000053, Tokens per sec:   2762
2023-03-15 05:55:02,234 - INFO - __main__ - Epoch  65, Step:  141500, Batch Loss:     1.813327, Lr: 0.000053, Tokens per sec:   2848
2023-03-15 05:55:20,866 - INFO - __main__ - Epoch  65, Step:  141600, Batch Loss:     2.769034, Lr: 0.000053, Tokens per sec:   2895
2023-03-15 05:55:27,450 - INFO - __main__ - Epoch  65: total training loss 6279.73
2023-03-15 05:55:27,451 - INFO - __main__ - Epoch 66
2023-03-15 05:55:40,134 - INFO - __main__ - Epoch  66, Step:  141700, Batch Loss:     2.537063, Lr: 0.000052, Tokens per sec:   2745
2023-03-15 05:55:58,579 - INFO - __main__ - Epoch  66, Step:  141800, Batch Loss:     3.729232, Lr: 0.000052, Tokens per sec:   2926
2023-03-15 05:56:17,435 - INFO - __main__ - Epoch  66, Step:  141900, Batch Loss:     1.459296, Lr: 0.000052, Tokens per sec:   2895
2023-03-15 05:56:36,363 - INFO - __main__ - Epoch  66, Step:  142000, Batch Loss:     2.866831, Lr: 0.000052, Tokens per sec:   2893
2023-03-15 05:56:55,202 - INFO - __main__ - Epoch  66, Step:  142100, Batch Loss:     1.798849, Lr: 0.000052, Tokens per sec:   2857
2023-03-15 05:57:13,638 - INFO - __main__ - Epoch  66, Step:  142200, Batch Loss:     2.804603, Lr: 0.000052, Tokens per sec:   2947
2023-03-15 05:57:32,749 - INFO - __main__ - Epoch  66, Step:  142300, Batch Loss:     2.849111, Lr: 0.000052, Tokens per sec:   2839
2023-03-15 05:57:52,298 - INFO - __main__ - Epoch  66, Step:  142400, Batch Loss:     1.820026, Lr: 0.000052, Tokens per sec:   2721
2023-03-15 05:58:11,298 - INFO - __main__ - Epoch  66, Step:  142500, Batch Loss:     2.812438, Lr: 0.000052, Tokens per sec:   2871
2023-03-15 05:58:30,149 - INFO - __main__ - Epoch  66, Step:  142600, Batch Loss:     2.751750, Lr: 0.000052, Tokens per sec:   2863
2023-03-15 05:58:49,347 - INFO - __main__ - Epoch  66, Step:  142700, Batch Loss:     2.046079, Lr: 0.000052, Tokens per sec:   2793
2023-03-15 05:59:08,123 - INFO - __main__ - Epoch  66, Step:  142800, Batch Loss:     2.732997, Lr: 0.000052, Tokens per sec:   2890
2023-03-15 05:59:26,394 - INFO - __main__ - Epoch  66, Step:  142900, Batch Loss:     3.776390, Lr: 0.000052, Tokens per sec:   2928
2023-03-15 05:59:45,268 - INFO - __main__ - Epoch  66, Step:  143000, Batch Loss:     4.082084, Lr: 0.000052, Tokens per sec:   2834
2023-03-15 06:00:04,458 - INFO - __main__ - Epoch  66, Step:  143100, Batch Loss:     3.167938, Lr: 0.000052, Tokens per sec:   2780
2023-03-15 06:00:23,519 - INFO - __main__ - Epoch  66, Step:  143200, Batch Loss:     3.518124, Lr: 0.000052, Tokens per sec:   2876
2023-03-15 06:00:42,365 - INFO - __main__ - Epoch  66, Step:  143300, Batch Loss:     3.210762, Lr: 0.000052, Tokens per sec:   2800
2023-03-15 06:01:01,908 - INFO - __main__ - Epoch  66, Step:  143400, Batch Loss:     2.535018, Lr: 0.000052, Tokens per sec:   2771
2023-03-15 06:01:21,283 - INFO - __main__ - Epoch  66, Step:  143500, Batch Loss:     4.050275, Lr: 0.000052, Tokens per sec:   2741
2023-03-15 06:01:39,932 - INFO - __main__ - Epoch  66, Step:  143600, Batch Loss:     3.427170, Lr: 0.000052, Tokens per sec:   2819
2023-03-15 06:01:59,065 - INFO - __main__ - Epoch  66, Step:  143700, Batch Loss:     2.841434, Lr: 0.000052, Tokens per sec:   2852
2023-03-15 06:02:18,284 - INFO - __main__ - Epoch  66, Step:  143800, Batch Loss:     2.379697, Lr: 0.000052, Tokens per sec:   2764
2023-03-15 06:02:20,966 - INFO - __main__ - Epoch  66: total training loss 6147.24
2023-03-15 06:02:20,967 - INFO - __main__ - Epoch 67
2023-03-15 06:02:38,132 - INFO - __main__ - Epoch  67, Step:  143900, Batch Loss:     2.533513, Lr: 0.000052, Tokens per sec:   2709
2023-03-15 06:02:57,032 - INFO - __main__ - Epoch  67, Step:  144000, Batch Loss:     1.749474, Lr: 0.000052, Tokens per sec:   2901
2023-03-15 06:03:16,323 - INFO - __main__ - Epoch  67, Step:  144100, Batch Loss:     2.758308, Lr: 0.000052, Tokens per sec:   2796
2023-03-15 06:03:35,580 - INFO - __main__ - Epoch  67, Step:  144200, Batch Loss:     2.117926, Lr: 0.000052, Tokens per sec:   2761
2023-03-15 06:03:54,509 - INFO - __main__ - Epoch  67, Step:  144300, Batch Loss:     2.595749, Lr: 0.000052, Tokens per sec:   2844
2023-03-15 06:04:13,825 - INFO - __main__ - Epoch  67, Step:  144400, Batch Loss:     1.679826, Lr: 0.000052, Tokens per sec:   2747
2023-03-15 06:04:32,862 - INFO - __main__ - Epoch  67, Step:  144500, Batch Loss:     2.948777, Lr: 0.000052, Tokens per sec:   2788
2023-03-15 06:04:52,278 - INFO - __main__ - Epoch  67, Step:  144600, Batch Loss:     2.588346, Lr: 0.000052, Tokens per sec:   2754
2023-03-15 06:05:10,545 - INFO - __main__ - Epoch  67, Step:  144700, Batch Loss:     2.067362, Lr: 0.000052, Tokens per sec:   2982
2023-03-15 06:05:29,618 - INFO - __main__ - Epoch  67, Step:  144800, Batch Loss:     2.953971, Lr: 0.000052, Tokens per sec:   2823
2023-03-15 06:05:48,552 - INFO - __main__ - Epoch  67, Step:  144900, Batch Loss:     2.524575, Lr: 0.000052, Tokens per sec:   2848
2023-03-15 06:06:08,322 - INFO - __main__ - Epoch  67, Step:  145000, Batch Loss:     3.152977, Lr: 0.000052, Tokens per sec:   2755
2023-03-15 06:06:28,143 - INFO - __main__ - Epoch  67, Step:  145100, Batch Loss:     3.935950, Lr: 0.000052, Tokens per sec:   2703
2023-03-15 06:06:47,407 - INFO - __main__ - Epoch  67, Step:  145200, Batch Loss:     2.260002, Lr: 0.000052, Tokens per sec:   2753
2023-03-15 06:07:06,160 - INFO - __main__ - Epoch  67, Step:  145300, Batch Loss:     1.939186, Lr: 0.000052, Tokens per sec:   2855
2023-03-15 06:07:25,517 - INFO - __main__ - Epoch  67, Step:  145400, Batch Loss:     2.857058, Lr: 0.000052, Tokens per sec:   2803
2023-03-15 06:07:45,310 - INFO - __main__ - Epoch  67, Step:  145500, Batch Loss:     2.662386, Lr: 0.000052, Tokens per sec:   2723
2023-03-15 06:08:04,413 - INFO - __main__ - Epoch  67, Step:  145600, Batch Loss:     2.801284, Lr: 0.000052, Tokens per sec:   2840
2023-03-15 06:08:23,451 - INFO - __main__ - Epoch  67, Step:  145700, Batch Loss:     2.978924, Lr: 0.000052, Tokens per sec:   2833
2023-03-15 06:08:42,274 - INFO - __main__ - Epoch  67, Step:  145800, Batch Loss:     2.722487, Lr: 0.000052, Tokens per sec:   2855
2023-03-15 06:09:01,105 - INFO - __main__ - Epoch  67, Step:  145900, Batch Loss:     3.876060, Lr: 0.000052, Tokens per sec:   2879
2023-03-15 06:09:18,456 - INFO - __main__ - Epoch  67: total training loss 6013.51
2023-03-15 06:09:18,457 - INFO - __main__ - Epoch 68
2023-03-15 06:09:20,222 - INFO - __main__ - Epoch  68, Step:  146000, Batch Loss:     3.093399, Lr: 0.000051, Tokens per sec:   2331
2023-03-15 06:09:39,555 - INFO - __main__ - Epoch  68, Step:  146100, Batch Loss:     3.046346, Lr: 0.000051, Tokens per sec:   2789
2023-03-15 06:09:58,507 - INFO - __main__ - Epoch  68, Step:  146200, Batch Loss:     2.955516, Lr: 0.000051, Tokens per sec:   2856
2023-03-15 06:10:17,804 - INFO - __main__ - Epoch  68, Step:  146300, Batch Loss:     2.264183, Lr: 0.000051, Tokens per sec:   2779
2023-03-15 06:10:37,204 - INFO - __main__ - Epoch  68, Step:  146400, Batch Loss:     1.699507, Lr: 0.000051, Tokens per sec:   2748
2023-03-15 06:10:55,554 - INFO - __main__ - Epoch  68, Step:  146500, Batch Loss:     2.419149, Lr: 0.000051, Tokens per sec:   2934
2023-03-15 06:11:14,720 - INFO - __main__ - Epoch  68, Step:  146600, Batch Loss:     3.292588, Lr: 0.000051, Tokens per sec:   2825
2023-03-15 06:11:33,202 - INFO - __main__ - Epoch  68, Step:  146700, Batch Loss:     2.758432, Lr: 0.000051, Tokens per sec:   2946
2023-03-15 06:11:51,920 - INFO - __main__ - Epoch  68, Step:  146800, Batch Loss:     2.785677, Lr: 0.000051, Tokens per sec:   2847
2023-03-15 06:12:11,521 - INFO - __main__ - Epoch  68, Step:  146900, Batch Loss:     2.168413, Lr: 0.000051, Tokens per sec:   2751
2023-03-15 06:12:30,346 - INFO - __main__ - Epoch  68, Step:  147000, Batch Loss:     3.060302, Lr: 0.000051, Tokens per sec:   2843
2023-03-15 06:12:49,238 - INFO - __main__ - Epoch  68, Step:  147100, Batch Loss:     2.228929, Lr: 0.000051, Tokens per sec:   2845
2023-03-15 06:13:07,922 - INFO - __main__ - Epoch  68, Step:  147200, Batch Loss:     3.056773, Lr: 0.000051, Tokens per sec:   2907
2023-03-15 06:13:26,776 - INFO - __main__ - Epoch  68, Step:  147300, Batch Loss:     3.193422, Lr: 0.000051, Tokens per sec:   2867
2023-03-15 06:13:45,396 - INFO - __main__ - Epoch  68, Step:  147400, Batch Loss:     2.228107, Lr: 0.000051, Tokens per sec:   2819
2023-03-15 06:14:03,950 - INFO - __main__ - Epoch  68, Step:  147500, Batch Loss:     3.179988, Lr: 0.000051, Tokens per sec:   2935
2023-03-15 06:14:22,206 - INFO - __main__ - Epoch  68, Step:  147600, Batch Loss:     3.145542, Lr: 0.000051, Tokens per sec:   3005
2023-03-15 06:14:41,354 - INFO - __main__ - Epoch  68, Step:  147700, Batch Loss:     2.701931, Lr: 0.000051, Tokens per sec:   2791
2023-03-15 06:15:00,490 - INFO - __main__ - Epoch  68, Step:  147800, Batch Loss:     3.303918, Lr: 0.000051, Tokens per sec:   2819
2023-03-15 06:15:20,326 - INFO - __main__ - Epoch  68, Step:  147900, Batch Loss:     2.745978, Lr: 0.000051, Tokens per sec:   2680
2023-03-15 06:15:39,458 - INFO - __main__ - Epoch  68, Step:  148000, Batch Loss:     2.671050, Lr: 0.000051, Tokens per sec:   2816
2023-03-15 06:15:58,294 - INFO - __main__ - Epoch  68, Step:  148100, Batch Loss:     2.325651, Lr: 0.000051, Tokens per sec:   2859
2023-03-15 06:16:11,707 - INFO - __main__ - Epoch  68: total training loss 5862.90
2023-03-15 06:16:11,708 - INFO - __main__ - Epoch 69
2023-03-15 06:16:17,646 - INFO - __main__ - Epoch  69, Step:  148200, Batch Loss:     2.622125, Lr: 0.000050, Tokens per sec:   2558
2023-03-15 06:16:36,956 - INFO - __main__ - Epoch  69, Step:  148300, Batch Loss:     1.759933, Lr: 0.000050, Tokens per sec:   2771
2023-03-15 06:16:56,711 - INFO - __main__ - Epoch  69, Step:  148400, Batch Loss:     2.996613, Lr: 0.000050, Tokens per sec:   2714
2023-03-15 06:17:15,546 - INFO - __main__ - Epoch  69, Step:  148500, Batch Loss:     2.887200, Lr: 0.000050, Tokens per sec:   2838
2023-03-15 06:17:34,922 - INFO - __main__ - Epoch  69, Step:  148600, Batch Loss:     2.115533, Lr: 0.000050, Tokens per sec:   2798
2023-03-15 06:17:53,810 - INFO - __main__ - Epoch  69, Step:  148700, Batch Loss:     1.760954, Lr: 0.000050, Tokens per sec:   2822
2023-03-15 06:18:12,678 - INFO - __main__ - Epoch  69, Step:  148800, Batch Loss:     2.256049, Lr: 0.000050, Tokens per sec:   2857
2023-03-15 06:18:31,488 - INFO - __main__ - Epoch  69, Step:  148900, Batch Loss:     2.306036, Lr: 0.000050, Tokens per sec:   2824
2023-03-15 06:18:50,332 - INFO - __main__ - Epoch  69, Step:  149000, Batch Loss:     2.814026, Lr: 0.000050, Tokens per sec:   2900
2023-03-15 06:19:08,875 - INFO - __main__ - Epoch  69, Step:  149100, Batch Loss:     2.292375, Lr: 0.000050, Tokens per sec:   2899
2023-03-15 06:19:27,219 - INFO - __main__ - Epoch  69, Step:  149200, Batch Loss:     2.140545, Lr: 0.000050, Tokens per sec:   2955
2023-03-15 06:19:46,256 - INFO - __main__ - Epoch  69, Step:  149300, Batch Loss:     3.496658, Lr: 0.000050, Tokens per sec:   2864
2023-03-15 06:20:04,838 - INFO - __main__ - Epoch  69, Step:  149400, Batch Loss:     1.623820, Lr: 0.000050, Tokens per sec:   2890
2023-03-15 06:20:23,627 - INFO - __main__ - Epoch  69, Step:  149500, Batch Loss:     2.333481, Lr: 0.000050, Tokens per sec:   2869
2023-03-15 06:20:42,788 - INFO - __main__ - Epoch  69, Step:  149600, Batch Loss:     2.024986, Lr: 0.000050, Tokens per sec:   2828
2023-03-15 06:21:01,798 - INFO - __main__ - Epoch  69, Step:  149700, Batch Loss:     3.180896, Lr: 0.000050, Tokens per sec:   2860
2023-03-15 06:21:20,267 - INFO - __main__ - Epoch  69, Step:  149800, Batch Loss:     2.855389, Lr: 0.000050, Tokens per sec:   2930
2023-03-15 06:21:39,293 - INFO - __main__ - Epoch  69, Step:  149900, Batch Loss:     1.103514, Lr: 0.000050, Tokens per sec:   2825
2023-03-15 06:21:58,639 - INFO - __main__ - Epoch  69, Step:  150000, Batch Loss:     2.881617, Lr: 0.000050, Tokens per sec:   2787
2023-03-15 06:22:17,314 - INFO - __main__ - Epoch  69, Step:  150100, Batch Loss:     2.975286, Lr: 0.000050, Tokens per sec:   2906
2023-03-15 06:22:36,092 - INFO - __main__ - Epoch  69, Step:  150200, Batch Loss:     2.470607, Lr: 0.000050, Tokens per sec:   2847
2023-03-15 06:22:54,960 - INFO - __main__ - Epoch  69, Step:  150300, Batch Loss:     3.223762, Lr: 0.000050, Tokens per sec:   2794
2023-03-15 06:23:04,856 - INFO - __main__ - Epoch  69: total training loss 5735.45
2023-03-15 06:23:04,857 - INFO - __main__ - Epoch 70
2023-03-15 06:23:14,545 - INFO - __main__ - Epoch  70, Step:  150400, Batch Loss:     2.181207, Lr: 0.000050, Tokens per sec:   2693
2023-03-15 06:23:33,047 - INFO - __main__ - Epoch  70, Step:  150500, Batch Loss:     2.244487, Lr: 0.000050, Tokens per sec:   2927
2023-03-15 06:23:51,276 - INFO - __main__ - Epoch  70, Step:  150600, Batch Loss:     2.552946, Lr: 0.000050, Tokens per sec:   2982
2023-03-15 06:24:09,896 - INFO - __main__ - Epoch  70, Step:  150700, Batch Loss:     2.591576, Lr: 0.000050, Tokens per sec:   2880
2023-03-15 06:24:29,413 - INFO - __main__ - Epoch  70, Step:  150800, Batch Loss:     2.348095, Lr: 0.000050, Tokens per sec:   2710
2023-03-15 06:24:48,766 - INFO - __main__ - Epoch  70, Step:  150900, Batch Loss:     3.549644, Lr: 0.000050, Tokens per sec:   2812
2023-03-15 06:25:08,344 - INFO - __main__ - Epoch  70, Step:  151000, Batch Loss:     2.576134, Lr: 0.000050, Tokens per sec:   2771
2023-03-15 06:25:27,188 - INFO - __main__ - Epoch  70, Step:  151100, Batch Loss:     2.711982, Lr: 0.000050, Tokens per sec:   2907
2023-03-15 06:25:46,258 - INFO - __main__ - Epoch  70, Step:  151200, Batch Loss:     3.047253, Lr: 0.000050, Tokens per sec:   2745
2023-03-15 06:26:05,176 - INFO - __main__ - Epoch  70, Step:  151300, Batch Loss:     3.321492, Lr: 0.000050, Tokens per sec:   2833
2023-03-15 06:26:23,787 - INFO - __main__ - Epoch  70, Step:  151400, Batch Loss:     2.028724, Lr: 0.000050, Tokens per sec:   2868
2023-03-15 06:26:42,787 - INFO - __main__ - Epoch  70, Step:  151500, Batch Loss:     2.499146, Lr: 0.000050, Tokens per sec:   2856
2023-03-15 06:27:01,301 - INFO - __main__ - Epoch  70, Step:  151600, Batch Loss:     2.980200, Lr: 0.000050, Tokens per sec:   2917
2023-03-15 06:27:20,051 - INFO - __main__ - Epoch  70, Step:  151700, Batch Loss:     2.857827, Lr: 0.000050, Tokens per sec:   2862
2023-03-15 06:27:39,603 - INFO - __main__ - Epoch  70, Step:  151800, Batch Loss:     2.461365, Lr: 0.000050, Tokens per sec:   2687
2023-03-15 06:27:58,745 - INFO - __main__ - Epoch  70, Step:  151900, Batch Loss:     2.325598, Lr: 0.000050, Tokens per sec:   2839
2023-03-15 06:28:17,575 - INFO - __main__ - Epoch  70, Step:  152000, Batch Loss:     3.094769, Lr: 0.000050, Tokens per sec:   2841
2023-03-15 06:28:36,179 - INFO - __main__ - Epoch  70, Step:  152100, Batch Loss:     2.701839, Lr: 0.000050, Tokens per sec:   2875
2023-03-15 06:28:54,950 - INFO - __main__ - Epoch  70, Step:  152200, Batch Loss:     2.919281, Lr: 0.000050, Tokens per sec:   2875
2023-03-15 06:29:13,538 - INFO - __main__ - Epoch  70, Step:  152300, Batch Loss:     2.567847, Lr: 0.000050, Tokens per sec:   2921
2023-03-15 06:29:32,336 - INFO - __main__ - Epoch  70, Step:  152400, Batch Loss:     3.147752, Lr: 0.000050, Tokens per sec:   2914
2023-03-15 06:29:51,455 - INFO - __main__ - Epoch  70, Step:  152500, Batch Loss:     2.231550, Lr: 0.000050, Tokens per sec:   2839
2023-03-15 06:29:56,966 - INFO - __main__ - Epoch  70: total training loss 5618.66
2023-03-15 06:29:56,967 - INFO - __main__ - Epoch 71
2023-03-15 06:30:10,897 - INFO - __main__ - Epoch  71, Step:  152600, Batch Loss:     3.114209, Lr: 0.000049, Tokens per sec:   2644
2023-03-15 06:30:29,770 - INFO - __main__ - Epoch  71, Step:  152700, Batch Loss:     2.308683, Lr: 0.000049, Tokens per sec:   2878
2023-03-15 06:30:49,767 - INFO - __main__ - Epoch  71, Step:  152800, Batch Loss:     2.475366, Lr: 0.000049, Tokens per sec:   2653
2023-03-15 06:31:09,860 - INFO - __main__ - Epoch  71, Step:  152900, Batch Loss:     1.977310, Lr: 0.000049, Tokens per sec:   2674
2023-03-15 06:31:28,903 - INFO - __main__ - Epoch  71, Step:  153000, Batch Loss:     2.088324, Lr: 0.000049, Tokens per sec:   2801
2023-03-15 06:31:47,060 - INFO - __main__ - Epoch  71, Step:  153100, Batch Loss:     1.831215, Lr: 0.000049, Tokens per sec:   2952
2023-03-15 06:32:06,043 - INFO - __main__ - Epoch  71, Step:  153200, Batch Loss:     1.767530, Lr: 0.000049, Tokens per sec:   2830
2023-03-15 06:32:25,129 - INFO - __main__ - Epoch  71, Step:  153300, Batch Loss:     2.180565, Lr: 0.000049, Tokens per sec:   2807
2023-03-15 06:32:44,295 - INFO - __main__ - Epoch  71, Step:  153400, Batch Loss:     2.931683, Lr: 0.000049, Tokens per sec:   2831
2023-03-15 06:33:03,411 - INFO - __main__ - Epoch  71, Step:  153500, Batch Loss:     2.190315, Lr: 0.000049, Tokens per sec:   2846
2023-03-15 06:33:22,044 - INFO - __main__ - Epoch  71, Step:  153600, Batch Loss:     3.408168, Lr: 0.000049, Tokens per sec:   2917
2023-03-15 06:33:40,852 - INFO - __main__ - Epoch  71, Step:  153700, Batch Loss:     2.983762, Lr: 0.000049, Tokens per sec:   2874
2023-03-15 06:34:00,391 - INFO - __main__ - Epoch  71, Step:  153800, Batch Loss:     1.548659, Lr: 0.000049, Tokens per sec:   2739
2023-03-15 06:34:20,006 - INFO - __main__ - Epoch  71, Step:  153900, Batch Loss:     2.238159, Lr: 0.000049, Tokens per sec:   2732
2023-03-15 06:34:39,482 - INFO - __main__ - Epoch  71, Step:  154000, Batch Loss:     2.592113, Lr: 0.000049, Tokens per sec:   2741
2023-03-15 06:34:58,192 - INFO - __main__ - Epoch  71, Step:  154100, Batch Loss:     2.808457, Lr: 0.000049, Tokens per sec:   2873
2023-03-15 06:35:16,607 - INFO - __main__ - Epoch  71, Step:  154200, Batch Loss:     2.088135, Lr: 0.000049, Tokens per sec:   2893
2023-03-15 06:35:35,218 - INFO - __main__ - Epoch  71, Step:  154300, Batch Loss:     3.150347, Lr: 0.000049, Tokens per sec:   2910
2023-03-15 06:35:53,506 - INFO - __main__ - Epoch  71, Step:  154400, Batch Loss:     3.595122, Lr: 0.000049, Tokens per sec:   2963
2023-03-15 06:36:12,174 - INFO - __main__ - Epoch  71, Step:  154500, Batch Loss:     2.916856, Lr: 0.000049, Tokens per sec:   2912
2023-03-15 06:36:30,619 - INFO - __main__ - Epoch  71, Step:  154600, Batch Loss:     1.953493, Lr: 0.000049, Tokens per sec:   2943
2023-03-15 06:36:50,196 - INFO - __main__ - Epoch  71, Step:  154700, Batch Loss:     3.110955, Lr: 0.000049, Tokens per sec:   2772
2023-03-15 06:36:51,961 - INFO - __main__ - Epoch  71: total training loss 5498.32
2023-03-15 06:36:51,962 - INFO - __main__ - Epoch 72
2023-03-15 06:37:09,554 - INFO - __main__ - Epoch  72, Step:  154800, Batch Loss:     1.428292, Lr: 0.000049, Tokens per sec:   2783
2023-03-15 06:37:28,767 - INFO - __main__ - Epoch  72, Step:  154900, Batch Loss:     2.179180, Lr: 0.000049, Tokens per sec:   2812
2023-03-15 06:37:47,769 - INFO - __main__ - Epoch  72, Step:  155000, Batch Loss:     1.927937, Lr: 0.000049, Tokens per sec:   2816
2023-03-15 06:38:07,648 - INFO - __main__ - Epoch  72, Step:  155100, Batch Loss:     2.263686, Lr: 0.000049, Tokens per sec:   2700
2023-03-15 06:38:27,346 - INFO - __main__ - Epoch  72, Step:  155200, Batch Loss:     1.864700, Lr: 0.000049, Tokens per sec:   2757
2023-03-15 06:38:45,743 - INFO - __main__ - Epoch  72, Step:  155300, Batch Loss:     2.612374, Lr: 0.000049, Tokens per sec:   2971
2023-03-15 06:39:04,573 - INFO - __main__ - Epoch  72, Step:  155400, Batch Loss:     2.396660, Lr: 0.000049, Tokens per sec:   2860
2023-03-15 06:39:24,044 - INFO - __main__ - Epoch  72, Step:  155500, Batch Loss:     2.000613, Lr: 0.000049, Tokens per sec:   2764
2023-03-15 06:39:42,483 - INFO - __main__ - Epoch  72, Step:  155600, Batch Loss:     1.358949, Lr: 0.000049, Tokens per sec:   2969
2023-03-15 06:40:02,175 - INFO - __main__ - Epoch  72, Step:  155700, Batch Loss:     1.401940, Lr: 0.000049, Tokens per sec:   2733
2023-03-15 06:40:21,643 - INFO - __main__ - Epoch  72, Step:  155800, Batch Loss:     2.692953, Lr: 0.000049, Tokens per sec:   2769
2023-03-15 06:40:40,646 - INFO - __main__ - Epoch  72, Step:  155900, Batch Loss:     2.269136, Lr: 0.000049, Tokens per sec:   2844
2023-03-15 06:40:59,858 - INFO - __main__ - Epoch  72, Step:  156000, Batch Loss:     2.247027, Lr: 0.000049, Tokens per sec:   2781
2023-03-15 06:41:18,674 - INFO - __main__ - Epoch  72, Step:  156100, Batch Loss:     2.016727, Lr: 0.000049, Tokens per sec:   2825
2023-03-15 06:41:37,868 - INFO - __main__ - Epoch  72, Step:  156200, Batch Loss:     3.074790, Lr: 0.000049, Tokens per sec:   2862
2023-03-15 06:41:56,376 - INFO - __main__ - Epoch  72, Step:  156300, Batch Loss:     2.208183, Lr: 0.000049, Tokens per sec:   2903
2023-03-15 06:42:14,861 - INFO - __main__ - Epoch  72, Step:  156400, Batch Loss:     3.811052, Lr: 0.000049, Tokens per sec:   2912
2023-03-15 06:42:33,664 - INFO - __main__ - Epoch  72, Step:  156500, Batch Loss:     2.842579, Lr: 0.000049, Tokens per sec:   2850
2023-03-15 06:42:52,896 - INFO - __main__ - Epoch  72, Step:  156600, Batch Loss:     3.085768, Lr: 0.000049, Tokens per sec:   2770
2023-03-15 06:43:11,117 - INFO - __main__ - Epoch  72, Step:  156700, Batch Loss:     2.171844, Lr: 0.000049, Tokens per sec:   2923
2023-03-15 06:43:29,565 - INFO - __main__ - Epoch  72, Step:  156800, Batch Loss:     3.343783, Lr: 0.000049, Tokens per sec:   2932
2023-03-15 06:43:45,912 - INFO - __main__ - Epoch  72: total training loss 5346.17
2023-03-15 06:43:45,913 - INFO - __main__ - Epoch 73
2023-03-15 06:43:48,662 - INFO - __main__ - Epoch  73, Step:  156900, Batch Loss:     1.807081, Lr: 0.000048, Tokens per sec:   2352
2023-03-15 06:44:08,249 - INFO - __main__ - Epoch  73, Step:  157000, Batch Loss:     2.287079, Lr: 0.000048, Tokens per sec:   2755
2023-03-15 06:44:27,328 - INFO - __main__ - Epoch  73, Step:  157100, Batch Loss:     1.803904, Lr: 0.000048, Tokens per sec:   2803
2023-03-15 06:44:47,165 - INFO - __main__ - Epoch  73, Step:  157200, Batch Loss:     2.620394, Lr: 0.000048, Tokens per sec:   2765
2023-03-15 06:45:05,903 - INFO - __main__ - Epoch  73, Step:  157300, Batch Loss:     2.286635, Lr: 0.000048, Tokens per sec:   2877
2023-03-15 06:45:24,091 - INFO - __main__ - Epoch  73, Step:  157400, Batch Loss:     1.412583, Lr: 0.000048, Tokens per sec:   2965
2023-03-15 06:45:43,166 - INFO - __main__ - Epoch  73, Step:  157500, Batch Loss:     2.110081, Lr: 0.000048, Tokens per sec:   2804
2023-03-15 06:46:02,004 - INFO - __main__ - Epoch  73, Step:  157600, Batch Loss:     2.476484, Lr: 0.000048, Tokens per sec:   2856
2023-03-15 06:46:21,138 - INFO - __main__ - Epoch  73, Step:  157700, Batch Loss:     1.973609, Lr: 0.000048, Tokens per sec:   2805
2023-03-15 06:46:39,531 - INFO - __main__ - Epoch  73, Step:  157800, Batch Loss:     2.812326, Lr: 0.000048, Tokens per sec:   2896
2023-03-15 06:46:58,479 - INFO - __main__ - Epoch  73, Step:  157900, Batch Loss:     2.232647, Lr: 0.000048, Tokens per sec:   2842
2023-03-15 06:47:18,478 - INFO - __main__ - Epoch  73, Step:  158000, Batch Loss:     2.035592, Lr: 0.000048, Tokens per sec:   2708
2023-03-15 06:47:37,783 - INFO - __main__ - Epoch  73, Step:  158100, Batch Loss:     2.356716, Lr: 0.000048, Tokens per sec:   2843
2023-03-15 06:47:56,405 - INFO - __main__ - Epoch  73, Step:  158200, Batch Loss:     3.054536, Lr: 0.000048, Tokens per sec:   2901
2023-03-15 06:48:15,247 - INFO - __main__ - Epoch  73, Step:  158300, Batch Loss:     2.594370, Lr: 0.000048, Tokens per sec:   2850
2023-03-15 06:48:33,370 - INFO - __main__ - Epoch  73, Step:  158400, Batch Loss:     3.101204, Lr: 0.000048, Tokens per sec:   2986
2023-03-15 06:48:52,224 - INFO - __main__ - Epoch  73, Step:  158500, Batch Loss:     2.152871, Lr: 0.000048, Tokens per sec:   2845
2023-03-15 06:49:11,084 - INFO - __main__ - Epoch  73, Step:  158600, Batch Loss:     3.125049, Lr: 0.000048, Tokens per sec:   2836
2023-03-15 06:49:30,116 - INFO - __main__ - Epoch  73, Step:  158700, Batch Loss:     1.802974, Lr: 0.000048, Tokens per sec:   2795
2023-03-15 06:49:49,023 - INFO - __main__ - Epoch  73, Step:  158800, Batch Loss:     2.790248, Lr: 0.000048, Tokens per sec:   2875
2023-03-15 06:50:07,469 - INFO - __main__ - Epoch  73, Step:  158900, Batch Loss:     2.668679, Lr: 0.000048, Tokens per sec:   2884
2023-03-15 06:50:26,663 - INFO - __main__ - Epoch  73, Step:  159000, Batch Loss:     1.954841, Lr: 0.000048, Tokens per sec:   2801
2023-03-15 06:50:39,428 - INFO - __main__ - Epoch  73: total training loss 5239.67
2023-03-15 06:50:39,429 - INFO - __main__ - Epoch 74
2023-03-15 06:50:46,098 - INFO - __main__ - Epoch  74, Step:  159100, Batch Loss:     2.522649, Lr: 0.000048, Tokens per sec:   2767
2023-03-15 06:51:05,759 - INFO - __main__ - Epoch  74, Step:  159200, Batch Loss:     1.772026, Lr: 0.000048, Tokens per sec:   2757
2023-03-15 06:51:25,302 - INFO - __main__ - Epoch  74, Step:  159300, Batch Loss:     2.044431, Lr: 0.000048, Tokens per sec:   2757
2023-03-15 06:51:44,233 - INFO - __main__ - Epoch  74, Step:  159400, Batch Loss:     2.414552, Lr: 0.000048, Tokens per sec:   2885
2023-03-15 06:52:03,304 - INFO - __main__ - Epoch  74, Step:  159500, Batch Loss:     2.700280, Lr: 0.000048, Tokens per sec:   2842
2023-03-15 06:52:22,257 - INFO - __main__ - Epoch  74, Step:  159600, Batch Loss:     2.015580, Lr: 0.000048, Tokens per sec:   2859
2023-03-15 06:52:41,250 - INFO - __main__ - Epoch  74, Step:  159700, Batch Loss:     2.738563, Lr: 0.000048, Tokens per sec:   2811
2023-03-15 06:53:01,104 - INFO - __main__ - Epoch  74, Step:  159800, Batch Loss:     2.024777, Lr: 0.000048, Tokens per sec:   2740
2023-03-15 06:53:20,577 - INFO - __main__ - Epoch  74, Step:  159900, Batch Loss:     2.551355, Lr: 0.000048, Tokens per sec:   2783
2023-03-15 06:53:39,951 - INFO - __main__ - Epoch  74, Step:  160000, Batch Loss:     2.358921, Lr: 0.000048, Tokens per sec:   2776
2023-03-15 06:53:58,885 - INFO - __main__ - Epoch  74, Step:  160100, Batch Loss:     2.682630, Lr: 0.000048, Tokens per sec:   2845
2023-03-15 06:54:18,099 - INFO - __main__ - Epoch  74, Step:  160200, Batch Loss:     2.725379, Lr: 0.000048, Tokens per sec:   2778
2023-03-15 06:54:38,120 - INFO - __main__ - Epoch  74, Step:  160300, Batch Loss:     1.648562, Lr: 0.000048, Tokens per sec:   2705
2023-03-15 06:54:57,594 - INFO - __main__ - Epoch  74, Step:  160400, Batch Loss:     2.896604, Lr: 0.000048, Tokens per sec:   2751
2023-03-15 06:55:17,005 - INFO - __main__ - Epoch  74, Step:  160500, Batch Loss:     1.742643, Lr: 0.000048, Tokens per sec:   2710
2023-03-15 06:55:36,766 - INFO - __main__ - Epoch  74, Step:  160600, Batch Loss:     3.037772, Lr: 0.000048, Tokens per sec:   2772
2023-03-15 06:55:56,199 - INFO - __main__ - Epoch  74, Step:  160700, Batch Loss:     1.655411, Lr: 0.000048, Tokens per sec:   2733
2023-03-15 06:56:14,554 - INFO - __main__ - Epoch  74, Step:  160800, Batch Loss:     1.939295, Lr: 0.000048, Tokens per sec:   2926
2023-03-15 06:56:33,639 - INFO - __main__ - Epoch  74, Step:  160900, Batch Loss:     2.816834, Lr: 0.000048, Tokens per sec:   2828
2023-03-15 06:56:53,338 - INFO - __main__ - Epoch  74, Step:  161000, Batch Loss:     2.196611, Lr: 0.000048, Tokens per sec:   2678
2023-03-15 06:57:12,469 - INFO - __main__ - Epoch  74, Step:  161100, Batch Loss:     2.697847, Lr: 0.000048, Tokens per sec:   2796
2023-03-15 06:57:32,258 - INFO - __main__ - Epoch  74, Step:  161200, Batch Loss:     1.441102, Lr: 0.000048, Tokens per sec:   2702
2023-03-15 06:57:41,201 - INFO - __main__ - Epoch  74: total training loss 5144.94
2023-03-15 06:57:41,202 - INFO - __main__ - Epoch 75
2023-03-15 06:57:51,717 - INFO - __main__ - Epoch  75, Step:  161300, Batch Loss:     2.524207, Lr: 0.000048, Tokens per sec:   2729
2023-03-15 06:58:10,892 - INFO - __main__ - Epoch  75, Step:  161400, Batch Loss:     1.187084, Lr: 0.000048, Tokens per sec:   2806
2023-03-15 06:58:29,626 - INFO - __main__ - Epoch  75, Step:  161500, Batch Loss:     1.712804, Lr: 0.000048, Tokens per sec:   2838
2023-03-15 06:58:48,114 - INFO - __main__ - Epoch  75, Step:  161600, Batch Loss:     2.158933, Lr: 0.000048, Tokens per sec:   2898
2023-03-15 06:59:06,910 - INFO - __main__ - Epoch  75, Step:  161700, Batch Loss:     2.173741, Lr: 0.000048, Tokens per sec:   2910
2023-03-15 06:59:26,406 - INFO - __main__ - Epoch  75, Step:  161800, Batch Loss:     3.359923, Lr: 0.000048, Tokens per sec:   2758
2023-03-15 06:59:45,705 - INFO - __main__ - Epoch  75, Step:  161900, Batch Loss:     2.383328, Lr: 0.000048, Tokens per sec:   2769
2023-03-15 07:00:04,876 - INFO - __main__ - Epoch  75, Step:  162000, Batch Loss:     3.285803, Lr: 0.000048, Tokens per sec:   2795
2023-03-15 07:00:24,206 - INFO - __main__ - Epoch  75, Step:  162100, Batch Loss:     3.470946, Lr: 0.000048, Tokens per sec:   2818
2023-03-15 07:00:43,704 - INFO - __main__ - Epoch  75, Step:  162200, Batch Loss:     2.700279, Lr: 0.000048, Tokens per sec:   2776
2023-03-15 07:01:03,403 - INFO - __main__ - Epoch  75, Step:  162300, Batch Loss:     2.595735, Lr: 0.000048, Tokens per sec:   2772
2023-03-15 07:01:23,301 - INFO - __main__ - Epoch  75, Step:  162400, Batch Loss:     3.128945, Lr: 0.000048, Tokens per sec:   2714
2023-03-15 07:01:42,874 - INFO - __main__ - Epoch  75, Step:  162500, Batch Loss:     2.635957, Lr: 0.000048, Tokens per sec:   2762
2023-03-15 07:02:01,199 - INFO - __main__ - Epoch  75, Step:  162600, Batch Loss:     2.673522, Lr: 0.000048, Tokens per sec:   2986
2023-03-15 07:02:20,388 - INFO - __main__ - Epoch  75, Step:  162700, Batch Loss:     2.265115, Lr: 0.000048, Tokens per sec:   2814
2023-03-15 07:02:39,003 - INFO - __main__ - Epoch  75, Step:  162800, Batch Loss:     2.761581, Lr: 0.000048, Tokens per sec:   2928
2023-03-15 07:02:58,097 - INFO - __main__ - Epoch  75, Step:  162900, Batch Loss:     2.086909, Lr: 0.000048, Tokens per sec:   2791
2023-03-15 07:03:17,163 - INFO - __main__ - Epoch  75, Step:  163000, Batch Loss:     2.042431, Lr: 0.000048, Tokens per sec:   2815
2023-03-15 07:03:36,323 - INFO - __main__ - Epoch  75, Step:  163100, Batch Loss:     1.605893, Lr: 0.000048, Tokens per sec:   2810
2023-03-15 07:03:55,775 - INFO - __main__ - Epoch  75, Step:  163200, Batch Loss:     3.564023, Lr: 0.000048, Tokens per sec:   2721
2023-03-15 07:04:14,888 - INFO - __main__ - Epoch  75, Step:  163300, Batch Loss:     2.465814, Lr: 0.000048, Tokens per sec:   2797
2023-03-15 07:04:33,788 - INFO - __main__ - Epoch  75, Step:  163400, Batch Loss:     2.124538, Lr: 0.000048, Tokens per sec:   2820
2023-03-15 07:04:38,491 - INFO - __main__ - Epoch  75: total training loss 5036.92
2023-03-15 07:04:38,492 - INFO - __main__ - Epoch 76
2023-03-15 07:04:53,320 - INFO - __main__ - Epoch  76, Step:  163500, Batch Loss:     2.706877, Lr: 0.000047, Tokens per sec:   2753
2023-03-15 07:05:12,016 - INFO - __main__ - Epoch  76, Step:  163600, Batch Loss:     2.105241, Lr: 0.000047, Tokens per sec:   2894
2023-03-15 07:05:30,533 - INFO - __main__ - Epoch  76, Step:  163700, Batch Loss:     2.673675, Lr: 0.000047, Tokens per sec:   2894
2023-03-15 07:05:50,316 - INFO - __main__ - Epoch  76, Step:  163800, Batch Loss:     2.440231, Lr: 0.000047, Tokens per sec:   2704
2023-03-15 07:06:09,333 - INFO - __main__ - Epoch  76, Step:  163900, Batch Loss:     2.554220, Lr: 0.000047, Tokens per sec:   2826
2023-03-15 07:06:28,744 - INFO - __main__ - Epoch  76, Step:  164000, Batch Loss:     2.227955, Lr: 0.000047, Tokens per sec:   2727
2023-03-15 07:06:47,701 - INFO - __main__ - Epoch  76, Step:  164100, Batch Loss:     2.306990, Lr: 0.000047, Tokens per sec:   2748
2023-03-15 07:07:07,536 - INFO - __main__ - Epoch  76, Step:  164200, Batch Loss:     1.810413, Lr: 0.000047, Tokens per sec:   2680
2023-03-15 07:07:25,793 - INFO - __main__ - Epoch  76, Step:  164300, Batch Loss:     2.768691, Lr: 0.000047, Tokens per sec:   2985
2023-03-15 07:07:44,926 - INFO - __main__ - Epoch  76, Step:  164400, Batch Loss:     1.929650, Lr: 0.000047, Tokens per sec:   2819
2023-03-15 07:08:04,096 - INFO - __main__ - Epoch  76, Step:  164500, Batch Loss:     1.887790, Lr: 0.000047, Tokens per sec:   2800
2023-03-15 07:08:22,895 - INFO - __main__ - Epoch  76, Step:  164600, Batch Loss:     2.030757, Lr: 0.000047, Tokens per sec:   2917
2023-03-15 07:08:42,209 - INFO - __main__ - Epoch  76, Step:  164700, Batch Loss:     2.598571, Lr: 0.000047, Tokens per sec:   2773
2023-03-15 07:09:01,684 - INFO - __main__ - Epoch  76, Step:  164800, Batch Loss:     2.193579, Lr: 0.000047, Tokens per sec:   2741
2023-03-15 07:09:20,331 - INFO - __main__ - Epoch  76, Step:  164900, Batch Loss:     2.231279, Lr: 0.000047, Tokens per sec:   2904
2023-03-15 07:09:39,455 - INFO - __main__ - Epoch  76, Step:  165000, Batch Loss:     1.703922, Lr: 0.000047, Tokens per sec:   2800
2023-03-15 07:09:59,222 - INFO - __main__ - Epoch  76, Step:  165100, Batch Loss:     2.480684, Lr: 0.000047, Tokens per sec:   2774
2023-03-15 07:10:17,600 - INFO - __main__ - Epoch  76, Step:  165200, Batch Loss:     1.848758, Lr: 0.000047, Tokens per sec:   2959
2023-03-15 07:10:36,572 - INFO - __main__ - Epoch  76, Step:  165300, Batch Loss:     1.718189, Lr: 0.000047, Tokens per sec:   2900
2023-03-15 07:10:55,914 - INFO - __main__ - Epoch  76, Step:  165400, Batch Loss:     2.411167, Lr: 0.000047, Tokens per sec:   2772
2023-03-15 07:11:13,989 - INFO - __main__ - Epoch  76, Step:  165500, Batch Loss:     2.435214, Lr: 0.000047, Tokens per sec:   2943
2023-03-15 07:11:33,128 - INFO - __main__ - Epoch  76, Step:  165600, Batch Loss:     1.936427, Lr: 0.000047, Tokens per sec:   2835
2023-03-15 07:11:33,994 - INFO - __main__ - Epoch  76: total training loss 4905.83
2023-03-15 07:11:33,995 - INFO - __main__ - Epoch 77
2023-03-15 07:11:52,114 - INFO - __main__ - Epoch  77, Step:  165700, Batch Loss:     1.994701, Lr: 0.000047, Tokens per sec:   2892
2023-03-15 07:12:10,671 - INFO - __main__ - Epoch  77, Step:  165800, Batch Loss:     1.362840, Lr: 0.000047, Tokens per sec:   2952
2023-03-15 07:12:29,453 - INFO - __main__ - Epoch  77, Step:  165900, Batch Loss:     2.767211, Lr: 0.000047, Tokens per sec:   2838
2023-03-15 07:12:48,527 - INFO - __main__ - Epoch  77, Step:  166000, Batch Loss:     2.362518, Lr: 0.000047, Tokens per sec:   2811
2023-03-15 07:13:07,630 - INFO - __main__ - Epoch  77, Step:  166100, Batch Loss:     2.561813, Lr: 0.000047, Tokens per sec:   2800
2023-03-15 07:13:26,348 - INFO - __main__ - Epoch  77, Step:  166200, Batch Loss:     2.170288, Lr: 0.000047, Tokens per sec:   2870
2023-03-15 07:13:45,255 - INFO - __main__ - Epoch  77, Step:  166300, Batch Loss:     2.606151, Lr: 0.000047, Tokens per sec:   2878
2023-03-15 07:14:03,576 - INFO - __main__ - Epoch  77, Step:  166400, Batch Loss:     2.390341, Lr: 0.000047, Tokens per sec:   2983
2023-03-15 07:14:22,266 - INFO - __main__ - Epoch  77, Step:  166500, Batch Loss:     3.094295, Lr: 0.000047, Tokens per sec:   2886
2023-03-15 07:14:40,936 - INFO - __main__ - Epoch  77, Step:  166600, Batch Loss:     2.146319, Lr: 0.000047, Tokens per sec:   2850
2023-03-15 07:14:59,929 - INFO - __main__ - Epoch  77, Step:  166700, Batch Loss:     2.608092, Lr: 0.000047, Tokens per sec:   2727
2023-03-15 07:15:18,883 - INFO - __main__ - Epoch  77, Step:  166800, Batch Loss:     3.146582, Lr: 0.000047, Tokens per sec:   2814
2023-03-15 07:15:38,420 - INFO - __main__ - Epoch  77, Step:  166900, Batch Loss:     2.109887, Lr: 0.000047, Tokens per sec:   2772
2023-03-15 07:15:57,827 - INFO - __main__ - Epoch  77, Step:  167000, Batch Loss:     1.877277, Lr: 0.000047, Tokens per sec:   2765
2023-03-15 07:16:17,218 - INFO - __main__ - Epoch  77, Step:  167100, Batch Loss:     1.492596, Lr: 0.000047, Tokens per sec:   2787
2023-03-15 07:16:35,417 - INFO - __main__ - Epoch  77, Step:  167200, Batch Loss:     2.741218, Lr: 0.000047, Tokens per sec:   3000
2023-03-15 07:16:54,190 - INFO - __main__ - Epoch  77, Step:  167300, Batch Loss:     1.934308, Lr: 0.000047, Tokens per sec:   2814
2023-03-15 07:17:13,390 - INFO - __main__ - Epoch  77, Step:  167400, Batch Loss:     2.154586, Lr: 0.000047, Tokens per sec:   2801
2023-03-15 07:17:32,087 - INFO - __main__ - Epoch  77, Step:  167500, Batch Loss:     2.317951, Lr: 0.000047, Tokens per sec:   2865
2023-03-15 07:17:50,896 - INFO - __main__ - Epoch  77, Step:  167600, Batch Loss:     2.468118, Lr: 0.000047, Tokens per sec:   2894
2023-03-15 07:18:09,572 - INFO - __main__ - Epoch  77, Step:  167700, Batch Loss:     1.999740, Lr: 0.000047, Tokens per sec:   2920
2023-03-15 07:18:24,763 - INFO - __main__ - Epoch  77: total training loss 4810.20
2023-03-15 07:18:24,764 - INFO - __main__ - Epoch 78
2023-03-15 07:18:28,405 - INFO - __main__ - Epoch  78, Step:  167800, Batch Loss:     2.665853, Lr: 0.000046, Tokens per sec:   2575
2023-03-15 07:18:47,858 - INFO - __main__ - Epoch  78, Step:  167900, Batch Loss:     1.332917, Lr: 0.000046, Tokens per sec:   2780
2023-03-15 07:19:06,792 - INFO - __main__ - Epoch  78, Step:  168000, Batch Loss:     2.505150, Lr: 0.000046, Tokens per sec:   2827
2023-03-15 07:19:25,591 - INFO - __main__ - Epoch  78, Step:  168100, Batch Loss:     2.354587, Lr: 0.000046, Tokens per sec:   2864
2023-03-15 07:19:44,462 - INFO - __main__ - Epoch  78, Step:  168200, Batch Loss:     2.213561, Lr: 0.000046, Tokens per sec:   2858
2023-03-15 07:20:03,648 - INFO - __main__ - Epoch  78, Step:  168300, Batch Loss:     2.408740, Lr: 0.000046, Tokens per sec:   2795
2023-03-15 07:20:22,551 - INFO - __main__ - Epoch  78, Step:  168400, Batch Loss:     1.881253, Lr: 0.000046, Tokens per sec:   2875
2023-03-15 07:20:40,986 - INFO - __main__ - Epoch  78, Step:  168500, Batch Loss:     1.697266, Lr: 0.000046, Tokens per sec:   2949
2023-03-15 07:20:59,936 - INFO - __main__ - Epoch  78, Step:  168600, Batch Loss:     2.681602, Lr: 0.000046, Tokens per sec:   2776
2023-03-15 07:21:19,060 - INFO - __main__ - Epoch  78, Step:  168700, Batch Loss:     1.830204, Lr: 0.000046, Tokens per sec:   2865
2023-03-15 07:21:37,687 - INFO - __main__ - Epoch  78, Step:  168800, Batch Loss:     2.607099, Lr: 0.000046, Tokens per sec:   2912
2023-03-15 07:21:56,009 - INFO - __main__ - Epoch  78, Step:  168900, Batch Loss:     1.798232, Lr: 0.000046, Tokens per sec:   2964
2023-03-15 07:22:15,215 - INFO - __main__ - Epoch  78, Step:  169000, Batch Loss:     1.795473, Lr: 0.000046, Tokens per sec:   2834
2023-03-15 07:22:34,147 - INFO - __main__ - Epoch  78, Step:  169100, Batch Loss:     2.492965, Lr: 0.000046, Tokens per sec:   2807
2023-03-15 07:22:53,022 - INFO - __main__ - Epoch  78, Step:  169200, Batch Loss:     2.085094, Lr: 0.000046, Tokens per sec:   2865
2023-03-15 07:23:12,525 - INFO - __main__ - Epoch  78, Step:  169300, Batch Loss:     2.566117, Lr: 0.000046, Tokens per sec:   2683
2023-03-15 07:23:32,364 - INFO - __main__ - Epoch  78, Step:  169400, Batch Loss:     2.009485, Lr: 0.000046, Tokens per sec:   2734
2023-03-15 07:23:51,581 - INFO - __main__ - Epoch  78, Step:  169500, Batch Loss:     2.130672, Lr: 0.000046, Tokens per sec:   2801
2023-03-15 07:24:11,326 - INFO - __main__ - Epoch  78, Step:  169600, Batch Loss:     2.052080, Lr: 0.000046, Tokens per sec:   2712
2023-03-15 07:24:30,050 - INFO - __main__ - Epoch  78, Step:  169700, Batch Loss:     2.601869, Lr: 0.000046, Tokens per sec:   2886
2023-03-15 07:24:48,981 - INFO - __main__ - Epoch  78, Step:  169800, Batch Loss:     4.154061, Lr: 0.000046, Tokens per sec:   2840
2023-03-15 07:25:07,904 - INFO - __main__ - Epoch  78, Step:  169900, Batch Loss:     3.285069, Lr: 0.000046, Tokens per sec:   2845
2023-03-15 07:25:20,119 - INFO - __main__ - Epoch  78: total training loss 4714.03
2023-03-15 07:25:20,120 - INFO - __main__ - Epoch 79
2023-03-15 07:25:27,748 - INFO - __main__ - Epoch  79, Step:  170000, Batch Loss:     2.030632, Lr: 0.000046, Tokens per sec:   2692
2023-03-15 07:25:46,341 - INFO - __main__ - Epoch  79, Step:  170100, Batch Loss:     2.139133, Lr: 0.000046, Tokens per sec:   2908
2023-03-15 07:26:04,762 - INFO - __main__ - Epoch  79, Step:  170200, Batch Loss:     1.899867, Lr: 0.000046, Tokens per sec:   2965
2023-03-15 07:26:23,416 - INFO - __main__ - Epoch  79, Step:  170300, Batch Loss:     1.681292, Lr: 0.000046, Tokens per sec:   2868
2023-03-15 07:26:42,066 - INFO - __main__ - Epoch  79, Step:  170400, Batch Loss:     3.017034, Lr: 0.000046, Tokens per sec:   2852
2023-03-15 07:27:01,823 - INFO - __main__ - Epoch  79, Step:  170500, Batch Loss:     2.392739, Lr: 0.000046, Tokens per sec:   2695
2023-03-15 07:27:21,448 - INFO - __main__ - Epoch  79, Step:  170600, Batch Loss:     2.337152, Lr: 0.000046, Tokens per sec:   2758
2023-03-15 07:27:40,208 - INFO - __main__ - Epoch  79, Step:  170700, Batch Loss:     2.782379, Lr: 0.000046, Tokens per sec:   2884
2023-03-15 07:27:59,184 - INFO - __main__ - Epoch  79, Step:  170800, Batch Loss:     2.667190, Lr: 0.000046, Tokens per sec:   2889
2023-03-15 07:28:18,470 - INFO - __main__ - Epoch  79, Step:  170900, Batch Loss:     2.629994, Lr: 0.000046, Tokens per sec:   2775
2023-03-15 07:28:37,629 - INFO - __main__ - Epoch  79, Step:  171000, Batch Loss:     2.158940, Lr: 0.000046, Tokens per sec:   2805
2023-03-15 07:28:56,462 - INFO - __main__ - Epoch  79, Step:  171100, Batch Loss:     1.647459, Lr: 0.000046, Tokens per sec:   2887
2023-03-15 07:29:15,654 - INFO - __main__ - Epoch  79, Step:  171200, Batch Loss:     3.138887, Lr: 0.000046, Tokens per sec:   2822
2023-03-15 07:29:34,981 - INFO - __main__ - Epoch  79, Step:  171300, Batch Loss:     2.025195, Lr: 0.000046, Tokens per sec:   2765
2023-03-15 07:29:54,467 - INFO - __main__ - Epoch  79, Step:  171400, Batch Loss:     1.998927, Lr: 0.000046, Tokens per sec:   2758
2023-03-15 07:30:13,270 - INFO - __main__ - Epoch  79, Step:  171500, Batch Loss:     2.080601, Lr: 0.000046, Tokens per sec:   2819
2023-03-15 07:30:32,582 - INFO - __main__ - Epoch  79, Step:  171600, Batch Loss:     2.433336, Lr: 0.000046, Tokens per sec:   2783
2023-03-15 07:30:51,864 - INFO - __main__ - Epoch  79, Step:  171700, Batch Loss:     1.687836, Lr: 0.000046, Tokens per sec:   2770
2023-03-15 07:31:11,584 - INFO - __main__ - Epoch  79, Step:  171800, Batch Loss:     3.503213, Lr: 0.000046, Tokens per sec:   2719
2023-03-15 07:31:31,199 - INFO - __main__ - Epoch  79, Step:  171900, Batch Loss:     2.555994, Lr: 0.000046, Tokens per sec:   2745
2023-03-15 07:31:50,702 - INFO - __main__ - Epoch  79, Step:  172000, Batch Loss:     2.283446, Lr: 0.000046, Tokens per sec:   2792
2023-03-15 07:32:09,265 - INFO - __main__ - Epoch  79, Step:  172100, Batch Loss:     1.911288, Lr: 0.000046, Tokens per sec:   2893
2023-03-15 07:32:16,735 - INFO - __main__ - Epoch  79: total training loss 4636.18
2023-03-15 07:32:16,736 - INFO - __main__ - Epoch 80
2023-03-15 07:32:28,157 - INFO - __main__ - Epoch  80, Step:  172200, Batch Loss:     2.709852, Lr: 0.000045, Tokens per sec:   2757
2023-03-15 07:32:47,894 - INFO - __main__ - Epoch  80, Step:  172300, Batch Loss:     1.451872, Lr: 0.000045, Tokens per sec:   2733
2023-03-15 07:33:06,865 - INFO - __main__ - Epoch  80, Step:  172400, Batch Loss:     2.578269, Lr: 0.000045, Tokens per sec:   2801
2023-03-15 07:33:26,079 - INFO - __main__ - Epoch  80, Step:  172500, Batch Loss:     2.999745, Lr: 0.000045, Tokens per sec:   2780
2023-03-15 07:33:45,018 - INFO - __main__ - Epoch  80, Step:  172600, Batch Loss:     1.500345, Lr: 0.000045, Tokens per sec:   2853
2023-03-15 07:34:04,272 - INFO - __main__ - Epoch  80, Step:  172700, Batch Loss:     2.152619, Lr: 0.000045, Tokens per sec:   2806
2023-03-15 07:34:23,190 - INFO - __main__ - Epoch  80, Step:  172800, Batch Loss:     1.569665, Lr: 0.000045, Tokens per sec:   2845
2023-03-15 07:34:41,863 - INFO - __main__ - Epoch  80, Step:  172900, Batch Loss:     2.387049, Lr: 0.000045, Tokens per sec:   2817
2023-03-15 07:35:00,649 - INFO - __main__ - Epoch  80, Step:  173000, Batch Loss:     2.282609, Lr: 0.000045, Tokens per sec:   2911
2023-03-15 07:35:19,987 - INFO - __main__ - Epoch  80, Step:  173100, Batch Loss:     1.323741, Lr: 0.000045, Tokens per sec:   2798
2023-03-15 07:35:38,961 - INFO - __main__ - Epoch  80, Step:  173200, Batch Loss:     2.183909, Lr: 0.000045, Tokens per sec:   2880
2023-03-15 07:35:57,750 - INFO - __main__ - Epoch  80, Step:  173300, Batch Loss:     1.773492, Lr: 0.000045, Tokens per sec:   2843
2023-03-15 07:36:16,982 - INFO - __main__ - Epoch  80, Step:  173400, Batch Loss:     2.583064, Lr: 0.000045, Tokens per sec:   2784
2023-03-15 07:36:35,701 - INFO - __main__ - Epoch  80, Step:  173500, Batch Loss:     2.484052, Lr: 0.000045, Tokens per sec:   2904
2023-03-15 07:36:54,958 - INFO - __main__ - Epoch  80, Step:  173600, Batch Loss:     1.754938, Lr: 0.000045, Tokens per sec:   2778
2023-03-15 07:37:13,962 - INFO - __main__ - Epoch  80, Step:  173700, Batch Loss:     1.833264, Lr: 0.000045, Tokens per sec:   2809
2023-03-15 07:37:32,561 - INFO - __main__ - Epoch  80, Step:  173800, Batch Loss:     2.328287, Lr: 0.000045, Tokens per sec:   2927
2023-03-15 07:37:52,039 - INFO - __main__ - Epoch  80, Step:  173900, Batch Loss:     2.788939, Lr: 0.000045, Tokens per sec:   2740
2023-03-15 07:38:11,454 - INFO - __main__ - Epoch  80, Step:  174000, Batch Loss:     1.949557, Lr: 0.000045, Tokens per sec:   2842
2023-03-15 07:38:29,641 - INFO - __main__ - Epoch  80, Step:  174100, Batch Loss:     2.206695, Lr: 0.000045, Tokens per sec:   2987
2023-03-15 07:38:48,104 - INFO - __main__ - Epoch  80, Step:  174200, Batch Loss:     2.868557, Lr: 0.000045, Tokens per sec:   2933
2023-03-15 07:39:07,350 - INFO - __main__ - Epoch  80, Step:  174300, Batch Loss:     1.629881, Lr: 0.000045, Tokens per sec:   2752
2023-03-15 07:39:11,408 - INFO - __main__ - Epoch  80: total training loss 4543.69
2023-03-15 07:39:11,409 - INFO - __main__ - Epoch 81
2023-03-15 07:39:26,986 - INFO - __main__ - Epoch  81, Step:  174400, Batch Loss:     2.084665, Lr: 0.000045, Tokens per sec:   2769
2023-03-15 07:39:45,980 - INFO - __main__ - Epoch  81, Step:  174500, Batch Loss:     1.501097, Lr: 0.000045, Tokens per sec:   2856
2023-03-15 07:40:04,394 - INFO - __main__ - Epoch  81, Step:  174600, Batch Loss:     1.772573, Lr: 0.000045, Tokens per sec:   2907
2023-03-15 07:40:22,584 - INFO - __main__ - Epoch  81, Step:  174700, Batch Loss:     2.306416, Lr: 0.000045, Tokens per sec:   2965
2023-03-15 07:40:41,891 - INFO - __main__ - Epoch  81, Step:  174800, Batch Loss:     1.317792, Lr: 0.000045, Tokens per sec:   2733
2023-03-15 07:41:00,600 - INFO - __main__ - Epoch  81, Step:  174900, Batch Loss:     1.355132, Lr: 0.000045, Tokens per sec:   2865
2023-03-15 07:41:19,814 - INFO - __main__ - Epoch  81, Step:  175000, Batch Loss:     1.781105, Lr: 0.000045, Tokens per sec:   2807
2023-03-15 07:41:38,830 - INFO - __main__ - Epoch  81, Step:  175100, Batch Loss:     2.202853, Lr: 0.000045, Tokens per sec:   2816
2023-03-15 07:41:57,378 - INFO - __main__ - Epoch  81, Step:  175200, Batch Loss:     1.982339, Lr: 0.000045, Tokens per sec:   2834
2023-03-15 07:42:16,831 - INFO - __main__ - Epoch  81, Step:  175300, Batch Loss:     1.890429, Lr: 0.000045, Tokens per sec:   2791
2023-03-15 07:42:35,350 - INFO - __main__ - Epoch  81, Step:  175400, Batch Loss:     1.406716, Lr: 0.000045, Tokens per sec:   2930
2023-03-15 07:42:54,184 - INFO - __main__ - Epoch  81, Step:  175500, Batch Loss:     1.815153, Lr: 0.000045, Tokens per sec:   2881
2023-03-15 07:43:12,954 - INFO - __main__ - Epoch  81, Step:  175600, Batch Loss:     2.159827, Lr: 0.000045, Tokens per sec:   2937
2023-03-15 07:43:32,228 - INFO - __main__ - Epoch  81, Step:  175700, Batch Loss:     1.622858, Lr: 0.000045, Tokens per sec:   2780
2023-03-15 07:43:52,162 - INFO - __main__ - Epoch  81, Step:  175800, Batch Loss:     1.431308, Lr: 0.000045, Tokens per sec:   2701
2023-03-15 07:44:10,817 - INFO - __main__ - Epoch  81, Step:  175900, Batch Loss:     3.265474, Lr: 0.000045, Tokens per sec:   2905
2023-03-15 07:44:30,083 - INFO - __main__ - Epoch  81, Step:  176000, Batch Loss:     2.150475, Lr: 0.000045, Tokens per sec:   2808
2023-03-15 07:44:49,091 - INFO - __main__ - Epoch  81, Step:  176100, Batch Loss:     2.450462, Lr: 0.000045, Tokens per sec:   2810
2023-03-15 07:45:07,661 - INFO - __main__ - Epoch  81, Step:  176200, Batch Loss:     2.980396, Lr: 0.000045, Tokens per sec:   2902
2023-03-15 07:45:27,055 - INFO - __main__ - Epoch  81, Step:  176300, Batch Loss:     1.994177, Lr: 0.000045, Tokens per sec:   2770
2023-03-15 07:45:45,766 - INFO - __main__ - Epoch  81, Step:  176400, Batch Loss:     2.010783, Lr: 0.000045, Tokens per sec:   2916
2023-03-15 07:46:04,211 - INFO - __main__ - Epoch  81: total training loss 4450.33
2023-03-15 07:46:04,211 - INFO - __main__ - Epoch 82
2023-03-15 07:46:04,742 - INFO - __main__ - Epoch  82, Step:  176500, Batch Loss:     1.635397, Lr: 0.000044, Tokens per sec:   1118
2023-03-15 07:46:23,367 - INFO - __main__ - Epoch  82, Step:  176600, Batch Loss:     1.312654, Lr: 0.000044, Tokens per sec:   2832
2023-03-15 07:46:42,010 - INFO - __main__ - Epoch  82, Step:  176700, Batch Loss:     2.302613, Lr: 0.000044, Tokens per sec:   2884
2023-03-15 07:47:00,976 - INFO - __main__ - Epoch  82, Step:  176800, Batch Loss:     2.426198, Lr: 0.000044, Tokens per sec:   2848
2023-03-15 07:47:20,014 - INFO - __main__ - Epoch  82, Step:  176900, Batch Loss:     1.989104, Lr: 0.000044, Tokens per sec:   2799
2023-03-15 07:47:39,155 - INFO - __main__ - Epoch  82, Step:  177000, Batch Loss:     2.615524, Lr: 0.000044, Tokens per sec:   2771
2023-03-15 07:47:58,347 - INFO - __main__ - Epoch  82, Step:  177100, Batch Loss:     2.601920, Lr: 0.000044, Tokens per sec:   2778
2023-03-15 07:48:17,903 - INFO - __main__ - Epoch  82, Step:  177200, Batch Loss:     1.835275, Lr: 0.000044, Tokens per sec:   2813
2023-03-15 07:48:36,349 - INFO - __main__ - Epoch  82, Step:  177300, Batch Loss:     2.143420, Lr: 0.000044, Tokens per sec:   2918
2023-03-15 07:48:56,084 - INFO - __main__ - Epoch  82, Step:  177400, Batch Loss:     2.742071, Lr: 0.000044, Tokens per sec:   2766
2023-03-15 07:49:15,721 - INFO - __main__ - Epoch  82, Step:  177500, Batch Loss:     2.999891, Lr: 0.000044, Tokens per sec:   2735
2023-03-15 07:49:34,489 - INFO - __main__ - Epoch  82, Step:  177600, Batch Loss:     2.480528, Lr: 0.000044, Tokens per sec:   2895
2023-03-15 07:49:52,901 - INFO - __main__ - Epoch  82, Step:  177700, Batch Loss:     1.616800, Lr: 0.000044, Tokens per sec:   2924
2023-03-15 07:50:12,654 - INFO - __main__ - Epoch  82, Step:  177800, Batch Loss:     1.974824, Lr: 0.000044, Tokens per sec:   2726
2023-03-15 07:50:32,259 - INFO - __main__ - Epoch  82, Step:  177900, Batch Loss:     1.749472, Lr: 0.000044, Tokens per sec:   2725
2023-03-15 07:50:50,642 - INFO - __main__ - Epoch  82, Step:  178000, Batch Loss:     2.720850, Lr: 0.000044, Tokens per sec:   2935
2023-03-15 07:51:09,075 - INFO - __main__ - Epoch  82, Step:  178100, Batch Loss:     2.391016, Lr: 0.000044, Tokens per sec:   2945
2023-03-15 07:51:27,484 - INFO - __main__ - Epoch  82, Step:  178200, Batch Loss:     2.021635, Lr: 0.000044, Tokens per sec:   2955
2023-03-15 07:51:46,409 - INFO - __main__ - Epoch  82, Step:  178300, Batch Loss:     1.634615, Lr: 0.000044, Tokens per sec:   2852
2023-03-15 07:52:05,643 - INFO - __main__ - Epoch  82, Step:  178400, Batch Loss:     1.441677, Lr: 0.000044, Tokens per sec:   2806
2023-03-15 07:52:24,494 - INFO - __main__ - Epoch  82, Step:  178500, Batch Loss:     1.758702, Lr: 0.000044, Tokens per sec:   2796
2023-03-15 07:52:44,210 - INFO - __main__ - Epoch  82, Step:  178600, Batch Loss:     2.131884, Lr: 0.000044, Tokens per sec:   2772
2023-03-15 07:52:59,154 - INFO - __main__ - Epoch  82: total training loss 4338.66
2023-03-15 07:52:59,155 - INFO - __main__ - Epoch 83
2023-03-15 07:53:03,663 - INFO - __main__ - Epoch  83, Step:  178700, Batch Loss:     1.448273, Lr: 0.000044, Tokens per sec:   2722
2023-03-15 07:53:22,142 - INFO - __main__ - Epoch  83, Step:  178800, Batch Loss:     2.082930, Lr: 0.000044, Tokens per sec:   2898
2023-03-15 07:53:41,645 - INFO - __main__ - Epoch  83, Step:  178900, Batch Loss:     1.428945, Lr: 0.000044, Tokens per sec:   2792
2023-03-15 07:54:00,515 - INFO - __main__ - Epoch  83, Step:  179000, Batch Loss:     1.801497, Lr: 0.000044, Tokens per sec:   2829
2023-03-15 07:54:19,637 - INFO - __main__ - Epoch  83, Step:  179100, Batch Loss:     2.520679, Lr: 0.000044, Tokens per sec:   2843
2023-03-15 07:54:38,095 - INFO - __main__ - Epoch  83, Step:  179200, Batch Loss:     1.673336, Lr: 0.000044, Tokens per sec:   2921
2023-03-15 07:54:56,866 - INFO - __main__ - Epoch  83, Step:  179300, Batch Loss:     1.128423, Lr: 0.000044, Tokens per sec:   2847
2023-03-15 07:55:15,423 - INFO - __main__ - Epoch  83, Step:  179400, Batch Loss:     2.921153, Lr: 0.000044, Tokens per sec:   2877
2023-03-15 07:55:34,829 - INFO - __main__ - Epoch  83, Step:  179500, Batch Loss:     2.340786, Lr: 0.000044, Tokens per sec:   2711
2023-03-15 07:55:54,149 - INFO - __main__ - Epoch  83, Step:  179600, Batch Loss:     1.926311, Lr: 0.000044, Tokens per sec:   2771
2023-03-15 07:56:12,797 - INFO - __main__ - Epoch  83, Step:  179700, Batch Loss:     1.294384, Lr: 0.000044, Tokens per sec:   2887
2023-03-15 07:56:32,651 - INFO - __main__ - Epoch  83, Step:  179800, Batch Loss:     3.219156, Lr: 0.000044, Tokens per sec:   2740
2023-03-15 07:56:51,893 - INFO - __main__ - Epoch  83, Step:  179900, Batch Loss:     2.081010, Lr: 0.000044, Tokens per sec:   2796
2023-03-15 07:57:10,930 - INFO - __main__ - Epoch  83, Step:  180000, Batch Loss:     2.292803, Lr: 0.000044, Tokens per sec:   2786
2023-03-15 07:57:30,308 - INFO - __main__ - Epoch  83, Step:  180100, Batch Loss:     1.615166, Lr: 0.000044, Tokens per sec:   2801
2023-03-15 07:57:49,685 - INFO - __main__ - Epoch  83, Step:  180200, Batch Loss:     2.168685, Lr: 0.000044, Tokens per sec:   2781
2023-03-15 07:58:08,645 - INFO - __main__ - Epoch  83, Step:  180300, Batch Loss:     1.817207, Lr: 0.000044, Tokens per sec:   2851
2023-03-15 07:58:27,238 - INFO - __main__ - Epoch  83, Step:  180400, Batch Loss:     1.756953, Lr: 0.000044, Tokens per sec:   2876
2023-03-15 07:58:45,654 - INFO - __main__ - Epoch  83, Step:  180500, Batch Loss:     2.684888, Lr: 0.000044, Tokens per sec:   2982
2023-03-15 07:59:04,567 - INFO - __main__ - Epoch  83, Step:  180600, Batch Loss:     1.639467, Lr: 0.000044, Tokens per sec:   2861
2023-03-15 07:59:24,202 - INFO - __main__ - Epoch  83, Step:  180700, Batch Loss:     1.706926, Lr: 0.000044, Tokens per sec:   2727
2023-03-15 07:59:43,472 - INFO - __main__ - Epoch  83, Step:  180800, Batch Loss:     1.898768, Lr: 0.000044, Tokens per sec:   2797
2023-03-15 07:59:54,781 - INFO - __main__ - Epoch  83: total training loss 4285.03
2023-03-15 07:59:54,783 - INFO - __main__ - Epoch 84
2023-03-15 08:00:03,748 - INFO - __main__ - Epoch  84, Step:  180900, Batch Loss:     1.876678, Lr: 0.000043, Tokens per sec:   2598
2023-03-15 08:00:22,083 - INFO - __main__ - Epoch  84, Step:  181000, Batch Loss:     1.817263, Lr: 0.000043, Tokens per sec:   2935
2023-03-15 08:00:40,965 - INFO - __main__ - Epoch  84, Step:  181100, Batch Loss:     1.527788, Lr: 0.000043, Tokens per sec:   2845
2023-03-15 08:01:00,348 - INFO - __main__ - Epoch  84, Step:  181200, Batch Loss:     1.695532, Lr: 0.000043, Tokens per sec:   2768
2023-03-15 08:01:19,387 - INFO - __main__ - Epoch  84, Step:  181300, Batch Loss:     1.928490, Lr: 0.000043, Tokens per sec:   2851
2023-03-15 08:01:37,832 - INFO - __main__ - Epoch  84, Step:  181400, Batch Loss:     1.480269, Lr: 0.000043, Tokens per sec:   2886
2023-03-15 08:01:56,170 - INFO - __main__ - Epoch  84, Step:  181500, Batch Loss:     2.026313, Lr: 0.000043, Tokens per sec:   2963
2023-03-15 08:02:15,164 - INFO - __main__ - Epoch  84, Step:  181600, Batch Loss:     2.749430, Lr: 0.000043, Tokens per sec:   2823
2023-03-15 08:02:34,295 - INFO - __main__ - Epoch  84, Step:  181700, Batch Loss:     1.887450, Lr: 0.000043, Tokens per sec:   2841
2023-03-15 08:02:52,470 - INFO - __main__ - Epoch  84, Step:  181800, Batch Loss:     1.226388, Lr: 0.000043, Tokens per sec:   2942
2023-03-15 08:03:11,942 - INFO - __main__ - Epoch  84, Step:  181900, Batch Loss:     1.200605, Lr: 0.000043, Tokens per sec:   2759
2023-03-15 08:03:30,891 - INFO - __main__ - Epoch  84, Step:  182000, Batch Loss:     2.390416, Lr: 0.000043, Tokens per sec:   2873
2023-03-15 08:03:49,529 - INFO - __main__ - Epoch  84, Step:  182100, Batch Loss:     2.056372, Lr: 0.000043, Tokens per sec:   2904
2023-03-15 08:04:08,571 - INFO - __main__ - Epoch  84, Step:  182200, Batch Loss:     2.080328, Lr: 0.000043, Tokens per sec:   2828
2023-03-15 08:04:27,534 - INFO - __main__ - Epoch  84, Step:  182300, Batch Loss:     3.627031, Lr: 0.000043, Tokens per sec:   2853
2023-03-15 08:04:45,852 - INFO - __main__ - Epoch  84, Step:  182400, Batch Loss:     4.008075, Lr: 0.000043, Tokens per sec:   2959
2023-03-15 08:05:04,209 - INFO - __main__ - Epoch  84, Step:  182500, Batch Loss:     1.582653, Lr: 0.000043, Tokens per sec:   2941
2023-03-15 08:05:23,958 - INFO - __main__ - Epoch  84, Step:  182600, Batch Loss:     1.413403, Lr: 0.000043, Tokens per sec:   2700
2023-03-15 08:05:42,603 - INFO - __main__ - Epoch  84, Step:  182700, Batch Loss:     1.871560, Lr: 0.000043, Tokens per sec:   2839
2023-03-15 08:06:01,759 - INFO - __main__ - Epoch  84, Step:  182800, Batch Loss:     2.280027, Lr: 0.000043, Tokens per sec:   2747
2023-03-15 08:06:20,132 - INFO - __main__ - Epoch  84, Step:  182900, Batch Loss:     1.780282, Lr: 0.000043, Tokens per sec:   2902
2023-03-15 08:06:39,064 - INFO - __main__ - Epoch  84, Step:  183000, Batch Loss:     2.231053, Lr: 0.000043, Tokens per sec:   2880
2023-03-15 08:06:45,620 - INFO - __main__ - Epoch  84: total training loss 4183.82
2023-03-15 08:06:45,621 - INFO - __main__ - Epoch 85
2023-03-15 08:06:58,598 - INFO - __main__ - Epoch  85, Step:  183100, Batch Loss:     1.647443, Lr: 0.000043, Tokens per sec:   2629
2023-03-15 08:07:17,955 - INFO - __main__ - Epoch  85, Step:  183200, Batch Loss:     1.721525, Lr: 0.000043, Tokens per sec:   2773
2023-03-15 08:07:37,106 - INFO - __main__ - Epoch  85, Step:  183300, Batch Loss:     2.486351, Lr: 0.000043, Tokens per sec:   2833
2023-03-15 08:07:55,245 - INFO - __main__ - Epoch  85, Step:  183400, Batch Loss:     1.528138, Lr: 0.000043, Tokens per sec:   2941
2023-03-15 08:08:14,334 - INFO - __main__ - Epoch  85, Step:  183500, Batch Loss:     2.363374, Lr: 0.000043, Tokens per sec:   2830
2023-03-15 08:08:33,746 - INFO - __main__ - Epoch  85, Step:  183600, Batch Loss:     2.281714, Lr: 0.000043, Tokens per sec:   2802
2023-03-15 08:08:52,336 - INFO - __main__ - Epoch  85, Step:  183700, Batch Loss:     2.211784, Lr: 0.000043, Tokens per sec:   2896
2023-03-15 08:09:11,491 - INFO - __main__ - Epoch  85, Step:  183800, Batch Loss:     1.795184, Lr: 0.000043, Tokens per sec:   2805
2023-03-15 08:09:30,481 - INFO - __main__ - Epoch  85, Step:  183900, Batch Loss:     2.006819, Lr: 0.000043, Tokens per sec:   2813
2023-03-15 08:09:49,969 - INFO - __main__ - Epoch  85, Step:  184000, Batch Loss:     2.184645, Lr: 0.000043, Tokens per sec:   2814
2023-03-15 08:10:08,176 - INFO - __main__ - Epoch  85, Step:  184100, Batch Loss:     1.301836, Lr: 0.000043, Tokens per sec:   3003
2023-03-15 08:10:27,404 - INFO - __main__ - Epoch  85, Step:  184200, Batch Loss:     1.758752, Lr: 0.000043, Tokens per sec:   2802
2023-03-15 08:10:46,782 - INFO - __main__ - Epoch  85, Step:  184300, Batch Loss:     2.138035, Lr: 0.000043, Tokens per sec:   2742
2023-03-15 08:11:06,083 - INFO - __main__ - Epoch  85, Step:  184400, Batch Loss:     1.986503, Lr: 0.000043, Tokens per sec:   2751
2023-03-15 08:11:24,562 - INFO - __main__ - Epoch  85, Step:  184500, Batch Loss:     1.616555, Lr: 0.000043, Tokens per sec:   2906
2023-03-15 08:11:42,753 - INFO - __main__ - Epoch  85, Step:  184600, Batch Loss:     1.546293, Lr: 0.000043, Tokens per sec:   2929
2023-03-15 08:12:00,832 - INFO - __main__ - Epoch  85, Step:  184700, Batch Loss:     2.484993, Lr: 0.000043, Tokens per sec:   2944
2023-03-15 08:12:19,378 - INFO - __main__ - Epoch  85, Step:  184800, Batch Loss:     1.701604, Lr: 0.000043, Tokens per sec:   2921
2023-03-15 08:12:37,703 - INFO - __main__ - Epoch  85, Step:  184900, Batch Loss:     1.684252, Lr: 0.000043, Tokens per sec:   2890
2023-03-15 08:12:55,968 - INFO - __main__ - Epoch  85, Step:  185000, Batch Loss:     2.182266, Lr: 0.000043, Tokens per sec:   2996
2023-03-15 08:13:15,030 - INFO - __main__ - Epoch  85, Step:  185100, Batch Loss:     1.585565, Lr: 0.000043, Tokens per sec:   2789
2023-03-15 08:13:33,444 - INFO - __main__ - Epoch  85, Step:  185200, Batch Loss:     1.159955, Lr: 0.000043, Tokens per sec:   2981
2023-03-15 08:13:36,531 - INFO - __main__ - Epoch  85: total training loss 4070.76
2023-03-15 08:13:36,532 - INFO - __main__ - Epoch 86
2023-03-15 08:13:52,938 - INFO - __main__ - Epoch  86, Step:  185300, Batch Loss:     2.344216, Lr: 0.000043, Tokens per sec:   2811
2023-03-15 08:14:11,376 - INFO - __main__ - Epoch  86, Step:  185400, Batch Loss:     0.812364, Lr: 0.000043, Tokens per sec:   2935
2023-03-15 08:14:30,826 - INFO - __main__ - Epoch  86, Step:  185500, Batch Loss:     2.686642, Lr: 0.000043, Tokens per sec:   2740
2023-03-15 08:14:49,907 - INFO - __main__ - Epoch  86, Step:  185600, Batch Loss:     2.083324, Lr: 0.000043, Tokens per sec:   2813
2023-03-15 08:15:08,188 - INFO - __main__ - Epoch  86, Step:  185700, Batch Loss:     1.600460, Lr: 0.000043, Tokens per sec:   2914
2023-03-15 08:15:26,523 - INFO - __main__ - Epoch  86, Step:  185800, Batch Loss:     2.040231, Lr: 0.000043, Tokens per sec:   2957
2023-03-15 08:15:44,687 - INFO - __main__ - Epoch  86, Step:  185900, Batch Loss:     2.677687, Lr: 0.000043, Tokens per sec:   2970
2023-03-15 08:16:03,199 - INFO - __main__ - Epoch  86, Step:  186000, Batch Loss:     1.908497, Lr: 0.000043, Tokens per sec:   2937
2023-03-15 08:16:22,165 - INFO - __main__ - Epoch  86, Step:  186100, Batch Loss:     1.472450, Lr: 0.000043, Tokens per sec:   2799
2023-03-15 08:16:41,452 - INFO - __main__ - Epoch  86, Step:  186200, Batch Loss:     1.502980, Lr: 0.000043, Tokens per sec:   2795
2023-03-15 08:17:00,296 - INFO - __main__ - Epoch  86, Step:  186300, Batch Loss:     1.942185, Lr: 0.000043, Tokens per sec:   2802
2023-03-15 08:17:19,217 - INFO - __main__ - Epoch  86, Step:  186400, Batch Loss:     1.721150, Lr: 0.000043, Tokens per sec:   2875
2023-03-15 08:17:38,450 - INFO - __main__ - Epoch  86, Step:  186500, Batch Loss:     2.676799, Lr: 0.000043, Tokens per sec:   2840
2023-03-15 08:17:57,093 - INFO - __main__ - Epoch  86, Step:  186600, Batch Loss:     1.986988, Lr: 0.000043, Tokens per sec:   2899
2023-03-15 08:18:15,834 - INFO - __main__ - Epoch  86, Step:  186700, Batch Loss:     1.945296, Lr: 0.000043, Tokens per sec:   2909
2023-03-15 08:18:34,235 - INFO - __main__ - Epoch  86, Step:  186800, Batch Loss:     1.986748, Lr: 0.000043, Tokens per sec:   2904
2023-03-15 08:18:52,723 - INFO - __main__ - Epoch  86, Step:  186900, Batch Loss:     1.522472, Lr: 0.000043, Tokens per sec:   2923
2023-03-15 08:19:11,491 - INFO - __main__ - Epoch  86, Step:  187000, Batch Loss:     1.871686, Lr: 0.000043, Tokens per sec:   2878
2023-03-15 08:19:30,029 - INFO - __main__ - Epoch  86, Step:  187100, Batch Loss:     2.473760, Lr: 0.000043, Tokens per sec:   2898
2023-03-15 08:19:48,632 - INFO - __main__ - Epoch  86, Step:  187200, Batch Loss:     1.879627, Lr: 0.000043, Tokens per sec:   2895
2023-03-15 08:20:07,865 - INFO - __main__ - Epoch  86, Step:  187300, Batch Loss:     1.355120, Lr: 0.000043, Tokens per sec:   2788
2023-03-15 08:20:25,107 - INFO - __main__ - Epoch  86: total training loss 4034.57
2023-03-15 08:20:25,108 - INFO - __main__ - Epoch 87
2023-03-15 08:20:26,650 - INFO - __main__ - Epoch  87, Step:  187400, Batch Loss:     1.113994, Lr: 0.000042, Tokens per sec:   1928
2023-03-15 08:20:45,514 - INFO - __main__ - Epoch  87, Step:  187500, Batch Loss:     1.167467, Lr: 0.000042, Tokens per sec:   2848
2023-03-15 08:21:04,603 - INFO - __main__ - Epoch  87, Step:  187600, Batch Loss:     2.177820, Lr: 0.000042, Tokens per sec:   2849
2023-03-15 08:21:23,721 - INFO - __main__ - Epoch  87, Step:  187700, Batch Loss:     1.825671, Lr: 0.000042, Tokens per sec:   2829
2023-03-15 08:21:43,670 - INFO - __main__ - Epoch  87, Step:  187800, Batch Loss:     1.828828, Lr: 0.000042, Tokens per sec:   2735
2023-03-15 08:22:02,731 - INFO - __main__ - Epoch  87, Step:  187900, Batch Loss:     1.968593, Lr: 0.000042, Tokens per sec:   2848
2023-03-15 08:22:21,132 - INFO - __main__ - Epoch  87, Step:  188000, Batch Loss:     2.237791, Lr: 0.000042, Tokens per sec:   2917
2023-03-15 08:22:40,576 - INFO - __main__ - Epoch  87, Step:  188100, Batch Loss:     2.055408, Lr: 0.000042, Tokens per sec:   2796
2023-03-15 08:22:59,135 - INFO - __main__ - Epoch  87, Step:  188200, Batch Loss:     1.810281, Lr: 0.000042, Tokens per sec:   2910
2023-03-15 08:23:19,167 - INFO - __main__ - Epoch  87, Step:  188300, Batch Loss:     1.534465, Lr: 0.000042, Tokens per sec:   2702
2023-03-15 08:23:37,795 - INFO - __main__ - Epoch  87, Step:  188400, Batch Loss:     1.845668, Lr: 0.000042, Tokens per sec:   2897
2023-03-15 08:23:57,604 - INFO - __main__ - Epoch  87, Step:  188500, Batch Loss:     2.930871, Lr: 0.000042, Tokens per sec:   2707
2023-03-15 08:24:16,861 - INFO - __main__ - Epoch  87, Step:  188600, Batch Loss:     1.348938, Lr: 0.000042, Tokens per sec:   2714
2023-03-15 08:24:36,414 - INFO - __main__ - Epoch  87, Step:  188700, Batch Loss:     2.254531, Lr: 0.000042, Tokens per sec:   2750
2023-03-15 08:24:55,732 - INFO - __main__ - Epoch  87, Step:  188800, Batch Loss:     1.863809, Lr: 0.000042, Tokens per sec:   2824
2023-03-15 08:25:14,204 - INFO - __main__ - Epoch  87, Step:  188900, Batch Loss:     1.813638, Lr: 0.000042, Tokens per sec:   2895
2023-03-15 08:25:32,449 - INFO - __main__ - Epoch  87, Step:  189000, Batch Loss:     1.732386, Lr: 0.000042, Tokens per sec:   2982
2023-03-15 08:25:52,138 - INFO - __main__ - Epoch  87, Step:  189100, Batch Loss:     2.138476, Lr: 0.000042, Tokens per sec:   2682
2023-03-15 08:26:11,239 - INFO - __main__ - Epoch  87, Step:  189200, Batch Loss:     2.725713, Lr: 0.000042, Tokens per sec:   2814
2023-03-15 08:26:30,316 - INFO - __main__ - Epoch  87, Step:  189300, Batch Loss:     1.843740, Lr: 0.000042, Tokens per sec:   2822
2023-03-15 08:26:48,581 - INFO - __main__ - Epoch  87, Step:  189400, Batch Loss:     2.092607, Lr: 0.000042, Tokens per sec:   2961
2023-03-15 08:27:07,066 - INFO - __main__ - Epoch  87, Step:  189500, Batch Loss:     2.030640, Lr: 0.000042, Tokens per sec:   2893
2023-03-15 08:27:21,132 - INFO - __main__ - Epoch  87: total training loss 3955.99
2023-03-15 08:27:21,133 - INFO - __main__ - Epoch 88
2023-03-15 08:27:26,904 - INFO - __main__ - Epoch  88, Step:  189600, Batch Loss:     2.111295, Lr: 0.000042, Tokens per sec:   2457
2023-03-15 08:27:45,594 - INFO - __main__ - Epoch  88, Step:  189700, Batch Loss:     1.246709, Lr: 0.000042, Tokens per sec:   2894
2023-03-15 08:28:05,376 - INFO - __main__ - Epoch  88, Step:  189800, Batch Loss:     1.031541, Lr: 0.000042, Tokens per sec:   2746
2023-03-15 08:28:24,144 - INFO - __main__ - Epoch  88, Step:  189900, Batch Loss:     1.361017, Lr: 0.000042, Tokens per sec:   2881
2023-03-15 08:28:42,700 - INFO - __main__ - Epoch  88, Step:  190000, Batch Loss:     2.800567, Lr: 0.000042, Tokens per sec:   2848
2023-03-15 08:29:00,918 - INFO - __main__ - Epoch  88, Step:  190100, Batch Loss:     1.223211, Lr: 0.000042, Tokens per sec:   2968
2023-03-15 08:29:19,089 - INFO - __main__ - Epoch  88, Step:  190200, Batch Loss:     2.140152, Lr: 0.000042, Tokens per sec:   2952
2023-03-15 08:29:38,138 - INFO - __main__ - Epoch  88, Step:  190300, Batch Loss:     1.879340, Lr: 0.000042, Tokens per sec:   2848
2023-03-15 08:29:56,837 - INFO - __main__ - Epoch  88, Step:  190400, Batch Loss:     1.332054, Lr: 0.000042, Tokens per sec:   2911
2023-03-15 08:30:16,028 - INFO - __main__ - Epoch  88, Step:  190500, Batch Loss:     1.231844, Lr: 0.000042, Tokens per sec:   2793
2023-03-15 08:30:34,651 - INFO - __main__ - Epoch  88, Step:  190600, Batch Loss:     2.136677, Lr: 0.000042, Tokens per sec:   2932
2023-03-15 08:30:54,375 - INFO - __main__ - Epoch  88, Step:  190700, Batch Loss:     1.909026, Lr: 0.000042, Tokens per sec:   2726
2023-03-15 08:31:13,469 - INFO - __main__ - Epoch  88, Step:  190800, Batch Loss:     1.592241, Lr: 0.000042, Tokens per sec:   2769
2023-03-15 08:31:32,157 - INFO - __main__ - Epoch  88, Step:  190900, Batch Loss:     1.703368, Lr: 0.000042, Tokens per sec:   2856
2023-03-15 08:31:50,391 - INFO - __main__ - Epoch  88, Step:  191000, Batch Loss:     1.811188, Lr: 0.000042, Tokens per sec:   2955
2023-03-15 08:32:09,179 - INFO - __main__ - Epoch  88, Step:  191100, Batch Loss:     1.453022, Lr: 0.000042, Tokens per sec:   2832
2023-03-15 08:32:28,163 - INFO - __main__ - Epoch  88, Step:  191200, Batch Loss:     2.558275, Lr: 0.000042, Tokens per sec:   2878
2023-03-15 08:32:46,740 - INFO - __main__ - Epoch  88, Step:  191300, Batch Loss:     2.502370, Lr: 0.000042, Tokens per sec:   2899
2023-03-15 08:33:05,279 - INFO - __main__ - Epoch  88, Step:  191400, Batch Loss:     1.097244, Lr: 0.000042, Tokens per sec:   2960
2023-03-15 08:33:24,588 - INFO - __main__ - Epoch  88, Step:  191500, Batch Loss:     1.082441, Lr: 0.000042, Tokens per sec:   2780
2023-03-15 08:33:43,280 - INFO - __main__ - Epoch  88, Step:  191600, Batch Loss:     1.784364, Lr: 0.000042, Tokens per sec:   2897
2023-03-15 08:34:02,220 - INFO - __main__ - Epoch  88, Step:  191700, Batch Loss:     1.496853, Lr: 0.000042, Tokens per sec:   2820
2023-03-15 08:34:11,914 - INFO - __main__ - Epoch  88: total training loss 3886.98
2023-03-15 08:34:11,915 - INFO - __main__ - Epoch 89
2023-03-15 08:34:21,545 - INFO - __main__ - Epoch  89, Step:  191800, Batch Loss:     1.442422, Lr: 0.000041, Tokens per sec:   2744
2023-03-15 08:34:40,048 - INFO - __main__ - Epoch  89, Step:  191900, Batch Loss:     1.624051, Lr: 0.000041, Tokens per sec:   2894
2023-03-15 08:34:58,824 - INFO - __main__ - Epoch  89, Step:  192000, Batch Loss:     1.187475, Lr: 0.000041, Tokens per sec:   2877
2023-03-15 08:35:17,337 - INFO - __main__ - Epoch  89, Step:  192100, Batch Loss:     1.559852, Lr: 0.000041, Tokens per sec:   2953
2023-03-15 08:35:36,346 - INFO - __main__ - Epoch  89, Step:  192200, Batch Loss:     2.087254, Lr: 0.000041, Tokens per sec:   2856
2023-03-15 08:35:55,043 - INFO - __main__ - Epoch  89, Step:  192300, Batch Loss:     1.512316, Lr: 0.000041, Tokens per sec:   2817
2023-03-15 08:36:13,583 - INFO - __main__ - Epoch  89, Step:  192400, Batch Loss:     2.255022, Lr: 0.000041, Tokens per sec:   2946
2023-03-15 08:36:32,873 - INFO - __main__ - Epoch  89, Step:  192500, Batch Loss:     1.651363, Lr: 0.000041, Tokens per sec:   2736
2023-03-15 08:36:51,450 - INFO - __main__ - Epoch  89, Step:  192600, Batch Loss:     1.861529, Lr: 0.000041, Tokens per sec:   2933
2023-03-15 08:37:09,973 - INFO - __main__ - Epoch  89, Step:  192700, Batch Loss:     1.971304, Lr: 0.000041, Tokens per sec:   2879
2023-03-15 08:37:29,036 - INFO - __main__ - Epoch  89, Step:  192800, Batch Loss:     1.066479, Lr: 0.000041, Tokens per sec:   2834
2023-03-15 08:37:47,913 - INFO - __main__ - Epoch  89, Step:  192900, Batch Loss:     1.416935, Lr: 0.000041, Tokens per sec:   2831
2023-03-15 08:38:06,277 - INFO - __main__ - Epoch  89, Step:  193000, Batch Loss:     1.657010, Lr: 0.000041, Tokens per sec:   2901
2023-03-15 08:38:25,051 - INFO - __main__ - Epoch  89, Step:  193100, Batch Loss:     1.586122, Lr: 0.000041, Tokens per sec:   2876
2023-03-15 08:38:44,549 - INFO - __main__ - Epoch  89, Step:  193200, Batch Loss:     1.390448, Lr: 0.000041, Tokens per sec:   2743
2023-03-15 08:39:03,220 - INFO - __main__ - Epoch  89, Step:  193300, Batch Loss:     1.785897, Lr: 0.000041, Tokens per sec:   2923
2023-03-15 08:39:22,931 - INFO - __main__ - Epoch  89, Step:  193400, Batch Loss:     1.068465, Lr: 0.000041, Tokens per sec:   2706
2023-03-15 08:39:42,320 - INFO - __main__ - Epoch  89, Step:  193500, Batch Loss:     1.658112, Lr: 0.000041, Tokens per sec:   2798
2023-03-15 08:40:00,559 - INFO - __main__ - Epoch  89, Step:  193600, Batch Loss:     2.036185, Lr: 0.000041, Tokens per sec:   2904
2023-03-15 08:40:19,660 - INFO - __main__ - Epoch  89, Step:  193700, Batch Loss:     2.771972, Lr: 0.000041, Tokens per sec:   2816
2023-03-15 08:40:38,732 - INFO - __main__ - Epoch  89, Step:  193800, Batch Loss:     1.861530, Lr: 0.000041, Tokens per sec:   2816
2023-03-15 08:40:58,066 - INFO - __main__ - Epoch  89, Step:  193900, Batch Loss:     2.034479, Lr: 0.000041, Tokens per sec:   2855
2023-03-15 08:41:03,766 - INFO - __main__ - Epoch  89: total training loss 3836.94
2023-03-15 08:41:03,767 - INFO - __main__ - Epoch 90
2023-03-15 08:41:17,636 - INFO - __main__ - Epoch  90, Step:  194000, Batch Loss:     1.676458, Lr: 0.000041, Tokens per sec:   2608
2023-03-15 08:41:36,664 - INFO - __main__ - Epoch  90, Step:  194100, Batch Loss:     1.953285, Lr: 0.000041, Tokens per sec:   2836
2023-03-15 08:41:55,148 - INFO - __main__ - Epoch  90, Step:  194200, Batch Loss:     1.959242, Lr: 0.000041, Tokens per sec:   2928
2023-03-15 08:42:14,025 - INFO - __main__ - Epoch  90, Step:  194300, Batch Loss:     2.398892, Lr: 0.000041, Tokens per sec:   2835
2023-03-15 08:42:32,461 - INFO - __main__ - Epoch  90, Step:  194400, Batch Loss:     0.955954, Lr: 0.000041, Tokens per sec:   2917
2023-03-15 08:42:50,971 - INFO - __main__ - Epoch  90, Step:  194500, Batch Loss:     2.427876, Lr: 0.000041, Tokens per sec:   2897
2023-03-15 08:43:09,594 - INFO - __main__ - Epoch  90, Step:  194600, Batch Loss:     1.040201, Lr: 0.000041, Tokens per sec:   2844
2023-03-15 08:43:28,538 - INFO - __main__ - Epoch  90, Step:  194700, Batch Loss:     1.895376, Lr: 0.000041, Tokens per sec:   2818
2023-03-15 08:43:46,989 - INFO - __main__ - Epoch  90, Step:  194800, Batch Loss:     1.757267, Lr: 0.000041, Tokens per sec:   2920
2023-03-15 08:44:05,772 - INFO - __main__ - Epoch  90, Step:  194900, Batch Loss:     1.185541, Lr: 0.000041, Tokens per sec:   2873
2023-03-15 08:44:24,382 - INFO - __main__ - Epoch  90, Step:  195000, Batch Loss:     2.013071, Lr: 0.000041, Tokens per sec:   2900
2023-03-15 08:44:43,876 - INFO - __main__ - Epoch  90, Step:  195100, Batch Loss:     1.838265, Lr: 0.000041, Tokens per sec:   2761
2023-03-15 08:45:03,041 - INFO - __main__ - Epoch  90, Step:  195200, Batch Loss:     1.235092, Lr: 0.000041, Tokens per sec:   2829
2023-03-15 08:45:21,249 - INFO - __main__ - Epoch  90, Step:  195300, Batch Loss:     1.809960, Lr: 0.000041, Tokens per sec:   2978
2023-03-15 08:45:39,639 - INFO - __main__ - Epoch  90, Step:  195400, Batch Loss:     1.943206, Lr: 0.000041, Tokens per sec:   2942
2023-03-15 08:45:58,812 - INFO - __main__ - Epoch  90, Step:  195500, Batch Loss:     2.009942, Lr: 0.000041, Tokens per sec:   2833
2023-03-15 08:46:17,408 - INFO - __main__ - Epoch  90, Step:  195600, Batch Loss:     1.976862, Lr: 0.000041, Tokens per sec:   2919
2023-03-15 08:46:36,431 - INFO - __main__ - Epoch  90, Step:  195700, Batch Loss:     2.006160, Lr: 0.000041, Tokens per sec:   2842
2023-03-15 08:46:55,051 - INFO - __main__ - Epoch  90, Step:  195800, Batch Loss:     1.734441, Lr: 0.000041, Tokens per sec:   2882
2023-03-15 08:47:13,708 - INFO - __main__ - Epoch  90, Step:  195900, Batch Loss:     1.797952, Lr: 0.000041, Tokens per sec:   2938
2023-03-15 08:47:31,904 - INFO - __main__ - Epoch  90, Step:  196000, Batch Loss:     1.525318, Lr: 0.000041, Tokens per sec:   2931
2023-03-15 08:47:50,379 - INFO - __main__ - Epoch  90, Step:  196100, Batch Loss:     2.042476, Lr: 0.000041, Tokens per sec:   2901
2023-03-15 08:47:52,452 - INFO - __main__ - Epoch  90: total training loss 3761.46
2023-03-15 08:47:52,453 - INFO - __main__ - Epoch 91
2023-03-15 08:48:09,534 - INFO - __main__ - Epoch  91, Step:  196200, Batch Loss:     1.421344, Lr: 0.000040, Tokens per sec:   2849
2023-03-15 08:48:28,317 - INFO - __main__ - Epoch  91, Step:  196300, Batch Loss:     1.629166, Lr: 0.000040, Tokens per sec:   2872
2023-03-15 08:48:47,416 - INFO - __main__ - Epoch  91, Step:  196400, Batch Loss:     2.213620, Lr: 0.000040, Tokens per sec:   2831
2023-03-15 08:49:06,252 - INFO - __main__ - Epoch  91, Step:  196500, Batch Loss:     2.364247, Lr: 0.000040, Tokens per sec:   2911
2023-03-15 08:49:25,746 - INFO - __main__ - Epoch  91, Step:  196600, Batch Loss:     1.653095, Lr: 0.000040, Tokens per sec:   2768
2023-03-15 08:49:45,151 - INFO - __main__ - Epoch  91, Step:  196700, Batch Loss:     1.616814, Lr: 0.000040, Tokens per sec:   2801
2023-03-15 08:50:03,791 - INFO - __main__ - Epoch  91, Step:  196800, Batch Loss:     1.631917, Lr: 0.000040, Tokens per sec:   2893
2023-03-15 08:50:22,827 - INFO - __main__ - Epoch  91, Step:  196900, Batch Loss:     1.485511, Lr: 0.000040, Tokens per sec:   2908
2023-03-15 08:50:42,683 - INFO - __main__ - Epoch  91, Step:  197000, Batch Loss:     1.793851, Lr: 0.000040, Tokens per sec:   2692
2023-03-15 08:51:01,783 - INFO - __main__ - Epoch  91, Step:  197100, Batch Loss:     1.237219, Lr: 0.000040, Tokens per sec:   2788
2023-03-15 08:51:19,971 - INFO - __main__ - Epoch  91, Step:  197200, Batch Loss:     1.490159, Lr: 0.000040, Tokens per sec:   2945
2023-03-15 08:51:39,745 - INFO - __main__ - Epoch  91, Step:  197300, Batch Loss:     1.908670, Lr: 0.000040, Tokens per sec:   2728
2023-03-15 08:51:58,238 - INFO - __main__ - Epoch  91, Step:  197400, Batch Loss:     1.682257, Lr: 0.000040, Tokens per sec:   2905
2023-03-15 08:52:16,826 - INFO - __main__ - Epoch  91, Step:  197500, Batch Loss:     1.877559, Lr: 0.000040, Tokens per sec:   2827
2023-03-15 08:52:36,034 - INFO - __main__ - Epoch  91, Step:  197600, Batch Loss:     1.342380, Lr: 0.000040, Tokens per sec:   2833
2023-03-15 08:52:55,758 - INFO - __main__ - Epoch  91, Step:  197700, Batch Loss:     1.744999, Lr: 0.000040, Tokens per sec:   2770
2023-03-15 08:53:15,238 - INFO - __main__ - Epoch  91, Step:  197800, Batch Loss:     1.362100, Lr: 0.000040, Tokens per sec:   2759
2023-03-15 08:53:34,383 - INFO - __main__ - Epoch  91, Step:  197900, Batch Loss:     1.630263, Lr: 0.000040, Tokens per sec:   2807
2023-03-15 08:53:53,868 - INFO - __main__ - Epoch  91, Step:  198000, Batch Loss:     2.204972, Lr: 0.000040, Tokens per sec:   2742
2023-03-15 08:54:13,660 - INFO - __main__ - Epoch  91, Step:  198100, Batch Loss:     1.198907, Lr: 0.000040, Tokens per sec:   2718
2023-03-15 08:54:33,590 - INFO - __main__ - Epoch  91, Step:  198200, Batch Loss:     1.114879, Lr: 0.000040, Tokens per sec:   2664
2023-03-15 08:54:51,088 - INFO - __main__ - Epoch  91: total training loss 3685.75
2023-03-15 08:54:51,088 - INFO - __main__ - Epoch 92
2023-03-15 08:54:53,563 - INFO - __main__ - Epoch  92, Step:  198300, Batch Loss:     1.760121, Lr: 0.000040, Tokens per sec:   2557
2023-03-15 08:55:12,341 - INFO - __main__ - Epoch  92, Step:  198400, Batch Loss:     0.974466, Lr: 0.000040, Tokens per sec:   2850
2023-03-15 08:55:31,352 - INFO - __main__ - Epoch  92, Step:  198500, Batch Loss:     1.433109, Lr: 0.000040, Tokens per sec:   2830
2023-03-15 08:55:50,789 - INFO - __main__ - Epoch  92, Step:  198600, Batch Loss:     1.779057, Lr: 0.000040, Tokens per sec:   2790
2023-03-15 08:56:10,755 - INFO - __main__ - Epoch  92, Step:  198700, Batch Loss:     0.869598, Lr: 0.000040, Tokens per sec:   2741
2023-03-15 08:56:30,709 - INFO - __main__ - Epoch  92, Step:  198800, Batch Loss:     1.739645, Lr: 0.000040, Tokens per sec:   2654
2023-03-15 08:56:50,382 - INFO - __main__ - Epoch  92, Step:  198900, Batch Loss:     1.347752, Lr: 0.000040, Tokens per sec:   2742
2023-03-15 08:57:09,359 - INFO - __main__ - Epoch  92, Step:  199000, Batch Loss:     2.292990, Lr: 0.000040, Tokens per sec:   2813
2023-03-15 08:57:27,716 - INFO - __main__ - Epoch  92, Step:  199100, Batch Loss:     2.498417, Lr: 0.000040, Tokens per sec:   2931
2023-03-15 08:57:46,325 - INFO - __main__ - Epoch  92, Step:  199200, Batch Loss:     2.763495, Lr: 0.000040, Tokens per sec:   2858
2023-03-15 08:58:05,145 - INFO - __main__ - Epoch  92, Step:  199300, Batch Loss:     1.639689, Lr: 0.000040, Tokens per sec:   2891
2023-03-15 08:58:23,318 - INFO - __main__ - Epoch  92, Step:  199400, Batch Loss:     1.360596, Lr: 0.000040, Tokens per sec:   3004
2023-03-15 08:58:41,627 - INFO - __main__ - Epoch  92, Step:  199500, Batch Loss:     3.018327, Lr: 0.000040, Tokens per sec:   2944
2023-03-15 08:59:00,433 - INFO - __main__ - Epoch  92, Step:  199600, Batch Loss:     1.552323, Lr: 0.000040, Tokens per sec:   2840
2023-03-15 08:59:19,684 - INFO - __main__ - Epoch  92, Step:  199700, Batch Loss:     1.545408, Lr: 0.000040, Tokens per sec:   2775
2023-03-15 08:59:38,075 - INFO - __main__ - Epoch  92, Step:  199800, Batch Loss:     1.384571, Lr: 0.000040, Tokens per sec:   2921
2023-03-15 08:59:56,841 - INFO - __main__ - Epoch  92, Step:  199900, Batch Loss:     1.798975, Lr: 0.000040, Tokens per sec:   2924
2023-03-15 09:00:15,344 - INFO - __main__ - Epoch  92, Step:  200000, Batch Loss:     2.184733, Lr: 0.000040, Tokens per sec:   2910
2023-03-15 09:00:33,747 - INFO - __main__ - Epoch  92, Step:  200100, Batch Loss:     0.979177, Lr: 0.000040, Tokens per sec:   2957
2023-03-15 09:00:53,204 - INFO - __main__ - Epoch  92, Step:  200200, Batch Loss:     1.236779, Lr: 0.000040, Tokens per sec:   2724
2023-03-15 09:01:13,065 - INFO - __main__ - Epoch  92, Step:  200300, Batch Loss:     2.546415, Lr: 0.000040, Tokens per sec:   2691
2023-03-15 09:01:31,816 - INFO - __main__ - Epoch  92, Step:  200400, Batch Loss:     2.352124, Lr: 0.000040, Tokens per sec:   2816
2023-03-15 09:01:44,231 - INFO - __main__ - Epoch  92: total training loss 3592.38
2023-03-15 09:01:44,232 - INFO - __main__ - Epoch 93
2023-03-15 09:01:50,411 - INFO - __main__ - Epoch  93, Step:  200500, Batch Loss:     1.677213, Lr: 0.000040, Tokens per sec:   2789
2023-03-15 09:02:09,384 - INFO - __main__ - Epoch  93, Step:  200600, Batch Loss:     1.942229, Lr: 0.000040, Tokens per sec:   2815
2023-03-15 09:02:27,431 - INFO - __main__ - Epoch  93, Step:  200700, Batch Loss:     1.159838, Lr: 0.000040, Tokens per sec:   2962
2023-03-15 09:02:46,060 - INFO - __main__ - Epoch  93, Step:  200800, Batch Loss:     1.644149, Lr: 0.000040, Tokens per sec:   2918
2023-03-15 09:03:04,846 - INFO - __main__ - Epoch  93, Step:  200900, Batch Loss:     1.289109, Lr: 0.000040, Tokens per sec:   2855
2023-03-15 09:03:24,711 - INFO - __main__ - Epoch  93, Step:  201000, Batch Loss:     1.675539, Lr: 0.000040, Tokens per sec:   2746
2023-03-15 09:03:43,414 - INFO - __main__ - Epoch  93, Step:  201100, Batch Loss:     1.189985, Lr: 0.000040, Tokens per sec:   2862
2023-03-15 09:04:01,725 - INFO - __main__ - Epoch  93, Step:  201200, Batch Loss:     2.007001, Lr: 0.000040, Tokens per sec:   2971
2023-03-15 09:04:20,856 - INFO - __main__ - Epoch  93, Step:  201300, Batch Loss:     1.159597, Lr: 0.000040, Tokens per sec:   2821
2023-03-15 09:04:40,011 - INFO - __main__ - Epoch  93, Step:  201400, Batch Loss:     1.577324, Lr: 0.000040, Tokens per sec:   2800
2023-03-15 09:04:58,812 - INFO - __main__ - Epoch  93, Step:  201500, Batch Loss:     1.028528, Lr: 0.000040, Tokens per sec:   2856
2023-03-15 09:05:17,224 - INFO - __main__ - Epoch  93, Step:  201600, Batch Loss:     1.613394, Lr: 0.000040, Tokens per sec:   2911
2023-03-15 09:05:36,335 - INFO - __main__ - Epoch  93, Step:  201700, Batch Loss:     1.034949, Lr: 0.000040, Tokens per sec:   2772
2023-03-15 09:05:55,138 - INFO - __main__ - Epoch  93, Step:  201800, Batch Loss:     1.132088, Lr: 0.000040, Tokens per sec:   2876
2023-03-15 09:06:14,006 - INFO - __main__ - Epoch  93, Step:  201900, Batch Loss:     1.272297, Lr: 0.000040, Tokens per sec:   2860
2023-03-15 09:06:33,376 - INFO - __main__ - Epoch  93, Step:  202000, Batch Loss:     2.096029, Lr: 0.000040, Tokens per sec:   2776
2023-03-15 09:06:53,067 - INFO - __main__ - Epoch  93, Step:  202100, Batch Loss:     1.478137, Lr: 0.000040, Tokens per sec:   2787
2023-03-15 09:07:12,087 - INFO - __main__ - Epoch  93, Step:  202200, Batch Loss:     1.735522, Lr: 0.000040, Tokens per sec:   2762
2023-03-15 09:07:31,437 - INFO - __main__ - Epoch  93, Step:  202300, Batch Loss:     1.679920, Lr: 0.000040, Tokens per sec:   2812
2023-03-15 09:07:50,171 - INFO - __main__ - Epoch  93, Step:  202400, Batch Loss:     1.205976, Lr: 0.000040, Tokens per sec:   2928
2023-03-15 09:08:08,665 - INFO - __main__ - Epoch  93, Step:  202500, Batch Loss:     2.318326, Lr: 0.000040, Tokens per sec:   2868
2023-03-15 09:08:27,598 - INFO - __main__ - Epoch  93, Step:  202600, Batch Loss:     2.311886, Lr: 0.000040, Tokens per sec:   2854
2023-03-15 09:08:36,968 - INFO - __main__ - Epoch  93: total training loss 3550.78
2023-03-15 09:08:36,969 - INFO - __main__ - Epoch 94
2023-03-15 09:08:48,352 - INFO - __main__ - Epoch  94, Step:  202700, Batch Loss:     1.763550, Lr: 0.000039, Tokens per sec:   2553
2023-03-15 09:09:06,819 - INFO - __main__ - Epoch  94, Step:  202800, Batch Loss:     1.452838, Lr: 0.000039, Tokens per sec:   2970
2023-03-15 09:09:26,384 - INFO - __main__ - Epoch  94, Step:  202900, Batch Loss:     1.591276, Lr: 0.000039, Tokens per sec:   2766
2023-03-15 09:09:45,839 - INFO - __main__ - Epoch  94, Step:  203000, Batch Loss:     1.616364, Lr: 0.000039, Tokens per sec:   2765
2023-03-15 09:10:04,575 - INFO - __main__ - Epoch  94, Step:  203100, Batch Loss:     1.283492, Lr: 0.000039, Tokens per sec:   2878
2023-03-15 09:10:23,641 - INFO - __main__ - Epoch  94, Step:  203200, Batch Loss:     1.302787, Lr: 0.000039, Tokens per sec:   2883
2023-03-15 09:10:42,886 - INFO - __main__ - Epoch  94, Step:  203300, Batch Loss:     2.550325, Lr: 0.000039, Tokens per sec:   2827
2023-03-15 09:11:02,458 - INFO - __main__ - Epoch  94, Step:  203400, Batch Loss:     1.914545, Lr: 0.000039, Tokens per sec:   2718
2023-03-15 09:11:22,272 - INFO - __main__ - Epoch  94, Step:  203500, Batch Loss:     1.296159, Lr: 0.000039, Tokens per sec:   2696
2023-03-15 09:11:41,271 - INFO - __main__ - Epoch  94, Step:  203600, Batch Loss:     1.480547, Lr: 0.000039, Tokens per sec:   2873
2023-03-15 09:12:00,923 - INFO - __main__ - Epoch  94, Step:  203700, Batch Loss:     1.364569, Lr: 0.000039, Tokens per sec:   2700
2023-03-15 09:12:20,707 - INFO - __main__ - Epoch  94, Step:  203800, Batch Loss:     1.905553, Lr: 0.000039, Tokens per sec:   2708
2023-03-15 09:12:39,124 - INFO - __main__ - Epoch  94, Step:  203900, Batch Loss:     1.643571, Lr: 0.000039, Tokens per sec:   2911
2023-03-15 09:12:57,693 - INFO - __main__ - Epoch  94, Step:  204000, Batch Loss:     1.245632, Lr: 0.000039, Tokens per sec:   2966
2023-03-15 09:13:16,550 - INFO - __main__ - Epoch  94, Step:  204100, Batch Loss:     1.205105, Lr: 0.000039, Tokens per sec:   2836
2023-03-15 09:13:35,222 - INFO - __main__ - Epoch  94, Step:  204200, Batch Loss:     0.788230, Lr: 0.000039, Tokens per sec:   2913
2023-03-15 09:13:54,560 - INFO - __main__ - Epoch  94, Step:  204300, Batch Loss:     1.918353, Lr: 0.000039, Tokens per sec:   2738
2023-03-15 09:14:13,643 - INFO - __main__ - Epoch  94, Step:  204400, Batch Loss:     1.678149, Lr: 0.000039, Tokens per sec:   2839
2023-03-15 09:14:32,957 - INFO - __main__ - Epoch  94, Step:  204500, Batch Loss:     1.505839, Lr: 0.000039, Tokens per sec:   2747
2023-03-15 09:14:51,555 - INFO - __main__ - Epoch  94, Step:  204600, Batch Loss:     2.213522, Lr: 0.000039, Tokens per sec:   2864
2023-03-15 09:15:09,807 - INFO - __main__ - Epoch  94, Step:  204700, Batch Loss:     1.210264, Lr: 0.000039, Tokens per sec:   2938
2023-03-15 09:15:28,335 - INFO - __main__ - Epoch  94, Step:  204800, Batch Loss:     1.611972, Lr: 0.000039, Tokens per sec:   2835
2023-03-15 09:15:33,322 - INFO - __main__ - Epoch  94: total training loss 3471.79
2023-03-15 09:15:33,323 - INFO - __main__ - Epoch 95
2023-03-15 09:15:47,470 - INFO - __main__ - Epoch  95, Step:  204900, Batch Loss:     1.641950, Lr: 0.000039, Tokens per sec:   2863
2023-03-15 09:16:06,815 - INFO - __main__ - Epoch  95, Step:  205000, Batch Loss:     1.592826, Lr: 0.000039, Tokens per sec:   2787
2023-03-15 09:16:26,043 - INFO - __main__ - Epoch  95, Step:  205100, Batch Loss:     1.012517, Lr: 0.000039, Tokens per sec:   2852
2023-03-15 09:16:44,461 - INFO - __main__ - Epoch  95, Step:  205200, Batch Loss:     1.496603, Lr: 0.000039, Tokens per sec:   2925
2023-03-15 09:17:04,218 - INFO - __main__ - Epoch  95, Step:  205300, Batch Loss:     1.727017, Lr: 0.000039, Tokens per sec:   2756
2023-03-15 09:17:23,777 - INFO - __main__ - Epoch  95, Step:  205400, Batch Loss:     1.231645, Lr: 0.000039, Tokens per sec:   2779
2023-03-15 09:17:43,058 - INFO - __main__ - Epoch  95, Step:  205500, Batch Loss:     2.128462, Lr: 0.000039, Tokens per sec:   2832
2023-03-15 09:18:01,500 - INFO - __main__ - Epoch  95, Step:  205600, Batch Loss:     2.136251, Lr: 0.000039, Tokens per sec:   2882
2023-03-15 09:18:19,890 - INFO - __main__ - Epoch  95, Step:  205700, Batch Loss:     1.963267, Lr: 0.000039, Tokens per sec:   2905
2023-03-15 09:18:38,479 - INFO - __main__ - Epoch  95, Step:  205800, Batch Loss:     1.517180, Lr: 0.000039, Tokens per sec:   2846
2023-03-15 09:18:57,962 - INFO - __main__ - Epoch  95, Step:  205900, Batch Loss:     1.051671, Lr: 0.000039, Tokens per sec:   2765
2023-03-15 09:19:17,314 - INFO - __main__ - Epoch  95, Step:  206000, Batch Loss:     2.208972, Lr: 0.000039, Tokens per sec:   2758
2023-03-15 09:19:36,531 - INFO - __main__ - Epoch  95, Step:  206100, Batch Loss:     1.633274, Lr: 0.000039, Tokens per sec:   2798
2023-03-15 09:19:55,597 - INFO - __main__ - Epoch  95, Step:  206200, Batch Loss:     1.342630, Lr: 0.000039, Tokens per sec:   2827
2023-03-15 09:20:15,071 - INFO - __main__ - Epoch  95, Step:  206300, Batch Loss:     1.633215, Lr: 0.000039, Tokens per sec:   2761
2023-03-15 09:20:33,652 - INFO - __main__ - Epoch  95, Step:  206400, Batch Loss:     1.974423, Lr: 0.000039, Tokens per sec:   2904
2023-03-15 09:20:51,853 - INFO - __main__ - Epoch  95, Step:  206500, Batch Loss:     1.232439, Lr: 0.000039, Tokens per sec:   2901
2023-03-15 09:21:11,951 - INFO - __main__ - Epoch  95, Step:  206600, Batch Loss:     1.745744, Lr: 0.000039, Tokens per sec:   2710
2023-03-15 09:21:30,915 - INFO - __main__ - Epoch  95, Step:  206700, Batch Loss:     1.526359, Lr: 0.000039, Tokens per sec:   2770
2023-03-15 09:21:49,772 - INFO - __main__ - Epoch  95, Step:  206800, Batch Loss:     1.988248, Lr: 0.000039, Tokens per sec:   2866
2023-03-15 09:22:08,846 - INFO - __main__ - Epoch  95, Step:  206900, Batch Loss:     1.706040, Lr: 0.000039, Tokens per sec:   2798
2023-03-15 09:22:28,072 - INFO - __main__ - Epoch  95, Step:  207000, Batch Loss:     1.428293, Lr: 0.000039, Tokens per sec:   2835
2023-03-15 09:22:29,074 - INFO - __main__ - Epoch  95: total training loss 3438.81
2023-03-15 09:22:29,075 - INFO - __main__ - Epoch 96
2023-03-15 09:22:47,508 - INFO - __main__ - Epoch  96, Step:  207100, Batch Loss:     1.764832, Lr: 0.000038, Tokens per sec:   2786
2023-03-15 09:23:06,865 - INFO - __main__ - Epoch  96, Step:  207200, Batch Loss:     1.321589, Lr: 0.000038, Tokens per sec:   2802
2023-03-15 09:23:25,604 - INFO - __main__ - Epoch  96, Step:  207300, Batch Loss:     1.125649, Lr: 0.000038, Tokens per sec:   2814
2023-03-15 09:23:44,611 - INFO - __main__ - Epoch  96, Step:  207400, Batch Loss:     1.117612, Lr: 0.000038, Tokens per sec:   2775
2023-03-15 09:24:03,761 - INFO - __main__ - Epoch  96, Step:  207500, Batch Loss:     2.200120, Lr: 0.000038, Tokens per sec:   2805
2023-03-15 09:24:23,293 - INFO - __main__ - Epoch  96, Step:  207600, Batch Loss:     2.135936, Lr: 0.000038, Tokens per sec:   2722
2023-03-15 09:24:41,966 - INFO - __main__ - Epoch  96, Step:  207700, Batch Loss:     0.874991, Lr: 0.000038, Tokens per sec:   2866
2023-03-15 09:25:00,350 - INFO - __main__ - Epoch  96, Step:  207800, Batch Loss:     1.750398, Lr: 0.000038, Tokens per sec:   2970
2023-03-15 09:25:18,916 - INFO - __main__ - Epoch  96, Step:  207900, Batch Loss:     1.092830, Lr: 0.000038, Tokens per sec:   2883
2023-03-15 09:25:37,814 - INFO - __main__ - Epoch  96, Step:  208000, Batch Loss:     1.612217, Lr: 0.000038, Tokens per sec:   2872
2023-03-15 09:25:57,347 - INFO - __main__ - Epoch  96, Step:  208100, Batch Loss:     1.572806, Lr: 0.000038, Tokens per sec:   2751
2023-03-15 09:26:16,211 - INFO - __main__ - Epoch  96, Step:  208200, Batch Loss:     1.014765, Lr: 0.000038, Tokens per sec:   2844
2023-03-15 09:26:34,691 - INFO - __main__ - Epoch  96, Step:  208300, Batch Loss:     1.153257, Lr: 0.000038, Tokens per sec:   2929
2023-03-15 09:26:53,585 - INFO - __main__ - Epoch  96, Step:  208400, Batch Loss:     1.497671, Lr: 0.000038, Tokens per sec:   2886
2023-03-15 09:27:12,301 - INFO - __main__ - Epoch  96, Step:  208500, Batch Loss:     1.484785, Lr: 0.000038, Tokens per sec:   2891
2023-03-15 09:27:31,535 - INFO - __main__ - Epoch  96, Step:  208600, Batch Loss:     1.337418, Lr: 0.000038, Tokens per sec:   2734
2023-03-15 09:27:50,752 - INFO - __main__ - Epoch  96, Step:  208700, Batch Loss:     1.528848, Lr: 0.000038, Tokens per sec:   2789
2023-03-15 09:28:09,631 - INFO - __main__ - Epoch  96, Step:  208800, Batch Loss:     1.243207, Lr: 0.000038, Tokens per sec:   2873
2023-03-15 09:28:29,028 - INFO - __main__ - Epoch  96, Step:  208900, Batch Loss:     1.755365, Lr: 0.000038, Tokens per sec:   2869
2023-03-15 09:28:48,608 - INFO - __main__ - Epoch  96, Step:  209000, Batch Loss:     1.733630, Lr: 0.000038, Tokens per sec:   2734
2023-03-15 09:29:07,107 - INFO - __main__ - Epoch  96, Step:  209100, Batch Loss:     1.879780, Lr: 0.000038, Tokens per sec:   2913
2023-03-15 09:29:23,606 - INFO - __main__ - Epoch  96: total training loss 3379.71
2023-03-15 09:29:23,606 - INFO - __main__ - Epoch 97
2023-03-15 09:29:26,887 - INFO - __main__ - Epoch  97, Step:  209200, Batch Loss:     1.079524, Lr: 0.000038, Tokens per sec:   2573
2023-03-15 09:29:46,586 - INFO - __main__ - Epoch  97, Step:  209300, Batch Loss:     1.111593, Lr: 0.000038, Tokens per sec:   2754
2023-03-15 09:30:04,887 - INFO - __main__ - Epoch  97, Step:  209400, Batch Loss:     1.394289, Lr: 0.000038, Tokens per sec:   2921
2023-03-15 09:30:24,401 - INFO - __main__ - Epoch  97, Step:  209500, Batch Loss:     1.888153, Lr: 0.000038, Tokens per sec:   2781
2023-03-15 09:30:43,437 - INFO - __main__ - Epoch  97, Step:  209600, Batch Loss:     1.488741, Lr: 0.000038, Tokens per sec:   2831
2023-03-15 09:31:02,306 - INFO - __main__ - Epoch  97, Step:  209700, Batch Loss:     1.426881, Lr: 0.000038, Tokens per sec:   2823
2023-03-15 09:31:21,783 - INFO - __main__ - Epoch  97, Step:  209800, Batch Loss:     1.821295, Lr: 0.000038, Tokens per sec:   2715
2023-03-15 09:31:40,975 - INFO - __main__ - Epoch  97, Step:  209900, Batch Loss:     1.770291, Lr: 0.000038, Tokens per sec:   2846
2023-03-15 09:31:59,837 - INFO - __main__ - Epoch  97, Step:  210000, Batch Loss:     1.741186, Lr: 0.000038, Tokens per sec:   2835
2023-03-15 09:32:18,294 - INFO - __main__ - Epoch  97, Step:  210100, Batch Loss:     0.555978, Lr: 0.000038, Tokens per sec:   2863
2023-03-15 09:32:37,569 - INFO - __main__ - Epoch  97, Step:  210200, Batch Loss:     1.314139, Lr: 0.000038, Tokens per sec:   2800
2023-03-15 09:32:56,812 - INFO - __main__ - Epoch  97, Step:  210300, Batch Loss:     1.557242, Lr: 0.000038, Tokens per sec:   2762
2023-03-15 09:33:15,253 - INFO - __main__ - Epoch  97, Step:  210400, Batch Loss:     1.225662, Lr: 0.000038, Tokens per sec:   2944
2023-03-15 09:33:34,146 - INFO - __main__ - Epoch  97, Step:  210500, Batch Loss:     1.039011, Lr: 0.000038, Tokens per sec:   2869
2023-03-15 09:33:52,925 - INFO - __main__ - Epoch  97, Step:  210600, Batch Loss:     1.092296, Lr: 0.000038, Tokens per sec:   2892
2023-03-15 09:34:12,667 - INFO - __main__ - Epoch  97, Step:  210700, Batch Loss:     1.839624, Lr: 0.000038, Tokens per sec:   2697
2023-03-15 09:34:31,224 - INFO - __main__ - Epoch  97, Step:  210800, Batch Loss:     1.491881, Lr: 0.000038, Tokens per sec:   2946
2023-03-15 09:34:49,539 - INFO - __main__ - Epoch  97, Step:  210900, Batch Loss:     1.957733, Lr: 0.000038, Tokens per sec:   2941
2023-03-15 09:35:08,571 - INFO - __main__ - Epoch  97, Step:  211000, Batch Loss:     2.217993, Lr: 0.000038, Tokens per sec:   2795
2023-03-15 09:35:27,695 - INFO - __main__ - Epoch  97, Step:  211100, Batch Loss:     1.695853, Lr: 0.000038, Tokens per sec:   2796
2023-03-15 09:35:47,078 - INFO - __main__ - Epoch  97, Step:  211200, Batch Loss:     1.606689, Lr: 0.000038, Tokens per sec:   2797
2023-03-15 09:36:05,766 - INFO - __main__ - Epoch  97, Step:  211300, Batch Loss:     2.184170, Lr: 0.000038, Tokens per sec:   2957
2023-03-15 09:36:17,296 - INFO - __main__ - Epoch  97: total training loss 3318.29
2023-03-15 09:36:17,297 - INFO - __main__ - Epoch 98
2023-03-15 09:36:24,673 - INFO - __main__ - Epoch  98, Step:  211400, Batch Loss:     1.807242, Lr: 0.000038, Tokens per sec:   2765
2023-03-15 09:36:42,969 - INFO - __main__ - Epoch  98, Step:  211500, Batch Loss:     1.982888, Lr: 0.000038, Tokens per sec:   2954
2023-03-15 09:37:01,405 - INFO - __main__ - Epoch  98, Step:  211600, Batch Loss:     1.264706, Lr: 0.000038, Tokens per sec:   2912
2023-03-15 09:37:20,089 - INFO - __main__ - Epoch  98, Step:  211700, Batch Loss:     1.350953, Lr: 0.000038, Tokens per sec:   2894
2023-03-15 09:37:39,128 - INFO - __main__ - Epoch  98, Step:  211800, Batch Loss:     1.896689, Lr: 0.000038, Tokens per sec:   2833
2023-03-15 09:37:57,883 - INFO - __main__ - Epoch  98, Step:  211900, Batch Loss:     2.169020, Lr: 0.000038, Tokens per sec:   2887
2023-03-15 09:38:16,436 - INFO - __main__ - Epoch  98, Step:  212000, Batch Loss:     1.539262, Lr: 0.000038, Tokens per sec:   2930
2023-03-15 09:38:35,389 - INFO - __main__ - Epoch  98, Step:  212100, Batch Loss:     1.754900, Lr: 0.000038, Tokens per sec:   2786
2023-03-15 09:38:53,586 - INFO - __main__ - Epoch  98, Step:  212200, Batch Loss:     2.231725, Lr: 0.000038, Tokens per sec:   2959
2023-03-15 09:39:11,954 - INFO - __main__ - Epoch  98, Step:  212300, Batch Loss:     1.226142, Lr: 0.000038, Tokens per sec:   2940
2023-03-15 09:39:31,831 - INFO - __main__ - Epoch  98, Step:  212400, Batch Loss:     1.472764, Lr: 0.000038, Tokens per sec:   2708
2023-03-15 09:39:50,328 - INFO - __main__ - Epoch  98, Step:  212500, Batch Loss:     1.500021, Lr: 0.000038, Tokens per sec:   2892
2023-03-15 09:40:09,799 - INFO - __main__ - Epoch  98, Step:  212600, Batch Loss:     1.942489, Lr: 0.000038, Tokens per sec:   2801
2023-03-15 09:40:28,919 - INFO - __main__ - Epoch  98, Step:  212700, Batch Loss:     1.925484, Lr: 0.000038, Tokens per sec:   2795
2023-03-15 09:40:47,584 - INFO - __main__ - Epoch  98, Step:  212800, Batch Loss:     1.143141, Lr: 0.000038, Tokens per sec:   2915
2023-03-15 09:41:06,549 - INFO - __main__ - Epoch  98, Step:  212900, Batch Loss:     1.895136, Lr: 0.000038, Tokens per sec:   2836
2023-03-15 09:41:25,050 - INFO - __main__ - Epoch  98, Step:  213000, Batch Loss:     1.076482, Lr: 0.000038, Tokens per sec:   2890
2023-03-15 09:41:43,446 - INFO - __main__ - Epoch  98, Step:  213100, Batch Loss:     1.338227, Lr: 0.000038, Tokens per sec:   2867
2023-03-15 09:42:02,137 - INFO - __main__ - Epoch  98, Step:  213200, Batch Loss:     1.368105, Lr: 0.000038, Tokens per sec:   2865
2023-03-15 09:42:22,011 - INFO - __main__ - Epoch  98, Step:  213300, Batch Loss:     1.642299, Lr: 0.000038, Tokens per sec:   2716
2023-03-15 09:42:40,727 - INFO - __main__ - Epoch  98, Step:  213400, Batch Loss:     1.872132, Lr: 0.000038, Tokens per sec:   2915
2023-03-15 09:42:59,920 - INFO - __main__ - Epoch  98, Step:  213500, Batch Loss:     1.480134, Lr: 0.000038, Tokens per sec:   2769
2023-03-15 09:43:07,576 - INFO - __main__ - Epoch  98: total training loss 3233.86
2023-03-15 09:43:07,577 - INFO - __main__ - Epoch 99
2023-03-15 09:43:19,107 - INFO - __main__ - Epoch  99, Step:  213600, Batch Loss:     1.577155, Lr: 0.000037, Tokens per sec:   2682
2023-03-15 09:43:37,993 - INFO - __main__ - Epoch  99, Step:  213700, Batch Loss:     1.807824, Lr: 0.000037, Tokens per sec:   2818
2023-03-15 09:43:57,108 - INFO - __main__ - Epoch  99, Step:  213800, Batch Loss:     1.590856, Lr: 0.000037, Tokens per sec:   2819
2023-03-15 09:44:16,773 - INFO - __main__ - Epoch  99, Step:  213900, Batch Loss:     1.307769, Lr: 0.000037, Tokens per sec:   2771
2023-03-15 09:44:36,032 - INFO - __main__ - Epoch  99, Step:  214000, Batch Loss:     1.768743, Lr: 0.000037, Tokens per sec:   2786
2023-03-15 09:44:54,694 - INFO - __main__ - Epoch  99, Step:  214100, Batch Loss:     1.385661, Lr: 0.000037, Tokens per sec:   2857
2023-03-15 09:45:13,500 - INFO - __main__ - Epoch  99, Step:  214200, Batch Loss:     1.507882, Lr: 0.000037, Tokens per sec:   2874
2023-03-15 09:45:32,353 - INFO - __main__ - Epoch  99, Step:  214300, Batch Loss:     0.968762, Lr: 0.000037, Tokens per sec:   2873
2023-03-15 09:45:50,916 - INFO - __main__ - Epoch  99, Step:  214400, Batch Loss:     1.045493, Lr: 0.000037, Tokens per sec:   2933
2023-03-15 09:46:09,866 - INFO - __main__ - Epoch  99, Step:  214500, Batch Loss:     1.785948, Lr: 0.000037, Tokens per sec:   2857
2023-03-15 09:46:28,857 - INFO - __main__ - Epoch  99, Step:  214600, Batch Loss:     1.796365, Lr: 0.000037, Tokens per sec:   2816
2023-03-15 09:46:47,089 - INFO - __main__ - Epoch  99, Step:  214700, Batch Loss:     0.612605, Lr: 0.000037, Tokens per sec:   2966
2023-03-15 09:47:05,481 - INFO - __main__ - Epoch  99, Step:  214800, Batch Loss:     1.364634, Lr: 0.000037, Tokens per sec:   2949
2023-03-15 09:47:24,613 - INFO - __main__ - Epoch  99, Step:  214900, Batch Loss:     1.534680, Lr: 0.000037, Tokens per sec:   2821
2023-03-15 09:47:43,610 - INFO - __main__ - Epoch  99, Step:  215000, Batch Loss:     0.906736, Lr: 0.000037, Tokens per sec:   2812
2023-03-15 09:48:02,769 - INFO - __main__ - Epoch  99, Step:  215100, Batch Loss:     1.730104, Lr: 0.000037, Tokens per sec:   2836
2023-03-15 09:48:22,358 - INFO - __main__ - Epoch  99, Step:  215200, Batch Loss:     1.374888, Lr: 0.000037, Tokens per sec:   2719
2023-03-15 09:48:40,774 - INFO - __main__ - Epoch  99, Step:  215300, Batch Loss:     1.171368, Lr: 0.000037, Tokens per sec:   2964
2023-03-15 09:49:00,011 - INFO - __main__ - Epoch  99, Step:  215400, Batch Loss:     1.416433, Lr: 0.000037, Tokens per sec:   2794
2023-03-15 09:49:18,679 - INFO - __main__ - Epoch  99, Step:  215500, Batch Loss:     1.986844, Lr: 0.000037, Tokens per sec:   2881
2023-03-15 09:49:37,450 - INFO - __main__ - Epoch  99, Step:  215600, Batch Loss:     1.921381, Lr: 0.000037, Tokens per sec:   2858
2023-03-15 09:49:56,671 - INFO - __main__ - Epoch  99, Step:  215700, Batch Loss:     1.012849, Lr: 0.000037, Tokens per sec:   2754
2023-03-15 09:50:00,734 - INFO - __main__ - Epoch  99: total training loss 3196.01
2023-03-15 09:50:00,735 - INFO - __main__ - Epoch 100
2023-03-15 09:50:16,342 - INFO - __main__ - Epoch 100, Step:  215800, Batch Loss:     1.929588, Lr: 0.000037, Tokens per sec:   2683
2023-03-15 09:50:35,148 - INFO - __main__ - Epoch 100, Step:  215900, Batch Loss:     1.100612, Lr: 0.000037, Tokens per sec:   2860
2023-03-15 09:50:53,571 - INFO - __main__ - Epoch 100, Step:  216000, Batch Loss:     1.097630, Lr: 0.000037, Tokens per sec:   2888
2023-03-15 09:51:12,470 - INFO - __main__ - Epoch 100, Step:  216100, Batch Loss:     0.940214, Lr: 0.000037, Tokens per sec:   2823
2023-03-15 09:51:31,634 - INFO - __main__ - Epoch 100, Step:  216200, Batch Loss:     1.808197, Lr: 0.000037, Tokens per sec:   2839
2023-03-15 09:51:51,249 - INFO - __main__ - Epoch 100, Step:  216300, Batch Loss:     1.397910, Lr: 0.000037, Tokens per sec:   2780
2023-03-15 09:52:09,757 - INFO - __main__ - Epoch 100, Step:  216400, Batch Loss:     1.454838, Lr: 0.000037, Tokens per sec:   2893
2023-03-15 09:52:29,479 - INFO - __main__ - Epoch 100, Step:  216500, Batch Loss:     1.352192, Lr: 0.000037, Tokens per sec:   2738
2023-03-15 09:52:48,438 - INFO - __main__ - Epoch 100, Step:  216600, Batch Loss:     1.287832, Lr: 0.000037, Tokens per sec:   2869
2023-03-15 09:53:07,048 - INFO - __main__ - Epoch 100, Step:  216700, Batch Loss:     1.252654, Lr: 0.000037, Tokens per sec:   2827
2023-03-15 09:53:25,507 - INFO - __main__ - Epoch 100, Step:  216800, Batch Loss:     1.834966, Lr: 0.000037, Tokens per sec:   2948
2023-03-15 09:53:44,996 - INFO - __main__ - Epoch 100, Step:  216900, Batch Loss:     1.655321, Lr: 0.000037, Tokens per sec:   2705
2023-03-15 09:54:03,318 - INFO - __main__ - Epoch 100, Step:  217000, Batch Loss:     1.025237, Lr: 0.000037, Tokens per sec:   2925
2023-03-15 09:54:22,707 - INFO - __main__ - Epoch 100, Step:  217100, Batch Loss:     1.518877, Lr: 0.000037, Tokens per sec:   2745
2023-03-15 09:54:41,701 - INFO - __main__ - Epoch 100, Step:  217200, Batch Loss:     1.126578, Lr: 0.000037, Tokens per sec:   2843
2023-03-15 09:55:00,189 - INFO - __main__ - Epoch 100, Step:  217300, Batch Loss:     1.443469, Lr: 0.000037, Tokens per sec:   2955
2023-03-15 09:55:18,707 - INFO - __main__ - Epoch 100, Step:  217400, Batch Loss:     1.344054, Lr: 0.000037, Tokens per sec:   2931
2023-03-15 09:55:37,462 - INFO - __main__ - Epoch 100, Step:  217500, Batch Loss:     1.993466, Lr: 0.000037, Tokens per sec:   2895
2023-03-15 09:55:56,381 - INFO - __main__ - Epoch 100, Step:  217600, Batch Loss:     1.298279, Lr: 0.000037, Tokens per sec:   2855
2023-03-15 09:56:15,474 - INFO - __main__ - Epoch 100, Step:  217700, Batch Loss:     2.148112, Lr: 0.000037, Tokens per sec:   2847
2023-03-15 09:56:34,512 - INFO - __main__ - Epoch 100, Step:  217800, Batch Loss:     1.646621, Lr: 0.000037, Tokens per sec:   2841
2023-03-15 09:56:53,613 - INFO - __main__ - Epoch 100, Step:  217900, Batch Loss:     0.374823, Lr: 0.000037, Tokens per sec:   2799
2023-03-15 09:56:53,760 - INFO - __main__ - Epoch 100: total training loss 3144.13
2023-03-15 09:56:53,760 - INFO - __main__ - Epoch 101
