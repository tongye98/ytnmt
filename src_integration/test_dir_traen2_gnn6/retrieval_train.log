2023-03-16 16:42:32,264 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:42:35,763 - INFO - model - Build Model...
2023-03-16 16:42:36,333 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (2): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (3): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (4): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (5): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:42:36,337 - INFO - model - Total parameters number: 91563520
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:42:36,338 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.bias [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:42:36,339 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,340 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,341 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,342 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:42:36,343 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:42:36,343 - INFO - model - The model is built.
2023-03-16 16:42:36,344 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:42:36,344 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:42:36,344 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:42:38,768 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:43:08,090 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:43:11,508 - INFO - model - Build Model...
2023-03-16 16:43:12,144 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:43:12,149 - INFO - model - Total parameters number: 78953984
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:43:12,150 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,151 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,152 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:43:12,153 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:43:12,154 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:43:12,154 - INFO - model - The model is built.
2023-03-16 16:43:12,154 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:43:12,154 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:43:12,154 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:43:14,143 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:43:14,244 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-16 16:43:14,244 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-16 16:43:14,322 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-16 16:43:14,322 - INFO - __main__ - Scheduler = StepLR
2023-03-16 16:43:14,324 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-16 16:43:14,324 - INFO - __main__ - Epoch 1
2023-03-16 16:44:50,806 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:44:53,982 - INFO - model - Build Model...
2023-03-16 16:44:54,588 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:44:54,594 - INFO - model - Total parameters number: 78953984
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:44:54,595 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,596 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,597 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:44:54,598 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:44:54,599 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:44:54,600 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:44:54,600 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:44:54,600 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:44:54,600 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:44:54,600 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:44:54,600 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:44:54,600 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:44:54,600 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:44:54,600 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:44:54,600 - INFO - model - The model is built.
2023-03-16 16:44:54,600 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:44:54,600 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:44:54,600 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:44:56,744 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:44:56,785 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-16 16:44:56,785 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-16 16:44:56,860 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-16 16:44:56,861 - INFO - __main__ - Scheduler = StepLR
2023-03-16 16:44:56,862 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-16 16:44:56,862 - INFO - __main__ - Epoch 1
2023-03-16 16:48:03,335 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:48:06,871 - INFO - model - Build Model...
2023-03-16 16:48:07,424 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:48:07,428 - INFO - model - Total parameters number: 78953984
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:48:07,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:48:07,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:48:07,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:48:07,433 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:48:07,433 - INFO - model - The model is built.
2023-03-16 16:48:07,433 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:48:07,433 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:48:07,433 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:48:09,338 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:48:09,419 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-16 16:48:09,419 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-16 16:48:09,502 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-16 16:48:09,502 - INFO - __main__ - Scheduler = StepLR
2023-03-16 16:48:09,504 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-16 16:48:09,504 - INFO - __main__ - Epoch 1
2023-03-16 16:48:11,037 - WARNING - model - transformer_encoder_output shape = torch.Size([32, 200, 512])
2023-03-16 16:48:11,038 - WARNING - model - src_mask shape = torch.Size([32, 1, 200])
2023-03-16 16:48:11,038 - WARNING - model - gnn_encoder_output shape = torch.Size([32, 535, 512])
2023-03-16 16:48:11,038 - WARNING - model - node_mask shape = torch.Size([32, 535])
2023-03-16 16:48:11,038 - WARNING - model - decoder_representation shape = torch.Size([32, 39, 512])
2023-03-16 16:50:56,600 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:50:59,477 - INFO - model - Build Model...
2023-03-16 16:51:00,221 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:51:00,229 - INFO - model - Total parameters number: 78953984
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:51:00,230 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:51:00,231 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,232 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:51:00,233 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,234 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:51:00,235 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:51:00,236 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:51:00,236 - INFO - model - The model is built.
2023-03-16 16:51:00,236 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:51:00,236 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:51:00,236 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:51:02,346 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:51:02,582 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-16 16:51:02,582 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-16 16:51:02,677 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-16 16:51:02,678 - INFO - __main__ - Scheduler = StepLR
2023-03-16 16:51:02,679 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-16 16:51:02,679 - INFO - __main__ - Epoch 1
2023-03-16 16:52:19,564 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:52:22,798 - INFO - model - Build Model...
2023-03-16 16:52:23,422 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:52:23,427 - INFO - model - Total parameters number: 78953984
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:23,428 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,429 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,430 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:52:23,431 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:52:23,432 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:52:23,433 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:52:23,433 - INFO - model - The model is built.
2023-03-16 16:52:23,433 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:52:23,433 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:52:23,433 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:52:25,462 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:52:25,563 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-16 16:52:25,563 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-16 16:52:25,647 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-16 16:52:25,647 - INFO - __main__ - Scheduler = StepLR
2023-03-16 16:52:25,650 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-16 16:52:25,650 - INFO - __main__ - Epoch 1
2023-03-16 16:52:49,642 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:52:52,790 - INFO - model - Build Model...
2023-03-16 16:52:53,298 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:52:53,306 - INFO - model - Total parameters number: 78953984
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:53,307 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,308 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,309 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,310 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:52:53,311 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:52:53,312 - INFO - model - The model is built.
2023-03-16 16:52:53,312 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:52:53,312 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:52:53,312 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:52:55,120 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:52:55,177 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-16 16:52:55,177 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-16 16:52:55,253 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-16 16:52:55,253 - INFO - __main__ - Scheduler = StepLR
2023-03-16 16:52:55,254 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-16 16:52:55,254 - INFO - __main__ - Epoch 1
2023-03-16 16:53:13,782 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:53:17,523 - INFO - model - Build Model...
2023-03-16 16:53:18,060 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:53:18,065 - INFO - model - Total parameters number: 78953984
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:53:18,066 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,067 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,068 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,069 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:53:18,070 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:53:18,070 - INFO - model - The model is built.
2023-03-16 16:53:18,070 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:53:18,070 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:53:18,070 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:53:20,077 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:53:20,197 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-16 16:53:20,197 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-16 16:53:20,290 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-16 16:53:20,290 - INFO - __main__ - Scheduler = StepLR
2023-03-16 16:53:20,294 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-16 16:53:20,294 - INFO - __main__ - Epoch 1
2023-03-16 16:53:29,175 - INFO - __main__ - Epoch   1, Step:     100, Batch Loss:    55.131695, Lr: 0.000100, Tokens per sec:   6139
2023-03-16 16:53:36,660 - INFO - __main__ - Epoch   1, Step:     200, Batch Loss:    33.349789, Lr: 0.000100, Tokens per sec:   7114
2023-03-16 16:53:42,138 - INFO - __main__ - Epoch   1: total training loss 11232.29
2023-03-16 16:53:42,138 - INFO - __main__ - Epoch 2
2023-03-16 16:53:44,492 - INFO - __main__ - Epoch   2, Step:     300, Batch Loss:    49.268341, Lr: 0.000099, Tokens per sec:   6229
2023-03-16 16:53:52,069 - INFO - __main__ - Epoch   2, Step:     400, Batch Loss:    29.497295, Lr: 0.000099, Tokens per sec:   7156
2023-03-16 16:53:59,816 - INFO - __main__ - Epoch   2, Step:     500, Batch Loss:    44.662067, Lr: 0.000099, Tokens per sec:   6877
2023-03-16 16:54:03,439 - INFO - __main__ - Epoch   2: total training loss 11041.96
2023-03-16 16:54:03,440 - INFO - __main__ - Epoch 3
2023-03-16 16:54:08,008 - INFO - __main__ - Epoch   3, Step:     600, Batch Loss:    61.687954, Lr: 0.000098, Tokens per sec:   6328
2023-03-16 16:54:15,938 - INFO - __main__ - Epoch   3, Step:     700, Batch Loss:    52.851143, Lr: 0.000098, Tokens per sec:   6878
2023-03-16 16:54:23,996 - INFO - __main__ - Epoch   3, Step:     800, Batch Loss:    56.519482, Lr: 0.000098, Tokens per sec:   6621
2023-03-16 16:54:25,548 - INFO - __main__ - Epoch   3: total training loss 10924.06
2023-03-16 16:54:25,549 - INFO - __main__ - Epoch 4
2023-03-16 16:54:32,285 - INFO - __main__ - Epoch   4, Step:     900, Batch Loss:    35.138165, Lr: 0.000097, Tokens per sec:   6434
2023-03-16 16:54:40,285 - INFO - __main__ - Epoch   4, Step:    1000, Batch Loss:    30.045244, Lr: 0.000097, Tokens per sec:   6769
2023-03-16 16:54:47,526 - INFO - __main__ - Epoch   4: total training loss 10820.19
2023-03-16 16:54:47,527 - INFO - __main__ - Epoch 5
2023-03-16 16:54:48,394 - INFO - __main__ - Epoch   5, Step:    1100, Batch Loss:    41.813816, Lr: 0.000096, Tokens per sec:   5030
2023-03-16 16:54:56,151 - INFO - __main__ - Epoch   5, Step:    1200, Batch Loss:    26.059935, Lr: 0.000096, Tokens per sec:   6914
2023-03-16 16:55:03,901 - INFO - __main__ - Epoch   5, Step:    1300, Batch Loss:    39.478775, Lr: 0.000096, Tokens per sec:   6902
2023-03-16 16:55:09,079 - INFO - __main__ - Epoch   5: total training loss 10721.31
2023-03-16 16:55:09,079 - INFO - __main__ - Epoch 6
2023-03-16 16:55:12,102 - INFO - __main__ - Epoch   6, Step:    1400, Batch Loss:    41.114773, Lr: 0.000095, Tokens per sec:   6149
2023-03-16 16:55:28,837 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:55:31,455 - INFO - model - Build Model...
2023-03-16 16:55:32,012 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:55:32,016 - INFO - model - Total parameters number: 78953984
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:55:32,016 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,017 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,018 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,019 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:55:32,020 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:55:32,021 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:55:32,021 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:55:32,021 - INFO - model - The model is built.
2023-03-16 16:55:32,021 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:55:32,021 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:55:32,021 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:55:33,777 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:55:33,871 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-16 16:55:33,871 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-16 16:55:33,950 - INFO - __main__ - Adam(lr=1e-05, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-16 16:55:33,950 - INFO - __main__ - Scheduler = StepLR
2023-03-16 16:55:33,952 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-16 16:55:33,952 - INFO - __main__ - Epoch 1
2023-03-16 16:55:43,092 - INFO - __main__ - Epoch   1, Step:     100, Batch Loss:    56.095924, Lr: 0.000010, Tokens per sec:   5965
2023-03-16 16:55:51,168 - INFO - __main__ - Epoch   1, Step:     200, Batch Loss:    33.575176, Lr: 0.000010, Tokens per sec:   6594
2023-03-16 16:55:57,326 - INFO - __main__ - Epoch   1: total training loss 11469.21
2023-03-16 16:55:57,326 - INFO - __main__ - Epoch 2
2023-03-16 16:55:59,915 - INFO - __main__ - Epoch   2, Step:     300, Batch Loss:    50.144955, Lr: 0.000010, Tokens per sec:   5664
2023-03-16 16:56:08,094 - INFO - __main__ - Epoch   2, Step:     400, Batch Loss:    29.946630, Lr: 0.000010, Tokens per sec:   6630
2023-03-16 16:56:16,109 - INFO - __main__ - Epoch   2, Step:     500, Batch Loss:    45.301201, Lr: 0.000010, Tokens per sec:   6647
2023-03-16 16:56:19,771 - INFO - __main__ - Epoch   2: total training loss 11248.97
2023-03-16 16:56:19,772 - INFO - __main__ - Epoch 3
2023-03-16 16:56:24,564 - INFO - __main__ - Epoch   3, Step:     600, Batch Loss:    62.821411, Lr: 0.000010, Tokens per sec:   6032
2023-03-16 16:56:32,283 - INFO - __main__ - Epoch   3, Step:     700, Batch Loss:    53.970909, Lr: 0.000010, Tokens per sec:   7067
2023-03-16 16:56:40,671 - INFO - __main__ - Epoch   3, Step:     800, Batch Loss:    58.411491, Lr: 0.000010, Tokens per sec:   6360
2023-03-16 16:56:42,283 - INFO - __main__ - Epoch   3: total training loss 11210.02
2023-03-16 16:56:42,283 - INFO - __main__ - Epoch 4
2023-03-16 16:56:49,013 - INFO - __main__ - Epoch   4, Step:     900, Batch Loss:    36.505447, Lr: 0.000010, Tokens per sec:   6440
2023-03-16 16:57:02,710 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:57:05,679 - INFO - model - Build Model...
2023-03-16 16:57:06,485 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:57:06,491 - INFO - model - Total parameters number: 78953984
2023-03-16 16:57:06,492 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:57:06,492 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:57:06,493 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:57:06,494 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,495 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,496 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:57:06,497 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:57:06,498 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:57:06,498 - INFO - model - The model is built.
2023-03-16 16:57:06,498 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:57:06,498 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:57:06,499 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:57:08,662 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:57:08,970 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-16 16:57:08,970 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-16 16:57:09,044 - INFO - __main__ - Adam(lr=0.001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-16 16:57:09,044 - INFO - __main__ - Scheduler = StepLR
2023-03-16 16:57:09,046 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-16 16:57:09,046 - INFO - __main__ - Epoch 1
2023-03-16 16:57:18,215 - INFO - __main__ - Epoch   1, Step:     100, Batch Loss:    55.169540, Lr: 0.001000, Tokens per sec:   5946
2023-03-16 16:57:26,384 - INFO - __main__ - Epoch   1, Step:     200, Batch Loss:    33.695171, Lr: 0.001000, Tokens per sec:   6518
2023-03-16 16:57:32,275 - INFO - __main__ - Epoch   1: total training loss 11227.83
2023-03-16 16:57:32,275 - INFO - __main__ - Epoch 2
2023-03-16 16:57:34,725 - INFO - __main__ - Epoch   2, Step:     300, Batch Loss:    47.220688, Lr: 0.000990, Tokens per sec:   5985
2023-03-16 16:57:42,670 - INFO - __main__ - Epoch   2, Step:     400, Batch Loss:    28.228212, Lr: 0.000990, Tokens per sec:   6825
2023-03-16 16:57:50,788 - INFO - __main__ - Epoch   2, Step:     500, Batch Loss:    44.097656, Lr: 0.000990, Tokens per sec:   6563
2023-03-16 16:57:54,459 - INFO - __main__ - Epoch   2: total training loss 10733.10
2023-03-16 16:57:54,460 - INFO - __main__ - Epoch 3
2023-03-16 16:57:58,977 - INFO - __main__ - Epoch   3, Step:     600, Batch Loss:    57.008392, Lr: 0.000980, Tokens per sec:   6398
2023-03-16 16:58:06,809 - INFO - __main__ - Epoch   3, Step:     700, Batch Loss:    48.296532, Lr: 0.000980, Tokens per sec:   6965
2023-03-16 16:58:15,038 - INFO - __main__ - Epoch   3, Step:     800, Batch Loss:    54.145187, Lr: 0.000980, Tokens per sec:   6483
2023-03-16 16:58:16,654 - INFO - __main__ - Epoch   3: total training loss 10174.64
2023-03-16 16:58:16,654 - INFO - __main__ - Epoch 4
2023-03-16 16:58:23,429 - INFO - __main__ - Epoch   4, Step:     900, Batch Loss:    31.435694, Lr: 0.000970, Tokens per sec:   6397
2023-03-16 16:58:31,484 - INFO - __main__ - Epoch   4, Step:    1000, Batch Loss:    26.647892, Lr: 0.000970, Tokens per sec:   6723
2023-03-16 16:58:39,018 - INFO - __main__ - Epoch   4: total training loss 9604.34
2023-03-16 16:58:39,019 - INFO - __main__ - Epoch 5
2023-03-16 16:58:39,946 - INFO - __main__ - Epoch   5, Step:    1100, Batch Loss:    34.787342, Lr: 0.000961, Tokens per sec:   4705
2023-03-16 16:58:48,089 - INFO - __main__ - Epoch   5, Step:    1200, Batch Loss:    21.878292, Lr: 0.000961, Tokens per sec:   6586
2023-03-16 16:58:56,313 - INFO - __main__ - Epoch   5, Step:    1300, Batch Loss:    33.871971, Lr: 0.000961, Tokens per sec:   6503
2023-03-16 16:59:01,760 - INFO - __main__ - Epoch   5: total training loss 9077.93
2023-03-16 16:59:01,761 - INFO - __main__ - Epoch 6
2023-03-16 16:59:04,853 - INFO - __main__ - Epoch   6, Step:    1400, Batch Loss:    33.298504, Lr: 0.000951, Tokens per sec:   6011
2023-03-16 16:59:26,772 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-16 16:59:29,920 - INFO - model - Build Model...
2023-03-16 16:59:30,450 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-16 16:59:30,453 - INFO - model - Total parameters number: 78953984
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-16 16:59:30,454 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,455 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,456 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,457 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-16 16:59:30,458 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-16 16:59:30,458 - INFO - model - The model is built.
2023-03-16 16:59:30,459 - WARNING - __main__ - ckpt_path is not specified.
2023-03-16 16:59:30,459 - WARNING - __main__ - use load_model item in config yaml.
2023-03-16 16:59:30,459 - INFO - __main__ - ckpt_path = test_dir_traen2_gnn6/435800.ckpt
2023-03-16 16:59:32,349 - INFO - __main__ - Load model from /home/tongye2/ytnmt/src_integration/test_dir_traen2_gnn6/435800.ckpt.
2023-03-16 16:59:32,398 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-16 16:59:32,398 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-16 16:59:32,475 - INFO - __main__ - Adam(lr=0.01, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-16 16:59:32,476 - INFO - __main__ - Scheduler = StepLR
2023-03-16 16:59:32,477 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-16 16:59:32,477 - INFO - __main__ - Epoch 1
2023-03-16 16:59:41,763 - INFO - __main__ - Epoch   1, Step:     100, Batch Loss:    77.527184, Lr: 0.010000, Tokens per sec:   5871
2023-03-16 16:59:50,055 - INFO - __main__ - Epoch   1, Step:     200, Batch Loss:    51.371029, Lr: 0.010000, Tokens per sec:   6422
2023-03-16 16:59:56,144 - INFO - __main__ - Epoch   1: total training loss 17085.60
2023-03-16 16:59:56,144 - INFO - __main__ - Epoch 2
2023-03-16 16:59:58,642 - INFO - __main__ - Epoch   2, Step:     300, Batch Loss:    74.964325, Lr: 0.009900, Tokens per sec:   5869
2023-03-16 17:00:06,961 - INFO - __main__ - Epoch   2, Step:     400, Batch Loss:    52.978439, Lr: 0.009900, Tokens per sec:   6518
2023-03-16 17:00:15,326 - INFO - __main__ - Epoch   2, Step:     500, Batch Loss:    80.252937, Lr: 0.009900, Tokens per sec:   6369
2023-03-16 17:00:19,176 - INFO - __main__ - Epoch   2: total training loss 18610.49
2023-03-16 17:00:19,177 - INFO - __main__ - Epoch 3
