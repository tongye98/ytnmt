2023-03-14 21:46:43,613 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-14 21:47:51,826 - INFO - data - average code tokens = 109.28999515442095
2023-03-14 21:47:51,826 - INFO - data - average ast tokens = 188.85505342888476
2023-03-14 21:47:51,826 - INFO - data - average text tokens = 15.993139680191783
2023-03-14 21:47:51,826 - INFO - data - average position tokens = 188.85505342888476
2023-03-14 21:47:51,826 - INFO - data - average ast edges = 375.7101068577695
2023-03-14 21:48:05,345 - INFO - data - code vocab length = 26684
2023-03-14 21:48:05,345 - INFO - data - text vocab length = 13207
2023-03-14 21:48:05,346 - INFO - data - position vocab length = 20587
2023-03-14 21:48:17,302 - INFO - model - Build Model...
2023-03-14 21:48:18,157 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-14 21:48:18,163 - INFO - model - Total parameters number: 78953984
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:48:18,164 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,165 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-14 21:48:18,166 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,167 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-14 21:48:18,168 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-14 21:48:18,169 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-14 21:48:18,169 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-14 21:48:18,173 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-14 21:48:18,173 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-14 21:48:18,174 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-14 21:48:18,174 - INFO - model - The model is built.
2023-03-14 21:48:18,176 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-14 21:48:18,176 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-14 21:48:20,032 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-14 21:48:20,033 - INFO - __main__ - Scheduler = StepLR
2023-03-14 21:48:20,033 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-14 21:48:20,033 - INFO - __main__ - Epoch 1
2023-03-14 21:48:41,273 - INFO - __main__ - Epoch   1, Step:     100, Batch Loss:    89.414612, Lr: 0.000100, Tokens per sec:   2493
2023-03-14 21:49:00,044 - INFO - __main__ - Epoch   1, Step:     200, Batch Loss:    84.514175, Lr: 0.000100, Tokens per sec:   2830
2023-03-14 21:49:19,132 - INFO - __main__ - Epoch   1, Step:     300, Batch Loss:    74.457283, Lr: 0.000100, Tokens per sec:   2879
2023-03-14 21:49:39,027 - INFO - __main__ - Epoch   1, Step:     400, Batch Loss:    72.295036, Lr: 0.000100, Tokens per sec:   2730
2023-03-14 21:49:58,174 - INFO - __main__ - Epoch   1, Step:     500, Batch Loss:    67.472710, Lr: 0.000100, Tokens per sec:   2837
2023-03-14 21:50:17,439 - INFO - __main__ - Epoch   1, Step:     600, Batch Loss:    88.589081, Lr: 0.000100, Tokens per sec:   2797
2023-03-14 21:50:37,148 - INFO - __main__ - Epoch   1, Step:     700, Batch Loss:    78.066681, Lr: 0.000100, Tokens per sec:   2721
2023-03-14 21:50:56,758 - INFO - __main__ - Epoch   1, Step:     800, Batch Loss:    70.398201, Lr: 0.000100, Tokens per sec:   2748
2023-03-14 21:51:16,492 - INFO - __main__ - Epoch   1, Step:     900, Batch Loss:    73.444511, Lr: 0.000100, Tokens per sec:   2769
2023-03-14 21:51:36,402 - INFO - __main__ - Epoch   1, Step:    1000, Batch Loss:    81.829247, Lr: 0.000100, Tokens per sec:   2695
2023-03-14 21:51:56,096 - INFO - __main__ - Epoch   1, Step:    1100, Batch Loss:    79.119789, Lr: 0.000100, Tokens per sec:   2683
2023-03-14 21:52:16,394 - INFO - __main__ - Epoch   1, Step:    1200, Batch Loss:    68.611351, Lr: 0.000100, Tokens per sec:   2646
2023-03-14 21:52:36,570 - INFO - __main__ - Epoch   1, Step:    1300, Batch Loss:    73.380386, Lr: 0.000100, Tokens per sec:   2699
2023-03-14 21:52:57,028 - INFO - __main__ - Epoch   1, Step:    1400, Batch Loss:    75.834488, Lr: 0.000100, Tokens per sec:   2591
2023-03-14 21:53:17,660 - INFO - __main__ - Epoch   1, Step:    1500, Batch Loss:    87.687843, Lr: 0.000100, Tokens per sec:   2589
2023-03-14 21:53:38,196 - INFO - __main__ - Epoch   1, Step:    1600, Batch Loss:    96.339638, Lr: 0.000100, Tokens per sec:   2636
2023-03-14 21:53:58,580 - INFO - __main__ - Epoch   1, Step:    1700, Batch Loss:    86.529449, Lr: 0.000100, Tokens per sec:   2685
2023-03-14 21:54:19,119 - INFO - __main__ - Epoch   1, Step:    1800, Batch Loss:    85.796318, Lr: 0.000100, Tokens per sec:   2639
2023-03-14 21:54:39,709 - INFO - __main__ - Epoch   1, Step:    1900, Batch Loss:    90.128197, Lr: 0.000100, Tokens per sec:   2614
2023-03-14 21:55:00,364 - INFO - __main__ - Epoch   1, Step:    2000, Batch Loss:    70.970413, Lr: 0.000100, Tokens per sec:   2617
2023-03-14 21:55:20,836 - INFO - __main__ - Epoch   1, Step:    2100, Batch Loss:    71.669785, Lr: 0.000100, Tokens per sec:   2606
2023-03-14 21:55:36,843 - INFO - __main__ - Epoch   1: total training loss 170138.29
2023-03-14 21:55:36,844 - INFO - __main__ - Epoch 2
2023-03-14 21:55:41,464 - INFO - __main__ - Epoch   2, Step:    2200, Batch Loss:    69.899117, Lr: 0.000099, Tokens per sec:   2423
2023-03-14 21:56:01,895 - INFO - __main__ - Epoch   2, Step:    2300, Batch Loss:    63.978767, Lr: 0.000099, Tokens per sec:   2620
2023-03-14 21:56:22,356 - INFO - __main__ - Epoch   2, Step:    2400, Batch Loss:    60.929741, Lr: 0.000099, Tokens per sec:   2585
2023-03-14 21:56:42,713 - INFO - __main__ - Epoch   2, Step:    2500, Batch Loss:    49.471573, Lr: 0.000099, Tokens per sec:   2682
2023-03-14 21:57:03,427 - INFO - __main__ - Epoch   2, Step:    2600, Batch Loss:    61.779724, Lr: 0.000099, Tokens per sec:   2609
2023-03-14 21:57:23,954 - INFO - __main__ - Epoch   2, Step:    2700, Batch Loss:    58.575821, Lr: 0.000099, Tokens per sec:   2617
2023-03-14 21:57:44,380 - INFO - __main__ - Epoch   2, Step:    2800, Batch Loss:    72.354698, Lr: 0.000099, Tokens per sec:   2656
2023-03-14 21:58:04,771 - INFO - __main__ - Epoch   2, Step:    2900, Batch Loss:    70.402100, Lr: 0.000099, Tokens per sec:   2642
2023-03-14 21:58:25,431 - INFO - __main__ - Epoch   2, Step:    3000, Batch Loss:    67.481339, Lr: 0.000099, Tokens per sec:   2607
2023-03-14 21:58:46,006 - INFO - __main__ - Epoch   2, Step:    3100, Batch Loss:    57.185631, Lr: 0.000099, Tokens per sec:   2621
2023-03-14 21:59:07,403 - INFO - __main__ - Epoch   2, Step:    3200, Batch Loss:    52.496059, Lr: 0.000099, Tokens per sec:   2533
2023-03-14 21:59:28,211 - INFO - __main__ - Epoch   2, Step:    3300, Batch Loss:    60.398819, Lr: 0.000099, Tokens per sec:   2559
2023-03-14 21:59:48,676 - INFO - __main__ - Epoch   2, Step:    3400, Batch Loss:    64.340469, Lr: 0.000099, Tokens per sec:   2627
2023-03-14 22:00:09,383 - INFO - __main__ - Epoch   2, Step:    3500, Batch Loss:    62.750553, Lr: 0.000099, Tokens per sec:   2595
2023-03-14 22:00:30,055 - INFO - __main__ - Epoch   2, Step:    3600, Batch Loss:    50.082043, Lr: 0.000099, Tokens per sec:   2602
2023-03-14 22:00:50,859 - INFO - __main__ - Epoch   2, Step:    3700, Batch Loss:    58.464935, Lr: 0.000099, Tokens per sec:   2649
2023-03-14 22:01:11,581 - INFO - __main__ - Epoch   2, Step:    3800, Batch Loss:    78.846390, Lr: 0.000099, Tokens per sec:   2588
2023-03-14 22:01:33,147 - INFO - __main__ - Epoch   2, Step:    3900, Batch Loss:    56.035179, Lr: 0.000099, Tokens per sec:   2522
2023-03-14 22:01:54,329 - INFO - __main__ - Epoch   2, Step:    4000, Batch Loss:    54.347992, Lr: 0.000099, Tokens per sec:   2540
2023-03-14 22:02:15,035 - INFO - __main__ - Epoch   2, Step:    4100, Batch Loss:    51.294346, Lr: 0.000099, Tokens per sec:   2530
2023-03-14 22:02:35,805 - INFO - __main__ - Epoch   2, Step:    4200, Batch Loss:    67.094193, Lr: 0.000099, Tokens per sec:   2565
2023-03-14 22:02:56,257 - INFO - __main__ - Epoch   2, Step:    4300, Batch Loss:    79.286636, Lr: 0.000099, Tokens per sec:   2661
2023-03-14 22:03:08,150 - INFO - __main__ - Epoch   2: total training loss 138861.37
2023-03-14 22:03:08,151 - INFO - __main__ - Epoch 3
2023-03-14 22:03:17,190 - INFO - __main__ - Epoch   3, Step:    4400, Batch Loss:    69.938545, Lr: 0.000098, Tokens per sec:   2524
2023-03-14 22:03:37,860 - INFO - __main__ - Epoch   3, Step:    4500, Batch Loss:    62.818142, Lr: 0.000098, Tokens per sec:   2603
2023-03-14 22:03:58,626 - INFO - __main__ - Epoch   3, Step:    4600, Batch Loss:    46.581493, Lr: 0.000098, Tokens per sec:   2576
2023-03-14 22:04:19,202 - INFO - __main__ - Epoch   3, Step:    4700, Batch Loss:    53.368988, Lr: 0.000098, Tokens per sec:   2653
2023-03-14 22:04:39,708 - INFO - __main__ - Epoch   3, Step:    4800, Batch Loss:    68.071976, Lr: 0.000098, Tokens per sec:   2660
2023-03-14 22:05:01,455 - INFO - __main__ - Epoch   3, Step:    4900, Batch Loss:    54.828262, Lr: 0.000098, Tokens per sec:   2459
2023-03-14 22:05:22,545 - INFO - __main__ - Epoch   3, Step:    5000, Batch Loss:    43.254318, Lr: 0.000098, Tokens per sec:   2558
2023-03-14 22:05:43,464 - INFO - __main__ - Epoch   3, Step:    5100, Batch Loss:    60.810146, Lr: 0.000098, Tokens per sec:   2575
2023-03-14 22:06:04,182 - INFO - __main__ - Epoch   3, Step:    5200, Batch Loss:    58.628242, Lr: 0.000098, Tokens per sec:   2621
2023-03-14 22:06:25,201 - INFO - __main__ - Epoch   3, Step:    5300, Batch Loss:    53.433830, Lr: 0.000098, Tokens per sec:   2526
2023-03-14 22:06:46,321 - INFO - __main__ - Epoch   3, Step:    5400, Batch Loss:    54.085884, Lr: 0.000098, Tokens per sec:   2548
2023-03-14 22:07:07,741 - INFO - __main__ - Epoch   3, Step:    5500, Batch Loss:    68.931213, Lr: 0.000098, Tokens per sec:   2471
2023-03-14 22:07:28,234 - INFO - __main__ - Epoch   3, Step:    5600, Batch Loss:    51.910030, Lr: 0.000098, Tokens per sec:   2629
2023-03-14 22:07:49,006 - INFO - __main__ - Epoch   3, Step:    5700, Batch Loss:    42.364082, Lr: 0.000098, Tokens per sec:   2608
2023-03-14 22:08:09,693 - INFO - __main__ - Epoch   3, Step:    5800, Batch Loss:    74.021660, Lr: 0.000098, Tokens per sec:   2580
2023-03-14 22:08:30,598 - INFO - __main__ - Epoch   3, Step:    5900, Batch Loss:    58.953716, Lr: 0.000098, Tokens per sec:   2572
2023-03-14 22:08:52,203 - INFO - __main__ - Epoch   3, Step:    6000, Batch Loss:    60.824394, Lr: 0.000098, Tokens per sec:   2516
2023-03-14 22:09:13,294 - INFO - __main__ - Epoch   3, Step:    6100, Batch Loss:    44.265717, Lr: 0.000098, Tokens per sec:   2533
2023-03-14 22:09:33,996 - INFO - __main__ - Epoch   3, Step:    6200, Batch Loss:    54.181473, Lr: 0.000098, Tokens per sec:   2619
2023-03-14 22:09:55,156 - INFO - __main__ - Epoch   3, Step:    6300, Batch Loss:    61.770393, Lr: 0.000098, Tokens per sec:   2539
2023-03-14 22:10:16,353 - INFO - __main__ - Epoch   3, Step:    6400, Batch Loss:    48.537189, Lr: 0.000098, Tokens per sec:   2567
2023-03-14 22:10:36,728 - INFO - __main__ - Epoch   3, Step:    6500, Batch Loss:    61.020821, Lr: 0.000098, Tokens per sec:   2628
2023-03-14 22:10:44,834 - INFO - __main__ - Epoch   3: total training loss 124333.76
2023-03-14 22:10:44,835 - INFO - __main__ - Epoch 4
2023-03-14 22:10:58,344 - INFO - __main__ - Epoch   4, Step:    6600, Batch Loss:    43.481888, Lr: 0.000097, Tokens per sec:   2551
2023-03-14 22:11:19,187 - INFO - __main__ - Epoch   4, Step:    6700, Batch Loss:    58.071053, Lr: 0.000097, Tokens per sec:   2571
2023-03-14 22:11:39,604 - INFO - __main__ - Epoch   4, Step:    6800, Batch Loss:    54.174618, Lr: 0.000097, Tokens per sec:   2658
2023-03-14 22:12:00,083 - INFO - __main__ - Epoch   4, Step:    6900, Batch Loss:    53.223347, Lr: 0.000097, Tokens per sec:   2641
2023-03-14 22:12:21,180 - INFO - __main__ - Epoch   4, Step:    7000, Batch Loss:    53.700497, Lr: 0.000097, Tokens per sec:   2518
2023-03-14 22:12:42,343 - INFO - __main__ - Epoch   4, Step:    7100, Batch Loss:    64.252251, Lr: 0.000097, Tokens per sec:   2594
2023-03-14 22:13:03,005 - INFO - __main__ - Epoch   4, Step:    7200, Batch Loss:    45.217010, Lr: 0.000097, Tokens per sec:   2564
2023-03-14 22:13:24,522 - INFO - __main__ - Epoch   4, Step:    7300, Batch Loss:    59.115429, Lr: 0.000097, Tokens per sec:   2533
2023-03-14 22:13:45,205 - INFO - __main__ - Epoch   4, Step:    7400, Batch Loss:    53.654797, Lr: 0.000097, Tokens per sec:   2590
2023-03-14 22:14:05,627 - INFO - __main__ - Epoch   4, Step:    7500, Batch Loss:    47.130150, Lr: 0.000097, Tokens per sec:   2687
2023-03-14 22:14:26,493 - INFO - __main__ - Epoch   4, Step:    7600, Batch Loss:    56.930187, Lr: 0.000097, Tokens per sec:   2603
2023-03-14 22:14:47,695 - INFO - __main__ - Epoch   4, Step:    7700, Batch Loss:    49.023277, Lr: 0.000097, Tokens per sec:   2492
2023-03-14 22:15:08,141 - INFO - __main__ - Epoch   4, Step:    7800, Batch Loss:    52.647278, Lr: 0.000097, Tokens per sec:   2614
2023-03-14 22:15:28,751 - INFO - __main__ - Epoch   4, Step:    7900, Batch Loss:    64.214409, Lr: 0.000097, Tokens per sec:   2589
2023-03-14 22:15:50,218 - INFO - __main__ - Epoch   4, Step:    8000, Batch Loss:    58.242828, Lr: 0.000097, Tokens per sec:   2506
2023-03-14 22:16:10,971 - INFO - __main__ - Epoch   4, Step:    8100, Batch Loss:    56.931801, Lr: 0.000097, Tokens per sec:   2609
2023-03-14 22:16:31,512 - INFO - __main__ - Epoch   4, Step:    8200, Batch Loss:    42.611340, Lr: 0.000097, Tokens per sec:   2598
2023-03-14 22:16:52,309 - INFO - __main__ - Epoch   4, Step:    8300, Batch Loss:    41.150673, Lr: 0.000097, Tokens per sec:   2577
2023-03-14 22:17:13,148 - INFO - __main__ - Epoch   4, Step:    8400, Batch Loss:    45.907822, Lr: 0.000097, Tokens per sec:   2574
2023-03-14 22:17:33,532 - INFO - __main__ - Epoch   4, Step:    8500, Batch Loss:    52.841492, Lr: 0.000097, Tokens per sec:   2683
2023-03-14 22:17:54,064 - INFO - __main__ - Epoch   4, Step:    8600, Batch Loss:    38.675896, Lr: 0.000097, Tokens per sec:   2584
2023-03-14 22:18:15,311 - INFO - __main__ - Epoch   4, Step:    8700, Batch Loss:    42.232975, Lr: 0.000097, Tokens per sec:   2542
2023-03-14 22:18:18,439 - INFO - __main__ - Epoch   4: total training loss 113711.82
2023-03-14 22:18:18,441 - INFO - __main__ - Epoch 5
2023-03-14 22:18:36,668 - INFO - __main__ - Epoch   5, Step:    8800, Batch Loss:    48.965801, Lr: 0.000096, Tokens per sec:   2462
2023-03-14 22:18:57,001 - INFO - __main__ - Epoch   5, Step:    8900, Batch Loss:    55.813549, Lr: 0.000096, Tokens per sec:   2670
2023-03-14 22:19:18,198 - INFO - __main__ - Epoch   5, Step:    9000, Batch Loss:    49.726452, Lr: 0.000096, Tokens per sec:   2547
2023-03-14 22:19:38,799 - INFO - __main__ - Epoch   5, Step:    9100, Batch Loss:    42.135197, Lr: 0.000096, Tokens per sec:   2566
2023-03-14 22:19:59,260 - INFO - __main__ - Epoch   5, Step:    9200, Batch Loss:    57.187290, Lr: 0.000096, Tokens per sec:   2662
2023-03-14 22:20:20,399 - INFO - __main__ - Epoch   5, Step:    9300, Batch Loss:    43.801613, Lr: 0.000096, Tokens per sec:   2513
2023-03-14 22:20:41,176 - INFO - __main__ - Epoch   5, Step:    9400, Batch Loss:    41.254990, Lr: 0.000096, Tokens per sec:   2586
2023-03-14 22:21:01,762 - INFO - __main__ - Epoch   5, Step:    9500, Batch Loss:    41.217239, Lr: 0.000096, Tokens per sec:   2588
2023-03-14 22:21:22,431 - INFO - __main__ - Epoch   5, Step:    9600, Batch Loss:    63.540379, Lr: 0.000096, Tokens per sec:   2579
2023-03-14 22:21:43,074 - INFO - __main__ - Epoch   5, Step:    9700, Batch Loss:    51.450485, Lr: 0.000096, Tokens per sec:   2656
2023-03-14 22:22:03,888 - INFO - __main__ - Epoch   5, Step:    9800, Batch Loss:    35.622208, Lr: 0.000096, Tokens per sec:   2585
2023-03-14 22:22:24,229 - INFO - __main__ - Epoch   5, Step:    9900, Batch Loss:    52.115707, Lr: 0.000096, Tokens per sec:   2602
2023-03-14 22:22:45,138 - INFO - __main__ - Epoch   5, Step:   10000, Batch Loss:    49.014107, Lr: 0.000096, Tokens per sec:   2588
2023-03-14 22:23:06,430 - INFO - __main__ - Epoch   5, Step:   10100, Batch Loss:    55.432411, Lr: 0.000096, Tokens per sec:   2563
2023-03-14 22:23:26,827 - INFO - __main__ - Epoch   5, Step:   10200, Batch Loss:    50.080929, Lr: 0.000096, Tokens per sec:   2658
2023-03-14 22:23:47,134 - INFO - __main__ - Epoch   5, Step:   10300, Batch Loss:    49.675678, Lr: 0.000096, Tokens per sec:   2671
2023-03-14 22:24:07,578 - INFO - __main__ - Epoch   5, Step:   10400, Batch Loss:    54.897461, Lr: 0.000096, Tokens per sec:   2638
2023-03-14 22:24:28,812 - INFO - __main__ - Epoch   5, Step:   10500, Batch Loss:    50.097485, Lr: 0.000096, Tokens per sec:   2561
2023-03-14 22:24:49,229 - INFO - __main__ - Epoch   5, Step:   10600, Batch Loss:    36.479565, Lr: 0.000096, Tokens per sec:   2652
2023-03-14 22:25:09,544 - INFO - __main__ - Epoch   5, Step:   10700, Batch Loss:    47.247585, Lr: 0.000096, Tokens per sec:   2632
2023-03-14 22:25:30,858 - INFO - __main__ - Epoch   5, Step:   10800, Batch Loss:    38.346649, Lr: 0.000096, Tokens per sec:   2534
2023-03-14 22:25:50,849 - INFO - __main__ - Epoch   5: total training loss 105053.26
2023-03-14 22:25:50,851 - INFO - __main__ - Epoch 6
2023-03-14 22:25:52,035 - INFO - __main__ - Epoch   6, Step:   10900, Batch Loss:    52.005474, Lr: 0.000095, Tokens per sec:   2552
2023-03-14 22:26:12,626 - INFO - __main__ - Epoch   6, Step:   11000, Batch Loss:    33.009235, Lr: 0.000095, Tokens per sec:   2660
2023-03-14 22:26:33,957 - INFO - __main__ - Epoch   6, Step:   11100, Batch Loss:    49.194622, Lr: 0.000095, Tokens per sec:   2548
2023-03-14 22:26:54,242 - INFO - __main__ - Epoch   6, Step:   11200, Batch Loss:    41.959526, Lr: 0.000095, Tokens per sec:   2655
2023-03-14 22:27:14,532 - INFO - __main__ - Epoch   6, Step:   11300, Batch Loss:    62.514973, Lr: 0.000095, Tokens per sec:   2649
2023-03-14 22:27:35,606 - INFO - __main__ - Epoch   6, Step:   11400, Batch Loss:    34.546425, Lr: 0.000095, Tokens per sec:   2563
2023-03-14 22:27:56,517 - INFO - __main__ - Epoch   6, Step:   11500, Batch Loss:    32.740910, Lr: 0.000095, Tokens per sec:   2564
2023-03-14 22:28:16,896 - INFO - __main__ - Epoch   6, Step:   11600, Batch Loss:    36.934174, Lr: 0.000095, Tokens per sec:   2607
2023-03-14 22:28:37,110 - INFO - __main__ - Epoch   6, Step:   11700, Batch Loss:    49.002747, Lr: 0.000095, Tokens per sec:   2654
2023-03-14 22:28:57,832 - INFO - __main__ - Epoch   6, Step:   11800, Batch Loss:    36.902733, Lr: 0.000095, Tokens per sec:   2579
2023-03-14 22:29:18,788 - INFO - __main__ - Epoch   6, Step:   11900, Batch Loss:    48.779640, Lr: 0.000095, Tokens per sec:   2552
2023-03-14 22:29:39,149 - INFO - __main__ - Epoch   6, Step:   12000, Batch Loss:    49.657368, Lr: 0.000095, Tokens per sec:   2640
2023-03-14 22:29:59,231 - INFO - __main__ - Epoch   6, Step:   12100, Batch Loss:    39.787533, Lr: 0.000095, Tokens per sec:   2668
2023-03-14 22:30:20,459 - INFO - __main__ - Epoch   6, Step:   12200, Batch Loss:    35.289818, Lr: 0.000095, Tokens per sec:   2552
2023-03-14 22:30:40,872 - INFO - __main__ - Epoch   6, Step:   12300, Batch Loss:    57.544182, Lr: 0.000095, Tokens per sec:   2686
2023-03-14 22:31:01,075 - INFO - __main__ - Epoch   6, Step:   12400, Batch Loss:    37.831547, Lr: 0.000095, Tokens per sec:   2662
2023-03-14 22:31:21,715 - INFO - __main__ - Epoch   6, Step:   12500, Batch Loss:    39.760559, Lr: 0.000095, Tokens per sec:   2581
2023-03-14 22:31:42,563 - INFO - __main__ - Epoch   6, Step:   12600, Batch Loss:    37.682262, Lr: 0.000095, Tokens per sec:   2596
2023-03-14 22:32:02,681 - INFO - __main__ - Epoch   6, Step:   12700, Batch Loss:    54.143593, Lr: 0.000095, Tokens per sec:   2673
2023-03-14 22:32:23,203 - INFO - __main__ - Epoch   6, Step:   12800, Batch Loss:    47.859955, Lr: 0.000095, Tokens per sec:   2604
2023-03-14 22:32:44,859 - INFO - __main__ - Epoch   6, Step:   12900, Batch Loss:    39.814709, Lr: 0.000095, Tokens per sec:   2465
2023-03-14 22:33:05,243 - INFO - __main__ - Epoch   6, Step:   13000, Batch Loss:    44.607395, Lr: 0.000095, Tokens per sec:   2657
2023-03-14 22:33:20,594 - INFO - __main__ - Epoch   6: total training loss 97574.56
2023-03-14 22:33:20,595 - INFO - __main__ - Epoch 7
2023-03-14 22:33:26,182 - INFO - __main__ - Epoch   7, Step:   13100, Batch Loss:    33.861565, Lr: 0.000094, Tokens per sec:   2582
2023-03-14 22:33:47,470 - INFO - __main__ - Epoch   7, Step:   13200, Batch Loss:    43.362816, Lr: 0.000094, Tokens per sec:   2522
2023-03-14 22:34:08,393 - INFO - __main__ - Epoch   7, Step:   13300, Batch Loss:    47.537354, Lr: 0.000094, Tokens per sec:   2603
2023-03-14 22:34:28,990 - INFO - __main__ - Epoch   7, Step:   13400, Batch Loss:    48.030121, Lr: 0.000094, Tokens per sec:   2594
2023-03-14 22:34:49,966 - INFO - __main__ - Epoch   7, Step:   13500, Batch Loss:    41.142036, Lr: 0.000094, Tokens per sec:   2572
2023-03-14 22:35:11,391 - INFO - __main__ - Epoch   7, Step:   13600, Batch Loss:    32.195744, Lr: 0.000094, Tokens per sec:   2493
2023-03-14 22:35:32,013 - INFO - __main__ - Epoch   7, Step:   13700, Batch Loss:    38.611759, Lr: 0.000094, Tokens per sec:   2581
2023-03-14 22:35:53,036 - INFO - __main__ - Epoch   7, Step:   13800, Batch Loss:    54.915382, Lr: 0.000094, Tokens per sec:   2520
2023-03-14 22:36:14,208 - INFO - __main__ - Epoch   7, Step:   13900, Batch Loss:    35.781155, Lr: 0.000094, Tokens per sec:   2573
2023-03-14 22:36:34,909 - INFO - __main__ - Epoch   7, Step:   14000, Batch Loss:    47.250835, Lr: 0.000094, Tokens per sec:   2624
2023-03-14 22:36:55,931 - INFO - __main__ - Epoch   7, Step:   14100, Batch Loss:    40.828178, Lr: 0.000094, Tokens per sec:   2556
2023-03-14 22:37:16,946 - INFO - __main__ - Epoch   7, Step:   14200, Batch Loss:    30.823215, Lr: 0.000094, Tokens per sec:   2590
2023-03-14 22:37:37,408 - INFO - __main__ - Epoch   7, Step:   14300, Batch Loss:    39.995758, Lr: 0.000094, Tokens per sec:   2628
2023-03-14 22:37:58,692 - INFO - __main__ - Epoch   7, Step:   14400, Batch Loss:    48.673923, Lr: 0.000094, Tokens per sec:   2554
2023-03-14 22:38:19,782 - INFO - __main__ - Epoch   7, Step:   14500, Batch Loss:    48.703575, Lr: 0.000094, Tokens per sec:   2594
2023-03-14 22:38:40,390 - INFO - __main__ - Epoch   7, Step:   14600, Batch Loss:    48.154377, Lr: 0.000094, Tokens per sec:   2614
2023-03-14 22:39:01,026 - INFO - __main__ - Epoch   7, Step:   14700, Batch Loss:    40.645794, Lr: 0.000094, Tokens per sec:   2594
2023-03-14 22:39:22,568 - INFO - __main__ - Epoch   7, Step:   14800, Batch Loss:    43.347984, Lr: 0.000094, Tokens per sec:   2490
2023-03-14 22:39:43,131 - INFO - __main__ - Epoch   7, Step:   14900, Batch Loss:    40.630363, Lr: 0.000094, Tokens per sec:   2607
2023-03-14 22:40:04,167 - INFO - __main__ - Epoch   7, Step:   15000, Batch Loss:    38.026726, Lr: 0.000094, Tokens per sec:   2538
2023-03-14 22:40:25,431 - INFO - __main__ - Epoch   7, Step:   15100, Batch Loss:    31.684288, Lr: 0.000094, Tokens per sec:   2509
2023-03-14 22:40:45,921 - INFO - __main__ - Epoch   7, Step:   15200, Batch Loss:    41.429131, Lr: 0.000094, Tokens per sec:   2601
2023-03-14 22:40:57,097 - INFO - __main__ - Epoch   7: total training loss 90995.35
2023-03-14 22:40:57,098 - INFO - __main__ - Epoch 8
2023-03-14 22:41:07,299 - INFO - __main__ - Epoch   8, Step:   15300, Batch Loss:    43.011234, Lr: 0.000093, Tokens per sec:   2430
2023-03-14 22:41:28,218 - INFO - __main__ - Epoch   8, Step:   15400, Batch Loss:    42.248341, Lr: 0.000093, Tokens per sec:   2614
2023-03-14 22:41:48,641 - INFO - __main__ - Epoch   8, Step:   15500, Batch Loss:    55.141834, Lr: 0.000093, Tokens per sec:   2636
2023-03-14 22:42:09,486 - INFO - __main__ - Epoch   8, Step:   15600, Batch Loss:    42.151978, Lr: 0.000093, Tokens per sec:   2610
2023-03-14 22:42:30,882 - INFO - __main__ - Epoch   8, Step:   15700, Batch Loss:    40.802662, Lr: 0.000093, Tokens per sec:   2544
2023-03-14 22:42:51,536 - INFO - __main__ - Epoch   8, Step:   15800, Batch Loss:    31.532688, Lr: 0.000093, Tokens per sec:   2598
2023-03-14 22:43:12,614 - INFO - __main__ - Epoch   8, Step:   15900, Batch Loss:    54.190292, Lr: 0.000093, Tokens per sec:   2552
2023-03-14 22:43:33,703 - INFO - __main__ - Epoch   8, Step:   16000, Batch Loss:    38.423351, Lr: 0.000093, Tokens per sec:   2523
2023-03-14 22:43:54,342 - INFO - __main__ - Epoch   8, Step:   16100, Batch Loss:    41.871529, Lr: 0.000093, Tokens per sec:   2621
2023-03-14 22:44:15,115 - INFO - __main__ - Epoch   8, Step:   16200, Batch Loss:    41.478695, Lr: 0.000093, Tokens per sec:   2612
2023-03-14 22:44:36,375 - INFO - __main__ - Epoch   8, Step:   16300, Batch Loss:    36.338760, Lr: 0.000093, Tokens per sec:   2486
2023-03-14 22:44:57,104 - INFO - __main__ - Epoch   8, Step:   16400, Batch Loss:    31.888281, Lr: 0.000093, Tokens per sec:   2619
2023-03-14 22:45:18,101 - INFO - __main__ - Epoch   8, Step:   16500, Batch Loss:    44.905239, Lr: 0.000093, Tokens per sec:   2574
2023-03-14 22:45:39,523 - INFO - __main__ - Epoch   8, Step:   16600, Batch Loss:    44.433605, Lr: 0.000093, Tokens per sec:   2502
2023-03-14 22:45:59,921 - INFO - __main__ - Epoch   8, Step:   16700, Batch Loss:    45.208427, Lr: 0.000093, Tokens per sec:   2598
2023-03-14 22:46:20,369 - INFO - __main__ - Epoch   8, Step:   16800, Batch Loss:    41.786152, Lr: 0.000093, Tokens per sec:   2640
2023-03-14 22:46:41,500 - INFO - __main__ - Epoch   8, Step:   16900, Batch Loss:    32.172153, Lr: 0.000093, Tokens per sec:   2552
2023-03-14 22:47:02,642 - INFO - __main__ - Epoch   8, Step:   17000, Batch Loss:    47.396561, Lr: 0.000093, Tokens per sec:   2568
2023-03-14 22:47:23,113 - INFO - __main__ - Epoch   8, Step:   17100, Batch Loss:    50.553226, Lr: 0.000093, Tokens per sec:   2610
2023-03-14 22:47:43,485 - INFO - __main__ - Epoch   8, Step:   17200, Batch Loss:    34.371342, Lr: 0.000093, Tokens per sec:   2651
2023-03-14 22:48:04,262 - INFO - __main__ - Epoch   8, Step:   17300, Batch Loss:    44.387154, Lr: 0.000093, Tokens per sec:   2566
2023-03-14 22:48:25,505 - INFO - __main__ - Epoch   8, Step:   17400, Batch Loss:    46.984222, Lr: 0.000093, Tokens per sec:   2554
2023-03-14 22:48:31,754 - INFO - __main__ - Epoch   8: total training loss 85080.19
2023-03-14 22:48:31,754 - INFO - __main__ - Epoch 9
2023-03-14 22:48:46,459 - INFO - __main__ - Epoch   9, Step:   17500, Batch Loss:    38.519642, Lr: 0.000092, Tokens per sec:   2475
2023-03-14 22:49:07,020 - INFO - __main__ - Epoch   9, Step:   17600, Batch Loss:    29.916931, Lr: 0.000092, Tokens per sec:   2615
2023-03-14 22:49:27,707 - INFO - __main__ - Epoch   9, Step:   17700, Batch Loss:    31.950666, Lr: 0.000092, Tokens per sec:   2638
2023-03-14 22:49:48,874 - INFO - __main__ - Epoch   9, Step:   17800, Batch Loss:    26.531454, Lr: 0.000092, Tokens per sec:   2564
2023-03-14 22:50:09,351 - INFO - __main__ - Epoch   9, Step:   17900, Batch Loss:    35.146427, Lr: 0.000092, Tokens per sec:   2607
2023-03-14 22:50:29,685 - INFO - __main__ - Epoch   9, Step:   18000, Batch Loss:    36.376564, Lr: 0.000092, Tokens per sec:   2642
2023-03-14 22:50:50,360 - INFO - __main__ - Epoch   9, Step:   18100, Batch Loss:    42.622433, Lr: 0.000092, Tokens per sec:   2617
2023-03-14 22:51:11,338 - INFO - __main__ - Epoch   9, Step:   18200, Batch Loss:    23.850334, Lr: 0.000092, Tokens per sec:   2633
2023-03-14 22:51:32,179 - INFO - __main__ - Epoch   9, Step:   18300, Batch Loss:    22.417000, Lr: 0.000092, Tokens per sec:   2583
2023-03-14 22:51:52,210 - INFO - __main__ - Epoch   9, Step:   18400, Batch Loss:    42.050987, Lr: 0.000092, Tokens per sec:   2667
2023-03-14 22:52:12,344 - INFO - __main__ - Epoch   9, Step:   18500, Batch Loss:    42.819962, Lr: 0.000092, Tokens per sec:   2690
2023-03-14 22:52:32,440 - INFO - __main__ - Epoch   9, Step:   18600, Batch Loss:    55.103699, Lr: 0.000092, Tokens per sec:   2675
2023-03-14 22:52:52,803 - INFO - __main__ - Epoch   9, Step:   18700, Batch Loss:    21.816132, Lr: 0.000092, Tokens per sec:   2659
2023-03-14 22:53:13,222 - INFO - __main__ - Epoch   9, Step:   18800, Batch Loss:    44.037498, Lr: 0.000092, Tokens per sec:   2624
2023-03-14 22:53:34,088 - INFO - __main__ - Epoch   9, Step:   18900, Batch Loss:    35.462551, Lr: 0.000092, Tokens per sec:   2569
2023-03-14 22:53:54,763 - INFO - __main__ - Epoch   9, Step:   19000, Batch Loss:    41.420727, Lr: 0.000092, Tokens per sec:   2597
2023-03-14 22:54:14,988 - INFO - __main__ - Epoch   9, Step:   19100, Batch Loss:    41.056442, Lr: 0.000092, Tokens per sec:   2686
2023-03-14 22:54:35,141 - INFO - __main__ - Epoch   9, Step:   19200, Batch Loss:    42.824284, Lr: 0.000092, Tokens per sec:   2652
2023-03-14 22:54:55,263 - INFO - __main__ - Epoch   9, Step:   19300, Batch Loss:    28.298094, Lr: 0.000092, Tokens per sec:   2640
2023-03-14 22:55:15,596 - INFO - __main__ - Epoch   9, Step:   19400, Batch Loss:    36.008156, Lr: 0.000092, Tokens per sec:   2663
2023-03-14 22:55:35,758 - INFO - __main__ - Epoch   9, Step:   19500, Batch Loss:    32.609909, Lr: 0.000092, Tokens per sec:   2638
2023-03-14 22:55:55,403 - INFO - __main__ - Epoch   9, Step:   19600, Batch Loss:    34.110699, Lr: 0.000092, Tokens per sec:   2722
2023-03-14 22:55:57,562 - INFO - __main__ - Epoch   9: total training loss 79742.57
2023-03-14 22:55:57,563 - INFO - __main__ - Epoch 10
2023-03-14 22:56:16,421 - INFO - __main__ - Epoch  10, Step:   19700, Batch Loss:    33.698154, Lr: 0.000091, Tokens per sec:   2536
2023-03-14 22:56:36,536 - INFO - __main__ - Epoch  10, Step:   19800, Batch Loss:    37.739986, Lr: 0.000091, Tokens per sec:   2698
2023-03-14 22:56:56,764 - INFO - __main__ - Epoch  10, Step:   19900, Batch Loss:    31.900633, Lr: 0.000091, Tokens per sec:   2711
2023-03-14 22:57:17,079 - INFO - __main__ - Epoch  10, Step:   20000, Batch Loss:    30.141029, Lr: 0.000091, Tokens per sec:   2617
2023-03-14 22:57:37,400 - INFO - __main__ - Epoch  10, Step:   20100, Batch Loss:    37.119080, Lr: 0.000091, Tokens per sec:   2682
2023-03-14 22:57:58,222 - INFO - __main__ - Epoch  10, Step:   20200, Batch Loss:    36.682327, Lr: 0.000091, Tokens per sec:   2604
2023-03-14 22:58:19,286 - INFO - __main__ - Epoch  10, Step:   20300, Batch Loss:    37.965828, Lr: 0.000091, Tokens per sec:   2576
2023-03-14 22:58:39,520 - INFO - __main__ - Epoch  10, Step:   20400, Batch Loss:    35.601433, Lr: 0.000091, Tokens per sec:   2609
2023-03-14 22:58:59,832 - INFO - __main__ - Epoch  10, Step:   20500, Batch Loss:    23.449482, Lr: 0.000091, Tokens per sec:   2673
2023-03-14 22:59:20,359 - INFO - __main__ - Epoch  10, Step:   20600, Batch Loss:    41.959114, Lr: 0.000091, Tokens per sec:   2650
2023-03-14 22:59:41,355 - INFO - __main__ - Epoch  10, Step:   20700, Batch Loss:    41.989697, Lr: 0.000091, Tokens per sec:   2546
2023-03-14 23:00:01,861 - INFO - __main__ - Epoch  10, Step:   20800, Batch Loss:    43.203430, Lr: 0.000091, Tokens per sec:   2603
2023-03-14 23:00:22,033 - INFO - __main__ - Epoch  10, Step:   20900, Batch Loss:    30.500998, Lr: 0.000091, Tokens per sec:   2655
2023-03-14 23:00:42,337 - INFO - __main__ - Epoch  10, Step:   21000, Batch Loss:    30.760567, Lr: 0.000091, Tokens per sec:   2674
2023-03-14 23:01:02,375 - INFO - __main__ - Epoch  10, Step:   21100, Batch Loss:    39.368320, Lr: 0.000091, Tokens per sec:   2647
2023-03-14 23:01:22,646 - INFO - __main__ - Epoch  10, Step:   21200, Batch Loss:    24.724104, Lr: 0.000091, Tokens per sec:   2636
2023-03-14 23:01:43,191 - INFO - __main__ - Epoch  10, Step:   21300, Batch Loss:    35.338207, Lr: 0.000091, Tokens per sec:   2601
2023-03-14 23:02:03,931 - INFO - __main__ - Epoch  10, Step:   21400, Batch Loss:    43.389290, Lr: 0.000091, Tokens per sec:   2609
2023-03-14 23:02:24,698 - INFO - __main__ - Epoch  10, Step:   21500, Batch Loss:    37.128677, Lr: 0.000091, Tokens per sec:   2586
2023-03-14 23:02:44,845 - INFO - __main__ - Epoch  10, Step:   21600, Batch Loss:    15.246815, Lr: 0.000091, Tokens per sec:   2643
2023-03-14 23:03:05,008 - INFO - __main__ - Epoch  10, Step:   21700, Batch Loss:    37.957352, Lr: 0.000091, Tokens per sec:   2726
2023-03-14 23:03:23,800 - INFO - __main__ - Epoch  10: total training loss 74858.51
2023-03-14 23:03:23,800 - INFO - __main__ - Epoch 11
2023-03-14 23:03:26,307 - INFO - __main__ - Epoch  11, Step:   21800, Batch Loss:    26.864065, Lr: 0.000090, Tokens per sec:   2058
2023-03-14 23:03:47,131 - INFO - __main__ - Epoch  11, Step:   21900, Batch Loss:    28.662296, Lr: 0.000090, Tokens per sec:   2609
2023-03-14 23:04:07,170 - INFO - __main__ - Epoch  11, Step:   22000, Batch Loss:    34.688229, Lr: 0.000090, Tokens per sec:   2624
2023-03-14 23:04:24,920 - INFO - __main__ - Epoch  11, Step:   22100, Batch Loss:    34.404884, Lr: 0.000090, Tokens per sec:   3036
2023-03-14 23:04:40,307 - INFO - __main__ - Epoch  11, Step:   22200, Batch Loss:    24.422049, Lr: 0.000090, Tokens per sec:   3488
2023-03-14 23:04:55,814 - INFO - __main__ - Epoch  11, Step:   22300, Batch Loss:    27.659845, Lr: 0.000090, Tokens per sec:   3477
2023-03-14 23:05:11,201 - INFO - __main__ - Epoch  11, Step:   22400, Batch Loss:    28.059715, Lr: 0.000090, Tokens per sec:   3497
2023-03-14 23:05:26,592 - INFO - __main__ - Epoch  11, Step:   22500, Batch Loss:    28.068619, Lr: 0.000090, Tokens per sec:   3533
2023-03-14 23:05:41,828 - INFO - __main__ - Epoch  11, Step:   22600, Batch Loss:    27.302834, Lr: 0.000090, Tokens per sec:   3510
2023-03-14 23:05:57,183 - INFO - __main__ - Epoch  11, Step:   22700, Batch Loss:    36.118526, Lr: 0.000090, Tokens per sec:   3558
2023-03-14 23:06:12,599 - INFO - __main__ - Epoch  11, Step:   22800, Batch Loss:    29.432047, Lr: 0.000090, Tokens per sec:   3558
2023-03-14 23:06:27,468 - INFO - __main__ - Epoch  11, Step:   22900, Batch Loss:    37.706474, Lr: 0.000090, Tokens per sec:   3567
2023-03-14 23:06:42,831 - INFO - __main__ - Epoch  11, Step:   23000, Batch Loss:    23.764719, Lr: 0.000090, Tokens per sec:   3532
2023-03-14 23:06:57,733 - INFO - __main__ - Epoch  11, Step:   23100, Batch Loss:    25.261822, Lr: 0.000090, Tokens per sec:   3604
2023-03-14 23:07:12,699 - INFO - __main__ - Epoch  11, Step:   23200, Batch Loss:    36.906921, Lr: 0.000090, Tokens per sec:   3618
2023-03-14 23:07:28,129 - INFO - __main__ - Epoch  11, Step:   23300, Batch Loss:    29.540564, Lr: 0.000090, Tokens per sec:   3450
2023-03-14 23:07:43,479 - INFO - __main__ - Epoch  11, Step:   23400, Batch Loss:    33.626301, Lr: 0.000090, Tokens per sec:   3483
2023-03-14 23:07:58,836 - INFO - __main__ - Epoch  11, Step:   23500, Batch Loss:    36.951813, Lr: 0.000090, Tokens per sec:   3520
2023-03-14 23:08:14,064 - INFO - __main__ - Epoch  11, Step:   23600, Batch Loss:    30.654436, Lr: 0.000090, Tokens per sec:   3570
2023-03-14 23:08:29,268 - INFO - __main__ - Epoch  11, Step:   23700, Batch Loss:    26.958881, Lr: 0.000090, Tokens per sec:   3464
2023-03-14 23:08:44,540 - INFO - __main__ - Epoch  11, Step:   23800, Batch Loss:    34.193287, Lr: 0.000090, Tokens per sec:   3508
2023-03-14 23:08:59,656 - INFO - __main__ - Epoch  11, Step:   23900, Batch Loss:    33.930233, Lr: 0.000090, Tokens per sec:   3612
2023-03-14 23:09:10,388 - INFO - __main__ - Epoch  11: total training loss 70456.25
2023-03-14 23:09:10,389 - INFO - __main__ - Epoch 12
2023-03-14 23:09:14,908 - INFO - __main__ - Epoch  12, Step:   24000, Batch Loss:    23.895638, Lr: 0.000090, Tokens per sec:   3676
2023-03-14 23:09:30,220 - INFO - __main__ - Epoch  12, Step:   24100, Batch Loss:    31.560314, Lr: 0.000090, Tokens per sec:   3494
2023-03-14 23:09:45,507 - INFO - __main__ - Epoch  12, Step:   24200, Batch Loss:    30.648335, Lr: 0.000090, Tokens per sec:   3471
2023-03-14 23:10:00,991 - INFO - __main__ - Epoch  12, Step:   24300, Batch Loss:    34.778275, Lr: 0.000090, Tokens per sec:   3470
2023-03-14 23:10:16,423 - INFO - __main__ - Epoch  12, Step:   24400, Batch Loss:    31.820421, Lr: 0.000090, Tokens per sec:   3482
2023-03-14 23:10:31,691 - INFO - __main__ - Epoch  12, Step:   24500, Batch Loss:    30.607222, Lr: 0.000090, Tokens per sec:   3549
2023-03-14 23:10:46,599 - INFO - __main__ - Epoch  12, Step:   24600, Batch Loss:    22.199430, Lr: 0.000090, Tokens per sec:   3573
2023-03-14 23:11:01,832 - INFO - __main__ - Epoch  12, Step:   24700, Batch Loss:    25.273983, Lr: 0.000090, Tokens per sec:   3543
2023-03-14 23:11:17,208 - INFO - __main__ - Epoch  12, Step:   24800, Batch Loss:    31.270521, Lr: 0.000090, Tokens per sec:   3514
2023-03-14 23:11:32,551 - INFO - __main__ - Epoch  12, Step:   24900, Batch Loss:    34.783699, Lr: 0.000090, Tokens per sec:   3540
2023-03-14 23:11:47,740 - INFO - __main__ - Epoch  12, Step:   25000, Batch Loss:    16.354317, Lr: 0.000090, Tokens per sec:   3516
2023-03-14 23:12:03,047 - INFO - __main__ - Epoch  12, Step:   25100, Batch Loss:    33.654514, Lr: 0.000090, Tokens per sec:   3510
2023-03-14 23:12:18,449 - INFO - __main__ - Epoch  12, Step:   25200, Batch Loss:    33.458042, Lr: 0.000090, Tokens per sec:   3539
2023-03-14 23:12:33,830 - INFO - __main__ - Epoch  12, Step:   25300, Batch Loss:    25.520479, Lr: 0.000090, Tokens per sec:   3453
2023-03-14 23:12:49,135 - INFO - __main__ - Epoch  12, Step:   25400, Batch Loss:    26.171545, Lr: 0.000090, Tokens per sec:   3566
2023-03-14 23:13:04,366 - INFO - __main__ - Epoch  12, Step:   25500, Batch Loss:    23.637539, Lr: 0.000090, Tokens per sec:   3504
2023-03-14 23:13:19,671 - INFO - __main__ - Epoch  12, Step:   25600, Batch Loss:    27.728384, Lr: 0.000090, Tokens per sec:   3485
2023-03-14 23:13:34,975 - INFO - __main__ - Epoch  12, Step:   25700, Batch Loss:    42.548576, Lr: 0.000090, Tokens per sec:   3539
2023-03-14 23:13:50,355 - INFO - __main__ - Epoch  12, Step:   25800, Batch Loss:    33.030605, Lr: 0.000090, Tokens per sec:   3524
2023-03-14 23:14:05,701 - INFO - __main__ - Epoch  12, Step:   25900, Batch Loss:    37.739712, Lr: 0.000090, Tokens per sec:   3517
2023-03-14 23:14:20,842 - INFO - __main__ - Epoch  12, Step:   26000, Batch Loss:    27.597712, Lr: 0.000090, Tokens per sec:   3537
2023-03-14 23:14:35,827 - INFO - __main__ - Epoch  12, Step:   26100, Batch Loss:    23.557917, Lr: 0.000090, Tokens per sec:   3664
2023-03-14 23:14:43,233 - INFO - __main__ - Epoch  12: total training loss 66327.05
2023-03-14 23:14:43,234 - INFO - __main__ - Epoch 13
2023-03-14 23:14:51,434 - INFO - __main__ - Epoch  13, Step:   26200, Batch Loss:    20.454300, Lr: 0.000089, Tokens per sec:   3428
2023-03-14 23:15:06,797 - INFO - __main__ - Epoch  13, Step:   26300, Batch Loss:    23.353104, Lr: 0.000089, Tokens per sec:   3490
2023-03-14 23:15:22,054 - INFO - __main__ - Epoch  13, Step:   26400, Batch Loss:    30.213017, Lr: 0.000089, Tokens per sec:   3528
2023-03-14 23:15:37,340 - INFO - __main__ - Epoch  13, Step:   26500, Batch Loss:    22.747993, Lr: 0.000089, Tokens per sec:   3503
2023-03-14 23:15:52,534 - INFO - __main__ - Epoch  13, Step:   26600, Batch Loss:    32.575253, Lr: 0.000089, Tokens per sec:   3547
2023-03-14 23:16:07,760 - INFO - __main__ - Epoch  13, Step:   26700, Batch Loss:    20.900423, Lr: 0.000089, Tokens per sec:   3565
2023-03-14 23:16:22,876 - INFO - __main__ - Epoch  13, Step:   26800, Batch Loss:    31.192263, Lr: 0.000089, Tokens per sec:   3599
2023-03-14 23:16:38,055 - INFO - __main__ - Epoch  13, Step:   26900, Batch Loss:    29.202515, Lr: 0.000089, Tokens per sec:   3537
2023-03-14 23:16:53,353 - INFO - __main__ - Epoch  13, Step:   27000, Batch Loss:    28.976032, Lr: 0.000089, Tokens per sec:   3543
2023-03-14 23:17:08,609 - INFO - __main__ - Epoch  13, Step:   27100, Batch Loss:    23.208006, Lr: 0.000089, Tokens per sec:   3560
2023-03-14 23:17:23,976 - INFO - __main__ - Epoch  13, Step:   27200, Batch Loss:    27.913601, Lr: 0.000089, Tokens per sec:   3531
2023-03-14 23:17:39,289 - INFO - __main__ - Epoch  13, Step:   27300, Batch Loss:    34.808189, Lr: 0.000089, Tokens per sec:   3509
2023-03-14 23:17:54,506 - INFO - __main__ - Epoch  13, Step:   27400, Batch Loss:    32.675758, Lr: 0.000089, Tokens per sec:   3502
2023-03-14 23:18:09,766 - INFO - __main__ - Epoch  13, Step:   27500, Batch Loss:    25.828747, Lr: 0.000089, Tokens per sec:   3538
2023-03-14 23:18:24,994 - INFO - __main__ - Epoch  13, Step:   27600, Batch Loss:    30.176561, Lr: 0.000089, Tokens per sec:   3537
2023-03-14 23:18:40,376 - INFO - __main__ - Epoch  13, Step:   27700, Batch Loss:    27.248440, Lr: 0.000089, Tokens per sec:   3523
2023-03-14 23:18:55,899 - INFO - __main__ - Epoch  13, Step:   27800, Batch Loss:    33.065968, Lr: 0.000089, Tokens per sec:   3498
2023-03-14 23:19:11,446 - INFO - __main__ - Epoch  13, Step:   27900, Batch Loss:    22.894188, Lr: 0.000089, Tokens per sec:   3467
2023-03-14 23:19:27,062 - INFO - __main__ - Epoch  13, Step:   28000, Batch Loss:    23.193132, Lr: 0.000089, Tokens per sec:   3382
2023-03-14 23:19:42,335 - INFO - __main__ - Epoch  13, Step:   28100, Batch Loss:    27.228666, Lr: 0.000089, Tokens per sec:   3440
2023-03-14 23:19:57,299 - INFO - __main__ - Epoch  13, Step:   28200, Batch Loss:    31.663248, Lr: 0.000089, Tokens per sec:   3582
2023-03-14 23:20:12,644 - INFO - __main__ - Epoch  13, Step:   28300, Batch Loss:    24.317106, Lr: 0.000089, Tokens per sec:   3511
2023-03-14 23:20:16,819 - INFO - __main__ - Epoch  13: total training loss 62513.37
2023-03-14 23:20:16,820 - INFO - __main__ - Epoch 14
2023-03-14 23:20:28,216 - INFO - __main__ - Epoch  14, Step:   28400, Batch Loss:    24.140743, Lr: 0.000088, Tokens per sec:   3475
2023-03-14 23:20:43,420 - INFO - __main__ - Epoch  14, Step:   28500, Batch Loss:    31.898851, Lr: 0.000088, Tokens per sec:   3563
2023-03-14 23:20:58,576 - INFO - __main__ - Epoch  14, Step:   28600, Batch Loss:    27.229002, Lr: 0.000088, Tokens per sec:   3546
2023-03-14 23:21:13,972 - INFO - __main__ - Epoch  14, Step:   28700, Batch Loss:    25.164663, Lr: 0.000088, Tokens per sec:   3482
2023-03-14 23:21:29,277 - INFO - __main__ - Epoch  14, Step:   28800, Batch Loss:    27.695055, Lr: 0.000088, Tokens per sec:   3508
2023-03-14 23:21:44,454 - INFO - __main__ - Epoch  14, Step:   28900, Batch Loss:    33.118382, Lr: 0.000088, Tokens per sec:   3512
2023-03-14 23:21:59,709 - INFO - __main__ - Epoch  14, Step:   29000, Batch Loss:    29.874859, Lr: 0.000088, Tokens per sec:   3564
2023-03-14 23:22:14,894 - INFO - __main__ - Epoch  14, Step:   29100, Batch Loss:    20.977520, Lr: 0.000088, Tokens per sec:   3537
2023-03-14 23:22:30,109 - INFO - __main__ - Epoch  14, Step:   29200, Batch Loss:    25.655758, Lr: 0.000088, Tokens per sec:   3555
2023-03-14 23:22:45,371 - INFO - __main__ - Epoch  14, Step:   29300, Batch Loss:    20.522469, Lr: 0.000088, Tokens per sec:   3495
2023-03-14 23:23:00,663 - INFO - __main__ - Epoch  14, Step:   29400, Batch Loss:    34.090080, Lr: 0.000088, Tokens per sec:   3515
2023-03-14 23:23:16,099 - INFO - __main__ - Epoch  14, Step:   29500, Batch Loss:    28.821556, Lr: 0.000088, Tokens per sec:   3502
2023-03-14 23:23:31,644 - INFO - __main__ - Epoch  14, Step:   29600, Batch Loss:    26.757273, Lr: 0.000088, Tokens per sec:   3475
2023-03-14 23:23:47,060 - INFO - __main__ - Epoch  14, Step:   29700, Batch Loss:    30.984766, Lr: 0.000088, Tokens per sec:   3461
2023-03-14 23:24:02,181 - INFO - __main__ - Epoch  14, Step:   29800, Batch Loss:    28.553190, Lr: 0.000088, Tokens per sec:   3533
2023-03-14 23:24:17,138 - INFO - __main__ - Epoch  14, Step:   29900, Batch Loss:    33.325314, Lr: 0.000088, Tokens per sec:   3656
2023-03-14 23:24:31,318 - INFO - __main__ - Epoch  14, Step:   30000, Batch Loss:    25.250952, Lr: 0.000088, Tokens per sec:   3742
2023-03-14 23:24:46,014 - INFO - __main__ - Epoch  14, Step:   30100, Batch Loss:    28.219015, Lr: 0.000088, Tokens per sec:   3710
2023-03-14 23:25:00,880 - INFO - __main__ - Epoch  14, Step:   30200, Batch Loss:    26.376167, Lr: 0.000088, Tokens per sec:   3665
2023-03-14 23:25:15,483 - INFO - __main__ - Epoch  14, Step:   30300, Batch Loss:    24.558222, Lr: 0.000088, Tokens per sec:   3686
2023-03-14 23:25:30,472 - INFO - __main__ - Epoch  14, Step:   30400, Batch Loss:    28.716978, Lr: 0.000088, Tokens per sec:   3571
2023-03-14 23:25:44,915 - INFO - __main__ - Epoch  14, Step:   30500, Batch Loss:    36.123924, Lr: 0.000088, Tokens per sec:   3704
2023-03-14 23:25:45,979 - INFO - __main__ - Epoch  14: total training loss 59143.48
2023-03-14 23:25:45,979 - INFO - __main__ - Epoch 15
2023-03-14 23:26:00,256 - INFO - __main__ - Epoch  15, Step:   30600, Batch Loss:    23.063953, Lr: 0.000087, Tokens per sec:   3522
2023-03-14 23:26:14,786 - INFO - __main__ - Epoch  15, Step:   30700, Batch Loss:    22.460352, Lr: 0.000087, Tokens per sec:   3709
2023-03-14 23:26:29,385 - INFO - __main__ - Epoch  15, Step:   30800, Batch Loss:    13.718689, Lr: 0.000087, Tokens per sec:   3615
2023-03-14 23:26:43,981 - INFO - __main__ - Epoch  15, Step:   30900, Batch Loss:    23.715231, Lr: 0.000087, Tokens per sec:   3619
2023-03-14 23:26:59,473 - INFO - __main__ - Epoch  15, Step:   31000, Batch Loss:    27.574989, Lr: 0.000087, Tokens per sec:   3508
2023-03-14 23:27:14,386 - INFO - __main__ - Epoch  15, Step:   31100, Batch Loss:    24.505747, Lr: 0.000087, Tokens per sec:   3640
2023-03-14 23:27:29,214 - INFO - __main__ - Epoch  15, Step:   31200, Batch Loss:    23.205202, Lr: 0.000087, Tokens per sec:   3660
2023-03-14 23:27:44,156 - INFO - __main__ - Epoch  15, Step:   31300, Batch Loss:    36.530636, Lr: 0.000087, Tokens per sec:   3601
2023-03-14 23:27:58,813 - INFO - __main__ - Epoch  15, Step:   31400, Batch Loss:    27.200457, Lr: 0.000087, Tokens per sec:   3688
2023-03-14 23:28:13,742 - INFO - __main__ - Epoch  15, Step:   31500, Batch Loss:    25.809847, Lr: 0.000087, Tokens per sec:   3618
2023-03-14 23:28:28,488 - INFO - __main__ - Epoch  15, Step:   31600, Batch Loss:    32.819042, Lr: 0.000087, Tokens per sec:   3615
2023-03-14 23:28:43,640 - INFO - __main__ - Epoch  15, Step:   31700, Batch Loss:    37.931049, Lr: 0.000087, Tokens per sec:   3560
2023-03-14 23:28:58,919 - INFO - __main__ - Epoch  15, Step:   31800, Batch Loss:    29.004889, Lr: 0.000087, Tokens per sec:   3508
2023-03-14 23:29:13,800 - INFO - __main__ - Epoch  15, Step:   31900, Batch Loss:    27.906717, Lr: 0.000087, Tokens per sec:   3626
2023-03-14 23:29:28,762 - INFO - __main__ - Epoch  15, Step:   32000, Batch Loss:    19.247679, Lr: 0.000087, Tokens per sec:   3626
2023-03-14 23:29:44,032 - INFO - __main__ - Epoch  15, Step:   32100, Batch Loss:    20.329288, Lr: 0.000087, Tokens per sec:   3514
2023-03-14 23:29:59,355 - INFO - __main__ - Epoch  15, Step:   32200, Batch Loss:    30.390108, Lr: 0.000087, Tokens per sec:   3542
2023-03-14 23:30:14,782 - INFO - __main__ - Epoch  15, Step:   32300, Batch Loss:    21.119408, Lr: 0.000087, Tokens per sec:   3482
2023-03-14 23:30:30,219 - INFO - __main__ - Epoch  15, Step:   32400, Batch Loss:    26.492931, Lr: 0.000087, Tokens per sec:   3518
2023-03-14 23:30:45,900 - INFO - __main__ - Epoch  15, Step:   32500, Batch Loss:    23.991959, Lr: 0.000087, Tokens per sec:   3460
2023-03-14 23:31:01,215 - INFO - __main__ - Epoch  15, Step:   32600, Batch Loss:    31.148161, Lr: 0.000087, Tokens per sec:   3530
2023-03-14 23:31:14,356 - INFO - __main__ - Epoch  15: total training loss 55853.49
2023-03-14 23:31:14,357 - INFO - __main__ - Epoch 16
2023-03-14 23:31:17,022 - INFO - __main__ - Epoch  16, Step:   32700, Batch Loss:    26.236212, Lr: 0.000086, Tokens per sec:   3109
2023-03-14 23:31:32,044 - INFO - __main__ - Epoch  16, Step:   32800, Batch Loss:    25.387556, Lr: 0.000086, Tokens per sec:   3574
2023-03-14 23:31:46,806 - INFO - __main__ - Epoch  16, Step:   32900, Batch Loss:    22.563349, Lr: 0.000086, Tokens per sec:   3652
2023-03-14 23:32:01,566 - INFO - __main__ - Epoch  16, Step:   33000, Batch Loss:    34.372601, Lr: 0.000086, Tokens per sec:   3668
2023-03-14 23:32:15,944 - INFO - __main__ - Epoch  16, Step:   33100, Batch Loss:    25.105436, Lr: 0.000086, Tokens per sec:   3725
2023-03-14 23:32:30,559 - INFO - __main__ - Epoch  16, Step:   33200, Batch Loss:    20.598749, Lr: 0.000086, Tokens per sec:   3719
2023-03-14 23:32:45,658 - INFO - __main__ - Epoch  16, Step:   33300, Batch Loss:    23.264488, Lr: 0.000086, Tokens per sec:   3506
2023-03-14 23:33:00,894 - INFO - __main__ - Epoch  16, Step:   33400, Batch Loss:    22.186968, Lr: 0.000086, Tokens per sec:   3532
2023-03-14 23:33:16,362 - INFO - __main__ - Epoch  16, Step:   33500, Batch Loss:    20.479780, Lr: 0.000086, Tokens per sec:   3478
2023-03-14 23:33:31,592 - INFO - __main__ - Epoch  16, Step:   33600, Batch Loss:    26.777241, Lr: 0.000086, Tokens per sec:   3524
2023-03-14 23:33:47,004 - INFO - __main__ - Epoch  16, Step:   33700, Batch Loss:    21.683439, Lr: 0.000086, Tokens per sec:   3476
2023-03-14 23:34:02,358 - INFO - __main__ - Epoch  16, Step:   33800, Batch Loss:    23.756315, Lr: 0.000086, Tokens per sec:   3513
2023-03-14 23:34:17,552 - INFO - __main__ - Epoch  16, Step:   33900, Batch Loss:    34.360825, Lr: 0.000086, Tokens per sec:   3590
2023-03-14 23:34:33,030 - INFO - __main__ - Epoch  16, Step:   34000, Batch Loss:    16.882423, Lr: 0.000086, Tokens per sec:   3386
2023-03-14 23:34:47,932 - INFO - __main__ - Epoch  16, Step:   34100, Batch Loss:    25.553427, Lr: 0.000086, Tokens per sec:   3639
2023-03-14 23:35:03,283 - INFO - __main__ - Epoch  16, Step:   34200, Batch Loss:    26.530315, Lr: 0.000086, Tokens per sec:   3537
2023-03-14 23:35:18,135 - INFO - __main__ - Epoch  16, Step:   34300, Batch Loss:    32.733631, Lr: 0.000086, Tokens per sec:   3576
2023-03-14 23:35:33,335 - INFO - __main__ - Epoch  16, Step:   34400, Batch Loss:    21.054951, Lr: 0.000086, Tokens per sec:   3536
2023-03-14 23:35:48,203 - INFO - __main__ - Epoch  16, Step:   34500, Batch Loss:    21.090021, Lr: 0.000086, Tokens per sec:   3697
2023-03-14 23:36:03,472 - INFO - __main__ - Epoch  16, Step:   34600, Batch Loss:    25.060720, Lr: 0.000086, Tokens per sec:   3546
2023-03-14 23:36:18,474 - INFO - __main__ - Epoch  16, Step:   34700, Batch Loss:    21.172211, Lr: 0.000086, Tokens per sec:   3605
2023-03-14 23:36:33,683 - INFO - __main__ - Epoch  16, Step:   34800, Batch Loss:    16.814680, Lr: 0.000086, Tokens per sec:   3536
2023-03-14 23:36:43,439 - INFO - __main__ - Epoch  16: total training loss 52876.05
2023-03-14 23:36:43,440 - INFO - __main__ - Epoch 17
2023-03-14 23:36:49,180 - INFO - __main__ - Epoch  17, Step:   34900, Batch Loss:    16.699766, Lr: 0.000085, Tokens per sec:   3373
2023-03-14 23:37:03,954 - INFO - __main__ - Epoch  17, Step:   35000, Batch Loss:    15.799604, Lr: 0.000085, Tokens per sec:   3606
2023-03-14 23:37:18,882 - INFO - __main__ - Epoch  17, Step:   35100, Batch Loss:    13.821325, Lr: 0.000085, Tokens per sec:   3549
2023-03-14 23:37:33,522 - INFO - __main__ - Epoch  17, Step:   35200, Batch Loss:    17.494688, Lr: 0.000085, Tokens per sec:   3693
2023-03-14 23:37:48,524 - INFO - __main__ - Epoch  17, Step:   35300, Batch Loss:    26.674797, Lr: 0.000085, Tokens per sec:   3588
2023-03-14 23:38:03,218 - INFO - __main__ - Epoch  17, Step:   35400, Batch Loss:    24.108917, Lr: 0.000085, Tokens per sec:   3627
2023-03-14 23:38:18,174 - INFO - __main__ - Epoch  17, Step:   35500, Batch Loss:    16.698896, Lr: 0.000085, Tokens per sec:   3591
2023-03-14 23:38:32,997 - INFO - __main__ - Epoch  17, Step:   35600, Batch Loss:    22.885201, Lr: 0.000085, Tokens per sec:   3693
2023-03-14 23:38:47,512 - INFO - __main__ - Epoch  17, Step:   35700, Batch Loss:    21.580585, Lr: 0.000085, Tokens per sec:   3821
2023-03-14 23:39:02,060 - INFO - __main__ - Epoch  17, Step:   35800, Batch Loss:    33.636082, Lr: 0.000085, Tokens per sec:   3749
2023-03-14 23:39:16,712 - INFO - __main__ - Epoch  17, Step:   35900, Batch Loss:    22.002075, Lr: 0.000085, Tokens per sec:   3684
2023-03-14 23:39:31,662 - INFO - __main__ - Epoch  17, Step:   36000, Batch Loss:    14.835425, Lr: 0.000085, Tokens per sec:   3565
2023-03-14 23:39:46,536 - INFO - __main__ - Epoch  17, Step:   36100, Batch Loss:    23.593960, Lr: 0.000085, Tokens per sec:   3621
2023-03-14 23:40:01,565 - INFO - __main__ - Epoch  17, Step:   36200, Batch Loss:    28.596354, Lr: 0.000085, Tokens per sec:   3615
2023-03-14 23:40:16,362 - INFO - __main__ - Epoch  17, Step:   36300, Batch Loss:    21.868484, Lr: 0.000085, Tokens per sec:   3677
2023-03-14 23:40:31,175 - INFO - __main__ - Epoch  17, Step:   36400, Batch Loss:    25.586220, Lr: 0.000085, Tokens per sec:   3586
2023-03-14 23:40:45,406 - INFO - __main__ - Epoch  17, Step:   36500, Batch Loss:    34.924313, Lr: 0.000085, Tokens per sec:   3752
2023-03-14 23:41:00,273 - INFO - __main__ - Epoch  17, Step:   36600, Batch Loss:    26.596598, Lr: 0.000085, Tokens per sec:   3643
2023-03-14 23:41:14,711 - INFO - __main__ - Epoch  17, Step:   36700, Batch Loss:    23.656759, Lr: 0.000085, Tokens per sec:   3693
2023-03-14 23:41:29,457 - INFO - __main__ - Epoch  17, Step:   36800, Batch Loss:    16.458347, Lr: 0.000085, Tokens per sec:   3599
2023-03-14 23:41:44,055 - INFO - __main__ - Epoch  17, Step:   36900, Batch Loss:    24.423525, Lr: 0.000085, Tokens per sec:   3735
2023-03-14 23:41:59,126 - INFO - __main__ - Epoch  17, Step:   37000, Batch Loss:    24.577480, Lr: 0.000085, Tokens per sec:   3556
2023-03-14 23:42:05,754 - INFO - __main__ - Epoch  17: total training loss 50091.24
2023-03-14 23:42:05,754 - INFO - __main__ - Epoch 18
2023-03-14 23:42:14,200 - INFO - __main__ - Epoch  18, Step:   37100, Batch Loss:    17.269468, Lr: 0.000084, Tokens per sec:   3577
2023-03-14 23:42:29,438 - INFO - __main__ - Epoch  18, Step:   37200, Batch Loss:    17.082859, Lr: 0.000084, Tokens per sec:   3553
2023-03-14 23:42:44,878 - INFO - __main__ - Epoch  18, Step:   37300, Batch Loss:    29.690979, Lr: 0.000084, Tokens per sec:   3476
2023-03-14 23:43:00,054 - INFO - __main__ - Epoch  18, Step:   37400, Batch Loss:    26.624319, Lr: 0.000084, Tokens per sec:   3581
2023-03-14 23:43:15,499 - INFO - __main__ - Epoch  18, Step:   37500, Batch Loss:    26.299162, Lr: 0.000084, Tokens per sec:   3485
2023-03-14 23:43:31,409 - INFO - __main__ - Epoch  18, Step:   37600, Batch Loss:    19.547958, Lr: 0.000084, Tokens per sec:   3345
2023-03-14 23:43:47,157 - INFO - __main__ - Epoch  18, Step:   37700, Batch Loss:    29.379744, Lr: 0.000084, Tokens per sec:   3442
2023-03-14 23:44:02,429 - INFO - __main__ - Epoch  18, Step:   37800, Batch Loss:    17.124479, Lr: 0.000084, Tokens per sec:   3494
2023-03-14 23:44:17,554 - INFO - __main__ - Epoch  18, Step:   37900, Batch Loss:    28.457121, Lr: 0.000084, Tokens per sec:   3587
2023-03-14 23:44:32,872 - INFO - __main__ - Epoch  18, Step:   38000, Batch Loss:    16.543676, Lr: 0.000084, Tokens per sec:   3505
2023-03-14 23:44:48,327 - INFO - __main__ - Epoch  18, Step:   38100, Batch Loss:    13.933862, Lr: 0.000084, Tokens per sec:   3465
2023-03-14 23:45:03,830 - INFO - __main__ - Epoch  18, Step:   38200, Batch Loss:    19.545538, Lr: 0.000084, Tokens per sec:   3468
2023-03-14 23:45:18,817 - INFO - __main__ - Epoch  18, Step:   38300, Batch Loss:    23.433426, Lr: 0.000084, Tokens per sec:   3568
2023-03-14 23:45:34,108 - INFO - __main__ - Epoch  18, Step:   38400, Batch Loss:    20.767744, Lr: 0.000084, Tokens per sec:   3519
2023-03-14 23:45:49,026 - INFO - __main__ - Epoch  18, Step:   38500, Batch Loss:    22.797676, Lr: 0.000084, Tokens per sec:   3615
2023-03-14 23:46:04,491 - INFO - __main__ - Epoch  18, Step:   38600, Batch Loss:    29.844299, Lr: 0.000084, Tokens per sec:   3497
2023-03-14 23:46:19,839 - INFO - __main__ - Epoch  18, Step:   38700, Batch Loss:    25.981640, Lr: 0.000084, Tokens per sec:   3496
2023-03-14 23:46:35,174 - INFO - __main__ - Epoch  18, Step:   38800, Batch Loss:    20.496943, Lr: 0.000084, Tokens per sec:   3506
2023-03-14 23:46:50,002 - INFO - __main__ - Epoch  18, Step:   38900, Batch Loss:    18.926575, Lr: 0.000084, Tokens per sec:   3588
2023-03-14 23:47:04,740 - INFO - __main__ - Epoch  18, Step:   39000, Batch Loss:    15.430408, Lr: 0.000084, Tokens per sec:   3679
2023-03-14 23:47:20,228 - INFO - __main__ - Epoch  18, Step:   39100, Batch Loss:    19.150900, Lr: 0.000084, Tokens per sec:   3559
2023-03-14 23:47:35,949 - INFO - __main__ - Epoch  18, Step:   39200, Batch Loss:    24.758518, Lr: 0.000084, Tokens per sec:   3399
2023-03-14 23:47:39,451 - INFO - __main__ - Epoch  18: total training loss 47539.28
2023-03-14 23:47:39,451 - INFO - __main__ - Epoch 19
2023-03-14 23:47:51,974 - INFO - __main__ - Epoch  19, Step:   39300, Batch Loss:    20.782835, Lr: 0.000083, Tokens per sec:   3352
2023-03-14 23:48:07,250 - INFO - __main__ - Epoch  19, Step:   39400, Batch Loss:    22.441753, Lr: 0.000083, Tokens per sec:   3500
2023-03-14 23:48:21,939 - INFO - __main__ - Epoch  19, Step:   39500, Batch Loss:    13.926192, Lr: 0.000083, Tokens per sec:   3664
2023-03-14 23:48:37,051 - INFO - __main__ - Epoch  19, Step:   39600, Batch Loss:    24.404943, Lr: 0.000083, Tokens per sec:   3610
2023-03-14 23:48:51,982 - INFO - __main__ - Epoch  19, Step:   39700, Batch Loss:    20.380037, Lr: 0.000083, Tokens per sec:   3578
2023-03-14 23:49:07,000 - INFO - __main__ - Epoch  19, Step:   39800, Batch Loss:    20.949677, Lr: 0.000083, Tokens per sec:   3606
2023-03-14 23:49:21,595 - INFO - __main__ - Epoch  19, Step:   39900, Batch Loss:    18.882927, Lr: 0.000083, Tokens per sec:   3668
2023-03-14 23:49:36,623 - INFO - __main__ - Epoch  19, Step:   40000, Batch Loss:    19.786432, Lr: 0.000083, Tokens per sec:   3571
2023-03-14 23:49:52,125 - INFO - __main__ - Epoch  19, Step:   40100, Batch Loss:    23.203924, Lr: 0.000083, Tokens per sec:   3539
2023-03-14 23:50:07,648 - INFO - __main__ - Epoch  19, Step:   40200, Batch Loss:    19.156013, Lr: 0.000083, Tokens per sec:   3505
2023-03-14 23:50:22,833 - INFO - __main__ - Epoch  19, Step:   40300, Batch Loss:    22.308929, Lr: 0.000083, Tokens per sec:   3506
2023-03-14 23:50:37,821 - INFO - __main__ - Epoch  19, Step:   40400, Batch Loss:    26.948713, Lr: 0.000083, Tokens per sec:   3561
2023-03-14 23:50:52,567 - INFO - __main__ - Epoch  19, Step:   40500, Batch Loss:    22.025106, Lr: 0.000083, Tokens per sec:   3649
2023-03-14 23:51:07,388 - INFO - __main__ - Epoch  19, Step:   40600, Batch Loss:    22.360670, Lr: 0.000083, Tokens per sec:   3619
2023-03-14 23:51:22,400 - INFO - __main__ - Epoch  19, Step:   40700, Batch Loss:    28.814095, Lr: 0.000083, Tokens per sec:   3603
2023-03-14 23:51:37,918 - INFO - __main__ - Epoch  19, Step:   40800, Batch Loss:    23.800709, Lr: 0.000083, Tokens per sec:   3455
2023-03-14 23:51:53,550 - INFO - __main__ - Epoch  19, Step:   40900, Batch Loss:    22.846457, Lr: 0.000083, Tokens per sec:   3431
2023-03-14 23:52:09,041 - INFO - __main__ - Epoch  19, Step:   41000, Batch Loss:    21.335211, Lr: 0.000083, Tokens per sec:   3415
2023-03-14 23:52:24,482 - INFO - __main__ - Epoch  19, Step:   41100, Batch Loss:    21.180683, Lr: 0.000083, Tokens per sec:   3446
2023-03-14 23:52:40,002 - INFO - __main__ - Epoch  19, Step:   41200, Batch Loss:    14.862111, Lr: 0.000083, Tokens per sec:   3539
2023-03-14 23:52:56,076 - INFO - __main__ - Epoch  19, Step:   41300, Batch Loss:    19.764595, Lr: 0.000083, Tokens per sec:   3391
2023-03-14 23:53:11,915 - INFO - __main__ - Epoch  19, Step:   41400, Batch Loss:    24.667408, Lr: 0.000083, Tokens per sec:   3399
2023-03-14 23:53:12,148 - INFO - __main__ - Epoch  19: total training loss 45117.76
2023-03-14 23:53:12,148 - INFO - __main__ - Epoch 20
2023-03-14 23:53:27,227 - INFO - __main__ - Epoch  20, Step:   41500, Batch Loss:    16.123560, Lr: 0.000083, Tokens per sec:   3510
2023-03-14 23:53:42,671 - INFO - __main__ - Epoch  20, Step:   41600, Batch Loss:    15.527107, Lr: 0.000083, Tokens per sec:   3529
2023-03-14 23:53:58,084 - INFO - __main__ - Epoch  20, Step:   41700, Batch Loss:    18.942974, Lr: 0.000083, Tokens per sec:   3481
2023-03-14 23:54:14,109 - INFO - __main__ - Epoch  20, Step:   41800, Batch Loss:    22.747103, Lr: 0.000083, Tokens per sec:   3389
2023-03-14 23:54:29,283 - INFO - __main__ - Epoch  20, Step:   41900, Batch Loss:    15.067646, Lr: 0.000083, Tokens per sec:   3535
2023-03-14 23:54:44,259 - INFO - __main__ - Epoch  20, Step:   42000, Batch Loss:    21.097429, Lr: 0.000083, Tokens per sec:   3617
2023-03-14 23:55:00,141 - INFO - __main__ - Epoch  20, Step:   42100, Batch Loss:    21.893318, Lr: 0.000083, Tokens per sec:   3438
2023-03-14 23:55:16,142 - INFO - __main__ - Epoch  20, Step:   42200, Batch Loss:    16.698187, Lr: 0.000083, Tokens per sec:   3345
2023-03-14 23:55:31,871 - INFO - __main__ - Epoch  20, Step:   42300, Batch Loss:    22.171885, Lr: 0.000083, Tokens per sec:   3391
2023-03-14 23:55:47,857 - INFO - __main__ - Epoch  20, Step:   42400, Batch Loss:    16.487223, Lr: 0.000083, Tokens per sec:   3373
2023-03-14 23:56:03,380 - INFO - __main__ - Epoch  20, Step:   42500, Batch Loss:    24.857460, Lr: 0.000083, Tokens per sec:   3437
2023-03-14 23:56:19,047 - INFO - __main__ - Epoch  20, Step:   42600, Batch Loss:    18.080076, Lr: 0.000083, Tokens per sec:   3415
2023-03-14 23:56:34,367 - INFO - __main__ - Epoch  20, Step:   42700, Batch Loss:    17.115858, Lr: 0.000083, Tokens per sec:   3513
2023-03-14 23:56:50,007 - INFO - __main__ - Epoch  20, Step:   42800, Batch Loss:    17.892931, Lr: 0.000083, Tokens per sec:   3509
2023-03-14 23:57:05,654 - INFO - __main__ - Epoch  20, Step:   42900, Batch Loss:    20.490688, Lr: 0.000083, Tokens per sec:   3481
2023-03-14 23:57:21,235 - INFO - __main__ - Epoch  20, Step:   43000, Batch Loss:    16.708473, Lr: 0.000083, Tokens per sec:   3412
2023-03-14 23:57:36,199 - INFO - __main__ - Epoch  20, Step:   43100, Batch Loss:    22.394266, Lr: 0.000083, Tokens per sec:   3623
2023-03-14 23:57:51,494 - INFO - __main__ - Epoch  20, Step:   43200, Batch Loss:    21.515574, Lr: 0.000083, Tokens per sec:   3495
2023-03-14 23:58:06,612 - INFO - __main__ - Epoch  20, Step:   43300, Batch Loss:    21.204149, Lr: 0.000083, Tokens per sec:   3546
2023-03-14 23:58:22,093 - INFO - __main__ - Epoch  20, Step:   43400, Batch Loss:    18.389439, Lr: 0.000083, Tokens per sec:   3438
2023-03-14 23:58:37,800 - INFO - __main__ - Epoch  20, Step:   43500, Batch Loss:    29.803768, Lr: 0.000083, Tokens per sec:   3405
2023-03-14 23:58:50,464 - INFO - __main__ - Epoch  20: total training loss 42834.50
2023-03-14 23:58:50,465 - INFO - __main__ - Epoch 21
2023-03-14 23:58:54,055 - INFO - __main__ - Epoch  21, Step:   43600, Batch Loss:    14.994285, Lr: 0.000082, Tokens per sec:   2929
2023-03-14 23:59:09,666 - INFO - __main__ - Epoch  21, Step:   43700, Batch Loss:    16.871637, Lr: 0.000082, Tokens per sec:   3502
2023-03-14 23:59:24,797 - INFO - __main__ - Epoch  21, Step:   43800, Batch Loss:    14.805494, Lr: 0.000082, Tokens per sec:   3557
2023-03-14 23:59:40,178 - INFO - __main__ - Epoch  21, Step:   43900, Batch Loss:    14.132833, Lr: 0.000082, Tokens per sec:   3469
2023-03-14 23:59:55,916 - INFO - __main__ - Epoch  21, Step:   44000, Batch Loss:    18.323511, Lr: 0.000082, Tokens per sec:   3429
2023-03-15 00:00:11,552 - INFO - __main__ - Epoch  21, Step:   44100, Batch Loss:    15.074746, Lr: 0.000082, Tokens per sec:   3448
2023-03-15 00:00:26,927 - INFO - __main__ - Epoch  21, Step:   44200, Batch Loss:    14.396621, Lr: 0.000082, Tokens per sec:   3442
2023-03-15 00:00:42,570 - INFO - __main__ - Epoch  21, Step:   44300, Batch Loss:    15.448956, Lr: 0.000082, Tokens per sec:   3389
2023-03-15 00:00:58,209 - INFO - __main__ - Epoch  21, Step:   44400, Batch Loss:    17.035727, Lr: 0.000082, Tokens per sec:   3463
2023-03-15 00:01:14,005 - INFO - __main__ - Epoch  21, Step:   44500, Batch Loss:    17.172325, Lr: 0.000082, Tokens per sec:   3417
2023-03-15 00:01:30,050 - INFO - __main__ - Epoch  21, Step:   44600, Batch Loss:    22.737434, Lr: 0.000082, Tokens per sec:   3363
2023-03-15 00:01:46,098 - INFO - __main__ - Epoch  21, Step:   44700, Batch Loss:    17.459379, Lr: 0.000082, Tokens per sec:   3389
2023-03-15 00:02:02,168 - INFO - __main__ - Epoch  21, Step:   44800, Batch Loss:    26.799351, Lr: 0.000082, Tokens per sec:   3306
2023-03-15 00:02:18,024 - INFO - __main__ - Epoch  21, Step:   44900, Batch Loss:    23.186960, Lr: 0.000082, Tokens per sec:   3438
2023-03-15 00:02:33,502 - INFO - __main__ - Epoch  21, Step:   45000, Batch Loss:    22.368700, Lr: 0.000082, Tokens per sec:   3500
2023-03-15 00:02:48,962 - INFO - __main__ - Epoch  21, Step:   45100, Batch Loss:    16.153364, Lr: 0.000082, Tokens per sec:   3486
2023-03-15 00:03:04,167 - INFO - __main__ - Epoch  21, Step:   45200, Batch Loss:    20.587418, Lr: 0.000082, Tokens per sec:   3591
2023-03-15 00:03:19,843 - INFO - __main__ - Epoch  21, Step:   45300, Batch Loss:    19.226282, Lr: 0.000082, Tokens per sec:   3443
2023-03-15 00:03:35,143 - INFO - __main__ - Epoch  21, Step:   45400, Batch Loss:    22.753931, Lr: 0.000082, Tokens per sec:   3462
2023-03-15 00:03:50,420 - INFO - __main__ - Epoch  21, Step:   45500, Batch Loss:    20.722912, Lr: 0.000082, Tokens per sec:   3520
2023-03-15 00:04:05,837 - INFO - __main__ - Epoch  21, Step:   45600, Batch Loss:    17.349979, Lr: 0.000082, Tokens per sec:   3476
2023-03-15 00:04:21,466 - INFO - __main__ - Epoch  21, Step:   45700, Batch Loss:    22.399363, Lr: 0.000082, Tokens per sec:   3451
2023-03-15 00:04:30,640 - INFO - __main__ - Epoch  21: total training loss 40730.71
2023-03-15 00:04:30,640 - INFO - __main__ - Epoch 22
2023-03-15 00:04:37,321 - INFO - __main__ - Epoch  22, Step:   45800, Batch Loss:    15.431908, Lr: 0.000081, Tokens per sec:   3338
2023-03-15 00:04:52,887 - INFO - __main__ - Epoch  22, Step:   45900, Batch Loss:    15.107018, Lr: 0.000081, Tokens per sec:   3436
2023-03-15 00:05:09,086 - INFO - __main__ - Epoch  22, Step:   46000, Batch Loss:    19.039385, Lr: 0.000081, Tokens per sec:   3328
2023-03-15 00:05:25,028 - INFO - __main__ - Epoch  22, Step:   46100, Batch Loss:    15.856730, Lr: 0.000081, Tokens per sec:   3377
2023-03-15 00:05:40,832 - INFO - __main__ - Epoch  22, Step:   46200, Batch Loss:    15.098236, Lr: 0.000081, Tokens per sec:   3474
2023-03-15 00:05:56,534 - INFO - __main__ - Epoch  22, Step:   46300, Batch Loss:    20.930767, Lr: 0.000081, Tokens per sec:   3480
2023-03-15 00:06:12,567 - INFO - __main__ - Epoch  22, Step:   46400, Batch Loss:    18.403175, Lr: 0.000081, Tokens per sec:   3362
2023-03-15 00:06:28,587 - INFO - __main__ - Epoch  22, Step:   46500, Batch Loss:    20.925156, Lr: 0.000081, Tokens per sec:   3349
2023-03-15 00:06:43,873 - INFO - __main__ - Epoch  22, Step:   46600, Batch Loss:    20.601313, Lr: 0.000081, Tokens per sec:   3466
2023-03-15 00:06:59,421 - INFO - __main__ - Epoch  22, Step:   46700, Batch Loss:    21.355799, Lr: 0.000081, Tokens per sec:   3457
2023-03-15 00:07:14,731 - INFO - __main__ - Epoch  22, Step:   46800, Batch Loss:    17.990252, Lr: 0.000081, Tokens per sec:   3459
2023-03-15 00:07:30,427 - INFO - __main__ - Epoch  22, Step:   46900, Batch Loss:    16.618637, Lr: 0.000081, Tokens per sec:   3425
2023-03-15 00:07:46,084 - INFO - __main__ - Epoch  22, Step:   47000, Batch Loss:    23.912825, Lr: 0.000081, Tokens per sec:   3459
2023-03-15 00:08:01,989 - INFO - __main__ - Epoch  22, Step:   47100, Batch Loss:    17.495060, Lr: 0.000081, Tokens per sec:   3357
2023-03-15 00:08:17,483 - INFO - __main__ - Epoch  22, Step:   47200, Batch Loss:    12.409691, Lr: 0.000081, Tokens per sec:   3473
2023-03-15 00:08:32,804 - INFO - __main__ - Epoch  22, Step:   47300, Batch Loss:    11.583177, Lr: 0.000081, Tokens per sec:   3487
2023-03-15 00:08:48,521 - INFO - __main__ - Epoch  22, Step:   47400, Batch Loss:    16.537909, Lr: 0.000081, Tokens per sec:   3392
2023-03-15 00:09:04,537 - INFO - __main__ - Epoch  22, Step:   47500, Batch Loss:    24.216433, Lr: 0.000081, Tokens per sec:   3392
2023-03-15 00:09:20,164 - INFO - __main__ - Epoch  22, Step:   47600, Batch Loss:    18.524035, Lr: 0.000081, Tokens per sec:   3463
2023-03-15 00:09:35,861 - INFO - __main__ - Epoch  22, Step:   47700, Batch Loss:    22.385977, Lr: 0.000081, Tokens per sec:   3461
2023-03-15 00:09:51,305 - INFO - __main__ - Epoch  22, Step:   47800, Batch Loss:    20.350512, Lr: 0.000081, Tokens per sec:   3484
2023-03-15 00:10:06,822 - INFO - __main__ - Epoch  22, Step:   47900, Batch Loss:    15.237045, Lr: 0.000081, Tokens per sec:   3459
2023-03-15 00:10:12,587 - INFO - __main__ - Epoch  22: total training loss 38765.91
2023-03-15 00:10:12,588 - INFO - __main__ - Epoch 23
2023-03-15 00:10:22,717 - INFO - __main__ - Epoch  23, Step:   48000, Batch Loss:    18.079630, Lr: 0.000080, Tokens per sec:   3292
2023-03-15 00:10:38,129 - INFO - __main__ - Epoch  23, Step:   48100, Batch Loss:    15.899974, Lr: 0.000080, Tokens per sec:   3499
2023-03-15 00:10:53,866 - INFO - __main__ - Epoch  23, Step:   48200, Batch Loss:    14.614705, Lr: 0.000080, Tokens per sec:   3473
2023-03-15 00:11:09,494 - INFO - __main__ - Epoch  23, Step:   48300, Batch Loss:    13.877649, Lr: 0.000080, Tokens per sec:   3418
2023-03-15 00:11:25,095 - INFO - __main__ - Epoch  23, Step:   48400, Batch Loss:    14.841963, Lr: 0.000080, Tokens per sec:   3500
2023-03-15 00:11:40,708 - INFO - __main__ - Epoch  23, Step:   48500, Batch Loss:    19.243134, Lr: 0.000080, Tokens per sec:   3466
2023-03-15 00:11:56,521 - INFO - __main__ - Epoch  23, Step:   48600, Batch Loss:    17.915211, Lr: 0.000080, Tokens per sec:   3421
2023-03-15 00:12:12,360 - INFO - __main__ - Epoch  23, Step:   48700, Batch Loss:    18.469511, Lr: 0.000080, Tokens per sec:   3407
2023-03-15 00:12:27,955 - INFO - __main__ - Epoch  23, Step:   48800, Batch Loss:    13.444425, Lr: 0.000080, Tokens per sec:   3433
2023-03-15 00:12:43,698 - INFO - __main__ - Epoch  23, Step:   48900, Batch Loss:    13.926014, Lr: 0.000080, Tokens per sec:   3427
2023-03-15 00:12:59,590 - INFO - __main__ - Epoch  23, Step:   49000, Batch Loss:    16.770042, Lr: 0.000080, Tokens per sec:   3393
2023-03-15 00:13:15,119 - INFO - __main__ - Epoch  23, Step:   49100, Batch Loss:    18.273703, Lr: 0.000080, Tokens per sec:   3450
2023-03-15 00:13:30,866 - INFO - __main__ - Epoch  23, Step:   49200, Batch Loss:    10.335452, Lr: 0.000080, Tokens per sec:   3425
2023-03-15 00:13:46,878 - INFO - __main__ - Epoch  23, Step:   49300, Batch Loss:    20.867855, Lr: 0.000080, Tokens per sec:   3365
2023-03-15 00:14:02,588 - INFO - __main__ - Epoch  23, Step:   49400, Batch Loss:    18.625792, Lr: 0.000080, Tokens per sec:   3475
2023-03-15 00:14:18,174 - INFO - __main__ - Epoch  23, Step:   49500, Batch Loss:    19.326534, Lr: 0.000080, Tokens per sec:   3452
2023-03-15 00:14:34,038 - INFO - __main__ - Epoch  23, Step:   49600, Batch Loss:    11.805593, Lr: 0.000080, Tokens per sec:   3418
2023-03-15 00:14:49,669 - INFO - __main__ - Epoch  23, Step:   49700, Batch Loss:    19.095469, Lr: 0.000080, Tokens per sec:   3410
2023-03-15 00:15:05,179 - INFO - __main__ - Epoch  23, Step:   49800, Batch Loss:    17.429754, Lr: 0.000080, Tokens per sec:   3490
2023-03-15 00:15:21,006 - INFO - __main__ - Epoch  23, Step:   49900, Batch Loss:    16.731833, Lr: 0.000080, Tokens per sec:   3360
2023-03-15 00:15:36,687 - INFO - __main__ - Epoch  23, Step:   50000, Batch Loss:    14.916372, Lr: 0.000080, Tokens per sec:   3372
2023-03-15 00:15:52,226 - INFO - __main__ - Epoch  23, Step:   50100, Batch Loss:    15.995738, Lr: 0.000080, Tokens per sec:   3405
2023-03-15 00:15:54,977 - INFO - __main__ - Epoch  23: total training loss 36896.44
2023-03-15 00:15:54,978 - INFO - __main__ - Epoch 24
2023-03-15 00:16:08,082 - INFO - __main__ - Epoch  24, Step:   50200, Batch Loss:    14.401036, Lr: 0.000079, Tokens per sec:   3441
2023-03-15 00:16:23,516 - INFO - __main__ - Epoch  24, Step:   50300, Batch Loss:    18.456785, Lr: 0.000079, Tokens per sec:   3475
2023-03-15 00:16:39,091 - INFO - __main__ - Epoch  24, Step:   50400, Batch Loss:    19.715076, Lr: 0.000079, Tokens per sec:   3498
2023-03-15 00:16:54,283 - INFO - __main__ - Epoch  24, Step:   50500, Batch Loss:    11.628638, Lr: 0.000079, Tokens per sec:   3514
2023-03-15 00:17:09,386 - INFO - __main__ - Epoch  24, Step:   50600, Batch Loss:    15.516654, Lr: 0.000079, Tokens per sec:   3585
2023-03-15 00:17:24,377 - INFO - __main__ - Epoch  24, Step:   50700, Batch Loss:    14.311262, Lr: 0.000079, Tokens per sec:   3576
2023-03-15 00:17:39,638 - INFO - __main__ - Epoch  24, Step:   50800, Batch Loss:    14.600682, Lr: 0.000079, Tokens per sec:   3558
2023-03-15 00:17:55,245 - INFO - __main__ - Epoch  24, Step:   50900, Batch Loss:    15.701599, Lr: 0.000079, Tokens per sec:   3384
2023-03-15 00:18:10,705 - INFO - __main__ - Epoch  24, Step:   51000, Batch Loss:    14.892893, Lr: 0.000079, Tokens per sec:   3439
2023-03-15 00:18:26,232 - INFO - __main__ - Epoch  24, Step:   51100, Batch Loss:    17.858656, Lr: 0.000079, Tokens per sec:   3476
2023-03-15 00:18:41,702 - INFO - __main__ - Epoch  24, Step:   51200, Batch Loss:    15.286487, Lr: 0.000079, Tokens per sec:   3529
2023-03-15 00:18:57,366 - INFO - __main__ - Epoch  24, Step:   51300, Batch Loss:    10.842522, Lr: 0.000079, Tokens per sec:   3457
2023-03-15 00:19:12,837 - INFO - __main__ - Epoch  24, Step:   51400, Batch Loss:    10.420119, Lr: 0.000079, Tokens per sec:   3500
2023-03-15 00:19:28,555 - INFO - __main__ - Epoch  24, Step:   51500, Batch Loss:    12.915625, Lr: 0.000079, Tokens per sec:   3440
2023-03-15 00:19:44,395 - INFO - __main__ - Epoch  24, Step:   51600, Batch Loss:    21.261831, Lr: 0.000079, Tokens per sec:   3398
2023-03-15 00:20:00,120 - INFO - __main__ - Epoch  24, Step:   51700, Batch Loss:    17.258753, Lr: 0.000079, Tokens per sec:   3378
2023-03-15 00:20:15,873 - INFO - __main__ - Epoch  24, Step:   51800, Batch Loss:    17.290270, Lr: 0.000079, Tokens per sec:   3373
2023-03-15 00:20:31,551 - INFO - __main__ - Epoch  24, Step:   51900, Batch Loss:    16.067354, Lr: 0.000079, Tokens per sec:   3491
2023-03-15 00:20:47,109 - INFO - __main__ - Epoch  24, Step:   52000, Batch Loss:    18.077618, Lr: 0.000079, Tokens per sec:   3445
2023-03-15 00:21:02,551 - INFO - __main__ - Epoch  24, Step:   52100, Batch Loss:    10.583036, Lr: 0.000079, Tokens per sec:   3489
2023-03-15 00:21:17,839 - INFO - __main__ - Epoch  24, Step:   52200, Batch Loss:    13.207905, Lr: 0.000079, Tokens per sec:   3516
2023-03-15 00:21:33,144 - INFO - __main__ - Epoch  24: total training loss 35174.48
2023-03-15 00:21:33,145 - INFO - __main__ - Epoch 25
2023-03-15 00:21:34,129 - INFO - __main__ - Epoch  25, Step:   52300, Batch Loss:    12.299094, Lr: 0.000079, Tokens per sec:   2203
2023-03-15 00:21:49,949 - INFO - __main__ - Epoch  25, Step:   52400, Batch Loss:    15.548245, Lr: 0.000079, Tokens per sec:   3405
2023-03-15 00:22:05,651 - INFO - __main__ - Epoch  25, Step:   52500, Batch Loss:    13.096230, Lr: 0.000079, Tokens per sec:   3429
2023-03-15 00:22:21,756 - INFO - __main__ - Epoch  25, Step:   52600, Batch Loss:    14.607772, Lr: 0.000079, Tokens per sec:   3282
2023-03-15 00:22:38,123 - INFO - __main__ - Epoch  25, Step:   52700, Batch Loss:    18.415670, Lr: 0.000079, Tokens per sec:   3286
2023-03-15 00:22:54,170 - INFO - __main__ - Epoch  25, Step:   52800, Batch Loss:    17.768381, Lr: 0.000079, Tokens per sec:   3345
2023-03-15 00:23:10,332 - INFO - __main__ - Epoch  25, Step:   52900, Batch Loss:    21.009504, Lr: 0.000079, Tokens per sec:   3312
2023-03-15 00:23:26,517 - INFO - __main__ - Epoch  25, Step:   53000, Batch Loss:    14.580803, Lr: 0.000079, Tokens per sec:   3270
2023-03-15 00:23:42,429 - INFO - __main__ - Epoch  25, Step:   53100, Batch Loss:    24.755623, Lr: 0.000079, Tokens per sec:   3454
2023-03-15 00:23:58,435 - INFO - __main__ - Epoch  25, Step:   53200, Batch Loss:    14.507093, Lr: 0.000079, Tokens per sec:   3351
2023-03-15 00:24:14,252 - INFO - __main__ - Epoch  25, Step:   53300, Batch Loss:    13.169831, Lr: 0.000079, Tokens per sec:   3373
2023-03-15 00:24:30,169 - INFO - __main__ - Epoch  25, Step:   53400, Batch Loss:    21.351191, Lr: 0.000079, Tokens per sec:   3437
2023-03-15 00:24:46,061 - INFO - __main__ - Epoch  25, Step:   53500, Batch Loss:    18.155827, Lr: 0.000079, Tokens per sec:   3294
2023-03-15 00:25:01,670 - INFO - __main__ - Epoch  25, Step:   53600, Batch Loss:     9.139035, Lr: 0.000079, Tokens per sec:   3378
2023-03-15 00:25:17,568 - INFO - __main__ - Epoch  25, Step:   53700, Batch Loss:    14.289039, Lr: 0.000079, Tokens per sec:   3426
2023-03-15 00:25:33,645 - INFO - __main__ - Epoch  25, Step:   53800, Batch Loss:    20.593540, Lr: 0.000079, Tokens per sec:   3326
2023-03-15 00:25:49,514 - INFO - __main__ - Epoch  25, Step:   53900, Batch Loss:    13.933052, Lr: 0.000079, Tokens per sec:   3441
2023-03-15 00:26:05,188 - INFO - __main__ - Epoch  25, Step:   54000, Batch Loss:    18.436071, Lr: 0.000079, Tokens per sec:   3519
2023-03-15 00:26:20,794 - INFO - __main__ - Epoch  25, Step:   54100, Batch Loss:    15.813603, Lr: 0.000079, Tokens per sec:   3459
2023-03-15 00:26:36,830 - INFO - __main__ - Epoch  25, Step:   54200, Batch Loss:    15.546587, Lr: 0.000079, Tokens per sec:   3398
2023-03-15 00:26:52,898 - INFO - __main__ - Epoch  25, Step:   54300, Batch Loss:    27.595078, Lr: 0.000079, Tokens per sec:   3359
2023-03-15 00:27:08,597 - INFO - __main__ - Epoch  25, Step:   54400, Batch Loss:    18.491901, Lr: 0.000079, Tokens per sec:   3446
2023-03-15 00:27:20,591 - INFO - __main__ - Epoch  25: total training loss 33602.28
2023-03-15 00:27:20,592 - INFO - __main__ - Epoch 26
2023-03-15 00:27:24,880 - INFO - __main__ - Epoch  26, Step:   54500, Batch Loss:    11.773322, Lr: 0.000078, Tokens per sec:   3201
2023-03-15 00:27:40,785 - INFO - __main__ - Epoch  26, Step:   54600, Batch Loss:     8.774450, Lr: 0.000078, Tokens per sec:   3324
2023-03-15 00:27:56,693 - INFO - __main__ - Epoch  26, Step:   54700, Batch Loss:    12.103868, Lr: 0.000078, Tokens per sec:   3390
2023-03-15 00:28:12,775 - INFO - __main__ - Epoch  26, Step:   54800, Batch Loss:    12.634390, Lr: 0.000078, Tokens per sec:   3398
2023-03-15 00:28:28,655 - INFO - __main__ - Epoch  26, Step:   54900, Batch Loss:    16.262249, Lr: 0.000078, Tokens per sec:   3384
2023-03-15 00:28:44,499 - INFO - __main__ - Epoch  26, Step:   55000, Batch Loss:    15.088014, Lr: 0.000078, Tokens per sec:   3369
2023-03-15 00:29:00,565 - INFO - __main__ - Epoch  26, Step:   55100, Batch Loss:    10.023481, Lr: 0.000078, Tokens per sec:   3320
2023-03-15 00:29:16,322 - INFO - __main__ - Epoch  26, Step:   55200, Batch Loss:    15.984341, Lr: 0.000078, Tokens per sec:   3405
2023-03-15 00:29:32,466 - INFO - __main__ - Epoch  26, Step:   55300, Batch Loss:    15.647942, Lr: 0.000078, Tokens per sec:   3337
2023-03-15 00:29:47,716 - INFO - __main__ - Epoch  26, Step:   55400, Batch Loss:    13.959094, Lr: 0.000078, Tokens per sec:   3501
2023-03-15 00:30:03,772 - INFO - __main__ - Epoch  26, Step:   55500, Batch Loss:    13.534179, Lr: 0.000078, Tokens per sec:   3390
2023-03-15 00:30:19,804 - INFO - __main__ - Epoch  26, Step:   55600, Batch Loss:    11.833220, Lr: 0.000078, Tokens per sec:   3351
2023-03-15 00:30:35,429 - INFO - __main__ - Epoch  26, Step:   55700, Batch Loss:    10.303131, Lr: 0.000078, Tokens per sec:   3456
2023-03-15 00:30:50,888 - INFO - __main__ - Epoch  26, Step:   55800, Batch Loss:    12.869442, Lr: 0.000078, Tokens per sec:   3473
2023-03-15 00:31:06,489 - INFO - __main__ - Epoch  26, Step:   55900, Batch Loss:    16.593260, Lr: 0.000078, Tokens per sec:   3438
2023-03-15 00:31:22,021 - INFO - __main__ - Epoch  26, Step:   56000, Batch Loss:    20.777142, Lr: 0.000078, Tokens per sec:   3434
2023-03-15 00:31:37,638 - INFO - __main__ - Epoch  26, Step:   56100, Batch Loss:    12.893807, Lr: 0.000078, Tokens per sec:   3489
2023-03-15 00:31:53,103 - INFO - __main__ - Epoch  26, Step:   56200, Batch Loss:    15.425251, Lr: 0.000078, Tokens per sec:   3522
2023-03-15 00:32:08,867 - INFO - __main__ - Epoch  26, Step:   56300, Batch Loss:    14.579617, Lr: 0.000078, Tokens per sec:   3431
2023-03-15 00:32:24,465 - INFO - __main__ - Epoch  26, Step:   56400, Batch Loss:    20.774925, Lr: 0.000078, Tokens per sec:   3464
2023-03-15 00:32:40,100 - INFO - __main__ - Epoch  26, Step:   56500, Batch Loss:    13.165215, Lr: 0.000078, Tokens per sec:   3430
2023-03-15 00:32:55,993 - INFO - __main__ - Epoch  26, Step:   56600, Batch Loss:    14.340528, Lr: 0.000078, Tokens per sec:   3398
2023-03-15 00:33:04,681 - INFO - __main__ - Epoch  26: total training loss 32043.35
2023-03-15 00:33:04,682 - INFO - __main__ - Epoch 27
2023-03-15 00:33:12,414 - INFO - __main__ - Epoch  27, Step:   56700, Batch Loss:    13.755991, Lr: 0.000077, Tokens per sec:   3246
2023-03-15 00:33:28,290 - INFO - __main__ - Epoch  27, Step:   56800, Batch Loss:    13.634245, Lr: 0.000077, Tokens per sec:   3404
2023-03-15 00:33:44,216 - INFO - __main__ - Epoch  27, Step:   56900, Batch Loss:    12.400329, Lr: 0.000077, Tokens per sec:   3345
2023-03-15 00:34:00,109 - INFO - __main__ - Epoch  27, Step:   57000, Batch Loss:    16.184195, Lr: 0.000077, Tokens per sec:   3380
2023-03-15 00:34:16,076 - INFO - __main__ - Epoch  27, Step:   57100, Batch Loss:    10.226802, Lr: 0.000077, Tokens per sec:   3398
2023-03-15 00:34:31,966 - INFO - __main__ - Epoch  27, Step:   57200, Batch Loss:    17.378441, Lr: 0.000077, Tokens per sec:   3358
2023-03-15 00:34:47,589 - INFO - __main__ - Epoch  27, Step:   57300, Batch Loss:    20.202620, Lr: 0.000077, Tokens per sec:   3473
2023-03-15 00:35:03,225 - INFO - __main__ - Epoch  27, Step:   57400, Batch Loss:    18.186975, Lr: 0.000077, Tokens per sec:   3533
2023-03-15 00:35:18,565 - INFO - __main__ - Epoch  27, Step:   57500, Batch Loss:    13.861684, Lr: 0.000077, Tokens per sec:   3469
2023-03-15 00:35:34,202 - INFO - __main__ - Epoch  27, Step:   57600, Batch Loss:     8.691077, Lr: 0.000077, Tokens per sec:   3471
2023-03-15 00:35:49,983 - INFO - __main__ - Epoch  27, Step:   57700, Batch Loss:    14.014318, Lr: 0.000077, Tokens per sec:   3497
2023-03-15 00:36:05,437 - INFO - __main__ - Epoch  27, Step:   57800, Batch Loss:    11.011086, Lr: 0.000077, Tokens per sec:   3464
2023-03-15 00:36:21,151 - INFO - __main__ - Epoch  27, Step:   57900, Batch Loss:     9.771556, Lr: 0.000077, Tokens per sec:   3423
2023-03-15 00:36:36,903 - INFO - __main__ - Epoch  27, Step:   58000, Batch Loss:    12.183432, Lr: 0.000077, Tokens per sec:   3416
2023-03-15 00:36:52,332 - INFO - __main__ - Epoch  27, Step:   58100, Batch Loss:    15.153240, Lr: 0.000077, Tokens per sec:   3488
2023-03-15 00:37:07,928 - INFO - __main__ - Epoch  27, Step:   58200, Batch Loss:    13.905705, Lr: 0.000077, Tokens per sec:   3435
2023-03-15 00:37:23,621 - INFO - __main__ - Epoch  27, Step:   58300, Batch Loss:    16.504543, Lr: 0.000077, Tokens per sec:   3469
2023-03-15 00:37:40,039 - INFO - __main__ - Epoch  27, Step:   58400, Batch Loss:    12.644974, Lr: 0.000077, Tokens per sec:   3260
2023-03-15 00:37:56,054 - INFO - __main__ - Epoch  27, Step:   58500, Batch Loss:    12.768614, Lr: 0.000077, Tokens per sec:   3292
2023-03-15 00:38:11,975 - INFO - __main__ - Epoch  27, Step:   58600, Batch Loss:    11.797053, Lr: 0.000077, Tokens per sec:   3345
2023-03-15 00:38:28,105 - INFO - __main__ - Epoch  27, Step:   58700, Batch Loss:    16.346981, Lr: 0.000077, Tokens per sec:   3301
2023-03-15 00:38:43,907 - INFO - __main__ - Epoch  27, Step:   58800, Batch Loss:    13.847445, Lr: 0.000077, Tokens per sec:   3394
2023-03-15 00:38:49,194 - INFO - __main__ - Epoch  27: total training loss 30530.16
2023-03-15 00:38:49,195 - INFO - __main__ - Epoch 28
2023-03-15 00:39:00,028 - INFO - __main__ - Epoch  28, Step:   58900, Batch Loss:    15.567917, Lr: 0.000076, Tokens per sec:   3302
2023-03-15 00:39:15,160 - INFO - __main__ - Epoch  28, Step:   59000, Batch Loss:     9.779105, Lr: 0.000076, Tokens per sec:   3584
2023-03-15 00:39:30,494 - INFO - __main__ - Epoch  28, Step:   59100, Batch Loss:    16.418230, Lr: 0.000076, Tokens per sec:   3511
2023-03-15 00:39:45,674 - INFO - __main__ - Epoch  28, Step:   59200, Batch Loss:    12.558282, Lr: 0.000076, Tokens per sec:   3591
2023-03-15 00:40:01,125 - INFO - __main__ - Epoch  28, Step:   59300, Batch Loss:    12.174083, Lr: 0.000076, Tokens per sec:   3482
2023-03-15 00:40:16,687 - INFO - __main__ - Epoch  28, Step:   59400, Batch Loss:    13.116522, Lr: 0.000076, Tokens per sec:   3456
2023-03-15 00:40:32,211 - INFO - __main__ - Epoch  28, Step:   59500, Batch Loss:     9.464384, Lr: 0.000076, Tokens per sec:   3480
2023-03-15 00:40:47,693 - INFO - __main__ - Epoch  28, Step:   59600, Batch Loss:    14.303409, Lr: 0.000076, Tokens per sec:   3522
2023-03-15 00:41:03,124 - INFO - __main__ - Epoch  28, Step:   59700, Batch Loss:    15.075345, Lr: 0.000076, Tokens per sec:   3476
2023-03-15 00:41:18,815 - INFO - __main__ - Epoch  28, Step:   59800, Batch Loss:    11.396735, Lr: 0.000076, Tokens per sec:   3432
2023-03-15 00:41:34,253 - INFO - __main__ - Epoch  28, Step:   59900, Batch Loss:    16.462973, Lr: 0.000076, Tokens per sec:   3402
2023-03-15 00:41:49,951 - INFO - __main__ - Epoch  28, Step:   60000, Batch Loss:    13.260148, Lr: 0.000076, Tokens per sec:   3446
2023-03-15 00:42:05,885 - INFO - __main__ - Epoch  28, Step:   60100, Batch Loss:    15.573143, Lr: 0.000076, Tokens per sec:   3437
2023-03-15 00:42:21,920 - INFO - __main__ - Epoch  28, Step:   60200, Batch Loss:    15.749235, Lr: 0.000076, Tokens per sec:   3372
2023-03-15 00:42:37,889 - INFO - __main__ - Epoch  28, Step:   60300, Batch Loss:    12.957767, Lr: 0.000076, Tokens per sec:   3276
2023-03-15 00:42:54,378 - INFO - __main__ - Epoch  28, Step:   60400, Batch Loss:    14.262551, Lr: 0.000076, Tokens per sec:   3282
2023-03-15 00:43:10,447 - INFO - __main__ - Epoch  28, Step:   60500, Batch Loss:    14.825103, Lr: 0.000076, Tokens per sec:   3333
2023-03-15 00:43:26,636 - INFO - __main__ - Epoch  28, Step:   60600, Batch Loss:    14.446056, Lr: 0.000076, Tokens per sec:   3344
2023-03-15 00:43:42,195 - INFO - __main__ - Epoch  28, Step:   60700, Batch Loss:    16.484341, Lr: 0.000076, Tokens per sec:   3470
2023-03-15 00:43:57,799 - INFO - __main__ - Epoch  28, Step:   60800, Batch Loss:    15.399639, Lr: 0.000076, Tokens per sec:   3496
2023-03-15 00:44:14,069 - INFO - __main__ - Epoch  28, Step:   60900, Batch Loss:    13.901602, Lr: 0.000076, Tokens per sec:   3285
2023-03-15 00:44:29,854 - INFO - __main__ - Epoch  28, Step:   61000, Batch Loss:    15.024849, Lr: 0.000076, Tokens per sec:   3383
2023-03-15 00:44:31,865 - INFO - __main__ - Epoch  28: total training loss 29250.99
2023-03-15 00:44:31,865 - INFO - __main__ - Epoch 29
2023-03-15 00:44:46,572 - INFO - __main__ - Epoch  29, Step:   61100, Batch Loss:     8.680539, Lr: 0.000075, Tokens per sec:   3215
2023-03-15 00:45:02,299 - INFO - __main__ - Epoch  29, Step:   61200, Batch Loss:    12.513981, Lr: 0.000075, Tokens per sec:   3451
2023-03-15 00:45:18,280 - INFO - __main__ - Epoch  29, Step:   61300, Batch Loss:    12.192432, Lr: 0.000075, Tokens per sec:   3283
2023-03-15 00:45:34,761 - INFO - __main__ - Epoch  29, Step:   61400, Batch Loss:    15.461422, Lr: 0.000075, Tokens per sec:   3257
2023-03-15 00:45:51,123 - INFO - __main__ - Epoch  29, Step:   61500, Batch Loss:    10.692008, Lr: 0.000075, Tokens per sec:   3345
2023-03-15 00:46:07,443 - INFO - __main__ - Epoch  29, Step:   61600, Batch Loss:    17.018694, Lr: 0.000075, Tokens per sec:   3348
2023-03-15 00:46:23,284 - INFO - __main__ - Epoch  29, Step:   61700, Batch Loss:    13.506486, Lr: 0.000075, Tokens per sec:   3430
2023-03-15 00:46:39,211 - INFO - __main__ - Epoch  29, Step:   61800, Batch Loss:    12.787556, Lr: 0.000075, Tokens per sec:   3421
2023-03-15 00:46:55,268 - INFO - __main__ - Epoch  29, Step:   61900, Batch Loss:    18.316427, Lr: 0.000075, Tokens per sec:   3405
2023-03-15 00:47:11,399 - INFO - __main__ - Epoch  29, Step:   62000, Batch Loss:    12.584098, Lr: 0.000075, Tokens per sec:   3269
2023-03-15 00:47:27,199 - INFO - __main__ - Epoch  29, Step:   62100, Batch Loss:    12.008263, Lr: 0.000075, Tokens per sec:   3360
2023-03-15 00:47:42,461 - INFO - __main__ - Epoch  29, Step:   62200, Batch Loss:    11.370970, Lr: 0.000075, Tokens per sec:   3560
2023-03-15 00:47:58,253 - INFO - __main__ - Epoch  29, Step:   62300, Batch Loss:     9.634297, Lr: 0.000075, Tokens per sec:   3390
2023-03-15 00:48:13,798 - INFO - __main__ - Epoch  29, Step:   62400, Batch Loss:    13.998596, Lr: 0.000075, Tokens per sec:   3487
2023-03-15 00:48:29,363 - INFO - __main__ - Epoch  29, Step:   62500, Batch Loss:    14.077606, Lr: 0.000075, Tokens per sec:   3451
2023-03-15 00:48:45,467 - INFO - __main__ - Epoch  29, Step:   62600, Batch Loss:    15.538559, Lr: 0.000075, Tokens per sec:   3322
2023-03-15 00:49:00,900 - INFO - __main__ - Epoch  29, Step:   62700, Batch Loss:    14.951833, Lr: 0.000075, Tokens per sec:   3500
2023-03-15 00:49:16,588 - INFO - __main__ - Epoch  29, Step:   62800, Batch Loss:    15.718412, Lr: 0.000075, Tokens per sec:   3384
2023-03-15 00:49:32,564 - INFO - __main__ - Epoch  29, Step:   62900, Batch Loss:    15.792268, Lr: 0.000075, Tokens per sec:   3352
2023-03-15 00:49:48,399 - INFO - __main__ - Epoch  29, Step:   63000, Batch Loss:    14.694331, Lr: 0.000075, Tokens per sec:   3370
2023-03-15 00:50:04,511 - INFO - __main__ - Epoch  29, Step:   63100, Batch Loss:    13.749994, Lr: 0.000075, Tokens per sec:   3308
2023-03-15 00:50:18,892 - INFO - __main__ - Epoch  29: total training loss 28000.60
2023-03-15 00:50:18,893 - INFO - __main__ - Epoch 30
2023-03-15 00:50:20,589 - INFO - __main__ - Epoch  30, Step:   63200, Batch Loss:     8.993258, Lr: 0.000075, Tokens per sec:   2631
2023-03-15 00:50:36,412 - INFO - __main__ - Epoch  30, Step:   63300, Batch Loss:    10.354401, Lr: 0.000075, Tokens per sec:   3464
2023-03-15 00:50:51,836 - INFO - __main__ - Epoch  30, Step:   63400, Batch Loss:     9.172108, Lr: 0.000075, Tokens per sec:   3472
2023-03-15 00:51:07,231 - INFO - __main__ - Epoch  30, Step:   63500, Batch Loss:    15.689680, Lr: 0.000075, Tokens per sec:   3493
2023-03-15 00:51:22,794 - INFO - __main__ - Epoch  30, Step:   63600, Batch Loss:    12.426242, Lr: 0.000075, Tokens per sec:   3459
2023-03-15 00:51:38,304 - INFO - __main__ - Epoch  30, Step:   63700, Batch Loss:    14.031378, Lr: 0.000075, Tokens per sec:   3512
2023-03-15 00:51:53,748 - INFO - __main__ - Epoch  30, Step:   63800, Batch Loss:    14.977077, Lr: 0.000075, Tokens per sec:   3533
2023-03-15 00:52:09,339 - INFO - __main__ - Epoch  30, Step:   63900, Batch Loss:     9.760015, Lr: 0.000075, Tokens per sec:   3455
2023-03-15 00:52:24,977 - INFO - __main__ - Epoch  30, Step:   64000, Batch Loss:     9.557878, Lr: 0.000075, Tokens per sec:   3442
2023-03-15 00:52:40,241 - INFO - __main__ - Epoch  30, Step:   64100, Batch Loss:    10.071818, Lr: 0.000075, Tokens per sec:   3554
2023-03-15 00:52:55,710 - INFO - __main__ - Epoch  30, Step:   64200, Batch Loss:    14.387263, Lr: 0.000075, Tokens per sec:   3468
2023-03-15 00:53:11,721 - INFO - __main__ - Epoch  30, Step:   64300, Batch Loss:    15.344412, Lr: 0.000075, Tokens per sec:   3309
2023-03-15 00:53:27,692 - INFO - __main__ - Epoch  30, Step:   64400, Batch Loss:    13.820711, Lr: 0.000075, Tokens per sec:   3354
2023-03-15 00:53:43,269 - INFO - __main__ - Epoch  30, Step:   64500, Batch Loss:    17.745220, Lr: 0.000075, Tokens per sec:   3471
2023-03-15 00:53:59,201 - INFO - __main__ - Epoch  30, Step:   64600, Batch Loss:    13.172898, Lr: 0.000075, Tokens per sec:   3373
2023-03-15 00:54:14,980 - INFO - __main__ - Epoch  30, Step:   64700, Batch Loss:    11.568647, Lr: 0.000075, Tokens per sec:   3326
2023-03-15 00:54:30,621 - INFO - __main__ - Epoch  30, Step:   64800, Batch Loss:     7.155735, Lr: 0.000075, Tokens per sec:   3434
2023-03-15 00:54:46,149 - INFO - __main__ - Epoch  30, Step:   64900, Batch Loss:    16.063942, Lr: 0.000075, Tokens per sec:   3542
2023-03-15 00:55:01,699 - INFO - __main__ - Epoch  30, Step:   65000, Batch Loss:    15.032749, Lr: 0.000075, Tokens per sec:   3431
2023-03-15 00:55:17,360 - INFO - __main__ - Epoch  30, Step:   65100, Batch Loss:    10.656762, Lr: 0.000075, Tokens per sec:   3448
2023-03-15 00:55:33,279 - INFO - __main__ - Epoch  30, Step:   65200, Batch Loss:    12.917858, Lr: 0.000075, Tokens per sec:   3335
2023-03-15 00:55:48,840 - INFO - __main__ - Epoch  30, Step:   65300, Batch Loss:    15.911139, Lr: 0.000075, Tokens per sec:   3505
2023-03-15 00:55:59,750 - INFO - __main__ - Epoch  30: total training loss 26835.07
2023-03-15 00:55:59,750 - INFO - __main__ - Epoch 31
2023-03-15 00:56:04,693 - INFO - __main__ - Epoch  31, Step:   65400, Batch Loss:    12.793267, Lr: 0.000074, Tokens per sec:   3247
2023-03-15 00:56:20,448 - INFO - __main__ - Epoch  31, Step:   65500, Batch Loss:     8.679765, Lr: 0.000074, Tokens per sec:   3424
2023-03-15 00:56:36,133 - INFO - __main__ - Epoch  31, Step:   65600, Batch Loss:    11.924405, Lr: 0.000074, Tokens per sec:   3443
2023-03-15 00:56:51,816 - INFO - __main__ - Epoch  31, Step:   65700, Batch Loss:     9.756487, Lr: 0.000074, Tokens per sec:   3439
2023-03-15 00:57:07,179 - INFO - __main__ - Epoch  31, Step:   65800, Batch Loss:    11.413964, Lr: 0.000074, Tokens per sec:   3446
2023-03-15 00:57:22,766 - INFO - __main__ - Epoch  31, Step:   65900, Batch Loss:     8.920033, Lr: 0.000074, Tokens per sec:   3418
2023-03-15 00:57:38,451 - INFO - __main__ - Epoch  31, Step:   66000, Batch Loss:    11.097979, Lr: 0.000074, Tokens per sec:   3405
2023-03-15 00:57:54,433 - INFO - __main__ - Epoch  31, Step:   66100, Batch Loss:    14.258476, Lr: 0.000074, Tokens per sec:   3378
2023-03-15 00:58:10,320 - INFO - __main__ - Epoch  31, Step:   66200, Batch Loss:     8.268910, Lr: 0.000074, Tokens per sec:   3401
2023-03-15 00:58:26,324 - INFO - __main__ - Epoch  31, Step:   66300, Batch Loss:    14.661601, Lr: 0.000074, Tokens per sec:   3302
2023-03-15 00:58:42,215 - INFO - __main__ - Epoch  31, Step:   66400, Batch Loss:    12.876019, Lr: 0.000074, Tokens per sec:   3457
2023-03-15 00:58:58,081 - INFO - __main__ - Epoch  31, Step:   66500, Batch Loss:     9.926424, Lr: 0.000074, Tokens per sec:   3410
2023-03-15 00:59:13,703 - INFO - __main__ - Epoch  31, Step:   66600, Batch Loss:    12.805573, Lr: 0.000074, Tokens per sec:   3418
2023-03-15 00:59:29,514 - INFO - __main__ - Epoch  31, Step:   66700, Batch Loss:    11.637102, Lr: 0.000074, Tokens per sec:   3395
2023-03-15 00:59:45,451 - INFO - __main__ - Epoch  31, Step:   66800, Batch Loss:    10.314281, Lr: 0.000074, Tokens per sec:   3387
2023-03-15 01:00:00,618 - INFO - __main__ - Epoch  31, Step:   66900, Batch Loss:    12.176614, Lr: 0.000074, Tokens per sec:   3541
2023-03-15 01:00:15,678 - INFO - __main__ - Epoch  31, Step:   67000, Batch Loss:    12.740458, Lr: 0.000074, Tokens per sec:   3665
2023-03-15 01:00:31,297 - INFO - __main__ - Epoch  31, Step:   67100, Batch Loss:    13.546866, Lr: 0.000074, Tokens per sec:   3450
2023-03-15 01:00:47,307 - INFO - __main__ - Epoch  31, Step:   67200, Batch Loss:    13.572398, Lr: 0.000074, Tokens per sec:   3347
2023-03-15 01:01:03,327 - INFO - __main__ - Epoch  31, Step:   67300, Batch Loss:    16.057877, Lr: 0.000074, Tokens per sec:   3385
2023-03-15 01:01:19,344 - INFO - __main__ - Epoch  31, Step:   67400, Batch Loss:    11.381831, Lr: 0.000074, Tokens per sec:   3345
2023-03-15 01:01:35,425 - INFO - __main__ - Epoch  31, Step:   67500, Batch Loss:    10.962921, Lr: 0.000074, Tokens per sec:   3319
2023-03-15 01:01:43,193 - INFO - __main__ - Epoch  31: total training loss 25703.28
2023-03-15 01:01:43,193 - INFO - __main__ - Epoch 32
2023-03-15 01:01:51,457 - INFO - __main__ - Epoch  32, Step:   67600, Batch Loss:     8.696026, Lr: 0.000073, Tokens per sec:   3298
2023-03-15 01:02:06,990 - INFO - __main__ - Epoch  32, Step:   67700, Batch Loss:    11.367371, Lr: 0.000073, Tokens per sec:   3497
2023-03-15 01:02:22,915 - INFO - __main__ - Epoch  32, Step:   67800, Batch Loss:     9.284271, Lr: 0.000073, Tokens per sec:   3419
2023-03-15 01:02:38,674 - INFO - __main__ - Epoch  32, Step:   67900, Batch Loss:     9.848301, Lr: 0.000073, Tokens per sec:   3420
2023-03-15 01:02:54,095 - INFO - __main__ - Epoch  32, Step:   68000, Batch Loss:     9.774988, Lr: 0.000073, Tokens per sec:   3388
2023-03-15 01:03:10,045 - INFO - __main__ - Epoch  32, Step:   68100, Batch Loss:    11.368182, Lr: 0.000073, Tokens per sec:   3402
2023-03-15 01:03:26,162 - INFO - __main__ - Epoch  32, Step:   68200, Batch Loss:     9.653010, Lr: 0.000073, Tokens per sec:   3339
2023-03-15 01:03:41,858 - INFO - __main__ - Epoch  32, Step:   68300, Batch Loss:    14.640938, Lr: 0.000073, Tokens per sec:   3397
2023-03-15 01:03:57,531 - INFO - __main__ - Epoch  32, Step:   68400, Batch Loss:    10.662817, Lr: 0.000073, Tokens per sec:   3410
2023-03-15 01:04:13,533 - INFO - __main__ - Epoch  32, Step:   68500, Batch Loss:    12.621681, Lr: 0.000073, Tokens per sec:   3393
2023-03-15 01:04:29,302 - INFO - __main__ - Epoch  32, Step:   68600, Batch Loss:    12.928856, Lr: 0.000073, Tokens per sec:   3432
2023-03-15 01:04:44,849 - INFO - __main__ - Epoch  32, Step:   68700, Batch Loss:    13.960179, Lr: 0.000073, Tokens per sec:   3378
2023-03-15 01:05:00,562 - INFO - __main__ - Epoch  32, Step:   68800, Batch Loss:    13.408831, Lr: 0.000073, Tokens per sec:   3466
2023-03-15 01:05:16,485 - INFO - __main__ - Epoch  32, Step:   68900, Batch Loss:    12.202976, Lr: 0.000073, Tokens per sec:   3398
2023-03-15 01:05:32,076 - INFO - __main__ - Epoch  32, Step:   69000, Batch Loss:    13.811903, Lr: 0.000073, Tokens per sec:   3458
2023-03-15 01:05:48,012 - INFO - __main__ - Epoch  32, Step:   69100, Batch Loss:    11.288134, Lr: 0.000073, Tokens per sec:   3378
2023-03-15 01:06:03,935 - INFO - __main__ - Epoch  32, Step:   69200, Batch Loss:     9.890380, Lr: 0.000073, Tokens per sec:   3462
2023-03-15 01:06:19,570 - INFO - __main__ - Epoch  32, Step:   69300, Batch Loss:    14.304190, Lr: 0.000073, Tokens per sec:   3454
2023-03-15 01:06:35,292 - INFO - __main__ - Epoch  32, Step:   69400, Batch Loss:    11.254615, Lr: 0.000073, Tokens per sec:   3432
2023-03-15 01:06:50,771 - INFO - __main__ - Epoch  32, Step:   69500, Batch Loss:     8.704528, Lr: 0.000073, Tokens per sec:   3478
2023-03-15 01:07:06,107 - INFO - __main__ - Epoch  32, Step:   69600, Batch Loss:    10.815655, Lr: 0.000073, Tokens per sec:   3542
2023-03-15 01:07:21,245 - INFO - __main__ - Epoch  32, Step:   69700, Batch Loss:    12.099914, Lr: 0.000073, Tokens per sec:   3455
2023-03-15 01:07:25,659 - INFO - __main__ - Epoch  32: total training loss 24610.99
2023-03-15 01:07:25,659 - INFO - __main__ - Epoch 33
2023-03-15 01:07:37,418 - INFO - __main__ - Epoch  33, Step:   69800, Batch Loss:     9.777548, Lr: 0.000072, Tokens per sec:   3314
2023-03-15 01:07:53,377 - INFO - __main__ - Epoch  33, Step:   69900, Batch Loss:    10.430749, Lr: 0.000072, Tokens per sec:   3353
2023-03-15 01:08:09,407 - INFO - __main__ - Epoch  33, Step:   70000, Batch Loss:     7.255620, Lr: 0.000072, Tokens per sec:   3377
2023-03-15 01:08:25,222 - INFO - __main__ - Epoch  33, Step:   70100, Batch Loss:     9.560985, Lr: 0.000072, Tokens per sec:   3324
2023-03-15 01:08:41,297 - INFO - __main__ - Epoch  33, Step:   70200, Batch Loss:    12.600206, Lr: 0.000072, Tokens per sec:   3386
2023-03-15 01:08:56,745 - INFO - __main__ - Epoch  33, Step:   70300, Batch Loss:     9.826565, Lr: 0.000072, Tokens per sec:   3476
2023-03-15 01:09:12,100 - INFO - __main__ - Epoch  33, Step:   70400, Batch Loss:     7.522582, Lr: 0.000072, Tokens per sec:   3491
2023-03-15 01:09:27,591 - INFO - __main__ - Epoch  33, Step:   70500, Batch Loss:     9.196291, Lr: 0.000072, Tokens per sec:   3464
2023-03-15 01:09:43,413 - INFO - __main__ - Epoch  33, Step:   70600, Batch Loss:     9.032487, Lr: 0.000072, Tokens per sec:   3456
2023-03-15 01:09:59,314 - INFO - __main__ - Epoch  33, Step:   70700, Batch Loss:    13.679489, Lr: 0.000072, Tokens per sec:   3376
2023-03-15 01:10:15,068 - INFO - __main__ - Epoch  33, Step:   70800, Batch Loss:    15.602942, Lr: 0.000072, Tokens per sec:   3398
2023-03-15 01:10:30,479 - INFO - __main__ - Epoch  33, Step:   70900, Batch Loss:    12.167315, Lr: 0.000072, Tokens per sec:   3467
2023-03-15 01:10:45,865 - INFO - __main__ - Epoch  33, Step:   71000, Batch Loss:     8.940300, Lr: 0.000072, Tokens per sec:   3465
2023-03-15 01:11:01,159 - INFO - __main__ - Epoch  33, Step:   71100, Batch Loss:     7.982636, Lr: 0.000072, Tokens per sec:   3494
2023-03-15 01:11:16,401 - INFO - __main__ - Epoch  33, Step:   71200, Batch Loss:    11.042604, Lr: 0.000072, Tokens per sec:   3517
2023-03-15 01:11:31,748 - INFO - __main__ - Epoch  33, Step:   71300, Batch Loss:    10.705950, Lr: 0.000072, Tokens per sec:   3530
2023-03-15 01:11:47,247 - INFO - __main__ - Epoch  33, Step:   71400, Batch Loss:    10.948894, Lr: 0.000072, Tokens per sec:   3452
2023-03-15 01:12:02,815 - INFO - __main__ - Epoch  33, Step:   71500, Batch Loss:    12.310391, Lr: 0.000072, Tokens per sec:   3472
2023-03-15 01:12:18,489 - INFO - __main__ - Epoch  33, Step:   71600, Batch Loss:    10.183225, Lr: 0.000072, Tokens per sec:   3475
2023-03-15 01:12:33,698 - INFO - __main__ - Epoch  33, Step:   71700, Batch Loss:    10.876470, Lr: 0.000072, Tokens per sec:   3538
2023-03-15 01:12:48,941 - INFO - __main__ - Epoch  33, Step:   71800, Batch Loss:    13.263103, Lr: 0.000072, Tokens per sec:   3657
2023-03-15 01:13:04,497 - INFO - __main__ - Epoch  33, Step:   71900, Batch Loss:    10.823202, Lr: 0.000072, Tokens per sec:   3440
2023-03-15 01:13:05,651 - INFO - __main__ - Epoch  33: total training loss 23631.18
2023-03-15 01:13:05,651 - INFO - __main__ - Epoch 34
2023-03-15 01:13:20,380 - INFO - __main__ - Epoch  34, Step:   72000, Batch Loss:    12.798787, Lr: 0.000072, Tokens per sec:   3386
2023-03-15 01:13:35,711 - INFO - __main__ - Epoch  34, Step:   72100, Batch Loss:     9.715297, Lr: 0.000072, Tokens per sec:   3520
2023-03-15 01:13:51,160 - INFO - __main__ - Epoch  34, Step:   72200, Batch Loss:     8.235708, Lr: 0.000072, Tokens per sec:   3490
2023-03-15 01:14:06,316 - INFO - __main__ - Epoch  34, Step:   72300, Batch Loss:     9.208833, Lr: 0.000072, Tokens per sec:   3561
2023-03-15 01:14:22,250 - INFO - __main__ - Epoch  34, Step:   72400, Batch Loss:    13.673495, Lr: 0.000072, Tokens per sec:   3413
2023-03-15 01:14:37,776 - INFO - __main__ - Epoch  34, Step:   72500, Batch Loss:     9.362931, Lr: 0.000072, Tokens per sec:   3380
2023-03-15 01:14:53,043 - INFO - __main__ - Epoch  34, Step:   72600, Batch Loss:    10.189784, Lr: 0.000072, Tokens per sec:   3630
2023-03-15 01:15:08,113 - INFO - __main__ - Epoch  34, Step:   72700, Batch Loss:     8.174582, Lr: 0.000072, Tokens per sec:   3496
2023-03-15 01:15:23,071 - INFO - __main__ - Epoch  34, Step:   72800, Batch Loss:    10.184088, Lr: 0.000072, Tokens per sec:   3598
2023-03-15 01:15:38,364 - INFO - __main__ - Epoch  34, Step:   72900, Batch Loss:    10.415473, Lr: 0.000072, Tokens per sec:   3486
2023-03-15 01:15:53,705 - INFO - __main__ - Epoch  34, Step:   73000, Batch Loss:     8.999434, Lr: 0.000072, Tokens per sec:   3489
2023-03-15 01:16:09,197 - INFO - __main__ - Epoch  34, Step:   73100, Batch Loss:     9.085216, Lr: 0.000072, Tokens per sec:   3450
2023-03-15 01:16:24,726 - INFO - __main__ - Epoch  34, Step:   73200, Batch Loss:     9.482265, Lr: 0.000072, Tokens per sec:   3499
2023-03-15 01:16:40,328 - INFO - __main__ - Epoch  34, Step:   73300, Batch Loss:     8.600469, Lr: 0.000072, Tokens per sec:   3504
2023-03-15 01:16:55,622 - INFO - __main__ - Epoch  34, Step:   73400, Batch Loss:     8.881904, Lr: 0.000072, Tokens per sec:   3508
2023-03-15 01:17:10,980 - INFO - __main__ - Epoch  34, Step:   73500, Batch Loss:    12.449974, Lr: 0.000072, Tokens per sec:   3572
2023-03-15 01:17:26,448 - INFO - __main__ - Epoch  34, Step:   73600, Batch Loss:     8.463554, Lr: 0.000072, Tokens per sec:   3423
2023-03-15 01:17:41,952 - INFO - __main__ - Epoch  34, Step:   73700, Batch Loss:     9.299093, Lr: 0.000072, Tokens per sec:   3479
2023-03-15 01:17:57,239 - INFO - __main__ - Epoch  34, Step:   73800, Batch Loss:    10.075028, Lr: 0.000072, Tokens per sec:   3520
2023-03-15 01:18:12,604 - INFO - __main__ - Epoch  34, Step:   73900, Batch Loss:    13.429434, Lr: 0.000072, Tokens per sec:   3506
2023-03-15 01:18:27,878 - INFO - __main__ - Epoch  34, Step:   74000, Batch Loss:    13.138009, Lr: 0.000072, Tokens per sec:   3543
2023-03-15 01:18:41,209 - INFO - __main__ - Epoch  34: total training loss 22687.23
2023-03-15 01:18:41,209 - INFO - __main__ - Epoch 35
2023-03-15 01:18:43,724 - INFO - __main__ - Epoch  35, Step:   74100, Batch Loss:    11.611084, Lr: 0.000071, Tokens per sec:   3052
2023-03-15 01:18:59,147 - INFO - __main__ - Epoch  35, Step:   74200, Batch Loss:     9.128126, Lr: 0.000071, Tokens per sec:   3551
2023-03-15 01:19:14,236 - INFO - __main__ - Epoch  35, Step:   74300, Batch Loss:     8.377541, Lr: 0.000071, Tokens per sec:   3588
2023-03-15 01:19:29,388 - INFO - __main__ - Epoch  35, Step:   74400, Batch Loss:     9.163124, Lr: 0.000071, Tokens per sec:   3490
2023-03-15 01:19:44,038 - INFO - __main__ - Epoch  35, Step:   74500, Batch Loss:     9.890224, Lr: 0.000071, Tokens per sec:   3703
2023-03-15 01:19:59,522 - INFO - __main__ - Epoch  35, Step:   74600, Batch Loss:     9.084233, Lr: 0.000071, Tokens per sec:   3495
2023-03-15 01:20:14,959 - INFO - __main__ - Epoch  35, Step:   74700, Batch Loss:     9.168996, Lr: 0.000071, Tokens per sec:   3459
2023-03-15 01:20:30,355 - INFO - __main__ - Epoch  35, Step:   74800, Batch Loss:     8.980672, Lr: 0.000071, Tokens per sec:   3465
2023-03-15 01:20:45,563 - INFO - __main__ - Epoch  35, Step:   74900, Batch Loss:     8.801457, Lr: 0.000071, Tokens per sec:   3595
2023-03-15 01:21:00,951 - INFO - __main__ - Epoch  35, Step:   75000, Batch Loss:    10.160857, Lr: 0.000071, Tokens per sec:   3485
2023-03-15 01:21:16,635 - INFO - __main__ - Epoch  35, Step:   75100, Batch Loss:    10.901585, Lr: 0.000071, Tokens per sec:   3470
2023-03-15 01:21:31,999 - INFO - __main__ - Epoch  35, Step:   75200, Batch Loss:     9.764190, Lr: 0.000071, Tokens per sec:   3486
2023-03-15 01:21:47,396 - INFO - __main__ - Epoch  35, Step:   75300, Batch Loss:     9.831498, Lr: 0.000071, Tokens per sec:   3508
2023-03-15 01:22:02,768 - INFO - __main__ - Epoch  35, Step:   75400, Batch Loss:    12.610624, Lr: 0.000071, Tokens per sec:   3516
2023-03-15 01:22:18,230 - INFO - __main__ - Epoch  35, Step:   75500, Batch Loss:    10.336294, Lr: 0.000071, Tokens per sec:   3474
2023-03-15 01:22:33,756 - INFO - __main__ - Epoch  35, Step:   75600, Batch Loss:    13.874763, Lr: 0.000071, Tokens per sec:   3483
2023-03-15 01:22:49,097 - INFO - __main__ - Epoch  35, Step:   75700, Batch Loss:    10.961557, Lr: 0.000071, Tokens per sec:   3517
2023-03-15 01:23:04,664 - INFO - __main__ - Epoch  35, Step:   75800, Batch Loss:    13.200974, Lr: 0.000071, Tokens per sec:   3433
2023-03-15 01:23:19,722 - INFO - __main__ - Epoch  35, Step:   75900, Batch Loss:    17.612640, Lr: 0.000071, Tokens per sec:   3525
2023-03-15 01:23:35,252 - INFO - __main__ - Epoch  35, Step:   76000, Batch Loss:    11.478747, Lr: 0.000071, Tokens per sec:   3461
2023-03-15 01:23:50,666 - INFO - __main__ - Epoch  35, Step:   76100, Batch Loss:    12.102932, Lr: 0.000071, Tokens per sec:   3468
2023-03-15 01:24:05,931 - INFO - __main__ - Epoch  35, Step:   76200, Batch Loss:    12.360622, Lr: 0.000071, Tokens per sec:   3497
2023-03-15 01:24:15,930 - INFO - __main__ - Epoch  35: total training loss 21839.81
2023-03-15 01:24:15,931 - INFO - __main__ - Epoch 36
2023-03-15 01:24:21,792 - INFO - __main__ - Epoch  36, Step:   76300, Batch Loss:     8.516621, Lr: 0.000070, Tokens per sec:   3347
2023-03-15 01:24:37,426 - INFO - __main__ - Epoch  36, Step:   76400, Batch Loss:     8.037251, Lr: 0.000070, Tokens per sec:   3424
2023-03-15 01:24:52,947 - INFO - __main__ - Epoch  36, Step:   76500, Batch Loss:    14.728434, Lr: 0.000070, Tokens per sec:   3521
2023-03-15 01:25:07,678 - INFO - __main__ - Epoch  36, Step:   76600, Batch Loss:    10.344692, Lr: 0.000070, Tokens per sec:   3692
2023-03-15 01:25:23,048 - INFO - __main__ - Epoch  36, Step:   76700, Batch Loss:     8.467032, Lr: 0.000070, Tokens per sec:   3478
2023-03-15 01:25:38,402 - INFO - __main__ - Epoch  36, Step:   76800, Batch Loss:    11.380933, Lr: 0.000070, Tokens per sec:   3553
2023-03-15 01:25:53,711 - INFO - __main__ - Epoch  36, Step:   76900, Batch Loss:     9.382153, Lr: 0.000070, Tokens per sec:   3480
2023-03-15 01:26:09,099 - INFO - __main__ - Epoch  36, Step:   77000, Batch Loss:     7.565167, Lr: 0.000070, Tokens per sec:   3547
2023-03-15 01:26:24,665 - INFO - __main__ - Epoch  36, Step:   77100, Batch Loss:     6.674730, Lr: 0.000070, Tokens per sec:   3479
2023-03-15 01:26:40,036 - INFO - __main__ - Epoch  36, Step:   77200, Batch Loss:    11.780969, Lr: 0.000070, Tokens per sec:   3454
2023-03-15 01:26:54,734 - INFO - __main__ - Epoch  36, Step:   77300, Batch Loss:     9.741534, Lr: 0.000070, Tokens per sec:   3664
2023-03-15 01:27:10,181 - INFO - __main__ - Epoch  36, Step:   77400, Batch Loss:    12.563455, Lr: 0.000070, Tokens per sec:   3479
2023-03-15 01:27:25,781 - INFO - __main__ - Epoch  36, Step:   77500, Batch Loss:     8.861240, Lr: 0.000070, Tokens per sec:   3386
2023-03-15 01:27:40,968 - INFO - __main__ - Epoch  36, Step:   77600, Batch Loss:     9.769253, Lr: 0.000070, Tokens per sec:   3545
2023-03-15 01:27:56,326 - INFO - __main__ - Epoch  36, Step:   77700, Batch Loss:    13.070045, Lr: 0.000070, Tokens per sec:   3477
2023-03-15 01:28:11,584 - INFO - __main__ - Epoch  36, Step:   77800, Batch Loss:     7.010592, Lr: 0.000070, Tokens per sec:   3521
2023-03-15 01:28:26,976 - INFO - __main__ - Epoch  36, Step:   77900, Batch Loss:     9.867703, Lr: 0.000070, Tokens per sec:   3491
2023-03-15 01:28:42,212 - INFO - __main__ - Epoch  36, Step:   78000, Batch Loss:     8.852503, Lr: 0.000070, Tokens per sec:   3535
2023-03-15 01:28:57,219 - INFO - __main__ - Epoch  36, Step:   78100, Batch Loss:    10.780357, Lr: 0.000070, Tokens per sec:   3602
2023-03-15 01:29:12,588 - INFO - __main__ - Epoch  36, Step:   78200, Batch Loss:     7.837545, Lr: 0.000070, Tokens per sec:   3463
2023-03-15 01:29:27,887 - INFO - __main__ - Epoch  36, Step:   78300, Batch Loss:     9.266039, Lr: 0.000070, Tokens per sec:   3534
2023-03-15 01:29:43,626 - INFO - __main__ - Epoch  36, Step:   78400, Batch Loss:    10.143673, Lr: 0.000070, Tokens per sec:   3425
2023-03-15 01:29:50,649 - INFO - __main__ - Epoch  36: total training loss 21012.82
2023-03-15 01:29:50,650 - INFO - __main__ - Epoch 37
2023-03-15 01:29:59,226 - INFO - __main__ - Epoch  37, Step:   78500, Batch Loss:    10.712367, Lr: 0.000070, Tokens per sec:   3570
2023-03-15 01:30:14,619 - INFO - __main__ - Epoch  37, Step:   78600, Batch Loss:     9.219973, Lr: 0.000070, Tokens per sec:   3440
2023-03-15 01:30:30,006 - INFO - __main__ - Epoch  37, Step:   78700, Batch Loss:     9.376879, Lr: 0.000070, Tokens per sec:   3476
2023-03-15 01:30:45,359 - INFO - __main__ - Epoch  37, Step:   78800, Batch Loss:     9.069095, Lr: 0.000070, Tokens per sec:   3507
2023-03-15 01:31:00,524 - INFO - __main__ - Epoch  37, Step:   78900, Batch Loss:     8.909079, Lr: 0.000070, Tokens per sec:   3527
2023-03-15 01:31:15,455 - INFO - __main__ - Epoch  37, Step:   79000, Batch Loss:    10.577041, Lr: 0.000070, Tokens per sec:   3641
2023-03-15 01:31:30,743 - INFO - __main__ - Epoch  37, Step:   79100, Batch Loss:     7.286680, Lr: 0.000070, Tokens per sec:   3464
2023-03-15 01:31:46,298 - INFO - __main__ - Epoch  37, Step:   79200, Batch Loss:     6.785632, Lr: 0.000070, Tokens per sec:   3459
2023-03-15 01:32:01,767 - INFO - __main__ - Epoch  37, Step:   79300, Batch Loss:     9.164274, Lr: 0.000070, Tokens per sec:   3480
2023-03-15 01:32:17,035 - INFO - __main__ - Epoch  37, Step:   79400, Batch Loss:     7.013033, Lr: 0.000070, Tokens per sec:   3550
2023-03-15 01:32:32,081 - INFO - __main__ - Epoch  37, Step:   79500, Batch Loss:    11.753911, Lr: 0.000070, Tokens per sec:   3528
2023-03-15 01:32:47,086 - INFO - __main__ - Epoch  37, Step:   79600, Batch Loss:    10.608405, Lr: 0.000070, Tokens per sec:   3577
2023-03-15 01:33:02,619 - INFO - __main__ - Epoch  37, Step:   79700, Batch Loss:     8.395253, Lr: 0.000070, Tokens per sec:   3485
2023-03-15 01:33:17,896 - INFO - __main__ - Epoch  37, Step:   79800, Batch Loss:     9.346886, Lr: 0.000070, Tokens per sec:   3522
2023-03-15 01:33:33,426 - INFO - __main__ - Epoch  37, Step:   79900, Batch Loss:     9.761918, Lr: 0.000070, Tokens per sec:   3436
2023-03-15 01:33:48,869 - INFO - __main__ - Epoch  37, Step:   80000, Batch Loss:     6.895709, Lr: 0.000070, Tokens per sec:   3503
2023-03-15 01:34:04,245 - INFO - __main__ - Epoch  37, Step:   80100, Batch Loss:     6.758014, Lr: 0.000070, Tokens per sec:   3574
2023-03-15 01:34:19,595 - INFO - __main__ - Epoch  37, Step:   80200, Batch Loss:     7.625504, Lr: 0.000070, Tokens per sec:   3543
2023-03-15 01:34:34,691 - INFO - __main__ - Epoch  37, Step:   80300, Batch Loss:    11.287523, Lr: 0.000070, Tokens per sec:   3622
2023-03-15 01:34:49,959 - INFO - __main__ - Epoch  37, Step:   80400, Batch Loss:     8.705964, Lr: 0.000070, Tokens per sec:   3492
2023-03-15 01:35:04,974 - INFO - __main__ - Epoch  37, Step:   80500, Batch Loss:    10.152439, Lr: 0.000070, Tokens per sec:   3617
2023-03-15 01:35:20,294 - INFO - __main__ - Epoch  37, Step:   80600, Batch Loss:     9.536227, Lr: 0.000070, Tokens per sec:   3528
2023-03-15 01:35:23,882 - INFO - __main__ - Epoch  37: total training loss 20300.14
2023-03-15 01:35:23,883 - INFO - __main__ - Epoch 38
2023-03-15 01:35:35,895 - INFO - __main__ - Epoch  38, Step:   80700, Batch Loss:     8.597624, Lr: 0.000069, Tokens per sec:   3446
2023-03-15 01:35:51,176 - INFO - __main__ - Epoch  38, Step:   80800, Batch Loss:     5.804898, Lr: 0.000069, Tokens per sec:   3505
2023-03-15 01:36:06,585 - INFO - __main__ - Epoch  38, Step:   80900, Batch Loss:     6.857734, Lr: 0.000069, Tokens per sec:   3500
2023-03-15 01:36:21,936 - INFO - __main__ - Epoch  38, Step:   81000, Batch Loss:     7.889038, Lr: 0.000069, Tokens per sec:   3486
2023-03-15 01:36:37,007 - INFO - __main__ - Epoch  38, Step:   81100, Batch Loss:     8.862669, Lr: 0.000069, Tokens per sec:   3560
2023-03-15 01:36:52,577 - INFO - __main__ - Epoch  38, Step:   81200, Batch Loss:     6.849595, Lr: 0.000069, Tokens per sec:   3487
2023-03-15 01:37:07,961 - INFO - __main__ - Epoch  38, Step:   81300, Batch Loss:    11.695627, Lr: 0.000069, Tokens per sec:   3494
2023-03-15 01:37:23,457 - INFO - __main__ - Epoch  38, Step:   81400, Batch Loss:     7.860164, Lr: 0.000069, Tokens per sec:   3518
2023-03-15 01:37:38,917 - INFO - __main__ - Epoch  38, Step:   81500, Batch Loss:     6.410420, Lr: 0.000069, Tokens per sec:   3497
2023-03-15 01:37:54,406 - INFO - __main__ - Epoch  38, Step:   81600, Batch Loss:     9.108767, Lr: 0.000069, Tokens per sec:   3490
2023-03-15 01:38:09,832 - INFO - __main__ - Epoch  38, Step:   81700, Batch Loss:     8.322346, Lr: 0.000069, Tokens per sec:   3431
2023-03-15 01:38:24,766 - INFO - __main__ - Epoch  38, Step:   81800, Batch Loss:     9.145472, Lr: 0.000069, Tokens per sec:   3612
2023-03-15 01:38:39,985 - INFO - __main__ - Epoch  38, Step:   81900, Batch Loss:     8.539370, Lr: 0.000069, Tokens per sec:   3493
2023-03-15 01:38:55,430 - INFO - __main__ - Epoch  38, Step:   82000, Batch Loss:     7.935609, Lr: 0.000069, Tokens per sec:   3437
2023-03-15 01:39:10,720 - INFO - __main__ - Epoch  38, Step:   82100, Batch Loss:     9.533455, Lr: 0.000069, Tokens per sec:   3529
2023-03-15 01:39:26,113 - INFO - __main__ - Epoch  38, Step:   82200, Batch Loss:    10.627867, Lr: 0.000069, Tokens per sec:   3509
2023-03-15 01:39:41,459 - INFO - __main__ - Epoch  38, Step:   82300, Batch Loss:     6.998889, Lr: 0.000069, Tokens per sec:   3553
2023-03-15 01:39:56,620 - INFO - __main__ - Epoch  38, Step:   82400, Batch Loss:    10.490964, Lr: 0.000069, Tokens per sec:   3567
2023-03-15 01:40:11,977 - INFO - __main__ - Epoch  38, Step:   82500, Batch Loss:     7.541059, Lr: 0.000069, Tokens per sec:   3528
2023-03-15 01:40:27,149 - INFO - __main__ - Epoch  38, Step:   82600, Batch Loss:    10.854666, Lr: 0.000069, Tokens per sec:   3497
2023-03-15 01:40:42,501 - INFO - __main__ - Epoch  38, Step:   82700, Batch Loss:     9.734483, Lr: 0.000069, Tokens per sec:   3510
2023-03-15 01:40:57,371 - INFO - __main__ - Epoch  38, Step:   82800, Batch Loss:    10.293450, Lr: 0.000069, Tokens per sec:   3659
2023-03-15 01:40:57,784 - INFO - __main__ - Epoch  38: total training loss 19520.69
2023-03-15 01:40:57,785 - INFO - __main__ - Epoch 39
2023-03-15 01:41:13,185 - INFO - __main__ - Epoch  39, Step:   82900, Batch Loss:     7.420973, Lr: 0.000068, Tokens per sec:   3382
2023-03-15 01:41:28,339 - INFO - __main__ - Epoch  39, Step:   83000, Batch Loss:     8.139791, Lr: 0.000068, Tokens per sec:   3538
2023-03-15 01:41:43,687 - INFO - __main__ - Epoch  39, Step:   83100, Batch Loss:     7.267321, Lr: 0.000068, Tokens per sec:   3558
2023-03-15 01:41:58,908 - INFO - __main__ - Epoch  39, Step:   83200, Batch Loss:     6.113294, Lr: 0.000068, Tokens per sec:   3531
2023-03-15 01:42:14,122 - INFO - __main__ - Epoch  39, Step:   83300, Batch Loss:     8.850720, Lr: 0.000068, Tokens per sec:   3586
2023-03-15 01:42:29,417 - INFO - __main__ - Epoch  39, Step:   83400, Batch Loss:    10.322029, Lr: 0.000068, Tokens per sec:   3567
2023-03-15 01:42:44,798 - INFO - __main__ - Epoch  39, Step:   83500, Batch Loss:     8.315844, Lr: 0.000068, Tokens per sec:   3458
2023-03-15 01:42:59,963 - INFO - __main__ - Epoch  39, Step:   83600, Batch Loss:     7.953532, Lr: 0.000068, Tokens per sec:   3561
2023-03-15 01:43:15,306 - INFO - __main__ - Epoch  39, Step:   83700, Batch Loss:    10.662505, Lr: 0.000068, Tokens per sec:   3508
2023-03-15 01:43:30,665 - INFO - __main__ - Epoch  39, Step:   83800, Batch Loss:     8.194324, Lr: 0.000068, Tokens per sec:   3507
2023-03-15 01:43:45,740 - INFO - __main__ - Epoch  39, Step:   83900, Batch Loss:    10.476270, Lr: 0.000068, Tokens per sec:   3609
2023-03-15 01:44:01,170 - INFO - __main__ - Epoch  39, Step:   84000, Batch Loss:     8.988711, Lr: 0.000068, Tokens per sec:   3521
2023-03-15 01:44:15,785 - INFO - __main__ - Epoch  39, Step:   84100, Batch Loss:     9.168981, Lr: 0.000068, Tokens per sec:   3683
2023-03-15 01:44:30,993 - INFO - __main__ - Epoch  39, Step:   84200, Batch Loss:     5.071830, Lr: 0.000068, Tokens per sec:   3523
2023-03-15 01:44:46,010 - INFO - __main__ - Epoch  39, Step:   84300, Batch Loss:     9.517783, Lr: 0.000068, Tokens per sec:   3524
2023-03-15 01:45:01,357 - INFO - __main__ - Epoch  39, Step:   84400, Batch Loss:    12.683707, Lr: 0.000068, Tokens per sec:   3523
2023-03-15 01:45:16,688 - INFO - __main__ - Epoch  39, Step:   84500, Batch Loss:     8.756392, Lr: 0.000068, Tokens per sec:   3492
2023-03-15 01:45:31,799 - INFO - __main__ - Epoch  39, Step:   84600, Batch Loss:     8.528179, Lr: 0.000068, Tokens per sec:   3526
2023-03-15 01:45:47,286 - INFO - __main__ - Epoch  39, Step:   84700, Batch Loss:     8.422687, Lr: 0.000068, Tokens per sec:   3500
2023-03-15 01:46:02,623 - INFO - __main__ - Epoch  39, Step:   84800, Batch Loss:    11.349196, Lr: 0.000068, Tokens per sec:   3551
2023-03-15 01:46:18,102 - INFO - __main__ - Epoch  39, Step:   84900, Batch Loss:     9.330897, Lr: 0.000068, Tokens per sec:   3428
2023-03-15 01:46:30,454 - INFO - __main__ - Epoch  39: total training loss 18839.41
2023-03-15 01:46:30,455 - INFO - __main__ - Epoch 40
2023-03-15 01:46:33,640 - INFO - __main__ - Epoch  40, Step:   85000, Batch Loss:     6.008326, Lr: 0.000068, Tokens per sec:   3159
2023-03-15 01:46:48,946 - INFO - __main__ - Epoch  40, Step:   85100, Batch Loss:     6.560709, Lr: 0.000068, Tokens per sec:   3478
2023-03-15 01:47:04,441 - INFO - __main__ - Epoch  40, Step:   85200, Batch Loss:     8.693705, Lr: 0.000068, Tokens per sec:   3454
2023-03-15 01:47:19,806 - INFO - __main__ - Epoch  40, Step:   85300, Batch Loss:     8.164556, Lr: 0.000068, Tokens per sec:   3481
2023-03-15 01:47:35,228 - INFO - __main__ - Epoch  40, Step:   85400, Batch Loss:     7.764602, Lr: 0.000068, Tokens per sec:   3485
2023-03-15 01:47:50,620 - INFO - __main__ - Epoch  40, Step:   85500, Batch Loss:     7.178639, Lr: 0.000068, Tokens per sec:   3482
2023-03-15 01:48:06,150 - INFO - __main__ - Epoch  40, Step:   85600, Batch Loss:     6.299166, Lr: 0.000068, Tokens per sec:   3446
2023-03-15 01:48:21,533 - INFO - __main__ - Epoch  40, Step:   85700, Batch Loss:     8.761677, Lr: 0.000068, Tokens per sec:   3523
2023-03-15 01:48:36,950 - INFO - __main__ - Epoch  40, Step:   85800, Batch Loss:    12.963428, Lr: 0.000068, Tokens per sec:   3538
2023-03-15 01:48:52,257 - INFO - __main__ - Epoch  40, Step:   85900, Batch Loss:     6.365610, Lr: 0.000068, Tokens per sec:   3495
2023-03-15 01:49:07,607 - INFO - __main__ - Epoch  40, Step:   86000, Batch Loss:     9.106008, Lr: 0.000068, Tokens per sec:   3517
2023-03-15 01:49:22,799 - INFO - __main__ - Epoch  40, Step:   86100, Batch Loss:     9.195263, Lr: 0.000068, Tokens per sec:   3605
2023-03-15 01:49:38,244 - INFO - __main__ - Epoch  40, Step:   86200, Batch Loss:    10.489315, Lr: 0.000068, Tokens per sec:   3488
2023-03-15 01:49:53,522 - INFO - __main__ - Epoch  40, Step:   86300, Batch Loss:     5.742668, Lr: 0.000068, Tokens per sec:   3544
2023-03-15 01:50:08,648 - INFO - __main__ - Epoch  40, Step:   86400, Batch Loss:     9.716635, Lr: 0.000068, Tokens per sec:   3523
2023-03-15 01:50:23,692 - INFO - __main__ - Epoch  40, Step:   86500, Batch Loss:     8.392546, Lr: 0.000068, Tokens per sec:   3573
2023-03-15 01:50:38,919 - INFO - __main__ - Epoch  40, Step:   86600, Batch Loss:     9.575734, Lr: 0.000068, Tokens per sec:   3507
2023-03-15 01:50:54,041 - INFO - __main__ - Epoch  40, Step:   86700, Batch Loss:    10.141780, Lr: 0.000068, Tokens per sec:   3546
2023-03-15 01:51:08,866 - INFO - __main__ - Epoch  40, Step:   86800, Batch Loss:     9.581606, Lr: 0.000068, Tokens per sec:   3722
2023-03-15 01:51:24,168 - INFO - __main__ - Epoch  40, Step:   86900, Batch Loss:    10.610700, Lr: 0.000068, Tokens per sec:   3541
2023-03-15 01:51:39,519 - INFO - __main__ - Epoch  40, Step:   87000, Batch Loss:     8.691578, Lr: 0.000068, Tokens per sec:   3478
2023-03-15 01:51:54,852 - INFO - __main__ - Epoch  40, Step:   87100, Batch Loss:     5.423909, Lr: 0.000068, Tokens per sec:   3472
2023-03-15 01:52:04,141 - INFO - __main__ - Epoch  40: total training loss 18227.83
2023-03-15 01:52:04,142 - INFO - __main__ - Epoch 41
2023-03-15 01:52:10,666 - INFO - __main__ - Epoch  41, Step:   87200, Batch Loss:    10.165830, Lr: 0.000067, Tokens per sec:   3271
2023-03-15 01:52:26,092 - INFO - __main__ - Epoch  41, Step:   87300, Batch Loss:     7.946067, Lr: 0.000067, Tokens per sec:   3504
2023-03-15 01:52:41,526 - INFO - __main__ - Epoch  41, Step:   87400, Batch Loss:     7.229451, Lr: 0.000067, Tokens per sec:   3518
2023-03-15 01:52:56,798 - INFO - __main__ - Epoch  41, Step:   87500, Batch Loss:     5.380570, Lr: 0.000067, Tokens per sec:   3496
2023-03-15 01:53:11,990 - INFO - __main__ - Epoch  41, Step:   87600, Batch Loss:     8.898393, Lr: 0.000067, Tokens per sec:   3603
2023-03-15 01:53:27,376 - INFO - __main__ - Epoch  41, Step:   87700, Batch Loss:     5.113131, Lr: 0.000067, Tokens per sec:   3441
2023-03-15 01:53:42,731 - INFO - __main__ - Epoch  41, Step:   87800, Batch Loss:     5.241161, Lr: 0.000067, Tokens per sec:   3572
2023-03-15 01:53:57,884 - INFO - __main__ - Epoch  41, Step:   87900, Batch Loss:     6.725683, Lr: 0.000067, Tokens per sec:   3552
2023-03-15 01:54:13,140 - INFO - __main__ - Epoch  41, Step:   88000, Batch Loss:     8.505870, Lr: 0.000067, Tokens per sec:   3520
2023-03-15 01:54:28,592 - INFO - __main__ - Epoch  41, Step:   88100, Batch Loss:     7.331625, Lr: 0.000067, Tokens per sec:   3431
2023-03-15 01:54:44,013 - INFO - __main__ - Epoch  41, Step:   88200, Batch Loss:     7.489868, Lr: 0.000067, Tokens per sec:   3484
2023-03-15 01:54:59,307 - INFO - __main__ - Epoch  41, Step:   88300, Batch Loss:     5.856111, Lr: 0.000067, Tokens per sec:   3447
2023-03-15 01:55:14,606 - INFO - __main__ - Epoch  41, Step:   88400, Batch Loss:     9.540480, Lr: 0.000067, Tokens per sec:   3541
2023-03-15 01:55:30,029 - INFO - __main__ - Epoch  41, Step:   88500, Batch Loss:     8.528309, Lr: 0.000067, Tokens per sec:   3462
2023-03-15 01:55:45,368 - INFO - __main__ - Epoch  41, Step:   88600, Batch Loss:     6.993943, Lr: 0.000067, Tokens per sec:   3505
2023-03-15 01:56:00,406 - INFO - __main__ - Epoch  41, Step:   88700, Batch Loss:     9.437543, Lr: 0.000067, Tokens per sec:   3609
2023-03-15 01:56:15,167 - INFO - __main__ - Epoch  41, Step:   88800, Batch Loss:    10.641431, Lr: 0.000067, Tokens per sec:   3613
2023-03-15 01:56:30,843 - INFO - __main__ - Epoch  41, Step:   88900, Batch Loss:     7.516802, Lr: 0.000067, Tokens per sec:   3503
2023-03-15 01:56:46,230 - INFO - __main__ - Epoch  41, Step:   89000, Batch Loss:     5.155591, Lr: 0.000067, Tokens per sec:   3498
2023-03-15 01:57:01,453 - INFO - __main__ - Epoch  41, Step:   89100, Batch Loss:     5.816538, Lr: 0.000067, Tokens per sec:   3582
2023-03-15 01:57:16,771 - INFO - __main__ - Epoch  41, Step:   89200, Batch Loss:     8.194616, Lr: 0.000067, Tokens per sec:   3504
2023-03-15 01:57:32,026 - INFO - __main__ - Epoch  41, Step:   89300, Batch Loss:    10.298290, Lr: 0.000067, Tokens per sec:   3520
2023-03-15 01:57:38,024 - INFO - __main__ - Epoch  41: total training loss 17574.94
2023-03-15 01:57:38,025 - INFO - __main__ - Epoch 42
2023-03-15 01:57:47,569 - INFO - __main__ - Epoch  42, Step:   89400, Batch Loss:     5.733328, Lr: 0.000066, Tokens per sec:   3381
2023-03-15 01:58:02,897 - INFO - __main__ - Epoch  42, Step:   89500, Batch Loss:     3.906589, Lr: 0.000066, Tokens per sec:   3478
2023-03-15 01:58:18,452 - INFO - __main__ - Epoch  42, Step:   89600, Batch Loss:     7.501407, Lr: 0.000066, Tokens per sec:   3421
2023-03-15 01:58:33,942 - INFO - __main__ - Epoch  42, Step:   89700, Batch Loss:     7.272998, Lr: 0.000066, Tokens per sec:   3479
2023-03-15 01:58:49,318 - INFO - __main__ - Epoch  42, Step:   89800, Batch Loss:     9.811625, Lr: 0.000066, Tokens per sec:   3488
2023-03-15 01:59:04,461 - INFO - __main__ - Epoch  42, Step:   89900, Batch Loss:     7.168969, Lr: 0.000066, Tokens per sec:   3616
2023-03-15 01:59:19,742 - INFO - __main__ - Epoch  42, Step:   90000, Batch Loss:     6.425760, Lr: 0.000066, Tokens per sec:   3560
2023-03-15 01:59:35,034 - INFO - __main__ - Epoch  42, Step:   90100, Batch Loss:     7.837349, Lr: 0.000066, Tokens per sec:   3513
2023-03-15 01:59:49,955 - INFO - __main__ - Epoch  42, Step:   90200, Batch Loss:     7.501557, Lr: 0.000066, Tokens per sec:   3550
2023-03-15 02:00:04,987 - INFO - __main__ - Epoch  42, Step:   90300, Batch Loss:     5.492306, Lr: 0.000066, Tokens per sec:   3662
2023-03-15 02:00:20,209 - INFO - __main__ - Epoch  42, Step:   90400, Batch Loss:     6.301636, Lr: 0.000066, Tokens per sec:   3490
2023-03-15 02:00:35,435 - INFO - __main__ - Epoch  42, Step:   90500, Batch Loss:     7.244886, Lr: 0.000066, Tokens per sec:   3530
2023-03-15 02:00:50,826 - INFO - __main__ - Epoch  42, Step:   90600, Batch Loss:     9.082999, Lr: 0.000066, Tokens per sec:   3465
2023-03-15 02:01:06,163 - INFO - __main__ - Epoch  42, Step:   90700, Batch Loss:     5.509483, Lr: 0.000066, Tokens per sec:   3512
2023-03-15 02:01:21,462 - INFO - __main__ - Epoch  42, Step:   90800, Batch Loss:     7.286711, Lr: 0.000066, Tokens per sec:   3529
2023-03-15 02:01:36,672 - INFO - __main__ - Epoch  42, Step:   90900, Batch Loss:     8.673627, Lr: 0.000066, Tokens per sec:   3525
2023-03-15 02:01:51,549 - INFO - __main__ - Epoch  42, Step:   91000, Batch Loss:     5.462055, Lr: 0.000066, Tokens per sec:   3606
2023-03-15 02:02:06,942 - INFO - __main__ - Epoch  42, Step:   91100, Batch Loss:     8.496787, Lr: 0.000066, Tokens per sec:   3553
2023-03-15 02:02:22,183 - INFO - __main__ - Epoch  42, Step:   91200, Batch Loss:     8.179515, Lr: 0.000066, Tokens per sec:   3514
2023-03-15 02:02:37,477 - INFO - __main__ - Epoch  42, Step:   91300, Batch Loss:     7.707871, Lr: 0.000066, Tokens per sec:   3489
2023-03-15 02:02:52,845 - INFO - __main__ - Epoch  42, Step:   91400, Batch Loss:    10.575923, Lr: 0.000066, Tokens per sec:   3516
2023-03-15 02:03:08,098 - INFO - __main__ - Epoch  42, Step:   91500, Batch Loss:     7.447928, Lr: 0.000066, Tokens per sec:   3600
2023-03-15 02:03:10,925 - INFO - __main__ - Epoch  42: total training loss 17039.76
2023-03-15 02:03:10,926 - INFO - __main__ - Epoch 43
2023-03-15 02:03:23,878 - INFO - __main__ - Epoch  43, Step:   91600, Batch Loss:     7.176173, Lr: 0.000066, Tokens per sec:   3420
2023-03-15 02:03:38,968 - INFO - __main__ - Epoch  43, Step:   91700, Batch Loss:     6.385458, Lr: 0.000066, Tokens per sec:   3558
2023-03-15 02:03:54,508 - INFO - __main__ - Epoch  43, Step:   91800, Batch Loss:     4.638722, Lr: 0.000066, Tokens per sec:   3425
2023-03-15 02:04:09,998 - INFO - __main__ - Epoch  43, Step:   91900, Batch Loss:     5.555645, Lr: 0.000066, Tokens per sec:   3491
2023-03-15 02:04:25,404 - INFO - __main__ - Epoch  43, Step:   92000, Batch Loss:     7.517775, Lr: 0.000066, Tokens per sec:   3560
2023-03-15 02:04:40,821 - INFO - __main__ - Epoch  43, Step:   92100, Batch Loss:     5.230343, Lr: 0.000066, Tokens per sec:   3516
2023-03-15 02:04:56,120 - INFO - __main__ - Epoch  43, Step:   92200, Batch Loss:     8.510880, Lr: 0.000066, Tokens per sec:   3533
2023-03-15 02:05:11,493 - INFO - __main__ - Epoch  43, Step:   92300, Batch Loss:     7.754198, Lr: 0.000066, Tokens per sec:   3482
2023-03-15 02:05:26,954 - INFO - __main__ - Epoch  43, Step:   92400, Batch Loss:     7.586367, Lr: 0.000066, Tokens per sec:   3460
2023-03-15 02:05:42,005 - INFO - __main__ - Epoch  43, Step:   92500, Batch Loss:     9.155339, Lr: 0.000066, Tokens per sec:   3619
2023-03-15 02:05:57,588 - INFO - __main__ - Epoch  43, Step:   92600, Batch Loss:     8.291585, Lr: 0.000066, Tokens per sec:   3435
2023-03-15 02:06:12,548 - INFO - __main__ - Epoch  43, Step:   92700, Batch Loss:     6.597350, Lr: 0.000066, Tokens per sec:   3587
2023-03-15 02:06:27,607 - INFO - __main__ - Epoch  43, Step:   92800, Batch Loss:     7.657607, Lr: 0.000066, Tokens per sec:   3575
2023-03-15 02:06:42,941 - INFO - __main__ - Epoch  43, Step:   92900, Batch Loss:     7.618170, Lr: 0.000066, Tokens per sec:   3575
2023-03-15 02:06:58,215 - INFO - __main__ - Epoch  43, Step:   93000, Batch Loss:     6.832313, Lr: 0.000066, Tokens per sec:   3517
2023-03-15 02:07:13,160 - INFO - __main__ - Epoch  43, Step:   93100, Batch Loss:     9.749046, Lr: 0.000066, Tokens per sec:   3604
2023-03-15 02:07:28,479 - INFO - __main__ - Epoch  43, Step:   93200, Batch Loss:     7.990179, Lr: 0.000066, Tokens per sec:   3484
2023-03-15 02:07:43,790 - INFO - __main__ - Epoch  43, Step:   93300, Batch Loss:     7.807187, Lr: 0.000066, Tokens per sec:   3480
2023-03-15 02:07:58,996 - INFO - __main__ - Epoch  43, Step:   93400, Batch Loss:     6.554635, Lr: 0.000066, Tokens per sec:   3491
2023-03-15 02:08:14,143 - INFO - __main__ - Epoch  43, Step:   93500, Batch Loss:    11.902714, Lr: 0.000066, Tokens per sec:   3578
2023-03-15 02:08:29,416 - INFO - __main__ - Epoch  43, Step:   93600, Batch Loss:     4.943120, Lr: 0.000066, Tokens per sec:   3433
2023-03-15 02:08:44,424 - INFO - __main__ - Epoch  43: total training loss 16482.40
2023-03-15 02:08:44,425 - INFO - __main__ - Epoch 44
2023-03-15 02:08:45,287 - INFO - __main__ - Epoch  44, Step:   93700, Batch Loss:     6.757001, Lr: 0.000065, Tokens per sec:   1706
2023-03-15 02:09:01,212 - INFO - __main__ - Epoch  44, Step:   93800, Batch Loss:     5.598900, Lr: 0.000065, Tokens per sec:   3410
2023-03-15 02:09:16,759 - INFO - __main__ - Epoch  44, Step:   93900, Batch Loss:     4.672273, Lr: 0.000065, Tokens per sec:   3492
2023-03-15 02:09:32,089 - INFO - __main__ - Epoch  44, Step:   94000, Batch Loss:     5.382574, Lr: 0.000065, Tokens per sec:   3441
2023-03-15 02:09:47,499 - INFO - __main__ - Epoch  44, Step:   94100, Batch Loss:     5.749179, Lr: 0.000065, Tokens per sec:   3453
2023-03-15 02:10:02,720 - INFO - __main__ - Epoch  44, Step:   94200, Batch Loss:     5.237587, Lr: 0.000065, Tokens per sec:   3479
2023-03-15 02:10:18,137 - INFO - __main__ - Epoch  44, Step:   94300, Batch Loss:     8.869911, Lr: 0.000065, Tokens per sec:   3509
2023-03-15 02:10:33,726 - INFO - __main__ - Epoch  44, Step:   94400, Batch Loss:     7.701197, Lr: 0.000065, Tokens per sec:   3424
2023-03-15 02:10:49,090 - INFO - __main__ - Epoch  44, Step:   94500, Batch Loss:    10.734080, Lr: 0.000065, Tokens per sec:   3501
2023-03-15 02:11:04,489 - INFO - __main__ - Epoch  44, Step:   94600, Batch Loss:     8.068151, Lr: 0.000065, Tokens per sec:   3490
2023-03-15 02:11:19,821 - INFO - __main__ - Epoch  44, Step:   94700, Batch Loss:     4.806592, Lr: 0.000065, Tokens per sec:   3520
2023-03-15 02:11:35,094 - INFO - __main__ - Epoch  44, Step:   94800, Batch Loss:    10.080074, Lr: 0.000065, Tokens per sec:   3554
2023-03-15 02:11:50,453 - INFO - __main__ - Epoch  44, Step:   94900, Batch Loss:     9.345434, Lr: 0.000065, Tokens per sec:   3576
2023-03-15 02:12:05,396 - INFO - __main__ - Epoch  44, Step:   95000, Batch Loss:     9.162658, Lr: 0.000065, Tokens per sec:   3640
2023-03-15 02:12:20,661 - INFO - __main__ - Epoch  44, Step:   95100, Batch Loss:     6.943100, Lr: 0.000065, Tokens per sec:   3579
2023-03-15 02:12:35,919 - INFO - __main__ - Epoch  44, Step:   95200, Batch Loss:     6.444945, Lr: 0.000065, Tokens per sec:   3530
2023-03-15 02:12:51,177 - INFO - __main__ - Epoch  44, Step:   95300, Batch Loss:     7.702601, Lr: 0.000065, Tokens per sec:   3545
2023-03-15 02:13:06,576 - INFO - __main__ - Epoch  44, Step:   95400, Batch Loss:     6.237935, Lr: 0.000065, Tokens per sec:   3501
2023-03-15 02:13:22,017 - INFO - __main__ - Epoch  44, Step:   95500, Batch Loss:     9.743532, Lr: 0.000065, Tokens per sec:   3441
2023-03-15 02:13:37,248 - INFO - __main__ - Epoch  44, Step:   95600, Batch Loss:     9.168839, Lr: 0.000065, Tokens per sec:   3495
2023-03-15 02:13:52,626 - INFO - __main__ - Epoch  44, Step:   95700, Batch Loss:     7.549062, Lr: 0.000065, Tokens per sec:   3512
2023-03-15 02:14:08,026 - INFO - __main__ - Epoch  44, Step:   95800, Batch Loss:     7.660050, Lr: 0.000065, Tokens per sec:   3442
2023-03-15 02:14:19,541 - INFO - __main__ - Epoch  44: total training loss 15961.60
2023-03-15 02:14:19,542 - INFO - __main__ - Epoch 45
2023-03-15 02:14:23,622 - INFO - __main__ - Epoch  45, Step:   95900, Batch Loss:     5.140237, Lr: 0.000064, Tokens per sec:   3072
2023-03-15 02:14:38,926 - INFO - __main__ - Epoch  45, Step:   96000, Batch Loss:     5.775998, Lr: 0.000064, Tokens per sec:   3497
2023-03-15 02:14:53,912 - INFO - __main__ - Epoch  45, Step:   96100, Batch Loss:     7.013446, Lr: 0.000064, Tokens per sec:   3571
2023-03-15 02:15:09,267 - INFO - __main__ - Epoch  45, Step:   96200, Batch Loss:     6.993684, Lr: 0.000064, Tokens per sec:   3528
2023-03-15 02:15:24,512 - INFO - __main__ - Epoch  45, Step:   96300, Batch Loss:     6.062207, Lr: 0.000064, Tokens per sec:   3512
2023-03-15 02:15:40,150 - INFO - __main__ - Epoch  45, Step:   96400, Batch Loss:     7.501134, Lr: 0.000064, Tokens per sec:   3455
2023-03-15 02:15:55,643 - INFO - __main__ - Epoch  45, Step:   96500, Batch Loss:     6.407116, Lr: 0.000064, Tokens per sec:   3425
2023-03-15 02:16:10,882 - INFO - __main__ - Epoch  45, Step:   96600, Batch Loss:     5.198036, Lr: 0.000064, Tokens per sec:   3523
2023-03-15 02:16:26,006 - INFO - __main__ - Epoch  45, Step:   96700, Batch Loss:     7.783777, Lr: 0.000064, Tokens per sec:   3562
2023-03-15 02:16:41,588 - INFO - __main__ - Epoch  45, Step:   96800, Batch Loss:     7.409007, Lr: 0.000064, Tokens per sec:   3497
2023-03-15 02:16:56,951 - INFO - __main__ - Epoch  45, Step:   96900, Batch Loss:     5.780860, Lr: 0.000064, Tokens per sec:   3542
2023-03-15 02:17:12,262 - INFO - __main__ - Epoch  45, Step:   97000, Batch Loss:     7.212671, Lr: 0.000064, Tokens per sec:   3459
2023-03-15 02:17:27,522 - INFO - __main__ - Epoch  45, Step:   97100, Batch Loss:     6.891099, Lr: 0.000064, Tokens per sec:   3519
2023-03-15 02:17:42,655 - INFO - __main__ - Epoch  45, Step:   97200, Batch Loss:     8.241174, Lr: 0.000064, Tokens per sec:   3646
2023-03-15 02:17:57,897 - INFO - __main__ - Epoch  45, Step:   97300, Batch Loss:     6.080436, Lr: 0.000064, Tokens per sec:   3527
2023-03-15 02:18:12,779 - INFO - __main__ - Epoch  45, Step:   97400, Batch Loss:     7.375139, Lr: 0.000064, Tokens per sec:   3614
2023-03-15 02:18:28,120 - INFO - __main__ - Epoch  45, Step:   97500, Batch Loss:     5.147671, Lr: 0.000064, Tokens per sec:   3482
2023-03-15 02:18:43,152 - INFO - __main__ - Epoch  45, Step:   97600, Batch Loss:     6.331549, Lr: 0.000064, Tokens per sec:   3533
2023-03-15 02:18:58,442 - INFO - __main__ - Epoch  45, Step:   97700, Batch Loss:     6.520932, Lr: 0.000064, Tokens per sec:   3517
2023-03-15 02:19:13,826 - INFO - __main__ - Epoch  45, Step:   97800, Batch Loss:     9.587038, Lr: 0.000064, Tokens per sec:   3481
2023-03-15 02:19:28,998 - INFO - __main__ - Epoch  45, Step:   97900, Batch Loss:    12.216875, Lr: 0.000064, Tokens per sec:   3637
2023-03-15 02:19:44,169 - INFO - __main__ - Epoch  45, Step:   98000, Batch Loss:     7.827578, Lr: 0.000064, Tokens per sec:   3592
2023-03-15 02:19:52,618 - INFO - __main__ - Epoch  45: total training loss 15484.57
2023-03-15 02:19:52,619 - INFO - __main__ - Epoch 46
2023-03-15 02:19:59,785 - INFO - __main__ - Epoch  46, Step:   98100, Batch Loss:     7.211945, Lr: 0.000064, Tokens per sec:   3371
2023-03-15 02:20:14,852 - INFO - __main__ - Epoch  46, Step:   98200, Batch Loss:     4.831621, Lr: 0.000064, Tokens per sec:   3582
2023-03-15 02:20:29,531 - INFO - __main__ - Epoch  46, Step:   98300, Batch Loss:     6.937437, Lr: 0.000064, Tokens per sec:   3691
2023-03-15 02:20:44,858 - INFO - __main__ - Epoch  46, Step:   98400, Batch Loss:     7.869470, Lr: 0.000064, Tokens per sec:   3511
2023-03-15 02:21:00,392 - INFO - __main__ - Epoch  46, Step:   98500, Batch Loss:     6.178639, Lr: 0.000064, Tokens per sec:   3500
2023-03-15 02:21:15,889 - INFO - __main__ - Epoch  46, Step:   98600, Batch Loss:     6.473698, Lr: 0.000064, Tokens per sec:   3463
2023-03-15 02:21:31,354 - INFO - __main__ - Epoch  46, Step:   98700, Batch Loss:     6.879514, Lr: 0.000064, Tokens per sec:   3487
2023-03-15 02:21:46,651 - INFO - __main__ - Epoch  46, Step:   98800, Batch Loss:     5.490634, Lr: 0.000064, Tokens per sec:   3578
2023-03-15 02:22:01,693 - INFO - __main__ - Epoch  46, Step:   98900, Batch Loss:     8.839551, Lr: 0.000064, Tokens per sec:   3509
2023-03-15 02:22:16,762 - INFO - __main__ - Epoch  46, Step:   99000, Batch Loss:     7.153842, Lr: 0.000064, Tokens per sec:   3531
2023-03-15 02:22:31,901 - INFO - __main__ - Epoch  46, Step:   99100, Batch Loss:     6.142506, Lr: 0.000064, Tokens per sec:   3554
2023-03-15 02:22:47,057 - INFO - __main__ - Epoch  46, Step:   99200, Batch Loss:     6.851137, Lr: 0.000064, Tokens per sec:   3634
2023-03-15 02:23:02,455 - INFO - __main__ - Epoch  46, Step:   99300, Batch Loss:     5.341195, Lr: 0.000064, Tokens per sec:   3455
2023-03-15 02:23:17,390 - INFO - __main__ - Epoch  46, Step:   99400, Batch Loss:     9.494206, Lr: 0.000064, Tokens per sec:   3644
2023-03-15 02:23:32,790 - INFO - __main__ - Epoch  46, Step:   99500, Batch Loss:     6.267494, Lr: 0.000064, Tokens per sec:   3474
2023-03-15 02:23:48,138 - INFO - __main__ - Epoch  46, Step:   99600, Batch Loss:     7.651745, Lr: 0.000064, Tokens per sec:   3515
2023-03-15 02:24:03,435 - INFO - __main__ - Epoch  46, Step:   99700, Batch Loss:     6.962804, Lr: 0.000064, Tokens per sec:   3545
2023-03-15 02:24:18,224 - INFO - __main__ - Epoch  46, Step:   99800, Batch Loss:     7.977030, Lr: 0.000064, Tokens per sec:   3625
2023-03-15 02:24:33,343 - INFO - __main__ - Epoch  46, Step:   99900, Batch Loss:     6.942514, Lr: 0.000064, Tokens per sec:   3552
2023-03-15 02:24:48,775 - INFO - __main__ - Epoch  46, Step:  100000, Batch Loss:     8.808410, Lr: 0.000064, Tokens per sec:   3475
2023-03-15 02:25:04,205 - INFO - __main__ - Epoch  46, Step:  100100, Batch Loss:     6.611677, Lr: 0.000064, Tokens per sec:   3421
2023-03-15 02:25:19,703 - INFO - __main__ - Epoch  46, Step:  100200, Batch Loss:     6.430367, Lr: 0.000064, Tokens per sec:   3510
2023-03-15 02:25:24,991 - INFO - __main__ - Epoch  46: total training loss 14976.99
2023-03-15 02:25:24,991 - INFO - __main__ - Epoch 47
2023-03-15 02:25:35,428 - INFO - __main__ - Epoch  47, Step:  100300, Batch Loss:     9.753835, Lr: 0.000063, Tokens per sec:   3377
2023-03-15 02:25:50,970 - INFO - __main__ - Epoch  47, Step:  100400, Batch Loss:     7.091327, Lr: 0.000063, Tokens per sec:   3450
2023-03-15 02:26:06,171 - INFO - __main__ - Epoch  47, Step:  100500, Batch Loss:     4.196029, Lr: 0.000063, Tokens per sec:   3581
2023-03-15 02:26:21,669 - INFO - __main__ - Epoch  47, Step:  100600, Batch Loss:     4.785329, Lr: 0.000063, Tokens per sec:   3485
2023-03-15 02:26:36,619 - INFO - __main__ - Epoch  47, Step:  100700, Batch Loss:    11.170596, Lr: 0.000063, Tokens per sec:   3610
2023-03-15 02:26:52,133 - INFO - __main__ - Epoch  47, Step:  100800, Batch Loss:     5.253608, Lr: 0.000063, Tokens per sec:   3486
2023-03-15 02:27:07,460 - INFO - __main__ - Epoch  47, Step:  100900, Batch Loss:     7.692242, Lr: 0.000063, Tokens per sec:   3522
2023-03-15 02:27:22,931 - INFO - __main__ - Epoch  47, Step:  101000, Batch Loss:     5.365292, Lr: 0.000063, Tokens per sec:   3427
2023-03-15 02:27:38,378 - INFO - __main__ - Epoch  47, Step:  101100, Batch Loss:     6.031352, Lr: 0.000063, Tokens per sec:   3501
2023-03-15 02:27:53,750 - INFO - __main__ - Epoch  47, Step:  101200, Batch Loss:     4.526822, Lr: 0.000063, Tokens per sec:   3554
2023-03-15 02:28:09,326 - INFO - __main__ - Epoch  47, Step:  101300, Batch Loss:     8.006373, Lr: 0.000063, Tokens per sec:   3409
2023-03-15 02:28:24,484 - INFO - __main__ - Epoch  47, Step:  101400, Batch Loss:     9.174467, Lr: 0.000063, Tokens per sec:   3562
2023-03-15 02:28:39,458 - INFO - __main__ - Epoch  47, Step:  101500, Batch Loss:     5.499155, Lr: 0.000063, Tokens per sec:   3613
2023-03-15 02:28:54,746 - INFO - __main__ - Epoch  47, Step:  101600, Batch Loss:     6.128489, Lr: 0.000063, Tokens per sec:   3507
2023-03-15 02:29:10,111 - INFO - __main__ - Epoch  47, Step:  101700, Batch Loss:     9.653732, Lr: 0.000063, Tokens per sec:   3448
2023-03-15 02:29:25,431 - INFO - __main__ - Epoch  47, Step:  101800, Batch Loss:     6.166399, Lr: 0.000063, Tokens per sec:   3544
2023-03-15 02:29:40,661 - INFO - __main__ - Epoch  47, Step:  101900, Batch Loss:     5.376789, Lr: 0.000063, Tokens per sec:   3484
2023-03-15 02:29:55,890 - INFO - __main__ - Epoch  47, Step:  102000, Batch Loss:     8.614737, Lr: 0.000063, Tokens per sec:   3509
2023-03-15 02:30:11,328 - INFO - __main__ - Epoch  47, Step:  102100, Batch Loss:     6.942312, Lr: 0.000063, Tokens per sec:   3531
2023-03-15 02:30:25,894 - INFO - __main__ - Epoch  47, Step:  102200, Batch Loss:     7.064079, Lr: 0.000063, Tokens per sec:   3646
2023-03-15 02:30:41,334 - INFO - __main__ - Epoch  47, Step:  102300, Batch Loss:     4.704099, Lr: 0.000063, Tokens per sec:   3499
2023-03-15 02:30:56,454 - INFO - __main__ - Epoch  47, Step:  102400, Batch Loss:     8.953919, Lr: 0.000063, Tokens per sec:   3610
2023-03-15 02:30:58,543 - INFO - __main__ - Epoch  47: total training loss 14592.61
2023-03-15 02:30:58,544 - INFO - __main__ - Epoch 48
2023-03-15 02:31:12,356 - INFO - __main__ - Epoch  48, Step:  102500, Batch Loss:     7.686826, Lr: 0.000062, Tokens per sec:   3374
2023-03-15 02:31:28,018 - INFO - __main__ - Epoch  48, Step:  102600, Batch Loss:     6.904149, Lr: 0.000062, Tokens per sec:   3442
2023-03-15 02:31:43,665 - INFO - __main__ - Epoch  48, Step:  102700, Batch Loss:     4.479569, Lr: 0.000062, Tokens per sec:   3439
2023-03-15 02:31:59,164 - INFO - __main__ - Epoch  48, Step:  102800, Batch Loss:     7.516862, Lr: 0.000062, Tokens per sec:   3467
2023-03-15 02:32:14,554 - INFO - __main__ - Epoch  48, Step:  102900, Batch Loss:     6.430208, Lr: 0.000062, Tokens per sec:   3507
2023-03-15 02:32:30,004 - INFO - __main__ - Epoch  48, Step:  103000, Batch Loss:     5.262382, Lr: 0.000062, Tokens per sec:   3506
2023-03-15 02:32:45,471 - INFO - __main__ - Epoch  48, Step:  103100, Batch Loss:     4.567364, Lr: 0.000062, Tokens per sec:   3445
2023-03-15 02:33:00,922 - INFO - __main__ - Epoch  48, Step:  103200, Batch Loss:     4.940638, Lr: 0.000062, Tokens per sec:   3483
2023-03-15 02:33:16,380 - INFO - __main__ - Epoch  48, Step:  103300, Batch Loss:     7.653255, Lr: 0.000062, Tokens per sec:   3467
2023-03-15 02:33:31,647 - INFO - __main__ - Epoch  48, Step:  103400, Batch Loss:     5.346802, Lr: 0.000062, Tokens per sec:   3529
2023-03-15 02:33:47,200 - INFO - __main__ - Epoch  48, Step:  103500, Batch Loss:     7.096038, Lr: 0.000062, Tokens per sec:   3448
2023-03-15 02:34:02,544 - INFO - __main__ - Epoch  48, Step:  103600, Batch Loss:     6.920873, Lr: 0.000062, Tokens per sec:   3474
2023-03-15 02:34:17,900 - INFO - __main__ - Epoch  48, Step:  103700, Batch Loss:     6.615869, Lr: 0.000062, Tokens per sec:   3568
2023-03-15 02:34:33,324 - INFO - __main__ - Epoch  48, Step:  103800, Batch Loss:     8.930690, Lr: 0.000062, Tokens per sec:   3499
2023-03-15 02:34:48,791 - INFO - __main__ - Epoch  48, Step:  103900, Batch Loss:     7.223306, Lr: 0.000062, Tokens per sec:   3440
2023-03-15 02:35:04,295 - INFO - __main__ - Epoch  48, Step:  104000, Batch Loss:     5.144818, Lr: 0.000062, Tokens per sec:   3448
2023-03-15 02:35:19,782 - INFO - __main__ - Epoch  48, Step:  104100, Batch Loss:     7.155529, Lr: 0.000062, Tokens per sec:   3482
2023-03-15 02:35:35,299 - INFO - __main__ - Epoch  48, Step:  104200, Batch Loss:     6.443930, Lr: 0.000062, Tokens per sec:   3501
2023-03-15 02:35:50,623 - INFO - __main__ - Epoch  48, Step:  104300, Batch Loss:     5.474442, Lr: 0.000062, Tokens per sec:   3472
2023-03-15 02:36:05,959 - INFO - __main__ - Epoch  48, Step:  104400, Batch Loss:     8.051906, Lr: 0.000062, Tokens per sec:   3536
2023-03-15 02:36:21,519 - INFO - __main__ - Epoch  48, Step:  104500, Batch Loss:     9.510721, Lr: 0.000062, Tokens per sec:   3521
2023-03-15 02:36:35,832 - INFO - __main__ - Epoch  48: total training loss 14177.85
2023-03-15 02:36:35,833 - INFO - __main__ - Epoch 49
2023-03-15 02:36:37,373 - INFO - __main__ - Epoch  49, Step:  104600, Batch Loss:     7.028961, Lr: 0.000062, Tokens per sec:   2761
2023-03-15 02:36:52,931 - INFO - __main__ - Epoch  49, Step:  104700, Batch Loss:     4.898281, Lr: 0.000062, Tokens per sec:   3438
2023-03-15 02:37:08,202 - INFO - __main__ - Epoch  49, Step:  104800, Batch Loss:     5.173456, Lr: 0.000062, Tokens per sec:   3539
2023-03-15 02:37:23,568 - INFO - __main__ - Epoch  49, Step:  104900, Batch Loss:     8.785059, Lr: 0.000062, Tokens per sec:   3526
2023-03-15 02:37:38,935 - INFO - __main__ - Epoch  49, Step:  105000, Batch Loss:     6.184990, Lr: 0.000062, Tokens per sec:   3532
2023-03-15 02:37:54,473 - INFO - __main__ - Epoch  49, Step:  105100, Batch Loss:     6.305583, Lr: 0.000062, Tokens per sec:   3443
2023-03-15 02:38:09,921 - INFO - __main__ - Epoch  49, Step:  105200, Batch Loss:     6.608559, Lr: 0.000062, Tokens per sec:   3442
2023-03-15 02:38:25,208 - INFO - __main__ - Epoch  49, Step:  105300, Batch Loss:     4.768421, Lr: 0.000062, Tokens per sec:   3540
2023-03-15 02:38:40,653 - INFO - __main__ - Epoch  49, Step:  105400, Batch Loss:     5.613623, Lr: 0.000062, Tokens per sec:   3524
2023-03-15 02:38:55,997 - INFO - __main__ - Epoch  49, Step:  105500, Batch Loss:     5.186496, Lr: 0.000062, Tokens per sec:   3497
2023-03-15 02:39:11,610 - INFO - __main__ - Epoch  49, Step:  105600, Batch Loss:     5.100478, Lr: 0.000062, Tokens per sec:   3450
2023-03-15 02:39:27,065 - INFO - __main__ - Epoch  49, Step:  105700, Batch Loss:     6.378744, Lr: 0.000062, Tokens per sec:   3510
2023-03-15 02:39:42,113 - INFO - __main__ - Epoch  49, Step:  105800, Batch Loss:     7.229527, Lr: 0.000062, Tokens per sec:   3604
2023-03-15 02:39:57,579 - INFO - __main__ - Epoch  49, Step:  105900, Batch Loss:     5.878457, Lr: 0.000062, Tokens per sec:   3522
2023-03-15 02:40:12,933 - INFO - __main__ - Epoch  49, Step:  106000, Batch Loss:     6.542434, Lr: 0.000062, Tokens per sec:   3463
2023-03-15 02:40:28,261 - INFO - __main__ - Epoch  49, Step:  106100, Batch Loss:     6.359257, Lr: 0.000062, Tokens per sec:   3516
2023-03-15 02:40:43,658 - INFO - __main__ - Epoch  49, Step:  106200, Batch Loss:     5.057649, Lr: 0.000062, Tokens per sec:   3508
2023-03-15 02:40:59,186 - INFO - __main__ - Epoch  49, Step:  106300, Batch Loss:     5.339180, Lr: 0.000062, Tokens per sec:   3471
2023-03-15 02:41:14,526 - INFO - __main__ - Epoch  49, Step:  106400, Batch Loss:     6.523888, Lr: 0.000062, Tokens per sec:   3502
2023-03-15 02:41:29,937 - INFO - __main__ - Epoch  49, Step:  106500, Batch Loss:     8.514556, Lr: 0.000062, Tokens per sec:   3464
2023-03-15 02:41:45,038 - INFO - __main__ - Epoch  49, Step:  106600, Batch Loss:     7.923791, Lr: 0.000062, Tokens per sec:   3619
2023-03-15 02:42:00,338 - INFO - __main__ - Epoch  49, Step:  106700, Batch Loss:     7.280129, Lr: 0.000062, Tokens per sec:   3409
2023-03-15 02:42:11,402 - INFO - __main__ - Epoch  49: total training loss 13781.38
2023-03-15 02:42:11,403 - INFO - __main__ - Epoch 50
2023-03-15 02:42:16,251 - INFO - __main__ - Epoch  50, Step:  106800, Batch Loss:     5.644710, Lr: 0.000061, Tokens per sec:   3268
2023-03-15 02:42:31,629 - INFO - __main__ - Epoch  50, Step:  106900, Batch Loss:     6.245994, Lr: 0.000061, Tokens per sec:   3506
2023-03-15 02:42:47,090 - INFO - __main__ - Epoch  50, Step:  107000, Batch Loss:     5.463681, Lr: 0.000061, Tokens per sec:   3572
2023-03-15 02:43:02,508 - INFO - __main__ - Epoch  50, Step:  107100, Batch Loss:     4.848040, Lr: 0.000061, Tokens per sec:   3499
2023-03-15 02:43:17,942 - INFO - __main__ - Epoch  50, Step:  107200, Batch Loss:     5.663624, Lr: 0.000061, Tokens per sec:   3462
2023-03-15 02:43:33,280 - INFO - __main__ - Epoch  50, Step:  107300, Batch Loss:     4.961652, Lr: 0.000061, Tokens per sec:   3488
2023-03-15 02:43:48,290 - INFO - __main__ - Epoch  50, Step:  107400, Batch Loss:     4.140506, Lr: 0.000061, Tokens per sec:   3559
2023-03-15 02:44:03,487 - INFO - __main__ - Epoch  50, Step:  107500, Batch Loss:     5.908602, Lr: 0.000061, Tokens per sec:   3577
2023-03-15 02:44:19,046 - INFO - __main__ - Epoch  50, Step:  107600, Batch Loss:     5.824738, Lr: 0.000061, Tokens per sec:   3471
2023-03-15 02:44:34,634 - INFO - __main__ - Epoch  50, Step:  107700, Batch Loss:     6.185084, Lr: 0.000061, Tokens per sec:   3476
2023-03-15 02:44:50,099 - INFO - __main__ - Epoch  50, Step:  107800, Batch Loss:     4.423417, Lr: 0.000061, Tokens per sec:   3420
2023-03-15 02:45:05,486 - INFO - __main__ - Epoch  50, Step:  107900, Batch Loss:     6.785610, Lr: 0.000061, Tokens per sec:   3479
2023-03-15 02:45:20,793 - INFO - __main__ - Epoch  50, Step:  108000, Batch Loss:     6.020762, Lr: 0.000061, Tokens per sec:   3562
2023-03-15 02:45:36,219 - INFO - __main__ - Epoch  50, Step:  108100, Batch Loss:     7.332444, Lr: 0.000061, Tokens per sec:   3509
2023-03-15 02:45:51,821 - INFO - __main__ - Epoch  50, Step:  108200, Batch Loss:     4.406550, Lr: 0.000061, Tokens per sec:   3415
2023-03-15 02:46:07,289 - INFO - __main__ - Epoch  50, Step:  108300, Batch Loss:     4.978098, Lr: 0.000061, Tokens per sec:   3509
2023-03-15 02:46:22,800 - INFO - __main__ - Epoch  50, Step:  108400, Batch Loss:     7.925583, Lr: 0.000061, Tokens per sec:   3447
2023-03-15 02:46:38,315 - INFO - __main__ - Epoch  50, Step:  108500, Batch Loss:     7.024830, Lr: 0.000061, Tokens per sec:   3484
2023-03-15 02:46:53,781 - INFO - __main__ - Epoch  50, Step:  108600, Batch Loss:     5.779374, Lr: 0.000061, Tokens per sec:   3459
2023-03-15 02:47:09,146 - INFO - __main__ - Epoch  50, Step:  108700, Batch Loss:     4.975746, Lr: 0.000061, Tokens per sec:   3489
2023-03-15 02:47:24,426 - INFO - __main__ - Epoch  50, Step:  108800, Batch Loss:     8.079517, Lr: 0.000061, Tokens per sec:   3518
2023-03-15 02:47:39,909 - INFO - __main__ - Epoch  50, Step:  108900, Batch Loss:     4.027108, Lr: 0.000061, Tokens per sec:   3487
2023-03-15 02:47:47,673 - INFO - __main__ - Epoch  50: total training loss 13351.12
2023-03-15 02:47:47,675 - INFO - __main__ - Epoch 51
2023-03-15 02:47:55,786 - INFO - __main__ - Epoch  51, Step:  109000, Batch Loss:     7.118614, Lr: 0.000061, Tokens per sec:   3305
2023-03-15 02:48:11,115 - INFO - __main__ - Epoch  51, Step:  109100, Batch Loss:     4.860798, Lr: 0.000061, Tokens per sec:   3457
2023-03-15 02:48:26,609 - INFO - __main__ - Epoch  51, Step:  109200, Batch Loss:     4.907845, Lr: 0.000061, Tokens per sec:   3484
2023-03-15 02:48:42,056 - INFO - __main__ - Epoch  51, Step:  109300, Batch Loss:     6.097114, Lr: 0.000061, Tokens per sec:   3525
2023-03-15 02:48:57,479 - INFO - __main__ - Epoch  51, Step:  109400, Batch Loss:     6.352094, Lr: 0.000061, Tokens per sec:   3507
2023-03-15 02:49:12,622 - INFO - __main__ - Epoch  51, Step:  109500, Batch Loss:     8.096066, Lr: 0.000061, Tokens per sec:   3570
2023-03-15 02:49:28,133 - INFO - __main__ - Epoch  51, Step:  109600, Batch Loss:     5.386288, Lr: 0.000061, Tokens per sec:   3439
2023-03-15 02:49:43,588 - INFO - __main__ - Epoch  51, Step:  109700, Batch Loss:     5.466952, Lr: 0.000061, Tokens per sec:   3454
2023-03-15 02:49:58,726 - INFO - __main__ - Epoch  51, Step:  109800, Batch Loss:     4.458997, Lr: 0.000061, Tokens per sec:   3567
2023-03-15 02:50:14,360 - INFO - __main__ - Epoch  51, Step:  109900, Batch Loss:     5.869229, Lr: 0.000061, Tokens per sec:   3459
2023-03-15 02:50:29,456 - INFO - __main__ - Epoch  51, Step:  110000, Batch Loss:     5.997222, Lr: 0.000061, Tokens per sec:   3581
2023-03-15 02:50:44,922 - INFO - __main__ - Epoch  51, Step:  110100, Batch Loss:     6.852587, Lr: 0.000061, Tokens per sec:   3503
2023-03-15 02:51:00,296 - INFO - __main__ - Epoch  51, Step:  110200, Batch Loss:     3.694560, Lr: 0.000061, Tokens per sec:   3502
2023-03-15 02:51:15,595 - INFO - __main__ - Epoch  51, Step:  110300, Batch Loss:     4.760051, Lr: 0.000061, Tokens per sec:   3610
2023-03-15 02:51:30,524 - INFO - __main__ - Epoch  51, Step:  110400, Batch Loss:     6.998141, Lr: 0.000061, Tokens per sec:   3535
2023-03-15 02:51:45,735 - INFO - __main__ - Epoch  51, Step:  110500, Batch Loss:     6.286431, Lr: 0.000061, Tokens per sec:   3468
2023-03-15 02:52:01,037 - INFO - __main__ - Epoch  51, Step:  110600, Batch Loss:     7.710703, Lr: 0.000061, Tokens per sec:   3578
2023-03-15 02:52:15,948 - INFO - __main__ - Epoch  51, Step:  110700, Batch Loss:     5.627940, Lr: 0.000061, Tokens per sec:   3613
2023-03-15 02:52:30,864 - INFO - __main__ - Epoch  51, Step:  110800, Batch Loss:     7.213733, Lr: 0.000061, Tokens per sec:   3608
2023-03-15 02:52:45,996 - INFO - __main__ - Epoch  51, Step:  110900, Batch Loss:     4.679655, Lr: 0.000061, Tokens per sec:   3529
2023-03-15 02:53:01,250 - INFO - __main__ - Epoch  51, Step:  111000, Batch Loss:     8.089990, Lr: 0.000061, Tokens per sec:   3522
2023-03-15 02:53:16,425 - INFO - __main__ - Epoch  51, Step:  111100, Batch Loss:     6.923437, Lr: 0.000061, Tokens per sec:   3555
2023-03-15 02:53:20,999 - INFO - __main__ - Epoch  51: total training loss 13052.38
2023-03-15 02:53:21,000 - INFO - __main__ - Epoch 52
2023-03-15 02:53:32,239 - INFO - __main__ - Epoch  52, Step:  111200, Batch Loss:     4.318987, Lr: 0.000060, Tokens per sec:   3472
2023-03-15 02:53:47,683 - INFO - __main__ - Epoch  52, Step:  111300, Batch Loss:     5.449378, Lr: 0.000060, Tokens per sec:   3427
2023-03-15 02:54:02,946 - INFO - __main__ - Epoch  52, Step:  111400, Batch Loss:     5.259594, Lr: 0.000060, Tokens per sec:   3526
2023-03-15 02:54:18,401 - INFO - __main__ - Epoch  52, Step:  111500, Batch Loss:     5.758434, Lr: 0.000060, Tokens per sec:   3510
2023-03-15 02:54:33,792 - INFO - __main__ - Epoch  52, Step:  111600, Batch Loss:     5.694443, Lr: 0.000060, Tokens per sec:   3444
2023-03-15 02:54:49,151 - INFO - __main__ - Epoch  52, Step:  111700, Batch Loss:     7.695890, Lr: 0.000060, Tokens per sec:   3532
2023-03-15 02:55:04,592 - INFO - __main__ - Epoch  52, Step:  111800, Batch Loss:     4.038920, Lr: 0.000060, Tokens per sec:   3576
2023-03-15 02:55:19,956 - INFO - __main__ - Epoch  52, Step:  111900, Batch Loss:     3.761289, Lr: 0.000060, Tokens per sec:   3532
2023-03-15 02:55:35,189 - INFO - __main__ - Epoch  52, Step:  112000, Batch Loss:     7.141099, Lr: 0.000060, Tokens per sec:   3517
2023-03-15 02:55:50,612 - INFO - __main__ - Epoch  52, Step:  112100, Batch Loss:     5.273571, Lr: 0.000060, Tokens per sec:   3502
2023-03-15 02:56:05,840 - INFO - __main__ - Epoch  52, Step:  112200, Batch Loss:     7.148508, Lr: 0.000060, Tokens per sec:   3477
2023-03-15 02:56:21,444 - INFO - __main__ - Epoch  52, Step:  112300, Batch Loss:     5.887335, Lr: 0.000060, Tokens per sec:   3452
2023-03-15 02:56:36,699 - INFO - __main__ - Epoch  52, Step:  112400, Batch Loss:     5.368929, Lr: 0.000060, Tokens per sec:   3502
2023-03-15 02:56:52,070 - INFO - __main__ - Epoch  52, Step:  112500, Batch Loss:     6.045831, Lr: 0.000060, Tokens per sec:   3600
2023-03-15 02:57:07,542 - INFO - __main__ - Epoch  52, Step:  112600, Batch Loss:     7.822541, Lr: 0.000060, Tokens per sec:   3465
2023-03-15 02:57:22,800 - INFO - __main__ - Epoch  52, Step:  112700, Batch Loss:     4.703579, Lr: 0.000060, Tokens per sec:   3530
2023-03-15 02:57:38,196 - INFO - __main__ - Epoch  52, Step:  112800, Batch Loss:     4.790796, Lr: 0.000060, Tokens per sec:   3425
2023-03-15 02:57:53,751 - INFO - __main__ - Epoch  52, Step:  112900, Batch Loss:     4.813221, Lr: 0.000060, Tokens per sec:   3406
2023-03-15 02:58:09,104 - INFO - __main__ - Epoch  52, Step:  113000, Batch Loss:     8.878798, Lr: 0.000060, Tokens per sec:   3567
2023-03-15 02:58:24,316 - INFO - __main__ - Epoch  52, Step:  113100, Batch Loss:     7.334527, Lr: 0.000060, Tokens per sec:   3518
2023-03-15 02:58:39,540 - INFO - __main__ - Epoch  52, Step:  113200, Batch Loss:     5.289900, Lr: 0.000060, Tokens per sec:   3537
2023-03-15 02:58:54,656 - INFO - __main__ - Epoch  52, Step:  113300, Batch Loss:     6.082286, Lr: 0.000060, Tokens per sec:   3557
2023-03-15 02:58:55,774 - INFO - __main__ - Epoch  52: total training loss 12653.38
2023-03-15 02:58:55,775 - INFO - __main__ - Epoch 53
2023-03-15 02:59:10,206 - INFO - __main__ - Epoch  53, Step:  113400, Batch Loss:     4.474129, Lr: 0.000059, Tokens per sec:   3426
2023-03-15 02:59:25,698 - INFO - __main__ - Epoch  53, Step:  113500, Batch Loss:     4.495511, Lr: 0.000059, Tokens per sec:   3425
2023-03-15 02:59:40,903 - INFO - __main__ - Epoch  53, Step:  113600, Batch Loss:     6.208341, Lr: 0.000059, Tokens per sec:   3584
2023-03-15 02:59:56,462 - INFO - __main__ - Epoch  53, Step:  113700, Batch Loss:     4.700252, Lr: 0.000059, Tokens per sec:   3439
2023-03-15 03:00:11,704 - INFO - __main__ - Epoch  53, Step:  113800, Batch Loss:     5.794680, Lr: 0.000059, Tokens per sec:   3521
2023-03-15 03:00:26,920 - INFO - __main__ - Epoch  53, Step:  113900, Batch Loss:     6.360809, Lr: 0.000059, Tokens per sec:   3559
2023-03-15 03:00:42,264 - INFO - __main__ - Epoch  53, Step:  114000, Batch Loss:     6.637392, Lr: 0.000059, Tokens per sec:   3520
2023-03-15 03:00:56,822 - INFO - __main__ - Epoch  53, Step:  114100, Batch Loss:     7.391442, Lr: 0.000059, Tokens per sec:   3744
2023-03-15 03:01:11,980 - INFO - __main__ - Epoch  53, Step:  114200, Batch Loss:     5.186048, Lr: 0.000059, Tokens per sec:   3514
2023-03-15 03:01:27,238 - INFO - __main__ - Epoch  53, Step:  114300, Batch Loss:     5.860288, Lr: 0.000059, Tokens per sec:   3502
2023-03-15 03:01:42,619 - INFO - __main__ - Epoch  53, Step:  114400, Batch Loss:     6.763911, Lr: 0.000059, Tokens per sec:   3535
2023-03-15 03:01:57,960 - INFO - __main__ - Epoch  53, Step:  114500, Batch Loss:     3.720994, Lr: 0.000059, Tokens per sec:   3552
2023-03-15 03:02:13,370 - INFO - __main__ - Epoch  53, Step:  114600, Batch Loss:     4.780023, Lr: 0.000059, Tokens per sec:   3450
2023-03-15 03:02:28,682 - INFO - __main__ - Epoch  53, Step:  114700, Batch Loss:     5.295248, Lr: 0.000059, Tokens per sec:   3523
2023-03-15 03:02:43,938 - INFO - __main__ - Epoch  53, Step:  114800, Batch Loss:     6.026671, Lr: 0.000059, Tokens per sec:   3506
2023-03-15 03:02:58,765 - INFO - __main__ - Epoch  53, Step:  114900, Batch Loss:     6.052972, Lr: 0.000059, Tokens per sec:   3655
2023-03-15 03:03:14,144 - INFO - __main__ - Epoch  53, Step:  115000, Batch Loss:     6.454854, Lr: 0.000059, Tokens per sec:   3547
2023-03-15 03:03:29,455 - INFO - __main__ - Epoch  53, Step:  115100, Batch Loss:     7.157598, Lr: 0.000059, Tokens per sec:   3556
2023-03-15 03:03:44,900 - INFO - __main__ - Epoch  53, Step:  115200, Batch Loss:     4.511179, Lr: 0.000059, Tokens per sec:   3462
2023-03-15 03:04:00,232 - INFO - __main__ - Epoch  53, Step:  115300, Batch Loss:     5.534884, Lr: 0.000059, Tokens per sec:   3488
2023-03-15 03:04:15,545 - INFO - __main__ - Epoch  53, Step:  115400, Batch Loss:     3.680151, Lr: 0.000059, Tokens per sec:   3482
2023-03-15 03:04:28,885 - INFO - __main__ - Epoch  53: total training loss 12344.70
2023-03-15 03:04:28,886 - INFO - __main__ - Epoch 54
2023-03-15 03:04:31,155 - INFO - __main__ - Epoch  54, Step:  115500, Batch Loss:     5.970005, Lr: 0.000059, Tokens per sec:   3135
2023-03-15 03:04:46,621 - INFO - __main__ - Epoch  54, Step:  115600, Batch Loss:     4.345762, Lr: 0.000059, Tokens per sec:   3508
2023-03-15 03:05:01,811 - INFO - __main__ - Epoch  54, Step:  115700, Batch Loss:     5.223226, Lr: 0.000059, Tokens per sec:   3540
2023-03-15 03:05:17,385 - INFO - __main__ - Epoch  54, Step:  115800, Batch Loss:     5.490225, Lr: 0.000059, Tokens per sec:   3460
2023-03-15 03:05:32,868 - INFO - __main__ - Epoch  54, Step:  115900, Batch Loss:     3.557678, Lr: 0.000059, Tokens per sec:   3400
2023-03-15 03:05:48,523 - INFO - __main__ - Epoch  54, Step:  116000, Batch Loss:     4.786359, Lr: 0.000059, Tokens per sec:   3394
2023-03-15 03:06:03,950 - INFO - __main__ - Epoch  54, Step:  116100, Batch Loss:     6.214888, Lr: 0.000059, Tokens per sec:   3461
2023-03-15 03:06:19,186 - INFO - __main__ - Epoch  54, Step:  116200, Batch Loss:     4.641016, Lr: 0.000059, Tokens per sec:   3515
2023-03-15 03:06:34,567 - INFO - __main__ - Epoch  54, Step:  116300, Batch Loss:     7.815720, Lr: 0.000059, Tokens per sec:   3472
2023-03-15 03:06:49,660 - INFO - __main__ - Epoch  54, Step:  116400, Batch Loss:     7.635155, Lr: 0.000059, Tokens per sec:   3669
2023-03-15 03:07:05,135 - INFO - __main__ - Epoch  54, Step:  116500, Batch Loss:     6.068841, Lr: 0.000059, Tokens per sec:   3479
2023-03-15 03:07:20,410 - INFO - __main__ - Epoch  54, Step:  116600, Batch Loss:     4.325873, Lr: 0.000059, Tokens per sec:   3486
2023-03-15 03:07:35,754 - INFO - __main__ - Epoch  54, Step:  116700, Batch Loss:     3.499882, Lr: 0.000059, Tokens per sec:   3547
2023-03-15 03:07:51,191 - INFO - __main__ - Epoch  54, Step:  116800, Batch Loss:     5.055185, Lr: 0.000059, Tokens per sec:   3503
2023-03-15 03:08:06,521 - INFO - __main__ - Epoch  54, Step:  116900, Batch Loss:     4.958555, Lr: 0.000059, Tokens per sec:   3553
2023-03-15 03:08:21,829 - INFO - __main__ - Epoch  54, Step:  117000, Batch Loss:     5.887570, Lr: 0.000059, Tokens per sec:   3551
2023-03-15 03:08:37,511 - INFO - __main__ - Epoch  54, Step:  117100, Batch Loss:     4.915641, Lr: 0.000059, Tokens per sec:   3396
2023-03-15 03:08:52,805 - INFO - __main__ - Epoch  54, Step:  117200, Batch Loss:     4.760896, Lr: 0.000059, Tokens per sec:   3509
2023-03-15 03:09:08,042 - INFO - __main__ - Epoch  54, Step:  117300, Batch Loss:     6.656275, Lr: 0.000059, Tokens per sec:   3507
2023-03-15 03:09:23,164 - INFO - __main__ - Epoch  54, Step:  117400, Batch Loss:     5.060899, Lr: 0.000059, Tokens per sec:   3583
2023-03-15 03:09:38,557 - INFO - __main__ - Epoch  54, Step:  117500, Batch Loss:     5.878983, Lr: 0.000059, Tokens per sec:   3525
2023-03-15 03:09:53,900 - INFO - __main__ - Epoch  54, Step:  117600, Batch Loss:     7.572696, Lr: 0.000059, Tokens per sec:   3504
2023-03-15 03:10:03,967 - INFO - __main__ - Epoch  54: total training loss 12037.24
2023-03-15 03:10:03,969 - INFO - __main__ - Epoch 55
2023-03-15 03:10:09,558 - INFO - __main__ - Epoch  55, Step:  117700, Batch Loss:     7.995417, Lr: 0.000058, Tokens per sec:   3325
2023-03-15 03:10:24,739 - INFO - __main__ - Epoch  55, Step:  117800, Batch Loss:     5.423542, Lr: 0.000058, Tokens per sec:   3511
2023-03-15 03:10:40,005 - INFO - __main__ - Epoch  55, Step:  117900, Batch Loss:     4.007729, Lr: 0.000058, Tokens per sec:   3518
2023-03-15 03:10:55,295 - INFO - __main__ - Epoch  55, Step:  118000, Batch Loss:     5.099988, Lr: 0.000058, Tokens per sec:   3500
2023-03-15 03:11:10,246 - INFO - __main__ - Epoch  55, Step:  118100, Batch Loss:     5.322431, Lr: 0.000058, Tokens per sec:   3636
2023-03-15 03:11:25,541 - INFO - __main__ - Epoch  55, Step:  118200, Batch Loss:     5.264410, Lr: 0.000058, Tokens per sec:   3489
2023-03-15 03:11:40,636 - INFO - __main__ - Epoch  55, Step:  118300, Batch Loss:     5.597584, Lr: 0.000058, Tokens per sec:   3582
2023-03-15 03:11:55,944 - INFO - __main__ - Epoch  55, Step:  118400, Batch Loss:     3.115069, Lr: 0.000058, Tokens per sec:   3522
2023-03-15 03:12:11,295 - INFO - __main__ - Epoch  55, Step:  118500, Batch Loss:     5.886992, Lr: 0.000058, Tokens per sec:   3491
2023-03-15 03:12:26,259 - INFO - __main__ - Epoch  55, Step:  118600, Batch Loss:     4.986930, Lr: 0.000058, Tokens per sec:   3643
2023-03-15 03:12:41,438 - INFO - __main__ - Epoch  55, Step:  118700, Batch Loss:     3.742177, Lr: 0.000058, Tokens per sec:   3610
2023-03-15 03:12:56,648 - INFO - __main__ - Epoch  55, Step:  118800, Batch Loss:     5.807454, Lr: 0.000058, Tokens per sec:   3547
2023-03-15 03:13:12,162 - INFO - __main__ - Epoch  55, Step:  118900, Batch Loss:     4.204010, Lr: 0.000058, Tokens per sec:   3493
2023-03-15 03:13:27,655 - INFO - __main__ - Epoch  55, Step:  119000, Batch Loss:     4.396085, Lr: 0.000058, Tokens per sec:   3476
2023-03-15 03:13:42,861 - INFO - __main__ - Epoch  55, Step:  119100, Batch Loss:     6.458013, Lr: 0.000058, Tokens per sec:   3539
2023-03-15 03:13:58,398 - INFO - __main__ - Epoch  55, Step:  119200, Batch Loss:     6.160337, Lr: 0.000058, Tokens per sec:   3438
2023-03-15 03:14:14,000 - INFO - __main__ - Epoch  55, Step:  119300, Batch Loss:     4.594991, Lr: 0.000058, Tokens per sec:   3447
2023-03-15 03:14:29,239 - INFO - __main__ - Epoch  55, Step:  119400, Batch Loss:     5.541581, Lr: 0.000058, Tokens per sec:   3491
2023-03-15 03:14:44,726 - INFO - __main__ - Epoch  55, Step:  119500, Batch Loss:     5.395751, Lr: 0.000058, Tokens per sec:   3462
2023-03-15 03:15:00,274 - INFO - __main__ - Epoch  55, Step:  119600, Batch Loss:     3.852380, Lr: 0.000058, Tokens per sec:   3442
2023-03-15 03:15:15,543 - INFO - __main__ - Epoch  55, Step:  119700, Batch Loss:     5.586117, Lr: 0.000058, Tokens per sec:   3538
2023-03-15 03:15:30,926 - INFO - __main__ - Epoch  55, Step:  119800, Batch Loss:     5.411526, Lr: 0.000058, Tokens per sec:   3489
2023-03-15 03:15:37,968 - INFO - __main__ - Epoch  55: total training loss 11720.44
2023-03-15 03:15:37,969 - INFO - __main__ - Epoch 56
2023-03-15 03:15:46,886 - INFO - __main__ - Epoch  56, Step:  119900, Batch Loss:     6.876833, Lr: 0.000058, Tokens per sec:   3347
2023-03-15 03:16:02,472 - INFO - __main__ - Epoch  56, Step:  120000, Batch Loss:     4.629438, Lr: 0.000058, Tokens per sec:   3458
2023-03-15 03:16:17,722 - INFO - __main__ - Epoch  56, Step:  120100, Batch Loss:     5.063392, Lr: 0.000058, Tokens per sec:   3574
2023-03-15 03:16:33,248 - INFO - __main__ - Epoch  56, Step:  120200, Batch Loss:     4.598440, Lr: 0.000058, Tokens per sec:   3488
2023-03-15 03:16:48,571 - INFO - __main__ - Epoch  56, Step:  120300, Batch Loss:     7.248869, Lr: 0.000058, Tokens per sec:   3509
2023-03-15 03:17:04,142 - INFO - __main__ - Epoch  56, Step:  120400, Batch Loss:     6.449106, Lr: 0.000058, Tokens per sec:   3428
2023-03-15 03:17:19,733 - INFO - __main__ - Epoch  56, Step:  120500, Batch Loss:     4.611573, Lr: 0.000058, Tokens per sec:   3482
2023-03-15 03:17:35,343 - INFO - __main__ - Epoch  56, Step:  120600, Batch Loss:     4.981162, Lr: 0.000058, Tokens per sec:   3442
2023-03-15 03:17:50,811 - INFO - __main__ - Epoch  56, Step:  120700, Batch Loss:     4.452349, Lr: 0.000058, Tokens per sec:   3440
2023-03-15 03:18:06,090 - INFO - __main__ - Epoch  56, Step:  120800, Batch Loss:     4.360935, Lr: 0.000058, Tokens per sec:   3542
2023-03-15 03:18:21,565 - INFO - __main__ - Epoch  56, Step:  120900, Batch Loss:     4.342835, Lr: 0.000058, Tokens per sec:   3523
2023-03-15 03:18:37,143 - INFO - __main__ - Epoch  56, Step:  121000, Batch Loss:     2.917539, Lr: 0.000058, Tokens per sec:   3471
2023-03-15 03:18:52,743 - INFO - __main__ - Epoch  56, Step:  121100, Batch Loss:     5.259655, Lr: 0.000058, Tokens per sec:   3496
2023-03-15 03:19:08,433 - INFO - __main__ - Epoch  56, Step:  121200, Batch Loss:     6.124361, Lr: 0.000058, Tokens per sec:   3356
2023-03-15 03:19:24,089 - INFO - __main__ - Epoch  56, Step:  121300, Batch Loss:     5.351676, Lr: 0.000058, Tokens per sec:   3446
2023-03-15 03:19:39,878 - INFO - __main__ - Epoch  56, Step:  121400, Batch Loss:     6.158195, Lr: 0.000058, Tokens per sec:   3385
2023-03-15 03:19:55,080 - INFO - __main__ - Epoch  56, Step:  121500, Batch Loss:     5.939482, Lr: 0.000058, Tokens per sec:   3516
2023-03-15 03:20:10,590 - INFO - __main__ - Epoch  56, Step:  121600, Batch Loss:     4.529975, Lr: 0.000058, Tokens per sec:   3430
2023-03-15 03:20:26,047 - INFO - __main__ - Epoch  56, Step:  121700, Batch Loss:     5.461667, Lr: 0.000058, Tokens per sec:   3454
2023-03-15 03:20:41,630 - INFO - __main__ - Epoch  56, Step:  121800, Batch Loss:     4.122936, Lr: 0.000058, Tokens per sec:   3480
2023-03-15 03:20:57,225 - INFO - __main__ - Epoch  56, Step:  121900, Batch Loss:     4.217250, Lr: 0.000058, Tokens per sec:   3432
2023-03-15 03:21:12,491 - INFO - __main__ - Epoch  56, Step:  122000, Batch Loss:     7.814223, Lr: 0.000058, Tokens per sec:   3539
2023-03-15 03:21:16,293 - INFO - __main__ - Epoch  56: total training loss 11472.80
2023-03-15 03:21:16,294 - INFO - __main__ - Epoch 57
2023-03-15 03:21:28,456 - INFO - __main__ - Epoch  57, Step:  122100, Batch Loss:     6.079370, Lr: 0.000057, Tokens per sec:   3387
2023-03-15 03:21:43,833 - INFO - __main__ - Epoch  57, Step:  122200, Batch Loss:     5.054573, Lr: 0.000057, Tokens per sec:   3466
2023-03-15 03:21:59,411 - INFO - __main__ - Epoch  57, Step:  122300, Batch Loss:     3.985369, Lr: 0.000057, Tokens per sec:   3387
2023-03-15 03:22:15,012 - INFO - __main__ - Epoch  57, Step:  122400, Batch Loss:     7.263248, Lr: 0.000057, Tokens per sec:   3507
2023-03-15 03:22:30,358 - INFO - __main__ - Epoch  57, Step:  122500, Batch Loss:     4.094588, Lr: 0.000057, Tokens per sec:   3521
2023-03-15 03:22:45,646 - INFO - __main__ - Epoch  57, Step:  122600, Batch Loss:     5.045172, Lr: 0.000057, Tokens per sec:   3495
2023-03-15 03:23:00,785 - INFO - __main__ - Epoch  57, Step:  122700, Batch Loss:     6.416640, Lr: 0.000057, Tokens per sec:   3524
2023-03-15 03:23:16,055 - INFO - __main__ - Epoch  57, Step:  122800, Batch Loss:     4.213390, Lr: 0.000057, Tokens per sec:   3507
2023-03-15 03:23:31,478 - INFO - __main__ - Epoch  57, Step:  122900, Batch Loss:     4.738724, Lr: 0.000057, Tokens per sec:   3531
2023-03-15 03:23:46,689 - INFO - __main__ - Epoch  57, Step:  123000, Batch Loss:     5.394452, Lr: 0.000057, Tokens per sec:   3525
2023-03-15 03:24:01,988 - INFO - __main__ - Epoch  57, Step:  123100, Batch Loss:     5.276252, Lr: 0.000057, Tokens per sec:   3461
2023-03-15 03:24:17,345 - INFO - __main__ - Epoch  57, Step:  123200, Batch Loss:     4.210783, Lr: 0.000057, Tokens per sec:   3524
2023-03-15 03:24:32,739 - INFO - __main__ - Epoch  57, Step:  123300, Batch Loss:     4.588366, Lr: 0.000057, Tokens per sec:   3423
2023-03-15 03:24:48,206 - INFO - __main__ - Epoch  57, Step:  123400, Batch Loss:     4.823495, Lr: 0.000057, Tokens per sec:   3524
2023-03-15 03:25:03,546 - INFO - __main__ - Epoch  57, Step:  123500, Batch Loss:     5.066629, Lr: 0.000057, Tokens per sec:   3446
2023-03-15 03:25:19,101 - INFO - __main__ - Epoch  57, Step:  123600, Batch Loss:     5.316717, Lr: 0.000057, Tokens per sec:   3559
2023-03-15 03:25:33,962 - INFO - __main__ - Epoch  57, Step:  123700, Batch Loss:     5.978199, Lr: 0.000057, Tokens per sec:   3648
2023-03-15 03:25:49,477 - INFO - __main__ - Epoch  57, Step:  123800, Batch Loss:     5.781948, Lr: 0.000057, Tokens per sec:   3474
2023-03-15 03:26:04,805 - INFO - __main__ - Epoch  57, Step:  123900, Batch Loss:     5.879983, Lr: 0.000057, Tokens per sec:   3508
2023-03-15 03:26:20,376 - INFO - __main__ - Epoch  57, Step:  124000, Batch Loss:     5.978861, Lr: 0.000057, Tokens per sec:   3406
2023-03-15 03:26:35,704 - INFO - __main__ - Epoch  57, Step:  124100, Batch Loss:     4.787477, Lr: 0.000057, Tokens per sec:   3629
2023-03-15 03:26:51,103 - INFO - __main__ - Epoch  57, Step:  124200, Batch Loss:     8.198274, Lr: 0.000057, Tokens per sec:   3502
2023-03-15 03:26:51,626 - INFO - __main__ - Epoch  57: total training loss 11170.87
2023-03-15 03:26:51,628 - INFO - __main__ - Epoch 58
2023-03-15 03:27:07,076 - INFO - __main__ - Epoch  58, Step:  124300, Batch Loss:     4.439566, Lr: 0.000056, Tokens per sec:   3354
2023-03-15 03:27:22,690 - INFO - __main__ - Epoch  58, Step:  124400, Batch Loss:     4.133920, Lr: 0.000056, Tokens per sec:   3410
2023-03-15 03:27:38,271 - INFO - __main__ - Epoch  58, Step:  124500, Batch Loss:     4.293721, Lr: 0.000056, Tokens per sec:   3416
2023-03-15 03:27:53,831 - INFO - __main__ - Epoch  58, Step:  124600, Batch Loss:     4.587776, Lr: 0.000056, Tokens per sec:   3495
2023-03-15 03:28:09,450 - INFO - __main__ - Epoch  58, Step:  124700, Batch Loss:     5.607563, Lr: 0.000056, Tokens per sec:   3390
2023-03-15 03:28:24,954 - INFO - __main__ - Epoch  58, Step:  124800, Batch Loss:     6.616288, Lr: 0.000056, Tokens per sec:   3483
2023-03-15 03:28:40,162 - INFO - __main__ - Epoch  58, Step:  124900, Batch Loss:     2.969550, Lr: 0.000056, Tokens per sec:   3503
2023-03-15 03:28:55,599 - INFO - __main__ - Epoch  58, Step:  125000, Batch Loss:     6.181924, Lr: 0.000056, Tokens per sec:   3479
2023-03-15 03:29:11,221 - INFO - __main__ - Epoch  58, Step:  125100, Batch Loss:     6.513462, Lr: 0.000056, Tokens per sec:   3472
2023-03-15 03:29:26,572 - INFO - __main__ - Epoch  58, Step:  125200, Batch Loss:     4.790098, Lr: 0.000056, Tokens per sec:   3526
2023-03-15 03:29:42,265 - INFO - __main__ - Epoch  58, Step:  125300, Batch Loss:     7.148028, Lr: 0.000056, Tokens per sec:   3450
2023-03-15 03:29:57,655 - INFO - __main__ - Epoch  58, Step:  125400, Batch Loss:     4.404667, Lr: 0.000056, Tokens per sec:   3472
2023-03-15 03:30:13,241 - INFO - __main__ - Epoch  58, Step:  125500, Batch Loss:     5.984790, Lr: 0.000056, Tokens per sec:   3500
2023-03-15 03:30:28,965 - INFO - __main__ - Epoch  58, Step:  125600, Batch Loss:     5.615458, Lr: 0.000056, Tokens per sec:   3489
2023-03-15 03:30:44,547 - INFO - __main__ - Epoch  58, Step:  125700, Batch Loss:     6.396385, Lr: 0.000056, Tokens per sec:   3508
2023-03-15 03:31:00,042 - INFO - __main__ - Epoch  58, Step:  125800, Batch Loss:     5.718021, Lr: 0.000056, Tokens per sec:   3503
2023-03-15 03:31:15,688 - INFO - __main__ - Epoch  58, Step:  125900, Batch Loss:     5.368881, Lr: 0.000056, Tokens per sec:   3458
2023-03-15 03:31:31,287 - INFO - __main__ - Epoch  58, Step:  126000, Batch Loss:     6.569338, Lr: 0.000056, Tokens per sec:   3428
2023-03-15 03:31:46,712 - INFO - __main__ - Epoch  58, Step:  126100, Batch Loss:     5.904962, Lr: 0.000056, Tokens per sec:   3433
2023-03-15 03:32:02,101 - INFO - __main__ - Epoch  58, Step:  126200, Batch Loss:     5.619350, Lr: 0.000056, Tokens per sec:   3451
2023-03-15 03:32:17,865 - INFO - __main__ - Epoch  58, Step:  126300, Batch Loss:     5.011662, Lr: 0.000056, Tokens per sec:   3402
2023-03-15 03:32:30,413 - INFO - __main__ - Epoch  58: total training loss 10893.04
2023-03-15 03:32:30,414 - INFO - __main__ - Epoch 59
2023-03-15 03:32:33,627 - INFO - __main__ - Epoch  59, Step:  126400, Batch Loss:     4.489120, Lr: 0.000056, Tokens per sec:   2869
2023-03-15 03:32:48,981 - INFO - __main__ - Epoch  59, Step:  126500, Batch Loss:     5.136600, Lr: 0.000056, Tokens per sec:   3477
2023-03-15 03:33:04,352 - INFO - __main__ - Epoch  59, Step:  126600, Batch Loss:     4.285151, Lr: 0.000056, Tokens per sec:   3507
2023-03-15 03:33:19,832 - INFO - __main__ - Epoch  59, Step:  126700, Batch Loss:     3.779113, Lr: 0.000056, Tokens per sec:   3480
2023-03-15 03:33:35,385 - INFO - __main__ - Epoch  59, Step:  126800, Batch Loss:     4.551067, Lr: 0.000056, Tokens per sec:   3536
2023-03-15 03:33:50,964 - INFO - __main__ - Epoch  59, Step:  126900, Batch Loss:     4.369580, Lr: 0.000056, Tokens per sec:   3388
2023-03-15 03:34:06,709 - INFO - __main__ - Epoch  59, Step:  127000, Batch Loss:     5.362940, Lr: 0.000056, Tokens per sec:   3409
2023-03-15 03:34:22,177 - INFO - __main__ - Epoch  59, Step:  127100, Batch Loss:     6.753823, Lr: 0.000056, Tokens per sec:   3462
2023-03-15 03:34:37,634 - INFO - __main__ - Epoch  59, Step:  127200, Batch Loss:     4.670288, Lr: 0.000056, Tokens per sec:   3540
2023-03-15 03:34:53,245 - INFO - __main__ - Epoch  59, Step:  127300, Batch Loss:     5.029205, Lr: 0.000056, Tokens per sec:   3442
2023-03-15 03:35:08,777 - INFO - __main__ - Epoch  59, Step:  127400, Batch Loss:     4.710218, Lr: 0.000056, Tokens per sec:   3452
2023-03-15 03:35:24,316 - INFO - __main__ - Epoch  59, Step:  127500, Batch Loss:     4.687045, Lr: 0.000056, Tokens per sec:   3422
2023-03-15 03:35:39,929 - INFO - __main__ - Epoch  59, Step:  127600, Batch Loss:     5.854509, Lr: 0.000056, Tokens per sec:   3435
2023-03-15 03:35:55,283 - INFO - __main__ - Epoch  59, Step:  127700, Batch Loss:     4.552249, Lr: 0.000056, Tokens per sec:   3538
2023-03-15 03:36:10,618 - INFO - __main__ - Epoch  59, Step:  127800, Batch Loss:     4.564946, Lr: 0.000056, Tokens per sec:   3526
2023-03-15 03:36:26,139 - INFO - __main__ - Epoch  59, Step:  127900, Batch Loss:     4.703366, Lr: 0.000056, Tokens per sec:   3498
2023-03-15 03:36:41,601 - INFO - __main__ - Epoch  59, Step:  128000, Batch Loss:     7.109552, Lr: 0.000056, Tokens per sec:   3533
2023-03-15 03:36:57,070 - INFO - __main__ - Epoch  59, Step:  128100, Batch Loss:     3.394474, Lr: 0.000056, Tokens per sec:   3434
2023-03-15 03:37:12,596 - INFO - __main__ - Epoch  59, Step:  128200, Batch Loss:     6.189179, Lr: 0.000056, Tokens per sec:   3535
2023-03-15 03:37:28,058 - INFO - __main__ - Epoch  59, Step:  128300, Batch Loss:     5.151979, Lr: 0.000056, Tokens per sec:   3506
2023-03-15 03:37:43,346 - INFO - __main__ - Epoch  59, Step:  128400, Batch Loss:     3.338546, Lr: 0.000056, Tokens per sec:   3474
2023-03-15 03:37:58,819 - INFO - __main__ - Epoch  59, Step:  128500, Batch Loss:     4.969919, Lr: 0.000056, Tokens per sec:   3463
2023-03-15 03:38:08,199 - INFO - __main__ - Epoch  59: total training loss 10658.08
2023-03-15 03:38:08,200 - INFO - __main__ - Epoch 60
2023-03-15 03:38:14,606 - INFO - __main__ - Epoch  60, Step:  128600, Batch Loss:     3.935077, Lr: 0.000055, Tokens per sec:   3194
2023-03-15 03:38:30,160 - INFO - __main__ - Epoch  60, Step:  128700, Batch Loss:     5.362526, Lr: 0.000055, Tokens per sec:   3484
2023-03-15 03:38:45,763 - INFO - __main__ - Epoch  60, Step:  128800, Batch Loss:     5.472962, Lr: 0.000055, Tokens per sec:   3489
2023-03-15 03:39:01,438 - INFO - __main__ - Epoch  60, Step:  128900, Batch Loss:     4.643603, Lr: 0.000055, Tokens per sec:   3435
2023-03-15 03:39:16,741 - INFO - __main__ - Epoch  60, Step:  129000, Batch Loss:     4.102910, Lr: 0.000055, Tokens per sec:   3412
2023-03-15 03:39:32,088 - INFO - __main__ - Epoch  60, Step:  129100, Batch Loss:     4.637004, Lr: 0.000055, Tokens per sec:   3575
2023-03-15 03:39:47,549 - INFO - __main__ - Epoch  60, Step:  129200, Batch Loss:     4.854024, Lr: 0.000055, Tokens per sec:   3428
2023-03-15 03:40:02,767 - INFO - __main__ - Epoch  60, Step:  129300, Batch Loss:     6.096325, Lr: 0.000055, Tokens per sec:   3553
2023-03-15 03:40:18,276 - INFO - __main__ - Epoch  60, Step:  129400, Batch Loss:     4.943355, Lr: 0.000055, Tokens per sec:   3483
2023-03-15 03:40:33,790 - INFO - __main__ - Epoch  60, Step:  129500, Batch Loss:     6.458390, Lr: 0.000055, Tokens per sec:   3491
2023-03-15 03:40:49,403 - INFO - __main__ - Epoch  60, Step:  129600, Batch Loss:     6.741178, Lr: 0.000055, Tokens per sec:   3496
2023-03-15 03:41:04,857 - INFO - __main__ - Epoch  60, Step:  129700, Batch Loss:     4.160804, Lr: 0.000055, Tokens per sec:   3459
2023-03-15 03:41:20,350 - INFO - __main__ - Epoch  60, Step:  129800, Batch Loss:     6.602919, Lr: 0.000055, Tokens per sec:   3479
2023-03-15 03:41:35,969 - INFO - __main__ - Epoch  60, Step:  129900, Batch Loss:     5.150619, Lr: 0.000055, Tokens per sec:   3443
2023-03-15 03:41:51,500 - INFO - __main__ - Epoch  60, Step:  130000, Batch Loss:     5.719331, Lr: 0.000055, Tokens per sec:   3468
2023-03-15 03:42:07,077 - INFO - __main__ - Epoch  60, Step:  130100, Batch Loss:     4.653558, Lr: 0.000055, Tokens per sec:   3477
2023-03-15 03:42:22,580 - INFO - __main__ - Epoch  60, Step:  130200, Batch Loss:     4.112070, Lr: 0.000055, Tokens per sec:   3499
2023-03-15 03:42:37,918 - INFO - __main__ - Epoch  60, Step:  130300, Batch Loss:     4.919603, Lr: 0.000055, Tokens per sec:   3470
2023-03-15 03:42:53,586 - INFO - __main__ - Epoch  60, Step:  130400, Batch Loss:     3.397758, Lr: 0.000055, Tokens per sec:   3379
2023-03-15 03:43:09,314 - INFO - __main__ - Epoch  60, Step:  130500, Batch Loss:     4.946787, Lr: 0.000055, Tokens per sec:   3453
2023-03-15 03:43:24,774 - INFO - __main__ - Epoch  60, Step:  130600, Batch Loss:     4.095537, Lr: 0.000055, Tokens per sec:   3501
2023-03-15 03:43:40,341 - INFO - __main__ - Epoch  60, Step:  130700, Batch Loss:     4.743299, Lr: 0.000055, Tokens per sec:   3476
2023-03-15 03:43:46,635 - INFO - __main__ - Epoch  60: total training loss 10370.82
2023-03-15 03:43:46,636 - INFO - __main__ - Epoch 61
2023-03-15 03:43:56,401 - INFO - __main__ - Epoch  61, Step:  130800, Batch Loss:     3.895685, Lr: 0.000055, Tokens per sec:   3315
2023-03-15 03:44:11,871 - INFO - __main__ - Epoch  61, Step:  130900, Batch Loss:     4.296476, Lr: 0.000055, Tokens per sec:   3465
2023-03-15 03:44:27,637 - INFO - __main__ - Epoch  61, Step:  131000, Batch Loss:     2.821685, Lr: 0.000055, Tokens per sec:   3399
2023-03-15 03:44:42,860 - INFO - __main__ - Epoch  61, Step:  131100, Batch Loss:     3.157556, Lr: 0.000055, Tokens per sec:   3513
2023-03-15 03:44:58,243 - INFO - __main__ - Epoch  61, Step:  131200, Batch Loss:     3.786750, Lr: 0.000055, Tokens per sec:   3550
2023-03-15 03:45:13,866 - INFO - __main__ - Epoch  61, Step:  131300, Batch Loss:     4.099226, Lr: 0.000055, Tokens per sec:   3477
2023-03-15 03:45:29,370 - INFO - __main__ - Epoch  61, Step:  131400, Batch Loss:     6.198645, Lr: 0.000055, Tokens per sec:   3530
2023-03-15 03:45:44,739 - INFO - __main__ - Epoch  61, Step:  131500, Batch Loss:     3.922845, Lr: 0.000055, Tokens per sec:   3559
2023-03-15 03:46:00,313 - INFO - __main__ - Epoch  61, Step:  131600, Batch Loss:     5.223806, Lr: 0.000055, Tokens per sec:   3502
2023-03-15 03:46:15,767 - INFO - __main__ - Epoch  61, Step:  131700, Batch Loss:     5.058022, Lr: 0.000055, Tokens per sec:   3458
2023-03-15 03:46:30,926 - INFO - __main__ - Epoch  61, Step:  131800, Batch Loss:     6.350686, Lr: 0.000055, Tokens per sec:   3530
2023-03-15 03:46:46,471 - INFO - __main__ - Epoch  61, Step:  131900, Batch Loss:     3.449099, Lr: 0.000055, Tokens per sec:   3454
2023-03-15 03:47:02,031 - INFO - __main__ - Epoch  61, Step:  132000, Batch Loss:     3.816513, Lr: 0.000055, Tokens per sec:   3393
2023-03-15 03:47:17,512 - INFO - __main__ - Epoch  61, Step:  132100, Batch Loss:     3.965723, Lr: 0.000055, Tokens per sec:   3453
2023-03-15 03:47:33,121 - INFO - __main__ - Epoch  61, Step:  132200, Batch Loss:     5.023622, Lr: 0.000055, Tokens per sec:   3431
2023-03-15 03:47:48,642 - INFO - __main__ - Epoch  61, Step:  132300, Batch Loss:     3.717873, Lr: 0.000055, Tokens per sec:   3500
2023-03-15 03:48:04,345 - INFO - __main__ - Epoch  61, Step:  132400, Batch Loss:     4.492753, Lr: 0.000055, Tokens per sec:   3454
2023-03-15 03:48:19,893 - INFO - __main__ - Epoch  61, Step:  132500, Batch Loss:     5.573617, Lr: 0.000055, Tokens per sec:   3458
2023-03-15 03:48:35,456 - INFO - __main__ - Epoch  61, Step:  132600, Batch Loss:     4.805079, Lr: 0.000055, Tokens per sec:   3458
2023-03-15 03:48:51,086 - INFO - __main__ - Epoch  61, Step:  132700, Batch Loss:     4.723062, Lr: 0.000055, Tokens per sec:   3449
2023-03-15 03:49:06,484 - INFO - __main__ - Epoch  61, Step:  132800, Batch Loss:     6.512155, Lr: 0.000055, Tokens per sec:   3458
2023-03-15 03:49:22,168 - INFO - __main__ - Epoch  61, Step:  132900, Batch Loss:     4.210692, Lr: 0.000055, Tokens per sec:   3395
2023-03-15 03:49:25,211 - INFO - __main__ - Epoch  61: total training loss 10123.88
2023-03-15 03:49:25,212 - INFO - __main__ - Epoch 62
2023-03-15 03:49:38,111 - INFO - __main__ - Epoch  62, Step:  133000, Batch Loss:     5.342331, Lr: 0.000054, Tokens per sec:   3478
2023-03-15 03:49:53,707 - INFO - __main__ - Epoch  62, Step:  133100, Batch Loss:     4.942186, Lr: 0.000054, Tokens per sec:   3468
2023-03-15 03:50:09,408 - INFO - __main__ - Epoch  62, Step:  133200, Batch Loss:     2.730609, Lr: 0.000054, Tokens per sec:   3404
2023-03-15 03:50:24,922 - INFO - __main__ - Epoch  62, Step:  133300, Batch Loss:     4.226130, Lr: 0.000054, Tokens per sec:   3518
2023-03-15 03:50:40,217 - INFO - __main__ - Epoch  62, Step:  133400, Batch Loss:     4.927201, Lr: 0.000054, Tokens per sec:   3527
2023-03-15 03:50:55,696 - INFO - __main__ - Epoch  62, Step:  133500, Batch Loss:     4.504740, Lr: 0.000054, Tokens per sec:   3440
2023-03-15 03:51:11,178 - INFO - __main__ - Epoch  62, Step:  133600, Batch Loss:     6.492513, Lr: 0.000054, Tokens per sec:   3454
2023-03-15 03:51:26,546 - INFO - __main__ - Epoch  62, Step:  133700, Batch Loss:     4.738263, Lr: 0.000054, Tokens per sec:   3455
2023-03-15 03:51:42,005 - INFO - __main__ - Epoch  62, Step:  133800, Batch Loss:     4.310567, Lr: 0.000054, Tokens per sec:   3448
2023-03-15 03:51:57,394 - INFO - __main__ - Epoch  62, Step:  133900, Batch Loss:     5.667854, Lr: 0.000054, Tokens per sec:   3511
2023-03-15 03:52:12,962 - INFO - __main__ - Epoch  62, Step:  134000, Batch Loss:     4.496328, Lr: 0.000054, Tokens per sec:   3456
2023-03-15 03:52:28,350 - INFO - __main__ - Epoch  62, Step:  134100, Batch Loss:     3.859511, Lr: 0.000054, Tokens per sec:   3475
2023-03-15 03:52:43,655 - INFO - __main__ - Epoch  62, Step:  134200, Batch Loss:     4.041900, Lr: 0.000054, Tokens per sec:   3495
2023-03-15 03:52:59,276 - INFO - __main__ - Epoch  62, Step:  134300, Batch Loss:     7.122596, Lr: 0.000054, Tokens per sec:   3453
2023-03-15 03:53:14,891 - INFO - __main__ - Epoch  62, Step:  134400, Batch Loss:     4.754982, Lr: 0.000054, Tokens per sec:   3436
2023-03-15 03:53:30,373 - INFO - __main__ - Epoch  62, Step:  134500, Batch Loss:     5.040339, Lr: 0.000054, Tokens per sec:   3468
2023-03-15 03:53:45,896 - INFO - __main__ - Epoch  62, Step:  134600, Batch Loss:     7.206387, Lr: 0.000054, Tokens per sec:   3515
2023-03-15 03:54:01,298 - INFO - __main__ - Epoch  62, Step:  134700, Batch Loss:     5.261679, Lr: 0.000054, Tokens per sec:   3525
2023-03-15 03:54:16,808 - INFO - __main__ - Epoch  62, Step:  134800, Batch Loss:     5.796153, Lr: 0.000054, Tokens per sec:   3452
2023-03-15 03:54:32,335 - INFO - __main__ - Epoch  62, Step:  134900, Batch Loss:     4.372114, Lr: 0.000054, Tokens per sec:   3481
2023-03-15 03:54:47,739 - INFO - __main__ - Epoch  62, Step:  135000, Batch Loss:     6.434481, Lr: 0.000054, Tokens per sec:   3475
2023-03-15 03:55:02,715 - INFO - __main__ - Epoch  62: total training loss 9911.98
2023-03-15 03:55:02,715 - INFO - __main__ - Epoch 63
2023-03-15 03:55:03,416 - INFO - __main__ - Epoch  63, Step:  135100, Batch Loss:     3.265152, Lr: 0.000054, Tokens per sec:   1549
2023-03-15 03:55:18,787 - INFO - __main__ - Epoch  63, Step:  135200, Batch Loss:     3.381927, Lr: 0.000054, Tokens per sec:   3488
2023-03-15 03:55:34,039 - INFO - __main__ - Epoch  63, Step:  135300, Batch Loss:     5.721224, Lr: 0.000054, Tokens per sec:   3559
2023-03-15 03:55:49,482 - INFO - __main__ - Epoch  63, Step:  135400, Batch Loss:     4.109045, Lr: 0.000054, Tokens per sec:   3519
2023-03-15 03:56:04,787 - INFO - __main__ - Epoch  63, Step:  135500, Batch Loss:     5.374030, Lr: 0.000054, Tokens per sec:   3485
2023-03-15 03:56:19,907 - INFO - __main__ - Epoch  63, Step:  135600, Batch Loss:     3.980211, Lr: 0.000054, Tokens per sec:   3576
2023-03-15 03:56:35,284 - INFO - __main__ - Epoch  63, Step:  135700, Batch Loss:     4.189674, Lr: 0.000054, Tokens per sec:   3453
2023-03-15 03:56:50,481 - INFO - __main__ - Epoch  63, Step:  135800, Batch Loss:     3.088158, Lr: 0.000054, Tokens per sec:   3500
2023-03-15 03:57:05,743 - INFO - __main__ - Epoch  63, Step:  135900, Batch Loss:     3.754949, Lr: 0.000054, Tokens per sec:   3549
2023-03-15 03:57:21,318 - INFO - __main__ - Epoch  63, Step:  136000, Batch Loss:     4.593309, Lr: 0.000054, Tokens per sec:   3458
2023-03-15 03:57:36,865 - INFO - __main__ - Epoch  63, Step:  136100, Batch Loss:     5.694417, Lr: 0.000054, Tokens per sec:   3450
2023-03-15 03:57:52,384 - INFO - __main__ - Epoch  63, Step:  136200, Batch Loss:     3.298475, Lr: 0.000054, Tokens per sec:   3490
2023-03-15 03:58:07,904 - INFO - __main__ - Epoch  63, Step:  136300, Batch Loss:     6.281168, Lr: 0.000054, Tokens per sec:   3449
2023-03-15 03:58:23,450 - INFO - __main__ - Epoch  63, Step:  136400, Batch Loss:     4.030660, Lr: 0.000054, Tokens per sec:   3522
2023-03-15 03:58:38,971 - INFO - __main__ - Epoch  63, Step:  136500, Batch Loss:     3.608816, Lr: 0.000054, Tokens per sec:   3488
2023-03-15 03:58:54,406 - INFO - __main__ - Epoch  63, Step:  136600, Batch Loss:     5.081560, Lr: 0.000054, Tokens per sec:   3476
2023-03-15 03:59:09,961 - INFO - __main__ - Epoch  63, Step:  136700, Batch Loss:     5.009822, Lr: 0.000054, Tokens per sec:   3437
2023-03-15 03:59:25,458 - INFO - __main__ - Epoch  63, Step:  136800, Batch Loss:     3.715239, Lr: 0.000054, Tokens per sec:   3459
2023-03-15 03:59:41,044 - INFO - __main__ - Epoch  63, Step:  136900, Batch Loss:     3.991068, Lr: 0.000054, Tokens per sec:   3471
2023-03-15 03:59:56,396 - INFO - __main__ - Epoch  63, Step:  137000, Batch Loss:     4.094790, Lr: 0.000054, Tokens per sec:   3533
2023-03-15 04:00:11,929 - INFO - __main__ - Epoch  63, Step:  137100, Batch Loss:     4.099441, Lr: 0.000054, Tokens per sec:   3466
2023-03-15 04:00:27,467 - INFO - __main__ - Epoch  63, Step:  137200, Batch Loss:     6.872454, Lr: 0.000054, Tokens per sec:   3465
2023-03-15 04:00:39,558 - INFO - __main__ - Epoch  63: total training loss 9699.13
2023-03-15 04:00:39,559 - INFO - __main__ - Epoch 64
2023-03-15 04:00:43,571 - INFO - __main__ - Epoch  64, Step:  137300, Batch Loss:     3.967458, Lr: 0.000053, Tokens per sec:   3031
2023-03-15 04:00:59,004 - INFO - __main__ - Epoch  64, Step:  137400, Batch Loss:     3.046112, Lr: 0.000053, Tokens per sec:   3512
2023-03-15 04:01:14,313 - INFO - __main__ - Epoch  64, Step:  137500, Batch Loss:     3.799945, Lr: 0.000053, Tokens per sec:   3524
2023-03-15 04:01:30,106 - INFO - __main__ - Epoch  64, Step:  137600, Batch Loss:     4.955277, Lr: 0.000053, Tokens per sec:   3415
2023-03-15 04:01:45,303 - INFO - __main__ - Epoch  64, Step:  137700, Batch Loss:     3.992694, Lr: 0.000053, Tokens per sec:   3507
2023-03-15 04:02:00,872 - INFO - __main__ - Epoch  64, Step:  137800, Batch Loss:     3.005891, Lr: 0.000053, Tokens per sec:   3444
2023-03-15 04:02:16,107 - INFO - __main__ - Epoch  64, Step:  137900, Batch Loss:     3.672250, Lr: 0.000053, Tokens per sec:   3636
2023-03-15 04:02:31,433 - INFO - __main__ - Epoch  64, Step:  138000, Batch Loss:     5.209804, Lr: 0.000053, Tokens per sec:   3503
2023-03-15 04:02:46,967 - INFO - __main__ - Epoch  64, Step:  138100, Batch Loss:     5.016855, Lr: 0.000053, Tokens per sec:   3431
2023-03-15 04:03:02,288 - INFO - __main__ - Epoch  64, Step:  138200, Batch Loss:     3.583002, Lr: 0.000053, Tokens per sec:   3507
2023-03-15 04:03:17,804 - INFO - __main__ - Epoch  64, Step:  138300, Batch Loss:     5.454304, Lr: 0.000053, Tokens per sec:   3445
2023-03-15 04:03:33,337 - INFO - __main__ - Epoch  64, Step:  138400, Batch Loss:     4.674686, Lr: 0.000053, Tokens per sec:   3474
2023-03-15 04:03:48,924 - INFO - __main__ - Epoch  64, Step:  138500, Batch Loss:     4.303301, Lr: 0.000053, Tokens per sec:   3499
2023-03-15 04:04:04,331 - INFO - __main__ - Epoch  64, Step:  138600, Batch Loss:     6.164418, Lr: 0.000053, Tokens per sec:   3506
2023-03-15 04:04:19,496 - INFO - __main__ - Epoch  64, Step:  138700, Batch Loss:     4.424315, Lr: 0.000053, Tokens per sec:   3452
2023-03-15 04:04:34,970 - INFO - __main__ - Epoch  64, Step:  138800, Batch Loss:     3.958148, Lr: 0.000053, Tokens per sec:   3461
2023-03-15 04:04:50,152 - INFO - __main__ - Epoch  64, Step:  138900, Batch Loss:     2.816073, Lr: 0.000053, Tokens per sec:   3606
2023-03-15 04:05:05,516 - INFO - __main__ - Epoch  64, Step:  139000, Batch Loss:     4.725976, Lr: 0.000053, Tokens per sec:   3529
2023-03-15 04:05:21,083 - INFO - __main__ - Epoch  64, Step:  139100, Batch Loss:     3.229281, Lr: 0.000053, Tokens per sec:   3484
2023-03-15 04:05:36,494 - INFO - __main__ - Epoch  64, Step:  139200, Batch Loss:     5.325739, Lr: 0.000053, Tokens per sec:   3507
2023-03-15 04:05:51,959 - INFO - __main__ - Epoch  64, Step:  139300, Batch Loss:     4.842519, Lr: 0.000053, Tokens per sec:   3429
2023-03-15 04:06:07,260 - INFO - __main__ - Epoch  64, Step:  139400, Batch Loss:     5.671973, Lr: 0.000053, Tokens per sec:   3558
2023-03-15 04:06:16,033 - INFO - __main__ - Epoch  64: total training loss 9532.73
2023-03-15 04:06:16,034 - INFO - __main__ - Epoch 65
2023-03-15 04:06:23,332 - INFO - __main__ - Epoch  65, Step:  139500, Batch Loss:     3.970742, Lr: 0.000053, Tokens per sec:   3265
2023-03-15 04:06:38,660 - INFO - __main__ - Epoch  65, Step:  139600, Batch Loss:     3.836039, Lr: 0.000053, Tokens per sec:   3486
2023-03-15 04:06:54,230 - INFO - __main__ - Epoch  65, Step:  139700, Batch Loss:     3.912400, Lr: 0.000053, Tokens per sec:   3434
2023-03-15 04:07:09,562 - INFO - __main__ - Epoch  65, Step:  139800, Batch Loss:     4.154665, Lr: 0.000053, Tokens per sec:   3517
2023-03-15 04:07:25,038 - INFO - __main__ - Epoch  65, Step:  139900, Batch Loss:     4.242007, Lr: 0.000053, Tokens per sec:   3458
2023-03-15 04:07:40,541 - INFO - __main__ - Epoch  65, Step:  140000, Batch Loss:     3.797643, Lr: 0.000053, Tokens per sec:   3450
2023-03-15 04:07:56,020 - INFO - __main__ - Epoch  65, Step:  140100, Batch Loss:     4.174707, Lr: 0.000053, Tokens per sec:   3508
2023-03-15 04:08:11,580 - INFO - __main__ - Epoch  65, Step:  140200, Batch Loss:     3.302622, Lr: 0.000053, Tokens per sec:   3425
2023-03-15 04:08:27,012 - INFO - __main__ - Epoch  65, Step:  140300, Batch Loss:     4.998269, Lr: 0.000053, Tokens per sec:   3469
2023-03-15 04:08:42,035 - INFO - __main__ - Epoch  65, Step:  140400, Batch Loss:     4.646392, Lr: 0.000053, Tokens per sec:   3562
2023-03-15 04:08:57,505 - INFO - __main__ - Epoch  65, Step:  140500, Batch Loss:     3.887123, Lr: 0.000053, Tokens per sec:   3473
2023-03-15 04:09:12,910 - INFO - __main__ - Epoch  65, Step:  140600, Batch Loss:     3.524874, Lr: 0.000053, Tokens per sec:   3506
2023-03-15 04:09:28,392 - INFO - __main__ - Epoch  65, Step:  140700, Batch Loss:     4.383171, Lr: 0.000053, Tokens per sec:   3528
2023-03-15 04:09:43,993 - INFO - __main__ - Epoch  65, Step:  140800, Batch Loss:     5.289259, Lr: 0.000053, Tokens per sec:   3370
2023-03-15 04:09:59,354 - INFO - __main__ - Epoch  65, Step:  140900, Batch Loss:     4.796780, Lr: 0.000053, Tokens per sec:   3534
2023-03-15 04:10:14,764 - INFO - __main__ - Epoch  65, Step:  141000, Batch Loss:     5.838011, Lr: 0.000053, Tokens per sec:   3547
2023-03-15 04:10:30,271 - INFO - __main__ - Epoch  65, Step:  141100, Batch Loss:     5.947084, Lr: 0.000053, Tokens per sec:   3489
2023-03-15 04:10:45,794 - INFO - __main__ - Epoch  65, Step:  141200, Batch Loss:     4.740597, Lr: 0.000053, Tokens per sec:   3444
2023-03-15 04:11:01,313 - INFO - __main__ - Epoch  65, Step:  141300, Batch Loss:     3.774188, Lr: 0.000053, Tokens per sec:   3536
2023-03-15 04:11:16,976 - INFO - __main__ - Epoch  65, Step:  141400, Batch Loss:     5.733821, Lr: 0.000053, Tokens per sec:   3432
2023-03-15 04:11:32,493 - INFO - __main__ - Epoch  65, Step:  141500, Batch Loss:     3.354032, Lr: 0.000053, Tokens per sec:   3478
2023-03-15 04:11:47,780 - INFO - __main__ - Epoch  65, Step:  141600, Batch Loss:     5.458251, Lr: 0.000053, Tokens per sec:   3529
2023-03-15 04:11:53,287 - INFO - __main__ - Epoch  65: total training loss 9339.46
2023-03-15 04:11:53,288 - INFO - __main__ - Epoch 66
2023-03-15 04:12:03,733 - INFO - __main__ - Epoch  66, Step:  141700, Batch Loss:     4.682409, Lr: 0.000052, Tokens per sec:   3334
2023-03-15 04:12:19,234 - INFO - __main__ - Epoch  66, Step:  141800, Batch Loss:     4.579132, Lr: 0.000052, Tokens per sec:   3482
2023-03-15 04:12:34,830 - INFO - __main__ - Epoch  66, Step:  141900, Batch Loss:     2.330832, Lr: 0.000052, Tokens per sec:   3500
2023-03-15 04:12:50,377 - INFO - __main__ - Epoch  66, Step:  142000, Batch Loss:     4.579865, Lr: 0.000052, Tokens per sec:   3523
2023-03-15 04:13:05,801 - INFO - __main__ - Epoch  66, Step:  142100, Batch Loss:     3.572760, Lr: 0.000052, Tokens per sec:   3490
2023-03-15 04:13:21,342 - INFO - __main__ - Epoch  66, Step:  142200, Batch Loss:     4.386403, Lr: 0.000052, Tokens per sec:   3496
2023-03-15 04:13:36,795 - INFO - __main__ - Epoch  66, Step:  142300, Batch Loss:     4.786645, Lr: 0.000052, Tokens per sec:   3511
2023-03-15 04:13:52,275 - INFO - __main__ - Epoch  66, Step:  142400, Batch Loss:     3.880893, Lr: 0.000052, Tokens per sec:   3436
2023-03-15 04:14:07,820 - INFO - __main__ - Epoch  66, Step:  142500, Batch Loss:     6.779628, Lr: 0.000052, Tokens per sec:   3509
2023-03-15 04:14:23,312 - INFO - __main__ - Epoch  66, Step:  142600, Batch Loss:     3.944715, Lr: 0.000052, Tokens per sec:   3484
2023-03-15 04:14:38,774 - INFO - __main__ - Epoch  66, Step:  142700, Batch Loss:     3.514112, Lr: 0.000052, Tokens per sec:   3468
2023-03-15 04:14:54,168 - INFO - __main__ - Epoch  66, Step:  142800, Batch Loss:     4.572779, Lr: 0.000052, Tokens per sec:   3524
2023-03-15 04:15:09,759 - INFO - __main__ - Epoch  66, Step:  142900, Batch Loss:     4.544824, Lr: 0.000052, Tokens per sec:   3431
2023-03-15 04:15:24,970 - INFO - __main__ - Epoch  66, Step:  143000, Batch Loss:     5.780563, Lr: 0.000052, Tokens per sec:   3517
2023-03-15 04:15:40,014 - INFO - __main__ - Epoch  66, Step:  143100, Batch Loss:     4.319216, Lr: 0.000052, Tokens per sec:   3546
2023-03-15 04:15:55,146 - INFO - __main__ - Epoch  66, Step:  143200, Batch Loss:     6.704538, Lr: 0.000052, Tokens per sec:   3623
2023-03-15 04:16:10,267 - INFO - __main__ - Epoch  66, Step:  143300, Batch Loss:     5.127648, Lr: 0.000052, Tokens per sec:   3490
2023-03-15 04:16:25,614 - INFO - __main__ - Epoch  66, Step:  143400, Batch Loss:     4.528793, Lr: 0.000052, Tokens per sec:   3529
2023-03-15 04:16:41,068 - INFO - __main__ - Epoch  66, Step:  143500, Batch Loss:     4.261971, Lr: 0.000052, Tokens per sec:   3437
2023-03-15 04:16:56,632 - INFO - __main__ - Epoch  66, Step:  143600, Batch Loss:     5.040627, Lr: 0.000052, Tokens per sec:   3378
2023-03-15 04:17:12,067 - INFO - __main__ - Epoch  66, Step:  143700, Batch Loss:     4.378763, Lr: 0.000052, Tokens per sec:   3536
2023-03-15 04:17:27,511 - INFO - __main__ - Epoch  66, Step:  143800, Batch Loss:     3.570418, Lr: 0.000052, Tokens per sec:   3439
2023-03-15 04:17:29,646 - INFO - __main__ - Epoch  66: total training loss 9126.13
2023-03-15 04:17:29,647 - INFO - __main__ - Epoch 67
2023-03-15 04:17:43,452 - INFO - __main__ - Epoch  67, Step:  143900, Batch Loss:     3.674785, Lr: 0.000052, Tokens per sec:   3369
2023-03-15 04:17:58,973 - INFO - __main__ - Epoch  67, Step:  144000, Batch Loss:     2.848470, Lr: 0.000052, Tokens per sec:   3533
2023-03-15 04:18:14,123 - INFO - __main__ - Epoch  67, Step:  144100, Batch Loss:     3.794792, Lr: 0.000052, Tokens per sec:   3560
2023-03-15 04:18:29,761 - INFO - __main__ - Epoch  67, Step:  144200, Batch Loss:     3.746091, Lr: 0.000052, Tokens per sec:   3400
2023-03-15 04:18:45,370 - INFO - __main__ - Epoch  67, Step:  144300, Batch Loss:     3.936133, Lr: 0.000052, Tokens per sec:   3449
2023-03-15 04:19:00,522 - INFO - __main__ - Epoch  67, Step:  144400, Batch Loss:     3.014927, Lr: 0.000052, Tokens per sec:   3502
2023-03-15 04:19:16,084 - INFO - __main__ - Epoch  67, Step:  144500, Batch Loss:     4.297706, Lr: 0.000052, Tokens per sec:   3411
2023-03-15 04:19:31,453 - INFO - __main__ - Epoch  67, Step:  144600, Batch Loss:     3.264314, Lr: 0.000052, Tokens per sec:   3480
2023-03-15 04:19:46,668 - INFO - __main__ - Epoch  67, Step:  144700, Batch Loss:     3.323822, Lr: 0.000052, Tokens per sec:   3581
2023-03-15 04:20:02,214 - INFO - __main__ - Epoch  67, Step:  144800, Batch Loss:     3.590057, Lr: 0.000052, Tokens per sec:   3463
2023-03-15 04:20:17,652 - INFO - __main__ - Epoch  67, Step:  144900, Batch Loss:     4.104876, Lr: 0.000052, Tokens per sec:   3493
2023-03-15 04:20:33,002 - INFO - __main__ - Epoch  67, Step:  145000, Batch Loss:     4.469692, Lr: 0.000052, Tokens per sec:   3549
2023-03-15 04:20:48,569 - INFO - __main__ - Epoch  67, Step:  145100, Batch Loss:     3.614407, Lr: 0.000052, Tokens per sec:   3442
2023-03-15 04:21:03,642 - INFO - __main__ - Epoch  67, Step:  145200, Batch Loss:     3.766512, Lr: 0.000052, Tokens per sec:   3518
2023-03-15 04:21:19,122 - INFO - __main__ - Epoch  67, Step:  145300, Batch Loss:     3.143135, Lr: 0.000052, Tokens per sec:   3459
2023-03-15 04:21:34,058 - INFO - __main__ - Epoch  67, Step:  145400, Batch Loss:     4.451530, Lr: 0.000052, Tokens per sec:   3633
2023-03-15 04:21:49,558 - INFO - __main__ - Epoch  67, Step:  145500, Batch Loss:     3.869204, Lr: 0.000052, Tokens per sec:   3478
2023-03-15 04:22:05,188 - INFO - __main__ - Epoch  67, Step:  145600, Batch Loss:     4.672096, Lr: 0.000052, Tokens per sec:   3471
2023-03-15 04:22:20,663 - INFO - __main__ - Epoch  67, Step:  145700, Batch Loss:     4.252169, Lr: 0.000052, Tokens per sec:   3486
2023-03-15 04:22:36,238 - INFO - __main__ - Epoch  67, Step:  145800, Batch Loss:     4.386591, Lr: 0.000052, Tokens per sec:   3450
2023-03-15 04:22:51,854 - INFO - __main__ - Epoch  67, Step:  145900, Batch Loss:     4.548140, Lr: 0.000052, Tokens per sec:   3472
2023-03-15 04:23:06,232 - INFO - __main__ - Epoch  67: total training loss 8901.32
2023-03-15 04:23:06,233 - INFO - __main__ - Epoch 68
2023-03-15 04:23:07,727 - INFO - __main__ - Epoch  68, Step:  146000, Batch Loss:     4.323836, Lr: 0.000051, Tokens per sec:   2758
2023-03-15 04:23:22,977 - INFO - __main__ - Epoch  68, Step:  146100, Batch Loss:     4.034044, Lr: 0.000051, Tokens per sec:   3535
2023-03-15 04:23:38,626 - INFO - __main__ - Epoch  68, Step:  146200, Batch Loss:     4.865654, Lr: 0.000051, Tokens per sec:   3459
2023-03-15 04:23:54,289 - INFO - __main__ - Epoch  68, Step:  146300, Batch Loss:     4.171056, Lr: 0.000051, Tokens per sec:   3424
2023-03-15 04:24:09,852 - INFO - __main__ - Epoch  68, Step:  146400, Batch Loss:     2.140981, Lr: 0.000051, Tokens per sec:   3426
2023-03-15 04:24:25,192 - INFO - __main__ - Epoch  68, Step:  146500, Batch Loss:     4.448825, Lr: 0.000051, Tokens per sec:   3509
2023-03-15 04:24:40,950 - INFO - __main__ - Epoch  68, Step:  146600, Batch Loss:     4.467587, Lr: 0.000051, Tokens per sec:   3436
2023-03-15 04:24:56,302 - INFO - __main__ - Epoch  68, Step:  146700, Batch Loss:     4.117474, Lr: 0.000051, Tokens per sec:   3546
2023-03-15 04:25:11,904 - INFO - __main__ - Epoch  68, Step:  146800, Batch Loss:     4.424840, Lr: 0.000051, Tokens per sec:   3415
2023-03-15 04:25:27,336 - INFO - __main__ - Epoch  68, Step:  146900, Batch Loss:     3.110900, Lr: 0.000051, Tokens per sec:   3494
2023-03-15 04:25:42,486 - INFO - __main__ - Epoch  68, Step:  147000, Batch Loss:     3.937755, Lr: 0.000051, Tokens per sec:   3532
2023-03-15 04:25:57,996 - INFO - __main__ - Epoch  68, Step:  147100, Batch Loss:     3.818407, Lr: 0.000051, Tokens per sec:   3465
2023-03-15 04:26:13,464 - INFO - __main__ - Epoch  68, Step:  147200, Batch Loss:     4.080966, Lr: 0.000051, Tokens per sec:   3512
2023-03-15 04:26:28,665 - INFO - __main__ - Epoch  68, Step:  147300, Batch Loss:     5.047163, Lr: 0.000051, Tokens per sec:   3555
2023-03-15 04:26:43,844 - INFO - __main__ - Epoch  68, Step:  147400, Batch Loss:     4.183796, Lr: 0.000051, Tokens per sec:   3458
2023-03-15 04:26:59,175 - INFO - __main__ - Epoch  68, Step:  147500, Batch Loss:     5.358510, Lr: 0.000051, Tokens per sec:   3552
2023-03-15 04:27:14,054 - INFO - __main__ - Epoch  68, Step:  147600, Batch Loss:     5.216010, Lr: 0.000051, Tokens per sec:   3687
2023-03-15 04:27:29,497 - INFO - __main__ - Epoch  68, Step:  147700, Batch Loss:     4.338005, Lr: 0.000051, Tokens per sec:   3461
2023-03-15 04:27:44,929 - INFO - __main__ - Epoch  68, Step:  147800, Batch Loss:     3.681301, Lr: 0.000051, Tokens per sec:   3496
2023-03-15 04:28:00,620 - INFO - __main__ - Epoch  68, Step:  147900, Batch Loss:     3.569405, Lr: 0.000051, Tokens per sec:   3388
2023-03-15 04:28:16,296 - INFO - __main__ - Epoch  68, Step:  148000, Batch Loss:     3.409595, Lr: 0.000051, Tokens per sec:   3437
2023-03-15 04:28:31,737 - INFO - __main__ - Epoch  68, Step:  148100, Batch Loss:     4.301429, Lr: 0.000051, Tokens per sec:   3487
2023-03-15 04:28:42,959 - INFO - __main__ - Epoch  68: total training loss 8754.77
2023-03-15 04:28:42,960 - INFO - __main__ - Epoch 69
2023-03-15 04:28:47,688 - INFO - __main__ - Epoch  69, Step:  148200, Batch Loss:     3.953459, Lr: 0.000050, Tokens per sec:   3212
2023-03-15 04:29:03,337 - INFO - __main__ - Epoch  69, Step:  148300, Batch Loss:     3.019374, Lr: 0.000050, Tokens per sec:   3420
2023-03-15 04:29:18,842 - INFO - __main__ - Epoch  69, Step:  148400, Batch Loss:     5.246900, Lr: 0.000050, Tokens per sec:   3458
2023-03-15 04:29:34,461 - INFO - __main__ - Epoch  69, Step:  148500, Batch Loss:     4.000222, Lr: 0.000050, Tokens per sec:   3422
2023-03-15 04:29:50,147 - INFO - __main__ - Epoch  69, Step:  148600, Batch Loss:     3.697181, Lr: 0.000050, Tokens per sec:   3456
2023-03-15 04:30:05,649 - INFO - __main__ - Epoch  69, Step:  148700, Batch Loss:     3.890736, Lr: 0.000050, Tokens per sec:   3439
2023-03-15 04:30:21,313 - INFO - __main__ - Epoch  69, Step:  148800, Batch Loss:     4.105633, Lr: 0.000050, Tokens per sec:   3442
2023-03-15 04:30:36,982 - INFO - __main__ - Epoch  69, Step:  148900, Batch Loss:     3.845037, Lr: 0.000050, Tokens per sec:   3391
2023-03-15 04:30:52,326 - INFO - __main__ - Epoch  69, Step:  149000, Batch Loss:     3.317314, Lr: 0.000050, Tokens per sec:   3561
2023-03-15 04:31:07,658 - INFO - __main__ - Epoch  69, Step:  149100, Batch Loss:     3.452200, Lr: 0.000050, Tokens per sec:   3506
2023-03-15 04:31:23,293 - INFO - __main__ - Epoch  69, Step:  149200, Batch Loss:     3.506624, Lr: 0.000050, Tokens per sec:   3467
2023-03-15 04:31:38,753 - INFO - __main__ - Epoch  69, Step:  149300, Batch Loss:     6.391183, Lr: 0.000050, Tokens per sec:   3527
2023-03-15 04:31:54,342 - INFO - __main__ - Epoch  69, Step:  149400, Batch Loss:     2.322592, Lr: 0.000050, Tokens per sec:   3445
2023-03-15 04:32:09,762 - INFO - __main__ - Epoch  69, Step:  149500, Batch Loss:     3.675474, Lr: 0.000050, Tokens per sec:   3497
2023-03-15 04:32:25,193 - INFO - __main__ - Epoch  69, Step:  149600, Batch Loss:     3.219171, Lr: 0.000050, Tokens per sec:   3512
2023-03-15 04:32:40,803 - INFO - __main__ - Epoch  69, Step:  149700, Batch Loss:     4.477843, Lr: 0.000050, Tokens per sec:   3483
2023-03-15 04:32:56,337 - INFO - __main__ - Epoch  69, Step:  149800, Batch Loss:     3.601008, Lr: 0.000050, Tokens per sec:   3484
2023-03-15 04:33:12,091 - INFO - __main__ - Epoch  69, Step:  149900, Batch Loss:     3.025379, Lr: 0.000050, Tokens per sec:   3412
2023-03-15 04:33:27,651 - INFO - __main__ - Epoch  69, Step:  150000, Batch Loss:     4.666764, Lr: 0.000050, Tokens per sec:   3465
2023-03-15 04:33:43,344 - INFO - __main__ - Epoch  69, Step:  150100, Batch Loss:     4.236663, Lr: 0.000050, Tokens per sec:   3458
2023-03-15 04:33:58,935 - INFO - __main__ - Epoch  69, Step:  150200, Batch Loss:     3.796660, Lr: 0.000050, Tokens per sec:   3429
2023-03-15 04:34:14,196 - INFO - __main__ - Epoch  69, Step:  150300, Batch Loss:     4.424911, Lr: 0.000050, Tokens per sec:   3455
2023-03-15 04:34:22,149 - INFO - __main__ - Epoch  69: total training loss 8592.84
2023-03-15 04:34:22,150 - INFO - __main__ - Epoch 70
2023-03-15 04:34:29,803 - INFO - __main__ - Epoch  70, Step:  150400, Batch Loss:     2.655429, Lr: 0.000050, Tokens per sec:   3410
2023-03-15 04:34:45,298 - INFO - __main__ - Epoch  70, Step:  150500, Batch Loss:     3.350194, Lr: 0.000050, Tokens per sec:   3496
2023-03-15 04:35:00,945 - INFO - __main__ - Epoch  70, Step:  150600, Batch Loss:     4.087396, Lr: 0.000050, Tokens per sec:   3475
2023-03-15 04:35:16,534 - INFO - __main__ - Epoch  70, Step:  150700, Batch Loss:     3.411856, Lr: 0.000050, Tokens per sec:   3440
2023-03-15 04:35:32,160 - INFO - __main__ - Epoch  70, Step:  150800, Batch Loss:     4.162922, Lr: 0.000050, Tokens per sec:   3386
2023-03-15 04:35:47,635 - INFO - __main__ - Epoch  70, Step:  150900, Batch Loss:     4.684752, Lr: 0.000050, Tokens per sec:   3517
2023-03-15 04:36:03,236 - INFO - __main__ - Epoch  70, Step:  151000, Batch Loss:     3.693862, Lr: 0.000050, Tokens per sec:   3478
2023-03-15 04:36:18,835 - INFO - __main__ - Epoch  70, Step:  151100, Batch Loss:     4.400006, Lr: 0.000050, Tokens per sec:   3511
2023-03-15 04:36:34,395 - INFO - __main__ - Epoch  70, Step:  151200, Batch Loss:     4.240391, Lr: 0.000050, Tokens per sec:   3364
2023-03-15 04:36:49,640 - INFO - __main__ - Epoch  70, Step:  151300, Batch Loss:     4.719594, Lr: 0.000050, Tokens per sec:   3515
2023-03-15 04:37:04,974 - INFO - __main__ - Epoch  70, Step:  151400, Batch Loss:     3.728481, Lr: 0.000050, Tokens per sec:   3482
2023-03-15 04:37:20,431 - INFO - __main__ - Epoch  70, Step:  151500, Batch Loss:     4.946790, Lr: 0.000050, Tokens per sec:   3511
2023-03-15 04:37:35,666 - INFO - __main__ - Epoch  70, Step:  151600, Batch Loss:     2.978647, Lr: 0.000050, Tokens per sec:   3545
2023-03-15 04:37:51,065 - INFO - __main__ - Epoch  70, Step:  151700, Batch Loss:     3.664525, Lr: 0.000050, Tokens per sec:   3485
2023-03-15 04:38:06,582 - INFO - __main__ - Epoch  70, Step:  151800, Batch Loss:     3.640030, Lr: 0.000050, Tokens per sec:   3385
2023-03-15 04:38:22,024 - INFO - __main__ - Epoch  70, Step:  151900, Batch Loss:     3.728361, Lr: 0.000050, Tokens per sec:   3520
2023-03-15 04:38:37,631 - INFO - __main__ - Epoch  70, Step:  152000, Batch Loss:     3.404313, Lr: 0.000050, Tokens per sec:   3428
2023-03-15 04:38:52,853 - INFO - __main__ - Epoch  70, Step:  152100, Batch Loss:     3.779692, Lr: 0.000050, Tokens per sec:   3514
2023-03-15 04:39:08,332 - INFO - __main__ - Epoch  70, Step:  152200, Batch Loss:     3.100716, Lr: 0.000050, Tokens per sec:   3487
2023-03-15 04:39:23,905 - INFO - __main__ - Epoch  70, Step:  152300, Batch Loss:     5.088893, Lr: 0.000050, Tokens per sec:   3486
2023-03-15 04:39:39,466 - INFO - __main__ - Epoch  70, Step:  152400, Batch Loss:     3.323749, Lr: 0.000050, Tokens per sec:   3521
2023-03-15 04:39:54,926 - INFO - __main__ - Epoch  70, Step:  152500, Batch Loss:     3.915732, Lr: 0.000050, Tokens per sec:   3511
2023-03-15 04:39:59,653 - INFO - __main__ - Epoch  70: total training loss 8419.08
2023-03-15 04:39:59,654 - INFO - __main__ - Epoch 71
2023-03-15 04:40:10,868 - INFO - __main__ - Epoch  71, Step:  152600, Batch Loss:     2.980782, Lr: 0.000049, Tokens per sec:   3284
2023-03-15 04:40:26,362 - INFO - __main__ - Epoch  71, Step:  152700, Batch Loss:     3.483212, Lr: 0.000049, Tokens per sec:   3507
2023-03-15 04:40:41,685 - INFO - __main__ - Epoch  71, Step:  152800, Batch Loss:     3.687997, Lr: 0.000049, Tokens per sec:   3462
2023-03-15 04:40:57,310 - INFO - __main__ - Epoch  71, Step:  152900, Batch Loss:     2.808483, Lr: 0.000049, Tokens per sec:   3439
2023-03-15 04:41:12,523 - INFO - __main__ - Epoch  71, Step:  153000, Batch Loss:     3.494031, Lr: 0.000049, Tokens per sec:   3506
2023-03-15 04:41:27,643 - INFO - __main__ - Epoch  71, Step:  153100, Batch Loss:     3.310454, Lr: 0.000049, Tokens per sec:   3544
2023-03-15 04:41:42,887 - INFO - __main__ - Epoch  71, Step:  153200, Batch Loss:     3.475769, Lr: 0.000049, Tokens per sec:   3525
2023-03-15 04:41:58,241 - INFO - __main__ - Epoch  71, Step:  153300, Batch Loss:     3.381786, Lr: 0.000049, Tokens per sec:   3489
2023-03-15 04:42:13,431 - INFO - __main__ - Epoch  71, Step:  153400, Batch Loss:     3.375118, Lr: 0.000049, Tokens per sec:   3572
2023-03-15 04:42:28,269 - INFO - __main__ - Epoch  71, Step:  153500, Batch Loss:     4.793248, Lr: 0.000049, Tokens per sec:   3667
2023-03-15 04:42:43,716 - INFO - __main__ - Epoch  71, Step:  153600, Batch Loss:     5.347649, Lr: 0.000049, Tokens per sec:   3519
2023-03-15 04:42:59,068 - INFO - __main__ - Epoch  71, Step:  153700, Batch Loss:     4.319291, Lr: 0.000049, Tokens per sec:   3520
2023-03-15 04:43:14,326 - INFO - __main__ - Epoch  71, Step:  153800, Batch Loss:     2.451297, Lr: 0.000049, Tokens per sec:   3508
2023-03-15 04:43:29,894 - INFO - __main__ - Epoch  71, Step:  153900, Batch Loss:     3.214823, Lr: 0.000049, Tokens per sec:   3442
2023-03-15 04:43:45,244 - INFO - __main__ - Epoch  71, Step:  154000, Batch Loss:     3.979518, Lr: 0.000049, Tokens per sec:   3477
2023-03-15 04:44:00,618 - INFO - __main__ - Epoch  71, Step:  154100, Batch Loss:     5.369344, Lr: 0.000049, Tokens per sec:   3497
2023-03-15 04:44:15,766 - INFO - __main__ - Epoch  71, Step:  154200, Batch Loss:     2.916142, Lr: 0.000049, Tokens per sec:   3517
2023-03-15 04:44:30,933 - INFO - __main__ - Epoch  71, Step:  154300, Batch Loss:     3.648642, Lr: 0.000049, Tokens per sec:   3571
2023-03-15 04:44:46,262 - INFO - __main__ - Epoch  71, Step:  154400, Batch Loss:     4.382808, Lr: 0.000049, Tokens per sec:   3535
2023-03-15 04:45:01,498 - INFO - __main__ - Epoch  71, Step:  154500, Batch Loss:     3.912165, Lr: 0.000049, Tokens per sec:   3568
2023-03-15 04:45:16,808 - INFO - __main__ - Epoch  71, Step:  154600, Batch Loss:     3.042947, Lr: 0.000049, Tokens per sec:   3546
2023-03-15 04:45:32,229 - INFO - __main__ - Epoch  71, Step:  154700, Batch Loss:     4.799769, Lr: 0.000049, Tokens per sec:   3519
2023-03-15 04:45:33,544 - INFO - __main__ - Epoch  71: total training loss 8245.52
2023-03-15 04:45:33,545 - INFO - __main__ - Epoch 72
2023-03-15 04:45:47,865 - INFO - __main__ - Epoch  72, Step:  154800, Batch Loss:     2.400913, Lr: 0.000049, Tokens per sec:   3418
2023-03-15 04:46:03,290 - INFO - __main__ - Epoch  72, Step:  154900, Batch Loss:     3.674672, Lr: 0.000049, Tokens per sec:   3502
2023-03-15 04:46:18,822 - INFO - __main__ - Epoch  72, Step:  155000, Batch Loss:     2.903770, Lr: 0.000049, Tokens per sec:   3445
2023-03-15 04:46:34,342 - INFO - __main__ - Epoch  72, Step:  155100, Batch Loss:     2.753293, Lr: 0.000049, Tokens per sec:   3458
2023-03-15 04:46:49,538 - INFO - __main__ - Epoch  72, Step:  155200, Batch Loss:     3.013742, Lr: 0.000049, Tokens per sec:   3574
2023-03-15 04:47:04,796 - INFO - __main__ - Epoch  72, Step:  155300, Batch Loss:     3.829304, Lr: 0.000049, Tokens per sec:   3582
2023-03-15 04:47:20,338 - INFO - __main__ - Epoch  72, Step:  155400, Batch Loss:     3.679496, Lr: 0.000049, Tokens per sec:   3465
2023-03-15 04:47:35,788 - INFO - __main__ - Epoch  72, Step:  155500, Batch Loss:     3.297843, Lr: 0.000049, Tokens per sec:   3484
2023-03-15 04:47:51,130 - INFO - __main__ - Epoch  72, Step:  155600, Batch Loss:     2.925556, Lr: 0.000049, Tokens per sec:   3568
2023-03-15 04:48:06,338 - INFO - __main__ - Epoch  72, Step:  155700, Batch Loss:     4.014646, Lr: 0.000049, Tokens per sec:   3539
2023-03-15 04:48:21,567 - INFO - __main__ - Epoch  72, Step:  155800, Batch Loss:     3.467431, Lr: 0.000049, Tokens per sec:   3540
2023-03-15 04:48:36,913 - INFO - __main__ - Epoch  72, Step:  155900, Batch Loss:     2.675836, Lr: 0.000049, Tokens per sec:   3521
2023-03-15 04:48:52,215 - INFO - __main__ - Epoch  72, Step:  156000, Batch Loss:     2.764534, Lr: 0.000049, Tokens per sec:   3492
2023-03-15 04:49:07,613 - INFO - __main__ - Epoch  72, Step:  156100, Batch Loss:     2.752534, Lr: 0.000049, Tokens per sec:   3452
2023-03-15 04:49:22,683 - INFO - __main__ - Epoch  72, Step:  156200, Batch Loss:     4.661410, Lr: 0.000049, Tokens per sec:   3645
2023-03-15 04:49:38,227 - INFO - __main__ - Epoch  72, Step:  156300, Batch Loss:     2.940503, Lr: 0.000049, Tokens per sec:   3457
2023-03-15 04:49:53,407 - INFO - __main__ - Epoch  72, Step:  156400, Batch Loss:     5.389210, Lr: 0.000049, Tokens per sec:   3546
2023-03-15 04:50:08,862 - INFO - __main__ - Epoch  72, Step:  156500, Batch Loss:     4.971227, Lr: 0.000049, Tokens per sec:   3467
2023-03-15 04:50:23,829 - INFO - __main__ - Epoch  72, Step:  156600, Batch Loss:     4.019220, Lr: 0.000049, Tokens per sec:   3560
2023-03-15 04:50:39,007 - INFO - __main__ - Epoch  72, Step:  156700, Batch Loss:     3.093017, Lr: 0.000049, Tokens per sec:   3509
2023-03-15 04:50:54,403 - INFO - __main__ - Epoch  72, Step:  156800, Batch Loss:     4.404834, Lr: 0.000049, Tokens per sec:   3513
2023-03-15 04:51:08,161 - INFO - __main__ - Epoch  72: total training loss 8108.21
2023-03-15 04:51:08,162 - INFO - __main__ - Epoch 73
2023-03-15 04:51:10,408 - INFO - __main__ - Epoch  73, Step:  156900, Batch Loss:     2.756434, Lr: 0.000048, Tokens per sec:   2879
2023-03-15 04:51:25,839 - INFO - __main__ - Epoch  73, Step:  157000, Batch Loss:     2.985828, Lr: 0.000048, Tokens per sec:   3497
2023-03-15 04:51:41,451 - INFO - __main__ - Epoch  73, Step:  157100, Batch Loss:     3.920909, Lr: 0.000048, Tokens per sec:   3425
2023-03-15 04:51:56,766 - INFO - __main__ - Epoch  73, Step:  157200, Batch Loss:     3.991223, Lr: 0.000048, Tokens per sec:   3582
2023-03-15 04:52:12,264 - INFO - __main__ - Epoch  73, Step:  157300, Batch Loss:     2.939835, Lr: 0.000048, Tokens per sec:   3479
2023-03-15 04:52:27,836 - INFO - __main__ - Epoch  73, Step:  157400, Batch Loss:     3.100138, Lr: 0.000048, Tokens per sec:   3464
2023-03-15 04:52:43,428 - INFO - __main__ - Epoch  73, Step:  157500, Batch Loss:     2.700863, Lr: 0.000048, Tokens per sec:   3430
2023-03-15 04:52:58,752 - INFO - __main__ - Epoch  73, Step:  157600, Batch Loss:     3.285691, Lr: 0.000048, Tokens per sec:   3510
2023-03-15 04:53:14,227 - INFO - __main__ - Epoch  73, Step:  157700, Batch Loss:     3.050798, Lr: 0.000048, Tokens per sec:   3468
2023-03-15 04:53:29,758 - INFO - __main__ - Epoch  73, Step:  157800, Batch Loss:     4.240771, Lr: 0.000048, Tokens per sec:   3430
2023-03-15 04:53:45,275 - INFO - __main__ - Epoch  73, Step:  157900, Batch Loss:     3.191973, Lr: 0.000048, Tokens per sec:   3471
2023-03-15 04:54:00,569 - INFO - __main__ - Epoch  73, Step:  158000, Batch Loss:     2.569189, Lr: 0.000048, Tokens per sec:   3542
2023-03-15 04:54:16,003 - INFO - __main__ - Epoch  73, Step:  158100, Batch Loss:     4.128004, Lr: 0.000048, Tokens per sec:   3556
2023-03-15 04:54:31,350 - INFO - __main__ - Epoch  73, Step:  158200, Batch Loss:     3.808223, Lr: 0.000048, Tokens per sec:   3520
2023-03-15 04:54:46,845 - INFO - __main__ - Epoch  73, Step:  158300, Batch Loss:     4.348007, Lr: 0.000048, Tokens per sec:   3466
2023-03-15 04:55:02,054 - INFO - __main__ - Epoch  73, Step:  158400, Batch Loss:     4.341311, Lr: 0.000048, Tokens per sec:   3558
2023-03-15 04:55:17,555 - INFO - __main__ - Epoch  73, Step:  158500, Batch Loss:     2.605692, Lr: 0.000048, Tokens per sec:   3461
2023-03-15 04:55:32,995 - INFO - __main__ - Epoch  73, Step:  158600, Batch Loss:     3.818346, Lr: 0.000048, Tokens per sec:   3464
2023-03-15 04:55:48,595 - INFO - __main__ - Epoch  73, Step:  158700, Batch Loss:     2.553968, Lr: 0.000048, Tokens per sec:   3410
2023-03-15 04:56:04,078 - INFO - __main__ - Epoch  73, Step:  158800, Batch Loss:     4.384714, Lr: 0.000048, Tokens per sec:   3511
2023-03-15 04:56:19,644 - INFO - __main__ - Epoch  73, Step:  158900, Batch Loss:     3.088984, Lr: 0.000048, Tokens per sec:   3418
2023-03-15 04:56:35,160 - INFO - __main__ - Epoch  73, Step:  159000, Batch Loss:     3.206322, Lr: 0.000048, Tokens per sec:   3465
2023-03-15 04:56:45,455 - INFO - __main__ - Epoch  73: total training loss 7893.49
2023-03-15 04:56:45,456 - INFO - __main__ - Epoch 74
2023-03-15 04:56:50,890 - INFO - __main__ - Epoch  74, Step:  159100, Batch Loss:     3.494564, Lr: 0.000048, Tokens per sec:   3396
2023-03-15 04:57:06,064 - INFO - __main__ - Epoch  74, Step:  159200, Batch Loss:     2.452150, Lr: 0.000048, Tokens per sec:   3572
2023-03-15 04:57:21,602 - INFO - __main__ - Epoch  74, Step:  159300, Batch Loss:     2.467659, Lr: 0.000048, Tokens per sec:   3467
2023-03-15 04:57:37,006 - INFO - __main__ - Epoch  74, Step:  159400, Batch Loss:     3.613380, Lr: 0.000048, Tokens per sec:   3546
2023-03-15 04:57:52,320 - INFO - __main__ - Epoch  74, Step:  159500, Batch Loss:     4.426160, Lr: 0.000048, Tokens per sec:   3539
2023-03-15 04:58:07,830 - INFO - __main__ - Epoch  74, Step:  159600, Batch Loss:     3.679571, Lr: 0.000048, Tokens per sec:   3494
2023-03-15 04:58:23,574 - INFO - __main__ - Epoch  74, Step:  159700, Batch Loss:     3.480761, Lr: 0.000048, Tokens per sec:   3391
2023-03-15 04:58:39,200 - INFO - __main__ - Epoch  74, Step:  159800, Batch Loss:     3.188114, Lr: 0.000048, Tokens per sec:   3482
2023-03-15 04:58:54,603 - INFO - __main__ - Epoch  74, Step:  159900, Batch Loss:     3.149490, Lr: 0.000048, Tokens per sec:   3518
2023-03-15 04:59:09,964 - INFO - __main__ - Epoch  74, Step:  160000, Batch Loss:     3.757542, Lr: 0.000048, Tokens per sec:   3502
2023-03-15 04:59:25,567 - INFO - __main__ - Epoch  74, Step:  160100, Batch Loss:     5.733320, Lr: 0.000048, Tokens per sec:   3452
2023-03-15 04:59:41,037 - INFO - __main__ - Epoch  74, Step:  160200, Batch Loss:     3.826418, Lr: 0.000048, Tokens per sec:   3451
2023-03-15 04:59:56,507 - INFO - __main__ - Epoch  74, Step:  160300, Batch Loss:     2.129056, Lr: 0.000048, Tokens per sec:   3500
2023-03-15 05:00:11,811 - INFO - __main__ - Epoch  74, Step:  160400, Batch Loss:     3.928399, Lr: 0.000048, Tokens per sec:   3500
2023-03-15 05:00:27,413 - INFO - __main__ - Epoch  74, Step:  160500, Batch Loss:     3.116251, Lr: 0.000048, Tokens per sec:   3371
2023-03-15 05:00:42,832 - INFO - __main__ - Epoch  74, Step:  160600, Batch Loss:     3.650365, Lr: 0.000048, Tokens per sec:   3552
2023-03-15 05:00:58,369 - INFO - __main__ - Epoch  74, Step:  160700, Batch Loss:     2.577277, Lr: 0.000048, Tokens per sec:   3418
2023-03-15 05:01:13,922 - INFO - __main__ - Epoch  74, Step:  160800, Batch Loss:     2.717096, Lr: 0.000048, Tokens per sec:   3453
2023-03-15 05:01:29,449 - INFO - __main__ - Epoch  74, Step:  160900, Batch Loss:     4.657920, Lr: 0.000048, Tokens per sec:   3476
2023-03-15 05:01:44,990 - INFO - __main__ - Epoch  74, Step:  161000, Batch Loss:     2.880428, Lr: 0.000048, Tokens per sec:   3394
2023-03-15 05:02:00,479 - INFO - __main__ - Epoch  74, Step:  161100, Batch Loss:     2.656109, Lr: 0.000048, Tokens per sec:   3453
2023-03-15 05:02:15,847 - INFO - __main__ - Epoch  74, Step:  161200, Batch Loss:     2.555977, Lr: 0.000048, Tokens per sec:   3480
2023-03-15 05:02:23,062 - INFO - __main__ - Epoch  74: total training loss 7789.84
2023-03-15 05:02:23,063 - INFO - __main__ - Epoch 75
2023-03-15 05:02:31,783 - INFO - __main__ - Epoch  75, Step:  161300, Batch Loss:     4.390266, Lr: 0.000048, Tokens per sec:   3293
2023-03-15 05:02:47,358 - INFO - __main__ - Epoch  75, Step:  161400, Batch Loss:     2.061053, Lr: 0.000048, Tokens per sec:   3455
2023-03-15 05:03:02,943 - INFO - __main__ - Epoch  75, Step:  161500, Batch Loss:     2.657398, Lr: 0.000048, Tokens per sec:   3412
2023-03-15 05:03:18,560 - INFO - __main__ - Epoch  75, Step:  161600, Batch Loss:     3.820441, Lr: 0.000048, Tokens per sec:   3431
2023-03-15 05:03:34,118 - INFO - __main__ - Epoch  75, Step:  161700, Batch Loss:     3.339253, Lr: 0.000048, Tokens per sec:   3516
2023-03-15 05:03:49,687 - INFO - __main__ - Epoch  75, Step:  161800, Batch Loss:     4.431608, Lr: 0.000048, Tokens per sec:   3453
2023-03-15 05:04:05,204 - INFO - __main__ - Epoch  75, Step:  161900, Batch Loss:     4.180570, Lr: 0.000048, Tokens per sec:   3444
2023-03-15 05:04:20,717 - INFO - __main__ - Epoch  75, Step:  162000, Batch Loss:     3.851296, Lr: 0.000048, Tokens per sec:   3454
2023-03-15 05:04:35,963 - INFO - __main__ - Epoch  75, Step:  162100, Batch Loss:     5.288859, Lr: 0.000048, Tokens per sec:   3573
2023-03-15 05:04:51,158 - INFO - __main__ - Epoch  75, Step:  162200, Batch Loss:     3.621098, Lr: 0.000048, Tokens per sec:   3561
2023-03-15 05:05:06,672 - INFO - __main__ - Epoch  75, Step:  162300, Batch Loss:     3.555640, Lr: 0.000048, Tokens per sec:   3520
2023-03-15 05:05:22,069 - INFO - __main__ - Epoch  75, Step:  162400, Batch Loss:     4.272639, Lr: 0.000048, Tokens per sec:   3507
2023-03-15 05:05:37,561 - INFO - __main__ - Epoch  75, Step:  162500, Batch Loss:     4.388572, Lr: 0.000048, Tokens per sec:   3490
2023-03-15 05:05:53,134 - INFO - __main__ - Epoch  75, Step:  162600, Batch Loss:     3.654839, Lr: 0.000048, Tokens per sec:   3513
2023-03-15 05:06:08,703 - INFO - __main__ - Epoch  75, Step:  162700, Batch Loss:     4.176668, Lr: 0.000048, Tokens per sec:   3469
2023-03-15 05:06:24,272 - INFO - __main__ - Epoch  75, Step:  162800, Batch Loss:     3.630541, Lr: 0.000048, Tokens per sec:   3500
2023-03-15 05:06:39,825 - INFO - __main__ - Epoch  75, Step:  162900, Batch Loss:     3.609226, Lr: 0.000048, Tokens per sec:   3427
2023-03-15 05:06:55,371 - INFO - __main__ - Epoch  75, Step:  163000, Batch Loss:     3.741584, Lr: 0.000048, Tokens per sec:   3452
2023-03-15 05:07:11,001 - INFO - __main__ - Epoch  75, Step:  163100, Batch Loss:     2.964277, Lr: 0.000048, Tokens per sec:   3445
2023-03-15 05:07:26,557 - INFO - __main__ - Epoch  75, Step:  163200, Batch Loss:     4.522696, Lr: 0.000048, Tokens per sec:   3402
2023-03-15 05:07:42,156 - INFO - __main__ - Epoch  75, Step:  163300, Batch Loss:     3.994219, Lr: 0.000048, Tokens per sec:   3427
2023-03-15 05:07:57,769 - INFO - __main__ - Epoch  75, Step:  163400, Batch Loss:     3.118239, Lr: 0.000048, Tokens per sec:   3413
2023-03-15 05:08:01,633 - INFO - __main__ - Epoch  75: total training loss 7595.06
2023-03-15 05:08:01,634 - INFO - __main__ - Epoch 76
2023-03-15 05:08:13,740 - INFO - __main__ - Epoch  76, Step:  163500, Batch Loss:     3.747481, Lr: 0.000047, Tokens per sec:   3373
2023-03-15 05:08:29,182 - INFO - __main__ - Epoch  76, Step:  163600, Batch Loss:     3.206093, Lr: 0.000047, Tokens per sec:   3504
2023-03-15 05:08:44,991 - INFO - __main__ - Epoch  76, Step:  163700, Batch Loss:     2.781086, Lr: 0.000047, Tokens per sec:   3390
2023-03-15 05:09:00,625 - INFO - __main__ - Epoch  76, Step:  163800, Batch Loss:     3.057791, Lr: 0.000047, Tokens per sec:   3421
2023-03-15 05:09:15,969 - INFO - __main__ - Epoch  76, Step:  163900, Batch Loss:     2.880325, Lr: 0.000047, Tokens per sec:   3503
2023-03-15 05:09:31,302 - INFO - __main__ - Epoch  76, Step:  164000, Batch Loss:     4.025115, Lr: 0.000047, Tokens per sec:   3452
2023-03-15 05:09:46,787 - INFO - __main__ - Epoch  76, Step:  164100, Batch Loss:     3.509567, Lr: 0.000047, Tokens per sec:   3364
2023-03-15 05:10:02,368 - INFO - __main__ - Epoch  76, Step:  164200, Batch Loss:     2.293592, Lr: 0.000047, Tokens per sec:   3412
2023-03-15 05:10:17,776 - INFO - __main__ - Epoch  76, Step:  164300, Batch Loss:     4.277211, Lr: 0.000047, Tokens per sec:   3537
2023-03-15 05:10:33,298 - INFO - __main__ - Epoch  76, Step:  164400, Batch Loss:     3.244503, Lr: 0.000047, Tokens per sec:   3475
2023-03-15 05:10:48,940 - INFO - __main__ - Epoch  76, Step:  164500, Batch Loss:     4.341262, Lr: 0.000047, Tokens per sec:   3432
2023-03-15 05:11:04,577 - INFO - __main__ - Epoch  76, Step:  164600, Batch Loss:     2.602858, Lr: 0.000047, Tokens per sec:   3506
2023-03-15 05:11:20,249 - INFO - __main__ - Epoch  76, Step:  164700, Batch Loss:     3.690254, Lr: 0.000047, Tokens per sec:   3418
2023-03-15 05:11:35,788 - INFO - __main__ - Epoch  76, Step:  164800, Batch Loss:     3.555652, Lr: 0.000047, Tokens per sec:   3436
2023-03-15 05:11:51,256 - INFO - __main__ - Epoch  76, Step:  164900, Batch Loss:     3.793404, Lr: 0.000047, Tokens per sec:   3501
2023-03-15 05:12:06,573 - INFO - __main__ - Epoch  76, Step:  165000, Batch Loss:     2.910476, Lr: 0.000047, Tokens per sec:   3496
2023-03-15 05:12:22,144 - INFO - __main__ - Epoch  76, Step:  165100, Batch Loss:     3.059413, Lr: 0.000047, Tokens per sec:   3522
2023-03-15 05:12:37,594 - INFO - __main__ - Epoch  76, Step:  165200, Batch Loss:     2.943776, Lr: 0.000047, Tokens per sec:   3520
2023-03-15 05:12:53,107 - INFO - __main__ - Epoch  76, Step:  165300, Batch Loss:     3.444097, Lr: 0.000047, Tokens per sec:   3546
2023-03-15 05:13:08,557 - INFO - __main__ - Epoch  76, Step:  165400, Batch Loss:     3.694364, Lr: 0.000047, Tokens per sec:   3470
2023-03-15 05:13:23,645 - INFO - __main__ - Epoch  76, Step:  165500, Batch Loss:     3.344975, Lr: 0.000047, Tokens per sec:   3525
2023-03-15 05:13:38,954 - INFO - __main__ - Epoch  76, Step:  165600, Batch Loss:     3.963572, Lr: 0.000047, Tokens per sec:   3544
2023-03-15 05:13:39,692 - INFO - __main__ - Epoch  76: total training loss 7496.67
2023-03-15 05:13:39,693 - INFO - __main__ - Epoch 77
2023-03-15 05:13:54,704 - INFO - __main__ - Epoch  77, Step:  165700, Batch Loss:     3.550212, Lr: 0.000047, Tokens per sec:   3491
2023-03-15 05:14:10,181 - INFO - __main__ - Epoch  77, Step:  165800, Batch Loss:     2.743500, Lr: 0.000047, Tokens per sec:   3539
2023-03-15 05:14:25,429 - INFO - __main__ - Epoch  77, Step:  165900, Batch Loss:     4.464359, Lr: 0.000047, Tokens per sec:   3496
2023-03-15 05:14:40,872 - INFO - __main__ - Epoch  77, Step:  166000, Batch Loss:     3.552641, Lr: 0.000047, Tokens per sec:   3473
2023-03-15 05:14:56,045 - INFO - __main__ - Epoch  77, Step:  166100, Batch Loss:     3.733656, Lr: 0.000047, Tokens per sec:   3525
2023-03-15 05:15:11,549 - INFO - __main__ - Epoch  77, Step:  166200, Batch Loss:     3.548580, Lr: 0.000047, Tokens per sec:   3465
2023-03-15 05:15:26,984 - INFO - __main__ - Epoch  77, Step:  166300, Batch Loss:     3.762540, Lr: 0.000047, Tokens per sec:   3526
2023-03-15 05:15:42,580 - INFO - __main__ - Epoch  77, Step:  166400, Batch Loss:     3.546482, Lr: 0.000047, Tokens per sec:   3504
2023-03-15 05:15:58,022 - INFO - __main__ - Epoch  77, Step:  166500, Batch Loss:     3.647101, Lr: 0.000047, Tokens per sec:   3493
2023-03-15 05:16:12,713 - INFO - __main__ - Epoch  77, Step:  166600, Batch Loss:     3.991889, Lr: 0.000047, Tokens per sec:   3622
2023-03-15 05:16:28,150 - INFO - __main__ - Epoch  77, Step:  166700, Batch Loss:     4.037364, Lr: 0.000047, Tokens per sec:   3355
2023-03-15 05:16:43,422 - INFO - __main__ - Epoch  77, Step:  166800, Batch Loss:     4.061351, Lr: 0.000047, Tokens per sec:   3492
2023-03-15 05:16:58,998 - INFO - __main__ - Epoch  77, Step:  166900, Batch Loss:     3.591847, Lr: 0.000047, Tokens per sec:   3477
2023-03-15 05:17:14,680 - INFO - __main__ - Epoch  77, Step:  167000, Batch Loss:     2.489454, Lr: 0.000047, Tokens per sec:   3422
2023-03-15 05:17:30,083 - INFO - __main__ - Epoch  77, Step:  167100, Batch Loss:     3.130059, Lr: 0.000047, Tokens per sec:   3508
2023-03-15 05:17:45,570 - INFO - __main__ - Epoch  77, Step:  167200, Batch Loss:     3.249716, Lr: 0.000047, Tokens per sec:   3525
2023-03-15 05:18:01,277 - INFO - __main__ - Epoch  77, Step:  167300, Batch Loss:     3.513460, Lr: 0.000047, Tokens per sec:   3363
2023-03-15 05:18:16,583 - INFO - __main__ - Epoch  77, Step:  167400, Batch Loss:     3.908780, Lr: 0.000047, Tokens per sec:   3514
2023-03-15 05:18:32,166 - INFO - __main__ - Epoch  77, Step:  167500, Batch Loss:     3.759887, Lr: 0.000047, Tokens per sec:   3437
2023-03-15 05:18:47,701 - INFO - __main__ - Epoch  77, Step:  167600, Batch Loss:     3.412891, Lr: 0.000047, Tokens per sec:   3504
2023-03-15 05:19:02,909 - INFO - __main__ - Epoch  77, Step:  167700, Batch Loss:     3.644810, Lr: 0.000047, Tokens per sec:   3586
2023-03-15 05:19:15,790 - INFO - __main__ - Epoch  77: total training loss 7395.81
2023-03-15 05:19:15,791 - INFO - __main__ - Epoch 78
2023-03-15 05:19:18,810 - INFO - __main__ - Epoch  78, Step:  167800, Batch Loss:     4.032617, Lr: 0.000046, Tokens per sec:   3105
2023-03-15 05:19:34,255 - INFO - __main__ - Epoch  78, Step:  167900, Batch Loss:     2.842432, Lr: 0.000046, Tokens per sec:   3501
2023-03-15 05:19:49,712 - INFO - __main__ - Epoch  78, Step:  168000, Batch Loss:     3.485612, Lr: 0.000046, Tokens per sec:   3463
2023-03-15 05:20:05,083 - INFO - __main__ - Epoch  78, Step:  168100, Batch Loss:     3.733200, Lr: 0.000046, Tokens per sec:   3503
2023-03-15 05:20:20,162 - INFO - __main__ - Epoch  78, Step:  168200, Batch Loss:     2.962400, Lr: 0.000046, Tokens per sec:   3577
2023-03-15 05:20:35,707 - INFO - __main__ - Epoch  78, Step:  168300, Batch Loss:     3.565800, Lr: 0.000046, Tokens per sec:   3450
2023-03-15 05:20:51,148 - INFO - __main__ - Epoch  78, Step:  168400, Batch Loss:     2.622146, Lr: 0.000046, Tokens per sec:   3520
2023-03-15 05:21:06,554 - INFO - __main__ - Epoch  78, Step:  168500, Batch Loss:     3.685337, Lr: 0.000046, Tokens per sec:   3528
2023-03-15 05:21:21,802 - INFO - __main__ - Epoch  78, Step:  168600, Batch Loss:     4.011602, Lr: 0.000046, Tokens per sec:   3450
2023-03-15 05:21:37,173 - INFO - __main__ - Epoch  78, Step:  168700, Batch Loss:     1.634730, Lr: 0.000046, Tokens per sec:   3564
2023-03-15 05:21:52,805 - INFO - __main__ - Epoch  78, Step:  168800, Batch Loss:     4.110619, Lr: 0.000046, Tokens per sec:   3470
2023-03-15 05:22:08,277 - INFO - __main__ - Epoch  78, Step:  168900, Batch Loss:     3.888029, Lr: 0.000046, Tokens per sec:   3511
2023-03-15 05:22:23,768 - INFO - __main__ - Epoch  78, Step:  169000, Batch Loss:     2.425827, Lr: 0.000046, Tokens per sec:   3514
2023-03-15 05:22:39,297 - INFO - __main__ - Epoch  78, Step:  169100, Batch Loss:     3.534711, Lr: 0.000046, Tokens per sec:   3423
2023-03-15 05:22:54,841 - INFO - __main__ - Epoch  78, Step:  169200, Batch Loss:     3.039085, Lr: 0.000046, Tokens per sec:   3479
2023-03-15 05:23:10,188 - INFO - __main__ - Epoch  78, Step:  169300, Batch Loss:     3.321170, Lr: 0.000046, Tokens per sec:   3410
2023-03-15 05:23:25,688 - INFO - __main__ - Epoch  78, Step:  169400, Batch Loss:     3.688661, Lr: 0.000046, Tokens per sec:   3499
2023-03-15 05:23:41,198 - INFO - __main__ - Epoch  78, Step:  169500, Batch Loss:     4.237125, Lr: 0.000046, Tokens per sec:   3470
2023-03-15 05:23:56,638 - INFO - __main__ - Epoch  78, Step:  169600, Batch Loss:     3.123081, Lr: 0.000046, Tokens per sec:   3468
2023-03-15 05:24:12,184 - INFO - __main__ - Epoch  78, Step:  169700, Batch Loss:     3.368468, Lr: 0.000046, Tokens per sec:   3476
2023-03-15 05:24:27,583 - INFO - __main__ - Epoch  78, Step:  169800, Batch Loss:     4.517312, Lr: 0.000046, Tokens per sec:   3491
2023-03-15 05:24:42,818 - INFO - __main__ - Epoch  78, Step:  169900, Batch Loss:     4.149942, Lr: 0.000046, Tokens per sec:   3534
2023-03-15 05:24:52,391 - INFO - __main__ - Epoch  78: total training loss 7212.52
2023-03-15 05:24:52,393 - INFO - __main__ - Epoch 79
2023-03-15 05:24:58,635 - INFO - __main__ - Epoch  79, Step:  170000, Batch Loss:     2.669631, Lr: 0.000046, Tokens per sec:   3290
2023-03-15 05:25:14,052 - INFO - __main__ - Epoch  79, Step:  170100, Batch Loss:     3.451440, Lr: 0.000046, Tokens per sec:   3508
2023-03-15 05:25:29,607 - INFO - __main__ - Epoch  79, Step:  170200, Batch Loss:     2.836736, Lr: 0.000046, Tokens per sec:   3511
2023-03-15 05:25:45,133 - INFO - __main__ - Epoch  79, Step:  170300, Batch Loss:     2.872724, Lr: 0.000046, Tokens per sec:   3446
2023-03-15 05:26:00,456 - INFO - __main__ - Epoch  79, Step:  170400, Batch Loss:     3.177063, Lr: 0.000046, Tokens per sec:   3471
2023-03-15 05:26:15,928 - INFO - __main__ - Epoch  79, Step:  170500, Batch Loss:     3.442767, Lr: 0.000046, Tokens per sec:   3441
2023-03-15 05:26:31,326 - INFO - __main__ - Epoch  79, Step:  170600, Batch Loss:     2.826258, Lr: 0.000046, Tokens per sec:   3515
2023-03-15 05:26:46,475 - INFO - __main__ - Epoch  79, Step:  170700, Batch Loss:     2.844302, Lr: 0.000046, Tokens per sec:   3571
2023-03-15 05:27:02,263 - INFO - __main__ - Epoch  79, Step:  170800, Batch Loss:     2.380020, Lr: 0.000046, Tokens per sec:   3472
2023-03-15 05:27:17,797 - INFO - __main__ - Epoch  79, Step:  170900, Batch Loss:     3.404937, Lr: 0.000046, Tokens per sec:   3445
2023-03-15 05:27:32,821 - INFO - __main__ - Epoch  79, Step:  171000, Batch Loss:     3.683700, Lr: 0.000046, Tokens per sec:   3577
2023-03-15 05:27:48,301 - INFO - __main__ - Epoch  79, Step:  171100, Batch Loss:     2.622977, Lr: 0.000046, Tokens per sec:   3512
2023-03-15 05:28:03,787 - INFO - __main__ - Epoch  79, Step:  171200, Batch Loss:     3.647397, Lr: 0.000046, Tokens per sec:   3498
2023-03-15 05:28:19,361 - INFO - __main__ - Epoch  79, Step:  171300, Batch Loss:     3.877284, Lr: 0.000046, Tokens per sec:   3431
2023-03-15 05:28:34,600 - INFO - __main__ - Epoch  79, Step:  171400, Batch Loss:     3.497747, Lr: 0.000046, Tokens per sec:   3527
2023-03-15 05:28:49,972 - INFO - __main__ - Epoch  79, Step:  171500, Batch Loss:     3.931041, Lr: 0.000046, Tokens per sec:   3448
2023-03-15 05:29:05,419 - INFO - __main__ - Epoch  79, Step:  171600, Batch Loss:     2.500965, Lr: 0.000046, Tokens per sec:   3479
2023-03-15 05:29:20,777 - INFO - __main__ - Epoch  79, Step:  171700, Batch Loss:     3.011014, Lr: 0.000046, Tokens per sec:   3477
2023-03-15 05:29:36,259 - INFO - __main__ - Epoch  79, Step:  171800, Batch Loss:     4.460104, Lr: 0.000046, Tokens per sec:   3463
2023-03-15 05:29:51,414 - INFO - __main__ - Epoch  79, Step:  171900, Batch Loss:     3.960314, Lr: 0.000046, Tokens per sec:   3552
2023-03-15 05:30:06,599 - INFO - __main__ - Epoch  79, Step:  172000, Batch Loss:     3.315283, Lr: 0.000046, Tokens per sec:   3586
2023-03-15 05:30:22,183 - INFO - __main__ - Epoch  79, Step:  172100, Batch Loss:     2.581529, Lr: 0.000046, Tokens per sec:   3446
2023-03-15 05:30:28,631 - INFO - __main__ - Epoch  79: total training loss 7140.27
2023-03-15 05:30:28,632 - INFO - __main__ - Epoch 80
2023-03-15 05:30:38,121 - INFO - __main__ - Epoch  80, Step:  172200, Batch Loss:     3.505410, Lr: 0.000045, Tokens per sec:   3318
2023-03-15 05:30:53,544 - INFO - __main__ - Epoch  80, Step:  172300, Batch Loss:     2.086504, Lr: 0.000045, Tokens per sec:   3498
2023-03-15 05:31:08,934 - INFO - __main__ - Epoch  80, Step:  172400, Batch Loss:     3.442348, Lr: 0.000045, Tokens per sec:   3453
2023-03-15 05:31:24,481 - INFO - __main__ - Epoch  80, Step:  172500, Batch Loss:     3.541684, Lr: 0.000045, Tokens per sec:   3436
2023-03-15 05:31:39,650 - INFO - __main__ - Epoch  80, Step:  172600, Batch Loss:     2.668093, Lr: 0.000045, Tokens per sec:   3563
2023-03-15 05:31:54,632 - INFO - __main__ - Epoch  80, Step:  172700, Batch Loss:     3.970326, Lr: 0.000045, Tokens per sec:   3607
2023-03-15 05:32:09,974 - INFO - __main__ - Epoch  80, Step:  172800, Batch Loss:     2.285885, Lr: 0.000045, Tokens per sec:   3508
2023-03-15 05:32:25,167 - INFO - __main__ - Epoch  80, Step:  172900, Batch Loss:     2.816407, Lr: 0.000045, Tokens per sec:   3462
2023-03-15 05:32:40,479 - INFO - __main__ - Epoch  80, Step:  173000, Batch Loss:     3.477878, Lr: 0.000045, Tokens per sec:   3571
2023-03-15 05:32:55,147 - INFO - __main__ - Epoch  80, Step:  173100, Batch Loss:     2.318745, Lr: 0.000045, Tokens per sec:   3689
2023-03-15 05:33:10,549 - INFO - __main__ - Epoch  80, Step:  173200, Batch Loss:     2.627155, Lr: 0.000045, Tokens per sec:   3548
2023-03-15 05:33:26,089 - INFO - __main__ - Epoch  80, Step:  173300, Batch Loss:     3.222244, Lr: 0.000045, Tokens per sec:   3438
2023-03-15 05:33:41,504 - INFO - __main__ - Epoch  80, Step:  173400, Batch Loss:     3.356316, Lr: 0.000045, Tokens per sec:   3474
2023-03-15 05:33:56,797 - INFO - __main__ - Epoch  80, Step:  173500, Batch Loss:     4.766709, Lr: 0.000045, Tokens per sec:   3555
2023-03-15 05:34:12,188 - INFO - __main__ - Epoch  80, Step:  173600, Batch Loss:     2.890255, Lr: 0.000045, Tokens per sec:   3475
2023-03-15 05:34:27,506 - INFO - __main__ - Epoch  80, Step:  173700, Batch Loss:     3.693789, Lr: 0.000045, Tokens per sec:   3485
2023-03-15 05:34:42,702 - INFO - __main__ - Epoch  80, Step:  173800, Batch Loss:     3.479433, Lr: 0.000045, Tokens per sec:   3583
2023-03-15 05:34:57,702 - INFO - __main__ - Epoch  80, Step:  173900, Batch Loss:     4.809501, Lr: 0.000045, Tokens per sec:   3559
2023-03-15 05:35:13,058 - INFO - __main__ - Epoch  80, Step:  174000, Batch Loss:     2.953693, Lr: 0.000045, Tokens per sec:   3593
2023-03-15 05:35:28,066 - INFO - __main__ - Epoch  80, Step:  174100, Batch Loss:     4.033217, Lr: 0.000045, Tokens per sec:   3620
2023-03-15 05:35:43,450 - INFO - __main__ - Epoch  80, Step:  174200, Batch Loss:     4.604262, Lr: 0.000045, Tokens per sec:   3520
2023-03-15 05:35:58,379 - INFO - __main__ - Epoch  80, Step:  174300, Batch Loss:     2.989449, Lr: 0.000045, Tokens per sec:   3549
2023-03-15 05:36:01,576 - INFO - __main__ - Epoch  80: total training loss 7010.02
2023-03-15 05:36:01,576 - INFO - __main__ - Epoch 81
2023-03-15 05:36:14,348 - INFO - __main__ - Epoch  81, Step:  174400, Batch Loss:     3.922558, Lr: 0.000045, Tokens per sec:   3379
2023-03-15 05:36:29,385 - INFO - __main__ - Epoch  81, Step:  174500, Batch Loss:     2.328192, Lr: 0.000045, Tokens per sec:   3608
2023-03-15 05:36:44,766 - INFO - __main__ - Epoch  81, Step:  174600, Batch Loss:     1.500051, Lr: 0.000045, Tokens per sec:   3481
2023-03-15 05:37:00,148 - INFO - __main__ - Epoch  81, Step:  174700, Batch Loss:     3.043332, Lr: 0.000045, Tokens per sec:   3507
2023-03-15 05:37:15,531 - INFO - __main__ - Epoch  81, Step:  174800, Batch Loss:     2.859586, Lr: 0.000045, Tokens per sec:   3430
2023-03-15 05:37:30,998 - INFO - __main__ - Epoch  81, Step:  174900, Batch Loss:     2.868022, Lr: 0.000045, Tokens per sec:   3466
2023-03-15 05:37:46,437 - INFO - __main__ - Epoch  81, Step:  175000, Batch Loss:     2.907382, Lr: 0.000045, Tokens per sec:   3494
2023-03-15 05:38:01,838 - INFO - __main__ - Epoch  81, Step:  175100, Batch Loss:     3.248263, Lr: 0.000045, Tokens per sec:   3477
2023-03-15 05:38:17,206 - INFO - __main__ - Epoch  81, Step:  175200, Batch Loss:     3.573957, Lr: 0.000045, Tokens per sec:   3421
2023-03-15 05:38:32,643 - INFO - __main__ - Epoch  81, Step:  175300, Batch Loss:     2.682755, Lr: 0.000045, Tokens per sec:   3518
2023-03-15 05:38:48,093 - INFO - __main__ - Epoch  81, Step:  175400, Batch Loss:     2.943370, Lr: 0.000045, Tokens per sec:   3512
2023-03-15 05:39:03,726 - INFO - __main__ - Epoch  81, Step:  175500, Batch Loss:     2.347493, Lr: 0.000045, Tokens per sec:   3471
2023-03-15 05:39:19,206 - INFO - __main__ - Epoch  81, Step:  175600, Batch Loss:     3.138740, Lr: 0.000045, Tokens per sec:   3561
2023-03-15 05:39:34,939 - INFO - __main__ - Epoch  81, Step:  175700, Batch Loss:     2.473026, Lr: 0.000045, Tokens per sec:   3406
2023-03-15 05:39:50,091 - INFO - __main__ - Epoch  81, Step:  175800, Batch Loss:     1.745970, Lr: 0.000045, Tokens per sec:   3554
2023-03-15 05:40:05,456 - INFO - __main__ - Epoch  81, Step:  175900, Batch Loss:     4.188264, Lr: 0.000045, Tokens per sec:   3527
2023-03-15 05:40:20,710 - INFO - __main__ - Epoch  81, Step:  176000, Batch Loss:     3.321583, Lr: 0.000045, Tokens per sec:   3546
2023-03-15 05:40:36,181 - INFO - __main__ - Epoch  81, Step:  176100, Batch Loss:     4.178077, Lr: 0.000045, Tokens per sec:   3452
2023-03-15 05:40:51,494 - INFO - __main__ - Epoch  81, Step:  176200, Batch Loss:     3.850402, Lr: 0.000045, Tokens per sec:   3519
2023-03-15 05:41:06,834 - INFO - __main__ - Epoch  81, Step:  176300, Batch Loss:     2.586540, Lr: 0.000045, Tokens per sec:   3502
2023-03-15 05:41:22,203 - INFO - __main__ - Epoch  81, Step:  176400, Batch Loss:     3.563131, Lr: 0.000045, Tokens per sec:   3550
2023-03-15 05:41:37,532 - INFO - __main__ - Epoch  81: total training loss 6901.10
2023-03-15 05:41:37,533 - INFO - __main__ - Epoch 82
2023-03-15 05:41:38,084 - INFO - __main__ - Epoch  82, Step:  176500, Batch Loss:     3.109918, Lr: 0.000044, Tokens per sec:   1088
2023-03-15 05:41:53,731 - INFO - __main__ - Epoch  82, Step:  176600, Batch Loss:     1.524163, Lr: 0.000044, Tokens per sec:   3372
2023-03-15 05:42:09,185 - INFO - __main__ - Epoch  82, Step:  176700, Batch Loss:     2.460366, Lr: 0.000044, Tokens per sec:   3479
2023-03-15 05:42:24,710 - INFO - __main__ - Epoch  82, Step:  176800, Batch Loss:     4.088369, Lr: 0.000044, Tokens per sec:   3480
2023-03-15 05:42:40,315 - INFO - __main__ - Epoch  82, Step:  176900, Batch Loss:     2.769358, Lr: 0.000044, Tokens per sec:   3415
2023-03-15 05:42:55,648 - INFO - __main__ - Epoch  82, Step:  177000, Batch Loss:     4.574712, Lr: 0.000044, Tokens per sec:   3459
2023-03-15 05:43:10,863 - INFO - __main__ - Epoch  82, Step:  177100, Batch Loss:     3.784303, Lr: 0.000044, Tokens per sec:   3504
2023-03-15 05:43:26,226 - INFO - __main__ - Epoch  82, Step:  177200, Batch Loss:     3.613226, Lr: 0.000044, Tokens per sec:   3581
2023-03-15 05:43:41,680 - INFO - __main__ - Epoch  82, Step:  177300, Batch Loss:     3.653223, Lr: 0.000044, Tokens per sec:   3483
2023-03-15 05:43:56,668 - INFO - __main__ - Epoch  82, Step:  177400, Batch Loss:     5.024240, Lr: 0.000044, Tokens per sec:   3642
2023-03-15 05:44:12,056 - INFO - __main__ - Epoch  82, Step:  177500, Batch Loss:     5.116716, Lr: 0.000044, Tokens per sec:   3490
2023-03-15 05:44:26,953 - INFO - __main__ - Epoch  82, Step:  177600, Batch Loss:     3.814911, Lr: 0.000044, Tokens per sec:   3648
2023-03-15 05:44:42,351 - INFO - __main__ - Epoch  82, Step:  177700, Batch Loss:     2.735709, Lr: 0.000044, Tokens per sec:   3496
2023-03-15 05:44:57,874 - INFO - __main__ - Epoch  82, Step:  177800, Batch Loss:     2.034988, Lr: 0.000044, Tokens per sec:   3469
2023-03-15 05:45:13,196 - INFO - __main__ - Epoch  82, Step:  177900, Batch Loss:     3.312283, Lr: 0.000044, Tokens per sec:   3487
2023-03-15 05:45:28,663 - INFO - __main__ - Epoch  82, Step:  178000, Batch Loss:     3.476145, Lr: 0.000044, Tokens per sec:   3489
2023-03-15 05:45:44,251 - INFO - __main__ - Epoch  82, Step:  178100, Batch Loss:     4.217048, Lr: 0.000044, Tokens per sec:   3482
2023-03-15 05:45:59,855 - INFO - __main__ - Epoch  82, Step:  178200, Batch Loss:     2.104610, Lr: 0.000044, Tokens per sec:   3486
2023-03-15 05:46:15,311 - INFO - __main__ - Epoch  82, Step:  178300, Batch Loss:     1.892290, Lr: 0.000044, Tokens per sec:   3493
2023-03-15 05:46:30,963 - INFO - __main__ - Epoch  82, Step:  178400, Batch Loss:     3.617754, Lr: 0.000044, Tokens per sec:   3449
2023-03-15 05:46:46,429 - INFO - __main__ - Epoch  82, Step:  178500, Batch Loss:     2.992290, Lr: 0.000044, Tokens per sec:   3408
2023-03-15 05:47:01,947 - INFO - __main__ - Epoch  82, Step:  178600, Batch Loss:     2.262760, Lr: 0.000044, Tokens per sec:   3522
2023-03-15 05:47:14,011 - INFO - __main__ - Epoch  82: total training loss 6795.75
2023-03-15 05:47:14,012 - INFO - __main__ - Epoch 83
2023-03-15 05:47:17,782 - INFO - __main__ - Epoch  83, Step:  178700, Batch Loss:     1.945276, Lr: 0.000044, Tokens per sec:   3255
2023-03-15 05:47:33,167 - INFO - __main__ - Epoch  83, Step:  178800, Batch Loss:     3.937124, Lr: 0.000044, Tokens per sec:   3480
2023-03-15 05:47:48,516 - INFO - __main__ - Epoch  83, Step:  178900, Batch Loss:     3.616105, Lr: 0.000044, Tokens per sec:   3548
2023-03-15 05:48:04,084 - INFO - __main__ - Epoch  83, Step:  179000, Batch Loss:     3.798292, Lr: 0.000044, Tokens per sec:   3429
2023-03-15 05:48:19,329 - INFO - __main__ - Epoch  83, Step:  179100, Batch Loss:     3.784103, Lr: 0.000044, Tokens per sec:   3567
2023-03-15 05:48:34,749 - INFO - __main__ - Epoch  83, Step:  179200, Batch Loss:     2.632258, Lr: 0.000044, Tokens per sec:   3496
2023-03-15 05:48:50,272 - INFO - __main__ - Epoch  83, Step:  179300, Batch Loss:     2.762702, Lr: 0.000044, Tokens per sec:   3443
2023-03-15 05:49:05,594 - INFO - __main__ - Epoch  83, Step:  179400, Batch Loss:     4.920659, Lr: 0.000044, Tokens per sec:   3485
2023-03-15 05:49:20,872 - INFO - __main__ - Epoch  83, Step:  179500, Batch Loss:     3.598947, Lr: 0.000044, Tokens per sec:   3444
2023-03-15 05:49:36,241 - INFO - __main__ - Epoch  83, Step:  179600, Batch Loss:     2.917875, Lr: 0.000044, Tokens per sec:   3483
2023-03-15 05:49:51,649 - INFO - __main__ - Epoch  83, Step:  179700, Batch Loss:     2.486879, Lr: 0.000044, Tokens per sec:   3494
2023-03-15 05:50:06,749 - INFO - __main__ - Epoch  83, Step:  179800, Batch Loss:     3.019555, Lr: 0.000044, Tokens per sec:   3602
2023-03-15 05:50:22,186 - INFO - __main__ - Epoch  83, Step:  179900, Batch Loss:     2.782153, Lr: 0.000044, Tokens per sec:   3486
2023-03-15 05:50:37,781 - INFO - __main__ - Epoch  83, Step:  180000, Batch Loss:     3.772122, Lr: 0.000044, Tokens per sec:   3401
2023-03-15 05:50:53,084 - INFO - __main__ - Epoch  83, Step:  180100, Batch Loss:     3.487777, Lr: 0.000044, Tokens per sec:   3547
2023-03-15 05:51:08,542 - INFO - __main__ - Epoch  83, Step:  180200, Batch Loss:     2.471084, Lr: 0.000044, Tokens per sec:   3486
2023-03-15 05:51:23,995 - INFO - __main__ - Epoch  83, Step:  180300, Batch Loss:     4.484182, Lr: 0.000044, Tokens per sec:   3498
2023-03-15 05:51:39,474 - INFO - __main__ - Epoch  83, Step:  180400, Batch Loss:     4.426999, Lr: 0.000044, Tokens per sec:   3455
2023-03-15 05:51:54,278 - INFO - __main__ - Epoch  83, Step:  180500, Batch Loss:     3.466442, Lr: 0.000044, Tokens per sec:   3709
2023-03-15 05:52:09,255 - INFO - __main__ - Epoch  83, Step:  180600, Batch Loss:     3.476666, Lr: 0.000044, Tokens per sec:   3613
2023-03-15 05:52:24,192 - INFO - __main__ - Epoch  83, Step:  180700, Batch Loss:     2.121910, Lr: 0.000044, Tokens per sec:   3585
2023-03-15 05:52:39,490 - INFO - __main__ - Epoch  83, Step:  180800, Batch Loss:     3.420474, Lr: 0.000044, Tokens per sec:   3523
2023-03-15 05:52:48,338 - INFO - __main__ - Epoch  83: total training loss 6640.20
2023-03-15 05:52:48,339 - INFO - __main__ - Epoch 84
2023-03-15 05:52:55,456 - INFO - __main__ - Epoch  84, Step:  180900, Batch Loss:     2.499463, Lr: 0.000043, Tokens per sec:   3273
2023-03-15 05:53:11,121 - INFO - __main__ - Epoch  84, Step:  181000, Batch Loss:     3.458332, Lr: 0.000043, Tokens per sec:   3435
2023-03-15 05:53:26,708 - INFO - __main__ - Epoch  84, Step:  181100, Batch Loss:     1.951852, Lr: 0.000043, Tokens per sec:   3446
2023-03-15 05:53:42,160 - INFO - __main__ - Epoch  84, Step:  181200, Batch Loss:     2.315599, Lr: 0.000043, Tokens per sec:   3472
2023-03-15 05:53:57,681 - INFO - __main__ - Epoch  84, Step:  181300, Batch Loss:     3.186444, Lr: 0.000043, Tokens per sec:   3498
2023-03-15 05:54:13,212 - INFO - __main__ - Epoch  84, Step:  181400, Batch Loss:     2.636790, Lr: 0.000043, Tokens per sec:   3428
2023-03-15 05:54:28,608 - INFO - __main__ - Epoch  84, Step:  181500, Batch Loss:     2.984986, Lr: 0.000043, Tokens per sec:   3530
2023-03-15 05:54:44,085 - INFO - __main__ - Epoch  84, Step:  181600, Batch Loss:     4.756996, Lr: 0.000043, Tokens per sec:   3465
2023-03-15 05:54:59,607 - INFO - __main__ - Epoch  84, Step:  181700, Batch Loss:     3.852686, Lr: 0.000043, Tokens per sec:   3501
2023-03-15 05:55:14,872 - INFO - __main__ - Epoch  84, Step:  181800, Batch Loss:     3.110238, Lr: 0.000043, Tokens per sec:   3503
2023-03-15 05:55:30,379 - INFO - __main__ - Epoch  84, Step:  181900, Batch Loss:     2.873408, Lr: 0.000043, Tokens per sec:   3465
2023-03-15 05:55:45,361 - INFO - __main__ - Epoch  84, Step:  182000, Batch Loss:     3.726196, Lr: 0.000043, Tokens per sec:   3635
2023-03-15 05:56:00,700 - INFO - __main__ - Epoch  84, Step:  182100, Batch Loss:     3.924625, Lr: 0.000043, Tokens per sec:   3528
2023-03-15 05:56:16,117 - INFO - __main__ - Epoch  84, Step:  182200, Batch Loss:     4.085319, Lr: 0.000043, Tokens per sec:   3494
2023-03-15 05:56:31,469 - INFO - __main__ - Epoch  84, Step:  182300, Batch Loss:     3.248698, Lr: 0.000043, Tokens per sec:   3524
2023-03-15 05:56:46,928 - INFO - __main__ - Epoch  84, Step:  182400, Batch Loss:     4.100773, Lr: 0.000043, Tokens per sec:   3507
2023-03-15 05:57:02,440 - INFO - __main__ - Epoch  84, Step:  182500, Batch Loss:     3.982662, Lr: 0.000043, Tokens per sec:   3481
2023-03-15 05:57:18,078 - INFO - __main__ - Epoch  84, Step:  182600, Batch Loss:     1.865103, Lr: 0.000043, Tokens per sec:   3409
2023-03-15 05:57:33,666 - INFO - __main__ - Epoch  84, Step:  182700, Batch Loss:     3.534599, Lr: 0.000043, Tokens per sec:   3395
2023-03-15 05:57:49,292 - INFO - __main__ - Epoch  84, Step:  182800, Batch Loss:     2.954980, Lr: 0.000043, Tokens per sec:   3367
2023-03-15 05:58:04,587 - INFO - __main__ - Epoch  84, Step:  182900, Batch Loss:     2.810285, Lr: 0.000043, Tokens per sec:   3485
2023-03-15 05:58:19,927 - INFO - __main__ - Epoch  84, Step:  183000, Batch Loss:     3.596048, Lr: 0.000043, Tokens per sec:   3555
2023-03-15 05:58:25,386 - INFO - __main__ - Epoch  84: total training loss 6578.01
2023-03-15 05:58:25,387 - INFO - __main__ - Epoch 85
2023-03-15 05:58:35,476 - INFO - __main__ - Epoch  85, Step:  183100, Batch Loss:     2.363611, Lr: 0.000043, Tokens per sec:   3385
2023-03-15 05:58:51,000 - INFO - __main__ - Epoch  85, Step:  183200, Batch Loss:     2.707358, Lr: 0.000043, Tokens per sec:   3458
2023-03-15 05:59:06,400 - INFO - __main__ - Epoch  85, Step:  183300, Batch Loss:     3.427016, Lr: 0.000043, Tokens per sec:   3524
2023-03-15 05:59:21,952 - INFO - __main__ - Epoch  85, Step:  183400, Batch Loss:     3.558799, Lr: 0.000043, Tokens per sec:   3431
2023-03-15 05:59:37,466 - INFO - __main__ - Epoch  85, Step:  183500, Batch Loss:     4.087169, Lr: 0.000043, Tokens per sec:   3482
2023-03-15 05:59:52,802 - INFO - __main__ - Epoch  85, Step:  183600, Batch Loss:     3.745346, Lr: 0.000043, Tokens per sec:   3546
2023-03-15 06:00:08,300 - INFO - __main__ - Epoch  85, Step:  183700, Batch Loss:     2.731847, Lr: 0.000043, Tokens per sec:   3474
2023-03-15 06:00:23,795 - INFO - __main__ - Epoch  85, Step:  183800, Batch Loss:     3.327483, Lr: 0.000043, Tokens per sec:   3468
2023-03-15 06:00:39,069 - INFO - __main__ - Epoch  85, Step:  183900, Batch Loss:     2.723294, Lr: 0.000043, Tokens per sec:   3497
2023-03-15 06:00:54,451 - INFO - __main__ - Epoch  85, Step:  184000, Batch Loss:     3.770523, Lr: 0.000043, Tokens per sec:   3565
2023-03-15 06:01:09,925 - INFO - __main__ - Epoch  85, Step:  184100, Batch Loss:     2.321693, Lr: 0.000043, Tokens per sec:   3534
2023-03-15 06:01:25,524 - INFO - __main__ - Epoch  85, Step:  184200, Batch Loss:     2.846211, Lr: 0.000043, Tokens per sec:   3455
2023-03-15 06:01:41,120 - INFO - __main__ - Epoch  85, Step:  184300, Batch Loss:     2.579294, Lr: 0.000043, Tokens per sec:   3407
2023-03-15 06:01:56,590 - INFO - __main__ - Epoch  85, Step:  184400, Batch Loss:     3.751409, Lr: 0.000043, Tokens per sec:   3432
2023-03-15 06:02:12,082 - INFO - __main__ - Epoch  85, Step:  184500, Batch Loss:     2.834090, Lr: 0.000043, Tokens per sec:   3466
2023-03-15 06:02:27,417 - INFO - __main__ - Epoch  85, Step:  184600, Batch Loss:     2.329068, Lr: 0.000043, Tokens per sec:   3475
2023-03-15 06:02:42,518 - INFO - __main__ - Epoch  85, Step:  184700, Batch Loss:     3.356262, Lr: 0.000043, Tokens per sec:   3524
2023-03-15 06:02:57,917 - INFO - __main__ - Epoch  85, Step:  184800, Batch Loss:     2.075895, Lr: 0.000043, Tokens per sec:   3518
2023-03-15 06:03:13,330 - INFO - __main__ - Epoch  85, Step:  184900, Batch Loss:     3.395893, Lr: 0.000043, Tokens per sec:   3436
2023-03-15 06:03:28,734 - INFO - __main__ - Epoch  85, Step:  185000, Batch Loss:     2.984231, Lr: 0.000043, Tokens per sec:   3553
2023-03-15 06:03:44,231 - INFO - __main__ - Epoch  85, Step:  185100, Batch Loss:     3.152740, Lr: 0.000043, Tokens per sec:   3431
2023-03-15 06:03:59,231 - INFO - __main__ - Epoch  85, Step:  185200, Batch Loss:     3.229082, Lr: 0.000043, Tokens per sec:   3660
2023-03-15 06:04:01,578 - INFO - __main__ - Epoch  85: total training loss 6449.34
2023-03-15 06:04:01,579 - INFO - __main__ - Epoch 86
2023-03-15 06:04:15,040 - INFO - __main__ - Epoch  86, Step:  185300, Batch Loss:     3.517712, Lr: 0.000043, Tokens per sec:   3426
2023-03-15 06:04:30,525 - INFO - __main__ - Epoch  86, Step:  185400, Batch Loss:     2.194853, Lr: 0.000043, Tokens per sec:   3494
2023-03-15 06:04:45,771 - INFO - __main__ - Epoch  86, Step:  185500, Batch Loss:     3.258358, Lr: 0.000043, Tokens per sec:   3495
2023-03-15 06:05:01,210 - INFO - __main__ - Epoch  86, Step:  185600, Batch Loss:     3.580011, Lr: 0.000043, Tokens per sec:   3476
2023-03-15 06:05:16,638 - INFO - __main__ - Epoch  86, Step:  185700, Batch Loss:     2.484126, Lr: 0.000043, Tokens per sec:   3453
2023-03-15 06:05:31,981 - INFO - __main__ - Epoch  86, Step:  185800, Batch Loss:     3.031676, Lr: 0.000043, Tokens per sec:   3534
2023-03-15 06:05:47,391 - INFO - __main__ - Epoch  86, Step:  185900, Batch Loss:     3.290972, Lr: 0.000043, Tokens per sec:   3501
2023-03-15 06:06:02,770 - INFO - __main__ - Epoch  86, Step:  186000, Batch Loss:     2.807911, Lr: 0.000043, Tokens per sec:   3536
2023-03-15 06:06:18,164 - INFO - __main__ - Epoch  86, Step:  186100, Batch Loss:     3.293691, Lr: 0.000043, Tokens per sec:   3449
2023-03-15 06:06:33,595 - INFO - __main__ - Epoch  86, Step:  186200, Batch Loss:     2.626671, Lr: 0.000043, Tokens per sec:   3494
2023-03-15 06:06:49,008 - INFO - __main__ - Epoch  86, Step:  186300, Batch Loss:     3.429928, Lr: 0.000043, Tokens per sec:   3425
2023-03-15 06:07:04,301 - INFO - __main__ - Epoch  86, Step:  186400, Batch Loss:     3.726142, Lr: 0.000043, Tokens per sec:   3557
2023-03-15 06:07:19,480 - INFO - __main__ - Epoch  86, Step:  186500, Batch Loss:     4.880408, Lr: 0.000043, Tokens per sec:   3598
2023-03-15 06:07:34,837 - INFO - __main__ - Epoch  86, Step:  186600, Batch Loss:     4.375849, Lr: 0.000043, Tokens per sec:   3520
2023-03-15 06:07:49,540 - INFO - __main__ - Epoch  86, Step:  186700, Batch Loss:     3.719264, Lr: 0.000043, Tokens per sec:   3707
2023-03-15 06:08:04,951 - INFO - __main__ - Epoch  86, Step:  186800, Batch Loss:     4.382329, Lr: 0.000043, Tokens per sec:   3468
2023-03-15 06:08:20,293 - INFO - __main__ - Epoch  86, Step:  186900, Batch Loss:     2.680390, Lr: 0.000043, Tokens per sec:   3523
2023-03-15 06:08:35,938 - INFO - __main__ - Epoch  86, Step:  187000, Batch Loss:     3.439119, Lr: 0.000043, Tokens per sec:   3453
2023-03-15 06:08:51,622 - INFO - __main__ - Epoch  86, Step:  187100, Batch Loss:     5.209851, Lr: 0.000043, Tokens per sec:   3425
2023-03-15 06:09:06,914 - INFO - __main__ - Epoch  86, Step:  187200, Batch Loss:     3.645035, Lr: 0.000043, Tokens per sec:   3522
2023-03-15 06:09:22,285 - INFO - __main__ - Epoch  86, Step:  187300, Batch Loss:     2.055393, Lr: 0.000043, Tokens per sec:   3488
2023-03-15 06:09:36,239 - INFO - __main__ - Epoch  86: total training loss 6368.27
2023-03-15 06:09:36,239 - INFO - __main__ - Epoch 87
2023-03-15 06:09:37,524 - INFO - __main__ - Epoch  87, Step:  187400, Batch Loss:     2.599895, Lr: 0.000042, Tokens per sec:   2302
2023-03-15 06:09:52,983 - INFO - __main__ - Epoch  87, Step:  187500, Batch Loss:     2.256340, Lr: 0.000042, Tokens per sec:   3476
2023-03-15 06:10:08,368 - INFO - __main__ - Epoch  87, Step:  187600, Batch Loss:     2.078340, Lr: 0.000042, Tokens per sec:   3535
2023-03-15 06:10:23,724 - INFO - __main__ - Epoch  87, Step:  187700, Batch Loss:     2.250919, Lr: 0.000042, Tokens per sec:   3523
2023-03-15 06:10:39,278 - INFO - __main__ - Epoch  87, Step:  187800, Batch Loss:     3.233832, Lr: 0.000042, Tokens per sec:   3507
2023-03-15 06:10:54,734 - INFO - __main__ - Epoch  87, Step:  187900, Batch Loss:     2.376894, Lr: 0.000042, Tokens per sec:   3513
2023-03-15 06:11:10,422 - INFO - __main__ - Epoch  87, Step:  188000, Batch Loss:     3.372037, Lr: 0.000042, Tokens per sec:   3422
2023-03-15 06:11:25,353 - INFO - __main__ - Epoch  87, Step:  188100, Batch Loss:     3.636947, Lr: 0.000042, Tokens per sec:   3641
2023-03-15 06:11:40,523 - INFO - __main__ - Epoch  87, Step:  188200, Batch Loss:     2.436677, Lr: 0.000042, Tokens per sec:   3560
2023-03-15 06:11:56,040 - INFO - __main__ - Epoch  87, Step:  188300, Batch Loss:     1.958645, Lr: 0.000042, Tokens per sec:   3489
2023-03-15 06:12:11,571 - INFO - __main__ - Epoch  87, Step:  188400, Batch Loss:     2.355576, Lr: 0.000042, Tokens per sec:   3474
2023-03-15 06:12:27,158 - INFO - __main__ - Epoch  87, Step:  188500, Batch Loss:     3.664412, Lr: 0.000042, Tokens per sec:   3441
2023-03-15 06:12:42,340 - INFO - __main__ - Epoch  87, Step:  188600, Batch Loss:     1.516853, Lr: 0.000042, Tokens per sec:   3443
2023-03-15 06:12:57,933 - INFO - __main__ - Epoch  87, Step:  188700, Batch Loss:     2.626764, Lr: 0.000042, Tokens per sec:   3449
2023-03-15 06:13:13,558 - INFO - __main__ - Epoch  87, Step:  188800, Batch Loss:     2.741272, Lr: 0.000042, Tokens per sec:   3492
2023-03-15 06:13:29,063 - INFO - __main__ - Epoch  87, Step:  188900, Batch Loss:     2.219693, Lr: 0.000042, Tokens per sec:   3448
2023-03-15 06:13:44,712 - INFO - __main__ - Epoch  87, Step:  189000, Batch Loss:     3.132002, Lr: 0.000042, Tokens per sec:   3477
2023-03-15 06:14:00,261 - INFO - __main__ - Epoch  87, Step:  189100, Batch Loss:     3.028002, Lr: 0.000042, Tokens per sec:   3396
2023-03-15 06:14:14,888 - INFO - __main__ - Epoch  87, Step:  189200, Batch Loss:     3.927824, Lr: 0.000042, Tokens per sec:   3675
2023-03-15 06:14:30,147 - INFO - __main__ - Epoch  87, Step:  189300, Batch Loss:     3.948363, Lr: 0.000042, Tokens per sec:   3527
2023-03-15 06:14:45,586 - INFO - __main__ - Epoch  87, Step:  189400, Batch Loss:     2.472972, Lr: 0.000042, Tokens per sec:   3504
2023-03-15 06:15:00,803 - INFO - __main__ - Epoch  87, Step:  189500, Batch Loss:     2.614002, Lr: 0.000042, Tokens per sec:   3514
2023-03-15 06:15:12,057 - INFO - __main__ - Epoch  87: total training loss 6221.64
2023-03-15 06:15:12,058 - INFO - __main__ - Epoch 88
2023-03-15 06:15:16,478 - INFO - __main__ - Epoch  88, Step:  189600, Batch Loss:     2.367716, Lr: 0.000042, Tokens per sec:   3204
2023-03-15 06:15:31,989 - INFO - __main__ - Epoch  88, Step:  189700, Batch Loss:     2.173405, Lr: 0.000042, Tokens per sec:   3487
2023-03-15 06:15:47,343 - INFO - __main__ - Epoch  88, Step:  189800, Batch Loss:     2.752194, Lr: 0.000042, Tokens per sec:   3538
2023-03-15 06:16:02,737 - INFO - __main__ - Epoch  88, Step:  189900, Batch Loss:     2.311098, Lr: 0.000042, Tokens per sec:   3513
2023-03-15 06:16:18,084 - INFO - __main__ - Epoch  88, Step:  190000, Batch Loss:     3.559572, Lr: 0.000042, Tokens per sec:   3444
2023-03-15 06:16:33,652 - INFO - __main__ - Epoch  88, Step:  190100, Batch Loss:     2.329821, Lr: 0.000042, Tokens per sec:   3474
2023-03-15 06:16:49,213 - INFO - __main__ - Epoch  88, Step:  190200, Batch Loss:     2.991011, Lr: 0.000042, Tokens per sec:   3447
2023-03-15 06:17:04,668 - INFO - __main__ - Epoch  88, Step:  190300, Batch Loss:     3.021758, Lr: 0.000042, Tokens per sec:   3511
2023-03-15 06:17:19,816 - INFO - __main__ - Epoch  88, Step:  190400, Batch Loss:     2.942017, Lr: 0.000042, Tokens per sec:   3593
2023-03-15 06:17:34,944 - INFO - __main__ - Epoch  88, Step:  190500, Batch Loss:     1.802170, Lr: 0.000042, Tokens per sec:   3544
2023-03-15 06:17:50,188 - INFO - __main__ - Epoch  88, Step:  190600, Batch Loss:     3.071089, Lr: 0.000042, Tokens per sec:   3582
2023-03-15 06:18:05,661 - INFO - __main__ - Epoch  88, Step:  190700, Batch Loss:     3.810250, Lr: 0.000042, Tokens per sec:   3475
2023-03-15 06:18:21,029 - INFO - __main__ - Epoch  88, Step:  190800, Batch Loss:     3.721601, Lr: 0.000042, Tokens per sec:   3440
2023-03-15 06:18:36,216 - INFO - __main__ - Epoch  88, Step:  190900, Batch Loss:     3.417964, Lr: 0.000042, Tokens per sec:   3514
2023-03-15 06:18:51,360 - INFO - __main__ - Epoch  88, Step:  191000, Batch Loss:     3.353140, Lr: 0.000042, Tokens per sec:   3558
2023-03-15 06:19:06,783 - INFO - __main__ - Epoch  88, Step:  191100, Batch Loss:     2.882528, Lr: 0.000042, Tokens per sec:   3450
2023-03-15 06:19:22,245 - INFO - __main__ - Epoch  88, Step:  191200, Batch Loss:     3.176655, Lr: 0.000042, Tokens per sec:   3534
2023-03-15 06:19:37,694 - INFO - __main__ - Epoch  88, Step:  191300, Batch Loss:     4.323927, Lr: 0.000042, Tokens per sec:   3487
2023-03-15 06:19:53,210 - INFO - __main__ - Epoch  88, Step:  191400, Batch Loss:     2.468852, Lr: 0.000042, Tokens per sec:   3537
2023-03-15 06:20:08,892 - INFO - __main__ - Epoch  88, Step:  191500, Batch Loss:     2.542992, Lr: 0.000042, Tokens per sec:   3423
2023-03-15 06:20:24,095 - INFO - __main__ - Epoch  88, Step:  191600, Batch Loss:     3.080070, Lr: 0.000042, Tokens per sec:   3562
2023-03-15 06:20:39,461 - INFO - __main__ - Epoch  88, Step:  191700, Batch Loss:     3.333414, Lr: 0.000042, Tokens per sec:   3476
2023-03-15 06:20:47,411 - INFO - __main__ - Epoch  88: total training loss 6129.12
2023-03-15 06:20:47,412 - INFO - __main__ - Epoch 89
2023-03-15 06:20:55,309 - INFO - __main__ - Epoch  89, Step:  191800, Batch Loss:     1.723697, Lr: 0.000041, Tokens per sec:   3347
2023-03-15 06:21:10,963 - INFO - __main__ - Epoch  89, Step:  191900, Batch Loss:     2.496017, Lr: 0.000041, Tokens per sec:   3421
2023-03-15 06:21:26,573 - INFO - __main__ - Epoch  89, Step:  192000, Batch Loss:     3.347208, Lr: 0.000041, Tokens per sec:   3461
2023-03-15 06:21:42,154 - INFO - __main__ - Epoch  89, Step:  192100, Batch Loss:     2.772277, Lr: 0.000041, Tokens per sec:   3509
2023-03-15 06:21:57,598 - INFO - __main__ - Epoch  89, Step:  192200, Batch Loss:     3.299976, Lr: 0.000041, Tokens per sec:   3515
2023-03-15 06:22:13,120 - INFO - __main__ - Epoch  89, Step:  192300, Batch Loss:     2.751947, Lr: 0.000041, Tokens per sec:   3393
2023-03-15 06:22:28,693 - INFO - __main__ - Epoch  89, Step:  192400, Batch Loss:     3.627538, Lr: 0.000041, Tokens per sec:   3508
2023-03-15 06:22:44,235 - INFO - __main__ - Epoch  89, Step:  192500, Batch Loss:     2.537356, Lr: 0.000041, Tokens per sec:   3395
2023-03-15 06:22:59,829 - INFO - __main__ - Epoch  89, Step:  192600, Batch Loss:     3.882944, Lr: 0.000041, Tokens per sec:   3495
2023-03-15 06:23:15,254 - INFO - __main__ - Epoch  89, Step:  192700, Batch Loss:     2.896912, Lr: 0.000041, Tokens per sec:   3457
2023-03-15 06:23:30,796 - INFO - __main__ - Epoch  89, Step:  192800, Batch Loss:     1.833388, Lr: 0.000041, Tokens per sec:   3476
2023-03-15 06:23:45,756 - INFO - __main__ - Epoch  89, Step:  192900, Batch Loss:     3.345639, Lr: 0.000041, Tokens per sec:   3572
2023-03-15 06:24:01,010 - INFO - __main__ - Epoch  89, Step:  193000, Batch Loss:     1.812596, Lr: 0.000041, Tokens per sec:   3492
2023-03-15 06:24:16,634 - INFO - __main__ - Epoch  89, Step:  193100, Batch Loss:     2.044292, Lr: 0.000041, Tokens per sec:   3456
2023-03-15 06:24:32,204 - INFO - __main__ - Epoch  89, Step:  193200, Batch Loss:     2.000701, Lr: 0.000041, Tokens per sec:   3435
2023-03-15 06:24:47,592 - INFO - __main__ - Epoch  89, Step:  193300, Batch Loss:     2.822232, Lr: 0.000041, Tokens per sec:   3546
2023-03-15 06:25:03,115 - INFO - __main__ - Epoch  89, Step:  193400, Batch Loss:     1.432762, Lr: 0.000041, Tokens per sec:   3436
2023-03-15 06:25:18,492 - INFO - __main__ - Epoch  89, Step:  193500, Batch Loss:     2.477291, Lr: 0.000041, Tokens per sec:   3528
2023-03-15 06:25:34,023 - INFO - __main__ - Epoch  89, Step:  193600, Batch Loss:     2.752665, Lr: 0.000041, Tokens per sec:   3411
2023-03-15 06:25:49,699 - INFO - __main__ - Epoch  89, Step:  193700, Batch Loss:     3.662024, Lr: 0.000041, Tokens per sec:   3431
2023-03-15 06:26:05,036 - INFO - __main__ - Epoch  89, Step:  193800, Batch Loss:     2.316856, Lr: 0.000041, Tokens per sec:   3502
2023-03-15 06:26:20,511 - INFO - __main__ - Epoch  89, Step:  193900, Batch Loss:     3.063717, Lr: 0.000041, Tokens per sec:   3566
2023-03-15 06:26:25,408 - INFO - __main__ - Epoch  89: total training loss 6044.05
2023-03-15 06:26:25,409 - INFO - __main__ - Epoch 90
2023-03-15 06:26:36,300 - INFO - __main__ - Epoch  90, Step:  194000, Batch Loss:     3.108595, Lr: 0.000041, Tokens per sec:   3322
2023-03-15 06:26:51,854 - INFO - __main__ - Epoch  90, Step:  194100, Batch Loss:     3.419226, Lr: 0.000041, Tokens per sec:   3469
2023-03-15 06:27:07,578 - INFO - __main__ - Epoch  90, Step:  194200, Batch Loss:     2.650492, Lr: 0.000041, Tokens per sec:   3442
2023-03-15 06:27:23,179 - INFO - __main__ - Epoch  90, Step:  194300, Batch Loss:     3.141771, Lr: 0.000041, Tokens per sec:   3431
2023-03-15 06:27:38,672 - INFO - __main__ - Epoch  90, Step:  194400, Batch Loss:     1.480690, Lr: 0.000041, Tokens per sec:   3471
2023-03-15 06:27:54,300 - INFO - __main__ - Epoch  90, Step:  194500, Batch Loss:     3.642698, Lr: 0.000041, Tokens per sec:   3431
2023-03-15 06:28:09,623 - INFO - __main__ - Epoch  90, Step:  194600, Batch Loss:     1.639217, Lr: 0.000041, Tokens per sec:   3456
2023-03-15 06:28:25,317 - INFO - __main__ - Epoch  90, Step:  194700, Batch Loss:     3.251070, Lr: 0.000041, Tokens per sec:   3402
2023-03-15 06:28:40,858 - INFO - __main__ - Epoch  90, Step:  194800, Batch Loss:     2.612570, Lr: 0.000041, Tokens per sec:   3467
2023-03-15 06:28:56,467 - INFO - __main__ - Epoch  90, Step:  194900, Batch Loss:     2.606587, Lr: 0.000041, Tokens per sec:   3457
2023-03-15 06:29:12,134 - INFO - __main__ - Epoch  90, Step:  195000, Batch Loss:     2.568132, Lr: 0.000041, Tokens per sec:   3444
2023-03-15 06:29:27,762 - INFO - __main__ - Epoch  90, Step:  195100, Batch Loss:     2.707548, Lr: 0.000041, Tokens per sec:   3444
2023-03-15 06:29:43,290 - INFO - __main__ - Epoch  90, Step:  195200, Batch Loss:     3.407291, Lr: 0.000041, Tokens per sec:   3491
2023-03-15 06:29:58,753 - INFO - __main__ - Epoch  90, Step:  195300, Batch Loss:     2.498538, Lr: 0.000041, Tokens per sec:   3507
2023-03-15 06:30:14,204 - INFO - __main__ - Epoch  90, Step:  195400, Batch Loss:     3.631350, Lr: 0.000041, Tokens per sec:   3502
2023-03-15 06:30:29,669 - INFO - __main__ - Epoch  90, Step:  195500, Batch Loss:     2.505102, Lr: 0.000041, Tokens per sec:   3512
2023-03-15 06:30:44,913 - INFO - __main__ - Epoch  90, Step:  195600, Batch Loss:     2.418764, Lr: 0.000041, Tokens per sec:   3561
2023-03-15 06:31:00,282 - INFO - __main__ - Epoch  90, Step:  195700, Batch Loss:     3.477491, Lr: 0.000041, Tokens per sec:   3518
2023-03-15 06:31:15,653 - INFO - __main__ - Epoch  90, Step:  195800, Batch Loss:     2.403967, Lr: 0.000041, Tokens per sec:   3491
2023-03-15 06:31:31,061 - INFO - __main__ - Epoch  90, Step:  195900, Batch Loss:     3.537977, Lr: 0.000041, Tokens per sec:   3558
2023-03-15 06:31:46,270 - INFO - __main__ - Epoch  90, Step:  196000, Batch Loss:     2.633904, Lr: 0.000041, Tokens per sec:   3507
2023-03-15 06:32:01,742 - INFO - __main__ - Epoch  90, Step:  196100, Batch Loss:     3.271886, Lr: 0.000041, Tokens per sec:   3464
2023-03-15 06:32:03,342 - INFO - __main__ - Epoch  90: total training loss 5942.38
2023-03-15 06:32:03,343 - INFO - __main__ - Epoch 91
2023-03-15 06:32:17,678 - INFO - __main__ - Epoch  91, Step:  196200, Batch Loss:     2.367462, Lr: 0.000040, Tokens per sec:   3395
2023-03-15 06:32:33,153 - INFO - __main__ - Epoch  91, Step:  196300, Batch Loss:     2.826221, Lr: 0.000040, Tokens per sec:   3486
2023-03-15 06:32:48,734 - INFO - __main__ - Epoch  91, Step:  196400, Batch Loss:     3.383898, Lr: 0.000040, Tokens per sec:   3471
2023-03-15 06:33:04,355 - INFO - __main__ - Epoch  91, Step:  196500, Batch Loss:     2.828814, Lr: 0.000040, Tokens per sec:   3510
2023-03-15 06:33:20,031 - INFO - __main__ - Epoch  91, Step:  196600, Batch Loss:     2.510964, Lr: 0.000040, Tokens per sec:   3442
2023-03-15 06:33:35,591 - INFO - __main__ - Epoch  91, Step:  196700, Batch Loss:     2.796930, Lr: 0.000040, Tokens per sec:   3493
2023-03-15 06:33:51,290 - INFO - __main__ - Epoch  91, Step:  196800, Batch Loss:     1.127782, Lr: 0.000040, Tokens per sec:   3436
2023-03-15 06:34:06,654 - INFO - __main__ - Epoch  91, Step:  196900, Batch Loss:     2.631285, Lr: 0.000040, Tokens per sec:   3603
2023-03-15 06:34:22,065 - INFO - __main__ - Epoch  91, Step:  197000, Batch Loss:     2.794440, Lr: 0.000040, Tokens per sec:   3469
2023-03-15 06:34:37,323 - INFO - __main__ - Epoch  91, Step:  197100, Batch Loss:     2.068969, Lr: 0.000040, Tokens per sec:   3491
2023-03-15 06:34:52,717 - INFO - __main__ - Epoch  91, Step:  197200, Batch Loss:     3.149241, Lr: 0.000040, Tokens per sec:   3479
2023-03-15 06:35:08,110 - INFO - __main__ - Epoch  91, Step:  197300, Batch Loss:     2.466776, Lr: 0.000040, Tokens per sec:   3504
2023-03-15 06:35:23,536 - INFO - __main__ - Epoch  91, Step:  197400, Batch Loss:     1.870789, Lr: 0.000040, Tokens per sec:   3483
2023-03-15 06:35:39,067 - INFO - __main__ - Epoch  91, Step:  197500, Batch Loss:     3.115862, Lr: 0.000040, Tokens per sec:   3384
2023-03-15 06:35:54,637 - INFO - __main__ - Epoch  91, Step:  197600, Batch Loss:     1.794825, Lr: 0.000040, Tokens per sec:   3496
2023-03-15 06:36:10,244 - INFO - __main__ - Epoch  91, Step:  197700, Batch Loss:     2.682695, Lr: 0.000040, Tokens per sec:   3501
2023-03-15 06:36:25,680 - INFO - __main__ - Epoch  91, Step:  197800, Batch Loss:     1.281668, Lr: 0.000040, Tokens per sec:   3482
2023-03-15 06:36:41,252 - INFO - __main__ - Epoch  91, Step:  197900, Batch Loss:     2.846444, Lr: 0.000040, Tokens per sec:   3451
2023-03-15 06:36:56,836 - INFO - __main__ - Epoch  91, Step:  198000, Batch Loss:     2.253262, Lr: 0.000040, Tokens per sec:   3429
2023-03-15 06:37:12,433 - INFO - __main__ - Epoch  91, Step:  198100, Batch Loss:     3.528999, Lr: 0.000040, Tokens per sec:   3449
2023-03-15 06:37:27,784 - INFO - __main__ - Epoch  91, Step:  198200, Batch Loss:     1.827012, Lr: 0.000040, Tokens per sec:   3458
2023-03-15 06:37:41,702 - INFO - __main__ - Epoch  91: total training loss 5853.18
2023-03-15 06:37:41,703 - INFO - __main__ - Epoch 92
2023-03-15 06:37:43,790 - INFO - __main__ - Epoch  92, Step:  198300, Batch Loss:     3.504694, Lr: 0.000040, Tokens per sec:   3039
2023-03-15 06:37:59,206 - INFO - __main__ - Epoch  92, Step:  198400, Batch Loss:     1.144560, Lr: 0.000040, Tokens per sec:   3471
2023-03-15 06:38:14,367 - INFO - __main__ - Epoch  92, Step:  198500, Batch Loss:     2.424166, Lr: 0.000040, Tokens per sec:   3549
2023-03-15 06:38:29,533 - INFO - __main__ - Epoch  92, Step:  198600, Batch Loss:     2.176556, Lr: 0.000040, Tokens per sec:   3575
2023-03-15 06:38:44,988 - INFO - __main__ - Epoch  92, Step:  198700, Batch Loss:     2.067834, Lr: 0.000040, Tokens per sec:   3541
2023-03-15 06:39:00,315 - INFO - __main__ - Epoch  92, Step:  198800, Batch Loss:     3.047597, Lr: 0.000040, Tokens per sec:   3456
2023-03-15 06:39:15,574 - INFO - __main__ - Epoch  92, Step:  198900, Batch Loss:     1.803459, Lr: 0.000040, Tokens per sec:   3535
2023-03-15 06:39:30,787 - INFO - __main__ - Epoch  92, Step:  199000, Batch Loss:     2.510740, Lr: 0.000040, Tokens per sec:   3509
2023-03-15 06:39:46,280 - INFO - __main__ - Epoch  92, Step:  199100, Batch Loss:     3.058961, Lr: 0.000040, Tokens per sec:   3473
2023-03-15 06:40:01,468 - INFO - __main__ - Epoch  92, Step:  199200, Batch Loss:     3.949499, Lr: 0.000040, Tokens per sec:   3502
2023-03-15 06:40:16,899 - INFO - __main__ - Epoch  92, Step:  199300, Batch Loss:     2.269147, Lr: 0.000040, Tokens per sec:   3526
2023-03-15 06:40:32,396 - INFO - __main__ - Epoch  92, Step:  199400, Batch Loss:     2.721854, Lr: 0.000040, Tokens per sec:   3522
2023-03-15 06:40:47,756 - INFO - __main__ - Epoch  92, Step:  199500, Batch Loss:     3.711725, Lr: 0.000040, Tokens per sec:   3510
2023-03-15 06:41:03,174 - INFO - __main__ - Epoch  92, Step:  199600, Batch Loss:     1.759678, Lr: 0.000040, Tokens per sec:   3464
2023-03-15 06:41:18,656 - INFO - __main__ - Epoch  92, Step:  199700, Batch Loss:     2.720576, Lr: 0.000040, Tokens per sec:   3451
2023-03-15 06:41:34,207 - INFO - __main__ - Epoch  92, Step:  199800, Batch Loss:     3.128755, Lr: 0.000040, Tokens per sec:   3454
2023-03-15 06:41:49,739 - INFO - __main__ - Epoch  92, Step:  199900, Batch Loss:     2.806390, Lr: 0.000040, Tokens per sec:   3532
2023-03-15 06:42:05,422 - INFO - __main__ - Epoch  92, Step:  200000, Batch Loss:     3.134959, Lr: 0.000040, Tokens per sec:   3433
2023-03-15 06:42:20,990 - INFO - __main__ - Epoch  92, Step:  200100, Batch Loss:     1.798457, Lr: 0.000040, Tokens per sec:   3496
2023-03-15 06:42:36,454 - INFO - __main__ - Epoch  92, Step:  200200, Batch Loss:     2.160261, Lr: 0.000040, Tokens per sec:   3428
2023-03-15 06:42:52,020 - INFO - __main__ - Epoch  92, Step:  200300, Batch Loss:     3.846391, Lr: 0.000040, Tokens per sec:   3433
2023-03-15 06:43:07,347 - INFO - __main__ - Epoch  92, Step:  200400, Batch Loss:     3.609340, Lr: 0.000040, Tokens per sec:   3445
2023-03-15 06:43:17,868 - INFO - __main__ - Epoch  92: total training loss 5761.98
2023-03-15 06:43:17,869 - INFO - __main__ - Epoch 93
2023-03-15 06:43:23,262 - INFO - __main__ - Epoch  93, Step:  200500, Batch Loss:     3.868294, Lr: 0.000040, Tokens per sec:   3196
2023-03-15 06:43:38,800 - INFO - __main__ - Epoch  93, Step:  200600, Batch Loss:     3.609124, Lr: 0.000040, Tokens per sec:   3437
2023-03-15 06:43:54,244 - INFO - __main__ - Epoch  93, Step:  200700, Batch Loss:     2.347376, Lr: 0.000040, Tokens per sec:   3462
2023-03-15 06:44:09,815 - INFO - __main__ - Epoch  93, Step:  200800, Batch Loss:     2.848649, Lr: 0.000040, Tokens per sec:   3491
2023-03-15 06:44:25,283 - INFO - __main__ - Epoch  93, Step:  200900, Batch Loss:     2.434063, Lr: 0.000040, Tokens per sec:   3468
2023-03-15 06:44:40,759 - INFO - __main__ - Epoch  93, Step:  201000, Batch Loss:     2.968983, Lr: 0.000040, Tokens per sec:   3524
2023-03-15 06:44:56,479 - INFO - __main__ - Epoch  93, Step:  201100, Batch Loss:     1.901505, Lr: 0.000040, Tokens per sec:   3405
2023-03-15 06:45:11,964 - INFO - __main__ - Epoch  93, Step:  201200, Batch Loss:     2.433631, Lr: 0.000040, Tokens per sec:   3513
2023-03-15 06:45:27,397 - INFO - __main__ - Epoch  93, Step:  201300, Batch Loss:     2.468881, Lr: 0.000040, Tokens per sec:   3497
2023-03-15 06:45:43,001 - INFO - __main__ - Epoch  93, Step:  201400, Batch Loss:     3.147914, Lr: 0.000040, Tokens per sec:   3438
2023-03-15 06:45:58,514 - INFO - __main__ - Epoch  93, Step:  201500, Batch Loss:     3.182908, Lr: 0.000040, Tokens per sec:   3461
2023-03-15 06:46:14,016 - INFO - __main__ - Epoch  93, Step:  201600, Batch Loss:     2.551342, Lr: 0.000040, Tokens per sec:   3458
2023-03-15 06:46:29,620 - INFO - __main__ - Epoch  93, Step:  201700, Batch Loss:     1.996583, Lr: 0.000040, Tokens per sec:   3395
2023-03-15 06:46:45,091 - INFO - __main__ - Epoch  93, Step:  201800, Batch Loss:     2.199117, Lr: 0.000040, Tokens per sec:   3495
2023-03-15 06:47:00,510 - INFO - __main__ - Epoch  93, Step:  201900, Batch Loss:     2.627532, Lr: 0.000040, Tokens per sec:   3500
2023-03-15 06:47:15,874 - INFO - __main__ - Epoch  93, Step:  202000, Batch Loss:     2.480638, Lr: 0.000040, Tokens per sec:   3500
2023-03-15 06:47:31,521 - INFO - __main__ - Epoch  93, Step:  202100, Batch Loss:     2.904778, Lr: 0.000040, Tokens per sec:   3507
2023-03-15 06:47:47,078 - INFO - __main__ - Epoch  93, Step:  202200, Batch Loss:     2.494622, Lr: 0.000040, Tokens per sec:   3377
2023-03-15 06:48:02,677 - INFO - __main__ - Epoch  93, Step:  202300, Batch Loss:     2.465794, Lr: 0.000040, Tokens per sec:   3488
2023-03-15 06:48:18,375 - INFO - __main__ - Epoch  93, Step:  202400, Batch Loss:     2.600964, Lr: 0.000040, Tokens per sec:   3495
2023-03-15 06:48:33,899 - INFO - __main__ - Epoch  93, Step:  202500, Batch Loss:     2.299491, Lr: 0.000040, Tokens per sec:   3417
2023-03-15 06:48:49,435 - INFO - __main__ - Epoch  93, Step:  202600, Batch Loss:     2.492024, Lr: 0.000040, Tokens per sec:   3479
2023-03-15 06:48:56,753 - INFO - __main__ - Epoch  93: total training loss 5708.59
2023-03-15 06:48:56,754 - INFO - __main__ - Epoch 94
2023-03-15 06:49:05,402 - INFO - __main__ - Epoch  94, Step:  202700, Batch Loss:     2.812805, Lr: 0.000039, Tokens per sec:   3361
2023-03-15 06:49:20,927 - INFO - __main__ - Epoch  94, Step:  202800, Batch Loss:     3.114982, Lr: 0.000039, Tokens per sec:   3533
2023-03-15 06:49:36,411 - INFO - __main__ - Epoch  94, Step:  202900, Batch Loss:     2.887113, Lr: 0.000039, Tokens per sec:   3496
2023-03-15 06:49:51,946 - INFO - __main__ - Epoch  94, Step:  203000, Batch Loss:     2.325305, Lr: 0.000039, Tokens per sec:   3462
2023-03-15 06:50:07,364 - INFO - __main__ - Epoch  94, Step:  203100, Batch Loss:     2.093453, Lr: 0.000039, Tokens per sec:   3498
2023-03-15 06:50:22,704 - INFO - __main__ - Epoch  94, Step:  203200, Batch Loss:     2.415393, Lr: 0.000039, Tokens per sec:   3584
2023-03-15 06:50:38,072 - INFO - __main__ - Epoch  94, Step:  203300, Batch Loss:     2.997819, Lr: 0.000039, Tokens per sec:   3540
2023-03-15 06:50:53,561 - INFO - __main__ - Epoch  94, Step:  203400, Batch Loss:     2.806580, Lr: 0.000039, Tokens per sec:   3435
2023-03-15 06:51:09,140 - INFO - __main__ - Epoch  94, Step:  203500, Batch Loss:     2.420106, Lr: 0.000039, Tokens per sec:   3429
2023-03-15 06:51:24,412 - INFO - __main__ - Epoch  94, Step:  203600, Batch Loss:     2.696311, Lr: 0.000039, Tokens per sec:   3575
2023-03-15 06:51:39,739 - INFO - __main__ - Epoch  94, Step:  203700, Batch Loss:     2.034834, Lr: 0.000039, Tokens per sec:   3462
2023-03-15 06:51:55,122 - INFO - __main__ - Epoch  94, Step:  203800, Batch Loss:     3.011445, Lr: 0.000039, Tokens per sec:   3482
2023-03-15 06:52:10,469 - INFO - __main__ - Epoch  94, Step:  203900, Batch Loss:     2.875409, Lr: 0.000039, Tokens per sec:   3493
2023-03-15 06:52:25,565 - INFO - __main__ - Epoch  94, Step:  204000, Batch Loss:     3.021226, Lr: 0.000039, Tokens per sec:   3648
2023-03-15 06:52:40,886 - INFO - __main__ - Epoch  94, Step:  204100, Batch Loss:     1.485219, Lr: 0.000039, Tokens per sec:   3491
2023-03-15 06:52:56,301 - INFO - __main__ - Epoch  94, Step:  204200, Batch Loss:     1.490526, Lr: 0.000039, Tokens per sec:   3528
2023-03-15 06:53:11,546 - INFO - __main__ - Epoch  94, Step:  204300, Batch Loss:     2.471521, Lr: 0.000039, Tokens per sec:   3473
2023-03-15 06:53:26,847 - INFO - __main__ - Epoch  94, Step:  204400, Batch Loss:     2.943781, Lr: 0.000039, Tokens per sec:   3541
2023-03-15 06:53:42,091 - INFO - __main__ - Epoch  94, Step:  204500, Batch Loss:     2.981888, Lr: 0.000039, Tokens per sec:   3480
2023-03-15 06:53:57,500 - INFO - __main__ - Epoch  94, Step:  204600, Batch Loss:     2.691815, Lr: 0.000039, Tokens per sec:   3457
2023-03-15 06:54:12,825 - INFO - __main__ - Epoch  94, Step:  204700, Batch Loss:     2.327628, Lr: 0.000039, Tokens per sec:   3499
2023-03-15 06:54:28,046 - INFO - __main__ - Epoch  94, Step:  204800, Batch Loss:     2.130500, Lr: 0.000039, Tokens per sec:   3451
2023-03-15 06:54:32,062 - INFO - __main__ - Epoch  94: total training loss 5582.02
2023-03-15 06:54:32,063 - INFO - __main__ - Epoch 95
2023-03-15 06:54:43,698 - INFO - __main__ - Epoch  95, Step:  204900, Batch Loss:     2.534731, Lr: 0.000039, Tokens per sec:   3482
2023-03-15 06:54:58,840 - INFO - __main__ - Epoch  95, Step:  205000, Batch Loss:     3.523531, Lr: 0.000039, Tokens per sec:   3561
2023-03-15 06:55:13,965 - INFO - __main__ - Epoch  95, Step:  205100, Batch Loss:     1.937349, Lr: 0.000039, Tokens per sec:   3625
2023-03-15 06:55:29,137 - INFO - __main__ - Epoch  95, Step:  205200, Batch Loss:     1.587733, Lr: 0.000039, Tokens per sec:   3551
2023-03-15 06:55:44,522 - INFO - __main__ - Epoch  95, Step:  205300, Batch Loss:     3.395233, Lr: 0.000039, Tokens per sec:   3539
2023-03-15 06:55:59,966 - INFO - __main__ - Epoch  95, Step:  205400, Batch Loss:     2.100817, Lr: 0.000039, Tokens per sec:   3520
2023-03-15 06:56:15,415 - INFO - __main__ - Epoch  95, Step:  205500, Batch Loss:     3.667078, Lr: 0.000039, Tokens per sec:   3535
2023-03-15 06:56:30,659 - INFO - __main__ - Epoch  95, Step:  205600, Batch Loss:     2.504897, Lr: 0.000039, Tokens per sec:   3487
2023-03-15 06:56:46,188 - INFO - __main__ - Epoch  95, Step:  205700, Batch Loss:     2.752445, Lr: 0.000039, Tokens per sec:   3441
2023-03-15 06:57:01,777 - INFO - __main__ - Epoch  95, Step:  205800, Batch Loss:     3.477075, Lr: 0.000039, Tokens per sec:   3394
2023-03-15 06:57:16,836 - INFO - __main__ - Epoch  95, Step:  205900, Batch Loss:     2.085768, Lr: 0.000039, Tokens per sec:   3578
2023-03-15 06:57:32,338 - INFO - __main__ - Epoch  95, Step:  206000, Batch Loss:     2.361608, Lr: 0.000039, Tokens per sec:   3442
2023-03-15 06:57:47,645 - INFO - __main__ - Epoch  95, Step:  206100, Batch Loss:     2.443714, Lr: 0.000039, Tokens per sec:   3513
2023-03-15 06:58:02,978 - INFO - __main__ - Epoch  95, Step:  206200, Batch Loss:     2.717758, Lr: 0.000039, Tokens per sec:   3515
2023-03-15 06:58:18,502 - INFO - __main__ - Epoch  95, Step:  206300, Batch Loss:     2.720099, Lr: 0.000039, Tokens per sec:   3464
2023-03-15 06:58:34,163 - INFO - __main__ - Epoch  95, Step:  206400, Batch Loss:     3.625003, Lr: 0.000039, Tokens per sec:   3445
2023-03-15 06:58:49,580 - INFO - __main__ - Epoch  95, Step:  206500, Batch Loss:     2.017843, Lr: 0.000039, Tokens per sec:   3424
2023-03-15 06:59:04,693 - INFO - __main__ - Epoch  95, Step:  206600, Batch Loss:     3.532059, Lr: 0.000039, Tokens per sec:   3603
2023-03-15 06:59:19,580 - INFO - __main__ - Epoch  95, Step:  206700, Batch Loss:     2.297604, Lr: 0.000039, Tokens per sec:   3529
2023-03-15 06:59:34,967 - INFO - __main__ - Epoch  95, Step:  206800, Batch Loss:     2.715282, Lr: 0.000039, Tokens per sec:   3513
2023-03-15 06:59:50,410 - INFO - __main__ - Epoch  95, Step:  206900, Batch Loss:     2.380907, Lr: 0.000039, Tokens per sec:   3456
2023-03-15 07:00:05,854 - INFO - __main__ - Epoch  95, Step:  207000, Batch Loss:     3.003974, Lr: 0.000039, Tokens per sec:   3530
2023-03-15 07:00:06,633 - INFO - __main__ - Epoch  95: total training loss 5495.09
2023-03-15 07:00:06,634 - INFO - __main__ - Epoch 96
2023-03-15 07:00:21,594 - INFO - __main__ - Epoch  96, Step:  207100, Batch Loss:     2.246250, Lr: 0.000038, Tokens per sec:   3433
2023-03-15 07:00:37,187 - INFO - __main__ - Epoch  96, Step:  207200, Batch Loss:     1.991948, Lr: 0.000038, Tokens per sec:   3478
2023-03-15 07:00:52,731 - INFO - __main__ - Epoch  96, Step:  207300, Batch Loss:     1.938169, Lr: 0.000038, Tokens per sec:   3392
2023-03-15 07:01:08,087 - INFO - __main__ - Epoch  96, Step:  207400, Batch Loss:     2.002917, Lr: 0.000038, Tokens per sec:   3435
2023-03-15 07:01:23,483 - INFO - __main__ - Epoch  96, Step:  207500, Batch Loss:     3.922675, Lr: 0.000038, Tokens per sec:   3488
2023-03-15 07:01:38,855 - INFO - __main__ - Epoch  96, Step:  207600, Batch Loss:     2.157923, Lr: 0.000038, Tokens per sec:   3459
2023-03-15 07:01:54,332 - INFO - __main__ - Epoch  96, Step:  207700, Batch Loss:     1.752894, Lr: 0.000038, Tokens per sec:   3458
2023-03-15 07:02:09,872 - INFO - __main__ - Epoch  96, Step:  207800, Batch Loss:     2.452367, Lr: 0.000038, Tokens per sec:   3514
2023-03-15 07:02:25,359 - INFO - __main__ - Epoch  96, Step:  207900, Batch Loss:     2.284034, Lr: 0.000038, Tokens per sec:   3456
2023-03-15 07:02:40,710 - INFO - __main__ - Epoch  96, Step:  208000, Batch Loss:     2.626545, Lr: 0.000038, Tokens per sec:   3536
2023-03-15 07:02:56,065 - INFO - __main__ - Epoch  96, Step:  208100, Batch Loss:     2.416557, Lr: 0.000038, Tokens per sec:   3499
2023-03-15 07:03:11,515 - INFO - __main__ - Epoch  96, Step:  208200, Batch Loss:     1.869504, Lr: 0.000038, Tokens per sec:   3472
2023-03-15 07:03:27,031 - INFO - __main__ - Epoch  96, Step:  208300, Batch Loss:     2.062842, Lr: 0.000038, Tokens per sec:   3489
2023-03-15 07:03:42,603 - INFO - __main__ - Epoch  96, Step:  208400, Batch Loss:     2.330251, Lr: 0.000038, Tokens per sec:   3502
2023-03-15 07:03:57,933 - INFO - __main__ - Epoch  96, Step:  208500, Batch Loss:     1.263705, Lr: 0.000038, Tokens per sec:   3530
2023-03-15 07:04:13,156 - INFO - __main__ - Epoch  96, Step:  208600, Batch Loss:     1.830740, Lr: 0.000038, Tokens per sec:   3454
2023-03-15 07:04:28,780 - INFO - __main__ - Epoch  96, Step:  208700, Batch Loss:     2.585023, Lr: 0.000038, Tokens per sec:   3430
2023-03-15 07:04:44,156 - INFO - __main__ - Epoch  96, Step:  208800, Batch Loss:     2.011193, Lr: 0.000038, Tokens per sec:   3527
2023-03-15 07:04:59,326 - INFO - __main__ - Epoch  96, Step:  208900, Batch Loss:     2.901893, Lr: 0.000038, Tokens per sec:   3668
2023-03-15 07:05:14,635 - INFO - __main__ - Epoch  96, Step:  209000, Batch Loss:     2.212517, Lr: 0.000038, Tokens per sec:   3496
2023-03-15 07:05:30,056 - INFO - __main__ - Epoch  96, Step:  209100, Batch Loss:     2.670405, Lr: 0.000038, Tokens per sec:   3494
2023-03-15 07:05:43,039 - INFO - __main__ - Epoch  96: total training loss 5448.67
2023-03-15 07:05:43,041 - INFO - __main__ - Epoch 97
2023-03-15 07:05:45,889 - INFO - __main__ - Epoch  97, Step:  209200, Batch Loss:     2.103412, Lr: 0.000038, Tokens per sec:   2967
2023-03-15 07:06:01,375 - INFO - __main__ - Epoch  97, Step:  209300, Batch Loss:     1.702840, Lr: 0.000038, Tokens per sec:   3503
2023-03-15 07:06:16,753 - INFO - __main__ - Epoch  97, Step:  209400, Batch Loss:     2.582525, Lr: 0.000038, Tokens per sec:   3476
2023-03-15 07:06:32,289 - INFO - __main__ - Epoch  97, Step:  209500, Batch Loss:     3.573677, Lr: 0.000038, Tokens per sec:   3493
2023-03-15 07:06:47,900 - INFO - __main__ - Epoch  97, Step:  209600, Batch Loss:     2.007434, Lr: 0.000038, Tokens per sec:   3452
2023-03-15 07:07:03,489 - INFO - __main__ - Epoch  97, Step:  209700, Batch Loss:     2.728388, Lr: 0.000038, Tokens per sec:   3417
2023-03-15 07:07:18,825 - INFO - __main__ - Epoch  97, Step:  209800, Batch Loss:     2.689976, Lr: 0.000038, Tokens per sec:   3448
2023-03-15 07:07:34,527 - INFO - __main__ - Epoch  97, Step:  209900, Batch Loss:     2.452731, Lr: 0.000038, Tokens per sec:   3478
2023-03-15 07:07:50,173 - INFO - __main__ - Epoch  97, Step:  210000, Batch Loss:     3.318054, Lr: 0.000038, Tokens per sec:   3417
2023-03-15 07:08:05,635 - INFO - __main__ - Epoch  97, Step:  210100, Batch Loss:     1.086539, Lr: 0.000038, Tokens per sec:   3417
2023-03-15 07:08:21,063 - INFO - __main__ - Epoch  97, Step:  210200, Batch Loss:     1.372611, Lr: 0.000038, Tokens per sec:   3498
2023-03-15 07:08:36,028 - INFO - __main__ - Epoch  97, Step:  210300, Batch Loss:     1.903965, Lr: 0.000038, Tokens per sec:   3552
2023-03-15 07:08:51,585 - INFO - __main__ - Epoch  97, Step:  210400, Batch Loss:     2.758259, Lr: 0.000038, Tokens per sec:   3489
2023-03-15 07:09:07,138 - INFO - __main__ - Epoch  97, Step:  210500, Batch Loss:     3.014538, Lr: 0.000038, Tokens per sec:   3485
2023-03-15 07:09:22,378 - INFO - __main__ - Epoch  97, Step:  210600, Batch Loss:     2.467256, Lr: 0.000038, Tokens per sec:   3563
2023-03-15 07:09:37,709 - INFO - __main__ - Epoch  97, Step:  210700, Batch Loss:     2.566233, Lr: 0.000038, Tokens per sec:   3473
2023-03-15 07:09:52,781 - INFO - __main__ - Epoch  97, Step:  210800, Batch Loss:     2.189634, Lr: 0.000038, Tokens per sec:   3627
2023-03-15 07:10:08,223 - INFO - __main__ - Epoch  97, Step:  210900, Batch Loss:     2.548372, Lr: 0.000038, Tokens per sec:   3488
2023-03-15 07:10:23,446 - INFO - __main__ - Epoch  97, Step:  211000, Batch Loss:     2.681960, Lr: 0.000038, Tokens per sec:   3495
2023-03-15 07:10:39,012 - INFO - __main__ - Epoch  97, Step:  211100, Batch Loss:     1.604025, Lr: 0.000038, Tokens per sec:   3435
2023-03-15 07:10:54,611 - INFO - __main__ - Epoch  97, Step:  211200, Batch Loss:     2.623446, Lr: 0.000038, Tokens per sec:   3475
2023-03-15 07:11:10,325 - INFO - __main__ - Epoch  97, Step:  211300, Batch Loss:     2.568779, Lr: 0.000038, Tokens per sec:   3517
2023-03-15 07:11:20,240 - INFO - __main__ - Epoch  97: total training loss 5326.35
2023-03-15 07:11:20,241 - INFO - __main__ - Epoch 98
2023-03-15 07:11:26,318 - INFO - __main__ - Epoch  98, Step:  211400, Batch Loss:     3.730531, Lr: 0.000038, Tokens per sec:   3357
2023-03-15 07:11:41,920 - INFO - __main__ - Epoch  98, Step:  211500, Batch Loss:     3.254226, Lr: 0.000038, Tokens per sec:   3464
2023-03-15 07:11:57,579 - INFO - __main__ - Epoch  98, Step:  211600, Batch Loss:     2.020531, Lr: 0.000038, Tokens per sec:   3429
2023-03-15 07:12:13,227 - INFO - __main__ - Epoch  98, Step:  211700, Batch Loss:     2.713102, Lr: 0.000038, Tokens per sec:   3456
2023-03-15 07:12:28,889 - INFO - __main__ - Epoch  98, Step:  211800, Batch Loss:     2.402704, Lr: 0.000038, Tokens per sec:   3444
2023-03-15 07:12:44,258 - INFO - __main__ - Epoch  98, Step:  211900, Batch Loss:     2.753914, Lr: 0.000038, Tokens per sec:   3524
2023-03-15 07:12:59,733 - INFO - __main__ - Epoch  98, Step:  212000, Batch Loss:     2.975358, Lr: 0.000038, Tokens per sec:   3512
2023-03-15 07:13:15,366 - INFO - __main__ - Epoch  98, Step:  212100, Batch Loss:     2.281978, Lr: 0.000038, Tokens per sec:   3377
2023-03-15 07:13:30,774 - INFO - __main__ - Epoch  98, Step:  212200, Batch Loss:     3.497934, Lr: 0.000038, Tokens per sec:   3495
2023-03-15 07:13:46,365 - INFO - __main__ - Epoch  98, Step:  212300, Batch Loss:     1.888660, Lr: 0.000038, Tokens per sec:   3463
2023-03-15 07:14:01,825 - INFO - __main__ - Epoch  98, Step:  212400, Batch Loss:     2.021767, Lr: 0.000038, Tokens per sec:   3482
2023-03-15 07:14:17,311 - INFO - __main__ - Epoch  98, Step:  212500, Batch Loss:     2.157915, Lr: 0.000038, Tokens per sec:   3455
2023-03-15 07:14:32,978 - INFO - __main__ - Epoch  98, Step:  212600, Batch Loss:     3.605501, Lr: 0.000038, Tokens per sec:   3481
2023-03-15 07:14:48,550 - INFO - __main__ - Epoch  98, Step:  212700, Batch Loss:     2.237124, Lr: 0.000038, Tokens per sec:   3431
2023-03-15 07:15:03,609 - INFO - __main__ - Epoch  98, Step:  212800, Batch Loss:     2.518082, Lr: 0.000038, Tokens per sec:   3614
2023-03-15 07:15:19,177 - INFO - __main__ - Epoch  98, Step:  212900, Batch Loss:     3.709682, Lr: 0.000038, Tokens per sec:   3455
2023-03-15 07:15:34,600 - INFO - __main__ - Epoch  98, Step:  213000, Batch Loss:     2.600229, Lr: 0.000038, Tokens per sec:   3467
2023-03-15 07:15:50,121 - INFO - __main__ - Epoch  98, Step:  213100, Batch Loss:     2.680929, Lr: 0.000038, Tokens per sec:   3398
2023-03-15 07:16:05,575 - INFO - __main__ - Epoch  98, Step:  213200, Batch Loss:     2.283594, Lr: 0.000038, Tokens per sec:   3464
2023-03-15 07:16:20,881 - INFO - __main__ - Epoch  98, Step:  213300, Batch Loss:     2.025028, Lr: 0.000038, Tokens per sec:   3526
2023-03-15 07:16:36,482 - INFO - __main__ - Epoch  98, Step:  213400, Batch Loss:     2.643913, Lr: 0.000038, Tokens per sec:   3498
2023-03-15 07:16:51,625 - INFO - __main__ - Epoch  98, Step:  213500, Batch Loss:     3.046278, Lr: 0.000038, Tokens per sec:   3509
2023-03-15 07:16:58,125 - INFO - __main__ - Epoch  98: total training loss 5285.58
2023-03-15 07:16:58,126 - INFO - __main__ - Epoch 99
2023-03-15 07:17:07,519 - INFO - __main__ - Epoch  99, Step:  213600, Batch Loss:     2.134792, Lr: 0.000037, Tokens per sec:   3293
2023-03-15 07:17:23,165 - INFO - __main__ - Epoch  99, Step:  213700, Batch Loss:     2.925826, Lr: 0.000037, Tokens per sec:   3401
2023-03-15 07:17:38,556 - INFO - __main__ - Epoch  99, Step:  213800, Batch Loss:     2.332253, Lr: 0.000037, Tokens per sec:   3501
2023-03-15 07:17:54,155 - INFO - __main__ - Epoch  99, Step:  213900, Batch Loss:     2.021600, Lr: 0.000037, Tokens per sec:   3494
2023-03-15 07:18:09,637 - INFO - __main__ - Epoch  99, Step:  214000, Batch Loss:     3.306503, Lr: 0.000037, Tokens per sec:   3466
2023-03-15 07:18:25,166 - INFO - __main__ - Epoch  99, Step:  214100, Batch Loss:     2.256788, Lr: 0.000037, Tokens per sec:   3433
2023-03-15 07:18:40,763 - INFO - __main__ - Epoch  99, Step:  214200, Batch Loss:     2.340159, Lr: 0.000037, Tokens per sec:   3465
2023-03-15 07:18:56,293 - INFO - __main__ - Epoch  99, Step:  214300, Batch Loss:     2.339406, Lr: 0.000037, Tokens per sec:   3487
2023-03-15 07:19:11,749 - INFO - __main__ - Epoch  99, Step:  214400, Batch Loss:     1.621087, Lr: 0.000037, Tokens per sec:   3522
2023-03-15 07:19:27,356 - INFO - __main__ - Epoch  99, Step:  214500, Batch Loss:     2.602149, Lr: 0.000037, Tokens per sec:   3468
2023-03-15 07:19:43,037 - INFO - __main__ - Epoch  99, Step:  214600, Batch Loss:     2.365567, Lr: 0.000037, Tokens per sec:   3410
2023-03-15 07:19:58,679 - INFO - __main__ - Epoch  99, Step:  214700, Batch Loss:     2.079314, Lr: 0.000037, Tokens per sec:   3457
2023-03-15 07:20:14,196 - INFO - __main__ - Epoch  99, Step:  214800, Batch Loss:     2.505754, Lr: 0.000037, Tokens per sec:   3496
2023-03-15 07:20:29,364 - INFO - __main__ - Epoch  99, Step:  214900, Batch Loss:     2.340369, Lr: 0.000037, Tokens per sec:   3558
2023-03-15 07:20:44,908 - INFO - __main__ - Epoch  99, Step:  215000, Batch Loss:     2.558092, Lr: 0.000037, Tokens per sec:   3437
2023-03-15 07:21:00,428 - INFO - __main__ - Epoch  99, Step:  215100, Batch Loss:     1.959873, Lr: 0.000037, Tokens per sec:   3501
2023-03-15 07:21:15,827 - INFO - __main__ - Epoch  99, Step:  215200, Batch Loss:     3.123162, Lr: 0.000037, Tokens per sec:   3459
2023-03-15 07:21:31,389 - INFO - __main__ - Epoch  99, Step:  215300, Batch Loss:     2.917348, Lr: 0.000037, Tokens per sec:   3508
2023-03-15 07:21:46,979 - INFO - __main__ - Epoch  99, Step:  215400, Batch Loss:     2.408073, Lr: 0.000037, Tokens per sec:   3448
2023-03-15 07:22:02,571 - INFO - __main__ - Epoch  99, Step:  215500, Batch Loss:     3.470315, Lr: 0.000037, Tokens per sec:   3449
2023-03-15 07:22:18,071 - INFO - __main__ - Epoch  99, Step:  215600, Batch Loss:     3.775637, Lr: 0.000037, Tokens per sec:   3461
2023-03-15 07:22:33,557 - INFO - __main__ - Epoch  99, Step:  215700, Batch Loss:     2.090308, Lr: 0.000037, Tokens per sec:   3419
2023-03-15 07:22:36,839 - INFO - __main__ - Epoch  99: total training loss 5199.63
2023-03-15 07:22:36,839 - INFO - __main__ - Epoch 100
2023-03-15 07:22:49,561 - INFO - __main__ - Epoch 100, Step:  215800, Batch Loss:     3.113994, Lr: 0.000037, Tokens per sec:   3291
2023-03-15 07:23:05,191 - INFO - __main__ - Epoch 100, Step:  215900, Batch Loss:     1.573009, Lr: 0.000037, Tokens per sec:   3442
2023-03-15 07:23:20,743 - INFO - __main__ - Epoch 100, Step:  216000, Batch Loss:     2.459296, Lr: 0.000037, Tokens per sec:   3421
2023-03-15 07:23:35,946 - INFO - __main__ - Epoch 100, Step:  216100, Batch Loss:     1.978721, Lr: 0.000037, Tokens per sec:   3510
2023-03-15 07:23:51,421 - INFO - __main__ - Epoch 100, Step:  216200, Batch Loss:     3.356995, Lr: 0.000037, Tokens per sec:   3515
2023-03-15 07:24:06,901 - INFO - __main__ - Epoch 100, Step:  216300, Batch Loss:     1.901458, Lr: 0.000037, Tokens per sec:   3523
2023-03-15 07:24:22,326 - INFO - __main__ - Epoch 100, Step:  216400, Batch Loss:     2.661160, Lr: 0.000037, Tokens per sec:   3472
2023-03-15 07:24:37,798 - INFO - __main__ - Epoch 100, Step:  216500, Batch Loss:     2.022476, Lr: 0.000037, Tokens per sec:   3490
2023-03-15 07:24:53,211 - INFO - __main__ - Epoch 100, Step:  216600, Batch Loss:     2.162808, Lr: 0.000037, Tokens per sec:   3530
2023-03-15 07:25:08,544 - INFO - __main__ - Epoch 100, Step:  216700, Batch Loss:     2.138865, Lr: 0.000037, Tokens per sec:   3431
2023-03-15 07:25:23,894 - INFO - __main__ - Epoch 100, Step:  216800, Batch Loss:     2.821153, Lr: 0.000037, Tokens per sec:   3545
2023-03-15 07:25:39,347 - INFO - __main__ - Epoch 100, Step:  216900, Batch Loss:     3.885010, Lr: 0.000037, Tokens per sec:   3412
2023-03-15 07:25:54,842 - INFO - __main__ - Epoch 100, Step:  217000, Batch Loss:     2.190822, Lr: 0.000037, Tokens per sec:   3459
2023-03-15 07:26:10,378 - INFO - __main__ - Epoch 100, Step:  217100, Batch Loss:     1.781294, Lr: 0.000037, Tokens per sec:   3426
2023-03-15 07:26:25,873 - INFO - __main__ - Epoch 100, Step:  217200, Batch Loss:     2.837059, Lr: 0.000037, Tokens per sec:   3485
2023-03-15 07:26:41,204 - INFO - __main__ - Epoch 100, Step:  217300, Batch Loss:     2.713099, Lr: 0.000037, Tokens per sec:   3564
2023-03-15 07:26:56,652 - INFO - __main__ - Epoch 100, Step:  217400, Batch Loss:     3.719407, Lr: 0.000037, Tokens per sec:   3514
2023-03-15 07:27:12,151 - INFO - __main__ - Epoch 100, Step:  217500, Batch Loss:     3.036147, Lr: 0.000037, Tokens per sec:   3503
2023-03-15 07:27:27,498 - INFO - __main__ - Epoch 100, Step:  217600, Batch Loss:     2.302602, Lr: 0.000037, Tokens per sec:   3519
2023-03-15 07:27:42,784 - INFO - __main__ - Epoch 100, Step:  217700, Batch Loss:     2.250252, Lr: 0.000037, Tokens per sec:   3555
2023-03-15 07:27:58,100 - INFO - __main__ - Epoch 100, Step:  217800, Batch Loss:     2.123950, Lr: 0.000037, Tokens per sec:   3532
2023-03-15 07:28:13,130 - INFO - __main__ - Epoch 100, Step:  217900, Batch Loss:     0.647075, Lr: 0.000037, Tokens per sec:   3557
2023-03-15 07:28:13,276 - INFO - __main__ - Epoch 100: total training loss 5136.49
2023-03-15 07:28:13,277 - INFO - __main__ - Epoch 101
2023-03-15 07:28:28,990 - INFO - __main__ - Epoch 101, Step:  218000, Batch Loss:     1.805850, Lr: 0.000037, Tokens per sec:   3427
2023-03-15 07:28:44,437 - INFO - __main__ - Epoch 101, Step:  218100, Batch Loss:     2.940192, Lr: 0.000037, Tokens per sec:   3455
2023-03-15 07:29:00,004 - INFO - __main__ - Epoch 101, Step:  218200, Batch Loss:     2.884206, Lr: 0.000037, Tokens per sec:   3493
2023-03-15 07:29:15,526 - INFO - __main__ - Epoch 101, Step:  218300, Batch Loss:     2.058943, Lr: 0.000037, Tokens per sec:   3509
2023-03-15 07:29:30,947 - INFO - __main__ - Epoch 101, Step:  218400, Batch Loss:     2.465289, Lr: 0.000037, Tokens per sec:   3480
2023-03-15 07:29:46,156 - INFO - __main__ - Epoch 101, Step:  218500, Batch Loss:     1.794808, Lr: 0.000037, Tokens per sec:   3588
2023-03-15 07:30:01,563 - INFO - __main__ - Epoch 101, Step:  218600, Batch Loss:     2.165645, Lr: 0.000037, Tokens per sec:   3478
2023-03-15 07:30:17,167 - INFO - __main__ - Epoch 101, Step:  218700, Batch Loss:     3.541543, Lr: 0.000037, Tokens per sec:   3423
2023-03-15 07:30:32,545 - INFO - __main__ - Epoch 101, Step:  218800, Batch Loss:     2.929945, Lr: 0.000037, Tokens per sec:   3450
2023-03-15 07:30:48,022 - INFO - __main__ - Epoch 101, Step:  218900, Batch Loss:     2.040751, Lr: 0.000037, Tokens per sec:   3470
2023-03-15 07:31:03,213 - INFO - __main__ - Epoch 101, Step:  219000, Batch Loss:     2.196952, Lr: 0.000037, Tokens per sec:   3555
2023-03-15 07:31:18,825 - INFO - __main__ - Epoch 101, Step:  219100, Batch Loss:     4.267177, Lr: 0.000037, Tokens per sec:   3528
2023-03-15 07:31:34,080 - INFO - __main__ - Epoch 101, Step:  219200, Batch Loss:     1.526346, Lr: 0.000037, Tokens per sec:   3520
2023-03-15 07:31:49,494 - INFO - __main__ - Epoch 101, Step:  219300, Batch Loss:     2.330485, Lr: 0.000037, Tokens per sec:   3464
2023-03-15 07:32:04,951 - INFO - __main__ - Epoch 101, Step:  219400, Batch Loss:     2.241472, Lr: 0.000037, Tokens per sec:   3412
2023-03-15 07:32:20,458 - INFO - __main__ - Epoch 101, Step:  219500, Batch Loss:     2.149898, Lr: 0.000037, Tokens per sec:   3464
2023-03-15 07:32:35,914 - INFO - __main__ - Epoch 101, Step:  219600, Batch Loss:     2.275333, Lr: 0.000037, Tokens per sec:   3497
2023-03-15 07:32:51,385 - INFO - __main__ - Epoch 101, Step:  219700, Batch Loss:     2.833790, Lr: 0.000037, Tokens per sec:   3452
2023-03-15 07:33:06,807 - INFO - __main__ - Epoch 101, Step:  219800, Batch Loss:     2.158448, Lr: 0.000037, Tokens per sec:   3533
2023-03-15 07:33:22,187 - INFO - __main__ - Epoch 101, Step:  219900, Batch Loss:     2.497746, Lr: 0.000037, Tokens per sec:   3554
2023-03-15 07:33:37,545 - INFO - __main__ - Epoch 101, Step:  220000, Batch Loss:     1.654886, Lr: 0.000037, Tokens per sec:   3486
2023-03-15 07:33:49,727 - INFO - __main__ - Epoch 101: total training loss 5063.41
2023-03-15 07:33:49,728 - INFO - __main__ - Epoch 102
2023-03-15 07:33:53,307 - INFO - __main__ - Epoch 102, Step:  220100, Batch Loss:     2.293170, Lr: 0.000036, Tokens per sec:   3170
2023-03-15 07:34:08,767 - INFO - __main__ - Epoch 102, Step:  220200, Batch Loss:     1.740975, Lr: 0.000036, Tokens per sec:   3465
2023-03-15 07:34:24,332 - INFO - __main__ - Epoch 102, Step:  220300, Batch Loss:     2.220103, Lr: 0.000036, Tokens per sec:   3431
2023-03-15 07:34:39,938 - INFO - __main__ - Epoch 102, Step:  220400, Batch Loss:     3.023567, Lr: 0.000036, Tokens per sec:   3454
2023-03-15 07:34:55,492 - INFO - __main__ - Epoch 102, Step:  220500, Batch Loss:     1.616544, Lr: 0.000036, Tokens per sec:   3428
2023-03-15 07:35:10,506 - INFO - __main__ - Epoch 102, Step:  220600, Batch Loss:     1.736195, Lr: 0.000036, Tokens per sec:   3569
2023-03-15 07:35:26,119 - INFO - __main__ - Epoch 102, Step:  220700, Batch Loss:     2.671088, Lr: 0.000036, Tokens per sec:   3425
2023-03-15 07:35:41,438 - INFO - __main__ - Epoch 102, Step:  220800, Batch Loss:     2.035449, Lr: 0.000036, Tokens per sec:   3516
2023-03-15 07:35:56,871 - INFO - __main__ - Epoch 102, Step:  220900, Batch Loss:     1.648685, Lr: 0.000036, Tokens per sec:   3470
2023-03-15 07:36:12,067 - INFO - __main__ - Epoch 102, Step:  221000, Batch Loss:     1.998775, Lr: 0.000036, Tokens per sec:   3525
2023-03-15 07:36:27,283 - INFO - __main__ - Epoch 102, Step:  221100, Batch Loss:     1.707230, Lr: 0.000036, Tokens per sec:   3549
2023-03-15 07:36:42,783 - INFO - __main__ - Epoch 102, Step:  221200, Batch Loss:     2.571919, Lr: 0.000036, Tokens per sec:   3550
2023-03-15 07:36:58,090 - INFO - __main__ - Epoch 102, Step:  221300, Batch Loss:     2.668431, Lr: 0.000036, Tokens per sec:   3519
2023-03-15 07:37:13,647 - INFO - __main__ - Epoch 102, Step:  221400, Batch Loss:     2.556393, Lr: 0.000036, Tokens per sec:   3436
2023-03-15 07:37:29,144 - INFO - __main__ - Epoch 102, Step:  221500, Batch Loss:     2.552630, Lr: 0.000036, Tokens per sec:   3471
2023-03-15 07:37:44,456 - INFO - __main__ - Epoch 102, Step:  221600, Batch Loss:     1.886964, Lr: 0.000036, Tokens per sec:   3507
2023-03-15 07:37:59,694 - INFO - __main__ - Epoch 102, Step:  221700, Batch Loss:     2.442011, Lr: 0.000036, Tokens per sec:   3612
2023-03-15 07:38:15,012 - INFO - __main__ - Epoch 102, Step:  221800, Batch Loss:     2.347854, Lr: 0.000036, Tokens per sec:   3523
2023-03-15 07:38:30,477 - INFO - __main__ - Epoch 102, Step:  221900, Batch Loss:     2.456541, Lr: 0.000036, Tokens per sec:   3423
2023-03-15 07:38:45,978 - INFO - __main__ - Epoch 102, Step:  222000, Batch Loss:     2.669402, Lr: 0.000036, Tokens per sec:   3508
2023-03-15 07:39:01,500 - INFO - __main__ - Epoch 102, Step:  222100, Batch Loss:     1.976342, Lr: 0.000036, Tokens per sec:   3488
2023-03-15 07:39:17,088 - INFO - __main__ - Epoch 102, Step:  222200, Batch Loss:     1.987797, Lr: 0.000036, Tokens per sec:   3487
2023-03-15 07:39:26,112 - INFO - __main__ - Epoch 102: total training loss 5010.91
2023-03-15 07:39:26,113 - INFO - __main__ - Epoch 103
2023-03-15 07:39:33,049 - INFO - __main__ - Epoch 103, Step:  222300, Batch Loss:     2.978481, Lr: 0.000036, Tokens per sec:   3264
2023-03-15 07:39:48,529 - INFO - __main__ - Epoch 103, Step:  222400, Batch Loss:     2.229287, Lr: 0.000036, Tokens per sec:   3457
2023-03-15 07:40:04,096 - INFO - __main__ - Epoch 103, Step:  222500, Batch Loss:     2.327977, Lr: 0.000036, Tokens per sec:   3483
2023-03-15 07:40:19,811 - INFO - __main__ - Epoch 103, Step:  222600, Batch Loss:     1.333919, Lr: 0.000036, Tokens per sec:   3417
2023-03-15 07:40:35,324 - INFO - __main__ - Epoch 103, Step:  222700, Batch Loss:     2.554147, Lr: 0.000036, Tokens per sec:   3452
2023-03-15 07:40:50,694 - INFO - __main__ - Epoch 103, Step:  222800, Batch Loss:     2.032438, Lr: 0.000036, Tokens per sec:   3571
2023-03-15 07:41:06,308 - INFO - __main__ - Epoch 103, Step:  222900, Batch Loss:     1.221453, Lr: 0.000036, Tokens per sec:   3436
2023-03-15 07:41:21,956 - INFO - __main__ - Epoch 103, Step:  223000, Batch Loss:     2.345266, Lr: 0.000036, Tokens per sec:   3446
2023-03-15 07:41:37,104 - INFO - __main__ - Epoch 103, Step:  223100, Batch Loss:     1.864647, Lr: 0.000036, Tokens per sec:   3638
2023-03-15 07:41:52,683 - INFO - __main__ - Epoch 103, Step:  223200, Batch Loss:     1.969631, Lr: 0.000036, Tokens per sec:   3488
2023-03-15 07:42:08,124 - INFO - __main__ - Epoch 103, Step:  223300, Batch Loss:     2.777583, Lr: 0.000036, Tokens per sec:   3421
2023-03-15 07:42:23,637 - INFO - __main__ - Epoch 103, Step:  223400, Batch Loss:     2.124645, Lr: 0.000036, Tokens per sec:   3442
2023-03-15 07:42:39,135 - INFO - __main__ - Epoch 103, Step:  223500, Batch Loss:     2.416484, Lr: 0.000036, Tokens per sec:   3441
2023-03-15 07:42:54,655 - INFO - __main__ - Epoch 103, Step:  223600, Batch Loss:     3.003215, Lr: 0.000036, Tokens per sec:   3445
2023-03-15 07:43:10,322 - INFO - __main__ - Epoch 103, Step:  223700, Batch Loss:     1.768467, Lr: 0.000036, Tokens per sec:   3461
2023-03-15 07:43:25,853 - INFO - __main__ - Epoch 103, Step:  223800, Batch Loss:     2.431608, Lr: 0.000036, Tokens per sec:   3477
2023-03-15 07:43:41,294 - INFO - __main__ - Epoch 103, Step:  223900, Batch Loss:     1.906077, Lr: 0.000036, Tokens per sec:   3489
2023-03-15 07:43:56,910 - INFO - __main__ - Epoch 103, Step:  224000, Batch Loss:     4.040363, Lr: 0.000036, Tokens per sec:   3441
2023-03-15 07:44:12,313 - INFO - __main__ - Epoch 103, Step:  224100, Batch Loss:     1.557728, Lr: 0.000036, Tokens per sec:   3480
2023-03-15 07:44:27,849 - INFO - __main__ - Epoch 103, Step:  224200, Batch Loss:     2.637413, Lr: 0.000036, Tokens per sec:   3397
2023-03-15 07:44:43,275 - INFO - __main__ - Epoch 103, Step:  224300, Batch Loss:     0.929593, Lr: 0.000036, Tokens per sec:   3534
2023-03-15 07:44:58,876 - INFO - __main__ - Epoch 103, Step:  224400, Batch Loss:     2.424989, Lr: 0.000036, Tokens per sec:   3444
2023-03-15 07:45:04,522 - INFO - __main__ - Epoch 103: total training loss 4946.04
2023-03-15 07:45:04,523 - INFO - __main__ - Epoch 104
2023-03-15 07:45:14,594 - INFO - __main__ - Epoch 104, Step:  224500, Batch Loss:     2.926350, Lr: 0.000036, Tokens per sec:   3397
2023-03-15 07:45:30,085 - INFO - __main__ - Epoch 104, Step:  224600, Batch Loss:     2.184920, Lr: 0.000036, Tokens per sec:   3428
2023-03-15 07:45:45,538 - INFO - __main__ - Epoch 104, Step:  224700, Batch Loss:     2.450500, Lr: 0.000036, Tokens per sec:   3521
2023-03-15 07:46:01,122 - INFO - __main__ - Epoch 104, Step:  224800, Batch Loss:     1.926546, Lr: 0.000036, Tokens per sec:   3469
2023-03-15 07:46:16,494 - INFO - __main__ - Epoch 104, Step:  224900, Batch Loss:     3.093489, Lr: 0.000036, Tokens per sec:   3576
2023-03-15 07:46:31,743 - INFO - __main__ - Epoch 104, Step:  225000, Batch Loss:     2.632356, Lr: 0.000036, Tokens per sec:   3486
2023-03-15 07:46:47,412 - INFO - __main__ - Epoch 104, Step:  225100, Batch Loss:     2.097565, Lr: 0.000036, Tokens per sec:   3381
2023-03-15 07:47:02,609 - INFO - __main__ - Epoch 104, Step:  225200, Batch Loss:     2.879764, Lr: 0.000036, Tokens per sec:   3570
2023-03-15 07:47:18,106 - INFO - __main__ - Epoch 104, Step:  225300, Batch Loss:     2.165900, Lr: 0.000036, Tokens per sec:   3433
2023-03-15 07:47:33,507 - INFO - __main__ - Epoch 104, Step:  225400, Batch Loss:     2.057285, Lr: 0.000036, Tokens per sec:   3550
2023-03-15 07:47:48,881 - INFO - __main__ - Epoch 104, Step:  225500, Batch Loss:     2.442757, Lr: 0.000036, Tokens per sec:   3504
2023-03-15 07:48:04,132 - INFO - __main__ - Epoch 104, Step:  225600, Batch Loss:     3.206894, Lr: 0.000036, Tokens per sec:   3620
2023-03-15 07:48:19,207 - INFO - __main__ - Epoch 104, Step:  225700, Batch Loss:     2.435527, Lr: 0.000036, Tokens per sec:   3640
2023-03-15 07:48:34,699 - INFO - __main__ - Epoch 104, Step:  225800, Batch Loss:     2.304174, Lr: 0.000036, Tokens per sec:   3453
2023-03-15 07:48:50,266 - INFO - __main__ - Epoch 104, Step:  225900, Batch Loss:     2.317785, Lr: 0.000036, Tokens per sec:   3473
2023-03-15 07:49:05,885 - INFO - __main__ - Epoch 104, Step:  226000, Batch Loss:     2.353318, Lr: 0.000036, Tokens per sec:   3423
2023-03-15 07:49:21,321 - INFO - __main__ - Epoch 104, Step:  226100, Batch Loss:     2.125330, Lr: 0.000036, Tokens per sec:   3474
2023-03-15 07:49:36,831 - INFO - __main__ - Epoch 104, Step:  226200, Batch Loss:     1.947448, Lr: 0.000036, Tokens per sec:   3543
2023-03-15 07:49:52,301 - INFO - __main__ - Epoch 104, Step:  226300, Batch Loss:     2.357686, Lr: 0.000036, Tokens per sec:   3445
2023-03-15 07:50:07,958 - INFO - __main__ - Epoch 104, Step:  226400, Batch Loss:     2.473918, Lr: 0.000036, Tokens per sec:   3417
2023-03-15 07:50:23,493 - INFO - __main__ - Epoch 104, Step:  226500, Batch Loss:     2.756532, Lr: 0.000036, Tokens per sec:   3406
2023-03-15 07:50:38,893 - INFO - __main__ - Epoch 104, Step:  226600, Batch Loss:     4.608559, Lr: 0.000036, Tokens per sec:   3441
2023-03-15 07:50:41,241 - INFO - __main__ - Epoch 104: total training loss 4859.28
2023-03-15 07:50:41,242 - INFO - __main__ - Epoch 105
2023-03-15 07:50:54,678 - INFO - __main__ - Epoch 105, Step:  226700, Batch Loss:     2.445457, Lr: 0.000035, Tokens per sec:   3323
2023-03-15 07:51:10,208 - INFO - __main__ - Epoch 105, Step:  226800, Batch Loss:     1.826902, Lr: 0.000035, Tokens per sec:   3520
2023-03-15 07:51:25,709 - INFO - __main__ - Epoch 105, Step:  226900, Batch Loss:     1.930616, Lr: 0.000035, Tokens per sec:   3489
2023-03-15 07:51:41,005 - INFO - __main__ - Epoch 105, Step:  227000, Batch Loss:     1.806641, Lr: 0.000035, Tokens per sec:   3531
2023-03-15 07:51:56,566 - INFO - __main__ - Epoch 105, Step:  227100, Batch Loss:     2.360971, Lr: 0.000035, Tokens per sec:   3554
2023-03-15 07:52:12,106 - INFO - __main__ - Epoch 105, Step:  227200, Batch Loss:     1.959258, Lr: 0.000035, Tokens per sec:   3494
2023-03-15 07:52:27,548 - INFO - __main__ - Epoch 105, Step:  227300, Batch Loss:     1.853785, Lr: 0.000035, Tokens per sec:   3525
2023-03-15 07:52:42,667 - INFO - __main__ - Epoch 105, Step:  227400, Batch Loss:     2.055933, Lr: 0.000035, Tokens per sec:   3595
2023-03-15 07:52:58,237 - INFO - __main__ - Epoch 105, Step:  227500, Batch Loss:     2.027596, Lr: 0.000035, Tokens per sec:   3454
2023-03-15 07:53:13,679 - INFO - __main__ - Epoch 105, Step:  227600, Batch Loss:     2.123204, Lr: 0.000035, Tokens per sec:   3429
2023-03-15 07:53:29,032 - INFO - __main__ - Epoch 105, Step:  227700, Batch Loss:     2.215026, Lr: 0.000035, Tokens per sec:   3505
2023-03-15 07:53:44,551 - INFO - __main__ - Epoch 105, Step:  227800, Batch Loss:     1.940842, Lr: 0.000035, Tokens per sec:   3411
2023-03-15 07:54:00,174 - INFO - __main__ - Epoch 105, Step:  227900, Batch Loss:     2.549250, Lr: 0.000035, Tokens per sec:   3477
2023-03-15 07:54:15,726 - INFO - __main__ - Epoch 105, Step:  228000, Batch Loss:     1.209805, Lr: 0.000035, Tokens per sec:   3472
2023-03-15 07:54:31,308 - INFO - __main__ - Epoch 105, Step:  228100, Batch Loss:     2.640174, Lr: 0.000035, Tokens per sec:   3459
2023-03-15 07:54:46,749 - INFO - __main__ - Epoch 105, Step:  228200, Batch Loss:     1.531918, Lr: 0.000035, Tokens per sec:   3438
2023-03-15 07:55:02,201 - INFO - __main__ - Epoch 105, Step:  228300, Batch Loss:     2.131319, Lr: 0.000035, Tokens per sec:   3447
2023-03-15 07:55:17,756 - INFO - __main__ - Epoch 105, Step:  228400, Batch Loss:     2.715388, Lr: 0.000035, Tokens per sec:   3450
2023-03-15 07:55:33,259 - INFO - __main__ - Epoch 105, Step:  228500, Batch Loss:     2.173786, Lr: 0.000035, Tokens per sec:   3421
2023-03-15 07:55:48,590 - INFO - __main__ - Epoch 105, Step:  228600, Batch Loss:     1.984489, Lr: 0.000035, Tokens per sec:   3527
2023-03-15 07:56:03,830 - INFO - __main__ - Epoch 105, Step:  228700, Batch Loss:     2.652552, Lr: 0.000035, Tokens per sec:   3514
2023-03-15 07:56:18,310 - INFO - __main__ - Epoch 105: total training loss 4828.75
2023-03-15 07:56:18,311 - INFO - __main__ - Epoch 106
2023-03-15 07:56:19,476 - INFO - __main__ - Epoch 106, Step:  228800, Batch Loss:     2.619015, Lr: 0.000035, Tokens per sec:   2380
2023-03-15 07:56:35,020 - INFO - __main__ - Epoch 106, Step:  228900, Batch Loss:     2.781883, Lr: 0.000035, Tokens per sec:   3437
2023-03-15 07:56:50,532 - INFO - __main__ - Epoch 106, Step:  229000, Batch Loss:     2.183962, Lr: 0.000035, Tokens per sec:   3438
2023-03-15 07:57:05,902 - INFO - __main__ - Epoch 106, Step:  229100, Batch Loss:     1.949247, Lr: 0.000035, Tokens per sec:   3478
2023-03-15 07:57:21,296 - INFO - __main__ - Epoch 106, Step:  229200, Batch Loss:     2.068399, Lr: 0.000035, Tokens per sec:   3529
2023-03-15 07:57:36,672 - INFO - __main__ - Epoch 106, Step:  229300, Batch Loss:     1.938147, Lr: 0.000035, Tokens per sec:   3492
2023-03-15 07:57:52,321 - INFO - __main__ - Epoch 106, Step:  229400, Batch Loss:     1.965770, Lr: 0.000035, Tokens per sec:   3450
2023-03-15 07:58:07,742 - INFO - __main__ - Epoch 106, Step:  229500, Batch Loss:     2.066526, Lr: 0.000035, Tokens per sec:   3474
2023-03-15 07:58:23,265 - INFO - __main__ - Epoch 106, Step:  229600, Batch Loss:     1.840067, Lr: 0.000035, Tokens per sec:   3451
2023-03-15 07:58:38,816 - INFO - __main__ - Epoch 106, Step:  229700, Batch Loss:     1.763020, Lr: 0.000035, Tokens per sec:   3509
2023-03-15 07:58:54,183 - INFO - __main__ - Epoch 106, Step:  229800, Batch Loss:     2.239688, Lr: 0.000035, Tokens per sec:   3520
2023-03-15 07:59:09,550 - INFO - __main__ - Epoch 106, Step:  229900, Batch Loss:     2.092075, Lr: 0.000035, Tokens per sec:   3478
2023-03-15 07:59:24,785 - INFO - __main__ - Epoch 106, Step:  230000, Batch Loss:     2.587794, Lr: 0.000035, Tokens per sec:   3476
2023-03-15 07:59:40,125 - INFO - __main__ - Epoch 106, Step:  230100, Batch Loss:     2.580450, Lr: 0.000035, Tokens per sec:   3527
2023-03-15 07:59:55,273 - INFO - __main__ - Epoch 106, Step:  230200, Batch Loss:     2.746831, Lr: 0.000035, Tokens per sec:   3572
2023-03-15 08:00:10,625 - INFO - __main__ - Epoch 106, Step:  230300, Batch Loss:     1.557706, Lr: 0.000035, Tokens per sec:   3513
2023-03-15 08:00:26,174 - INFO - __main__ - Epoch 106, Step:  230400, Batch Loss:     3.133234, Lr: 0.000035, Tokens per sec:   3456
2023-03-15 08:00:41,575 - INFO - __main__ - Epoch 106, Step:  230500, Batch Loss:     2.398849, Lr: 0.000035, Tokens per sec:   3459
2023-03-15 08:00:57,002 - INFO - __main__ - Epoch 106, Step:  230600, Batch Loss:     1.966905, Lr: 0.000035, Tokens per sec:   3469
2023-03-15 08:01:12,533 - INFO - __main__ - Epoch 106, Step:  230700, Batch Loss:     1.664707, Lr: 0.000035, Tokens per sec:   3481
2023-03-15 08:01:28,044 - INFO - __main__ - Epoch 106, Step:  230800, Batch Loss:     1.385438, Lr: 0.000035, Tokens per sec:   3503
2023-03-15 08:01:43,614 - INFO - __main__ - Epoch 106, Step:  230900, Batch Loss:     2.600144, Lr: 0.000035, Tokens per sec:   3520
2023-03-15 08:01:55,217 - INFO - __main__ - Epoch 106: total training loss 4750.40
2023-03-15 08:01:55,218 - INFO - __main__ - Epoch 107
2023-03-15 08:01:59,591 - INFO - __main__ - Epoch 107, Step:  231000, Batch Loss:     1.911991, Lr: 0.000034, Tokens per sec:   3189
2023-03-15 08:02:14,879 - INFO - __main__ - Epoch 107, Step:  231100, Batch Loss:     2.009733, Lr: 0.000034, Tokens per sec:   3525
2023-03-15 08:02:30,126 - INFO - __main__ - Epoch 107, Step:  231200, Batch Loss:     2.419966, Lr: 0.000034, Tokens per sec:   3505
2023-03-15 08:02:45,250 - INFO - __main__ - Epoch 107, Step:  231300, Batch Loss:     2.670807, Lr: 0.000034, Tokens per sec:   3607
2023-03-15 08:03:00,529 - INFO - __main__ - Epoch 107, Step:  231400, Batch Loss:     1.236058, Lr: 0.000034, Tokens per sec:   3576
2023-03-15 08:03:15,916 - INFO - __main__ - Epoch 107, Step:  231500, Batch Loss:     1.446997, Lr: 0.000034, Tokens per sec:   3441
2023-03-15 08:03:31,490 - INFO - __main__ - Epoch 107, Step:  231600, Batch Loss:     1.940403, Lr: 0.000034, Tokens per sec:   3440
2023-03-15 08:03:46,767 - INFO - __main__ - Epoch 107, Step:  231700, Batch Loss:     2.734921, Lr: 0.000034, Tokens per sec:   3457
2023-03-15 08:04:02,231 - INFO - __main__ - Epoch 107, Step:  231800, Batch Loss:     1.399081, Lr: 0.000034, Tokens per sec:   3431
2023-03-15 08:04:17,665 - INFO - __main__ - Epoch 107, Step:  231900, Batch Loss:     3.258493, Lr: 0.000034, Tokens per sec:   3483
2023-03-15 08:04:33,101 - INFO - __main__ - Epoch 107, Step:  232000, Batch Loss:     2.079659, Lr: 0.000034, Tokens per sec:   3504
2023-03-15 08:04:48,500 - INFO - __main__ - Epoch 107, Step:  232100, Batch Loss:     2.191024, Lr: 0.000034, Tokens per sec:   3554
2023-03-15 08:05:04,057 - INFO - __main__ - Epoch 107, Step:  232200, Batch Loss:     2.315075, Lr: 0.000034, Tokens per sec:   3485
2023-03-15 08:05:19,226 - INFO - __main__ - Epoch 107, Step:  232300, Batch Loss:     2.535035, Lr: 0.000034, Tokens per sec:   3528
2023-03-15 08:05:34,624 - INFO - __main__ - Epoch 107, Step:  232400, Batch Loss:     2.788010, Lr: 0.000034, Tokens per sec:   3472
2023-03-15 08:05:50,021 - INFO - __main__ - Epoch 107, Step:  232500, Batch Loss:     1.961007, Lr: 0.000034, Tokens per sec:   3526
2023-03-15 08:06:05,439 - INFO - __main__ - Epoch 107, Step:  232600, Batch Loss:     2.121743, Lr: 0.000034, Tokens per sec:   3547
2023-03-15 08:06:20,924 - INFO - __main__ - Epoch 107, Step:  232700, Batch Loss:     3.134356, Lr: 0.000034, Tokens per sec:   3514
2023-03-15 08:06:36,204 - INFO - __main__ - Epoch 107, Step:  232800, Batch Loss:     2.668586, Lr: 0.000034, Tokens per sec:   3473
2023-03-15 08:06:51,777 - INFO - __main__ - Epoch 107, Step:  232900, Batch Loss:     2.022499, Lr: 0.000034, Tokens per sec:   3412
2023-03-15 08:07:07,260 - INFO - __main__ - Epoch 107, Step:  233000, Batch Loss:     2.806428, Lr: 0.000034, Tokens per sec:   3483
2023-03-15 08:07:22,605 - INFO - __main__ - Epoch 107, Step:  233100, Batch Loss:     1.745999, Lr: 0.000034, Tokens per sec:   3549
2023-03-15 08:07:30,927 - INFO - __main__ - Epoch 107: total training loss 4687.06
2023-03-15 08:07:30,927 - INFO - __main__ - Epoch 108
2023-03-15 08:07:38,551 - INFO - __main__ - Epoch 108, Step:  233200, Batch Loss:     1.858585, Lr: 0.000034, Tokens per sec:   3318
2023-03-15 08:07:54,026 - INFO - __main__ - Epoch 108, Step:  233300, Batch Loss:     1.101079, Lr: 0.000034, Tokens per sec:   3480
2023-03-15 08:08:09,741 - INFO - __main__ - Epoch 108, Step:  233400, Batch Loss:     2.307351, Lr: 0.000034, Tokens per sec:   3450
2023-03-15 08:08:25,103 - INFO - __main__ - Epoch 108, Step:  233500, Batch Loss:     1.810576, Lr: 0.000034, Tokens per sec:   3535
2023-03-15 08:08:40,470 - INFO - __main__ - Epoch 108, Step:  233600, Batch Loss:     2.593417, Lr: 0.000034, Tokens per sec:   3486
2023-03-15 08:08:55,875 - INFO - __main__ - Epoch 108, Step:  233700, Batch Loss:     2.486750, Lr: 0.000034, Tokens per sec:   3529
2023-03-15 08:09:11,430 - INFO - __main__ - Epoch 108, Step:  233800, Batch Loss:     1.715545, Lr: 0.000034, Tokens per sec:   3467
2023-03-15 08:09:26,987 - INFO - __main__ - Epoch 108, Step:  233900, Batch Loss:     2.243194, Lr: 0.000034, Tokens per sec:   3444
2023-03-15 08:09:42,281 - INFO - __main__ - Epoch 108, Step:  234000, Batch Loss:     1.664577, Lr: 0.000034, Tokens per sec:   3498
2023-03-15 08:09:57,282 - INFO - __main__ - Epoch 108, Step:  234100, Batch Loss:     1.689938, Lr: 0.000034, Tokens per sec:   3630
2023-03-15 08:10:12,758 - INFO - __main__ - Epoch 108, Step:  234200, Batch Loss:     2.981262, Lr: 0.000034, Tokens per sec:   3533
2023-03-15 08:10:28,106 - INFO - __main__ - Epoch 108, Step:  234300, Batch Loss:     1.772048, Lr: 0.000034, Tokens per sec:   3474
2023-03-15 08:10:43,626 - INFO - __main__ - Epoch 108, Step:  234400, Batch Loss:     1.330126, Lr: 0.000034, Tokens per sec:   3468
2023-03-15 08:10:58,829 - INFO - __main__ - Epoch 108, Step:  234500, Batch Loss:     2.690799, Lr: 0.000034, Tokens per sec:   3606
2023-03-15 08:11:14,116 - INFO - __main__ - Epoch 108, Step:  234600, Batch Loss:     1.752651, Lr: 0.000034, Tokens per sec:   3467
2023-03-15 08:11:29,591 - INFO - __main__ - Epoch 108, Step:  234700, Batch Loss:     2.805895, Lr: 0.000034, Tokens per sec:   3424
2023-03-15 08:11:45,191 - INFO - __main__ - Epoch 108, Step:  234800, Batch Loss:     2.072079, Lr: 0.000034, Tokens per sec:   3393
2023-03-15 08:12:00,239 - INFO - __main__ - Epoch 108, Step:  234900, Batch Loss:     2.141722, Lr: 0.000034, Tokens per sec:   3578
2023-03-15 08:12:15,709 - INFO - __main__ - Epoch 108, Step:  235000, Batch Loss:     2.068794, Lr: 0.000034, Tokens per sec:   3506
2023-03-15 08:12:31,192 - INFO - __main__ - Epoch 108, Step:  235100, Batch Loss:     2.526266, Lr: 0.000034, Tokens per sec:   3500
2023-03-15 08:12:46,701 - INFO - __main__ - Epoch 108, Step:  235200, Batch Loss:     2.401400, Lr: 0.000034, Tokens per sec:   3429
2023-03-15 08:13:02,283 - INFO - __main__ - Epoch 108, Step:  235300, Batch Loss:     1.415779, Lr: 0.000034, Tokens per sec:   3459
2023-03-15 08:13:07,269 - INFO - __main__ - Epoch 108: total training loss 4614.84
2023-03-15 08:13:07,270 - INFO - __main__ - Epoch 109
2023-03-15 08:13:18,070 - INFO - __main__ - Epoch 109, Step:  235400, Batch Loss:     2.408410, Lr: 0.000034, Tokens per sec:   3483
2023-03-15 08:13:33,481 - INFO - __main__ - Epoch 109, Step:  235500, Batch Loss:     2.892828, Lr: 0.000034, Tokens per sec:   3462
2023-03-15 08:13:48,838 - INFO - __main__ - Epoch 109, Step:  235600, Batch Loss:     2.169478, Lr: 0.000034, Tokens per sec:   3508
2023-03-15 08:14:04,356 - INFO - __main__ - Epoch 109, Step:  235700, Batch Loss:     2.115866, Lr: 0.000034, Tokens per sec:   3427
2023-03-15 08:14:19,879 - INFO - __main__ - Epoch 109, Step:  235800, Batch Loss:     1.884903, Lr: 0.000034, Tokens per sec:   3483
2023-03-15 08:14:35,618 - INFO - __main__ - Epoch 109, Step:  235900, Batch Loss:     1.966625, Lr: 0.000034, Tokens per sec:   3415
2023-03-15 08:14:51,211 - INFO - __main__ - Epoch 109, Step:  236000, Batch Loss:     2.483861, Lr: 0.000034, Tokens per sec:   3446
2023-03-15 08:15:06,236 - INFO - __main__ - Epoch 109, Step:  236100, Batch Loss:     1.881723, Lr: 0.000034, Tokens per sec:   3558
2023-03-15 08:15:21,570 - INFO - __main__ - Epoch 109, Step:  236200, Batch Loss:     1.459535, Lr: 0.000034, Tokens per sec:   3527
2023-03-15 08:15:37,061 - INFO - __main__ - Epoch 109, Step:  236300, Batch Loss:     1.337354, Lr: 0.000034, Tokens per sec:   3524
2023-03-15 08:15:52,589 - INFO - __main__ - Epoch 109, Step:  236400, Batch Loss:     2.781784, Lr: 0.000034, Tokens per sec:   3452
2023-03-15 08:16:07,950 - INFO - __main__ - Epoch 109, Step:  236500, Batch Loss:     2.877147, Lr: 0.000034, Tokens per sec:   3443
2023-03-15 08:16:23,290 - INFO - __main__ - Epoch 109, Step:  236600, Batch Loss:     2.826682, Lr: 0.000034, Tokens per sec:   3491
2023-03-15 08:16:38,396 - INFO - __main__ - Epoch 109, Step:  236700, Batch Loss:     3.683927, Lr: 0.000034, Tokens per sec:   3472
2023-03-15 08:16:53,697 - INFO - __main__ - Epoch 109, Step:  236800, Batch Loss:     2.816272, Lr: 0.000034, Tokens per sec:   3523
2023-03-15 08:17:09,233 - INFO - __main__ - Epoch 109, Step:  236900, Batch Loss:     2.687274, Lr: 0.000034, Tokens per sec:   3496
2023-03-15 08:17:24,681 - INFO - __main__ - Epoch 109, Step:  237000, Batch Loss:     2.351290, Lr: 0.000034, Tokens per sec:   3459
2023-03-15 08:17:40,237 - INFO - __main__ - Epoch 109, Step:  237100, Batch Loss:     2.375417, Lr: 0.000034, Tokens per sec:   3559
2023-03-15 08:17:55,601 - INFO - __main__ - Epoch 109, Step:  237200, Batch Loss:     1.575591, Lr: 0.000034, Tokens per sec:   3496
2023-03-15 08:18:11,331 - INFO - __main__ - Epoch 109, Step:  237300, Batch Loss:     2.046943, Lr: 0.000034, Tokens per sec:   3367
2023-03-15 08:18:26,992 - INFO - __main__ - Epoch 109, Step:  237400, Batch Loss:     2.289988, Lr: 0.000034, Tokens per sec:   3506
2023-03-15 08:18:42,450 - INFO - __main__ - Epoch 109, Step:  237500, Batch Loss:     2.059344, Lr: 0.000034, Tokens per sec:   3502
2023-03-15 08:18:44,250 - INFO - __main__ - Epoch 109: total training loss 4575.56
2023-03-15 08:18:44,251 - INFO - __main__ - Epoch 110
2023-03-15 08:18:58,591 - INFO - __main__ - Epoch 110, Step:  237600, Batch Loss:     0.861690, Lr: 0.000033, Tokens per sec:   3273
2023-03-15 08:19:14,093 - INFO - __main__ - Epoch 110, Step:  237700, Batch Loss:     1.620773, Lr: 0.000033, Tokens per sec:   3478
2023-03-15 08:19:29,616 - INFO - __main__ - Epoch 110, Step:  237800, Batch Loss:     1.932609, Lr: 0.000033, Tokens per sec:   3469
2023-03-15 08:19:45,146 - INFO - __main__ - Epoch 110, Step:  237900, Batch Loss:     1.753062, Lr: 0.000033, Tokens per sec:   3493
2023-03-15 08:20:00,703 - INFO - __main__ - Epoch 110, Step:  238000, Batch Loss:     2.576539, Lr: 0.000033, Tokens per sec:   3505
2023-03-15 08:20:16,054 - INFO - __main__ - Epoch 110, Step:  238100, Batch Loss:     2.493349, Lr: 0.000033, Tokens per sec:   3538
2023-03-15 08:20:31,754 - INFO - __main__ - Epoch 110, Step:  238200, Batch Loss:     2.932451, Lr: 0.000033, Tokens per sec:   3413
2023-03-15 08:20:47,389 - INFO - __main__ - Epoch 110, Step:  238300, Batch Loss:     2.587406, Lr: 0.000033, Tokens per sec:   3443
2023-03-15 08:21:02,827 - INFO - __main__ - Epoch 110, Step:  238400, Batch Loss:     1.925001, Lr: 0.000033, Tokens per sec:   3440
2023-03-15 08:21:18,523 - INFO - __main__ - Epoch 110, Step:  238500, Batch Loss:     2.365624, Lr: 0.000033, Tokens per sec:   3447
2023-03-15 08:21:33,906 - INFO - __main__ - Epoch 110, Step:  238600, Batch Loss:     1.442038, Lr: 0.000033, Tokens per sec:   3417
2023-03-15 08:21:49,337 - INFO - __main__ - Epoch 110, Step:  238700, Batch Loss:     1.094143, Lr: 0.000033, Tokens per sec:   3505
2023-03-15 08:22:04,769 - INFO - __main__ - Epoch 110, Step:  238800, Batch Loss:     2.324037, Lr: 0.000033, Tokens per sec:   3490
2023-03-15 08:22:20,112 - INFO - __main__ - Epoch 110, Step:  238900, Batch Loss:     2.062938, Lr: 0.000033, Tokens per sec:   3465
2023-03-15 08:22:35,690 - INFO - __main__ - Epoch 110, Step:  239000, Batch Loss:     2.235508, Lr: 0.000033, Tokens per sec:   3456
2023-03-15 08:22:51,097 - INFO - __main__ - Epoch 110, Step:  239100, Batch Loss:     2.264118, Lr: 0.000033, Tokens per sec:   3559
2023-03-15 08:23:06,066 - INFO - __main__ - Epoch 110, Step:  239200, Batch Loss:     2.596873, Lr: 0.000033, Tokens per sec:   3622
2023-03-15 08:23:21,447 - INFO - __main__ - Epoch 110, Step:  239300, Batch Loss:     2.039800, Lr: 0.000033, Tokens per sec:   3494
2023-03-15 08:23:37,135 - INFO - __main__ - Epoch 110, Step:  239400, Batch Loss:     2.737441, Lr: 0.000033, Tokens per sec:   3439
2023-03-15 08:23:52,721 - INFO - __main__ - Epoch 110, Step:  239500, Batch Loss:     1.598563, Lr: 0.000033, Tokens per sec:   3464
2023-03-15 08:24:07,650 - INFO - __main__ - Epoch 110, Step:  239600, Batch Loss:     2.340128, Lr: 0.000033, Tokens per sec:   3606
2023-03-15 08:24:21,787 - INFO - __main__ - Epoch 110: total training loss 4500.86
2023-03-15 08:24:21,788 - INFO - __main__ - Epoch 111
2023-03-15 08:24:23,697 - INFO - __main__ - Epoch 111, Step:  239700, Batch Loss:     1.921209, Lr: 0.000033, Tokens per sec:   2776
2023-03-15 08:24:39,206 - INFO - __main__ - Epoch 111, Step:  239800, Batch Loss:     2.514649, Lr: 0.000033, Tokens per sec:   3454
2023-03-15 08:24:54,362 - INFO - __main__ - Epoch 111, Step:  239900, Batch Loss:     2.226757, Lr: 0.000033, Tokens per sec:   3594
2023-03-15 08:25:09,875 - INFO - __main__ - Epoch 111, Step:  240000, Batch Loss:     1.495093, Lr: 0.000033, Tokens per sec:   3485
2023-03-15 08:25:25,260 - INFO - __main__ - Epoch 111, Step:  240100, Batch Loss:     2.123487, Lr: 0.000033, Tokens per sec:   3473
2023-03-15 08:25:40,662 - INFO - __main__ - Epoch 111, Step:  240200, Batch Loss:     2.223840, Lr: 0.000033, Tokens per sec:   3547
2023-03-15 08:25:56,139 - INFO - __main__ - Epoch 111, Step:  240300, Batch Loss:     1.110163, Lr: 0.000033, Tokens per sec:   3453
2023-03-15 08:26:11,667 - INFO - __main__ - Epoch 111, Step:  240400, Batch Loss:     1.828177, Lr: 0.000033, Tokens per sec:   3479
2023-03-15 08:26:26,898 - INFO - __main__ - Epoch 111, Step:  240500, Batch Loss:     1.153700, Lr: 0.000033, Tokens per sec:   3520
2023-03-15 08:26:42,378 - INFO - __main__ - Epoch 111, Step:  240600, Batch Loss:     2.495194, Lr: 0.000033, Tokens per sec:   3505
2023-03-15 08:26:57,883 - INFO - __main__ - Epoch 111, Step:  240700, Batch Loss:     1.975894, Lr: 0.000033, Tokens per sec:   3439
2023-03-15 08:27:13,355 - INFO - __main__ - Epoch 111, Step:  240800, Batch Loss:     1.689369, Lr: 0.000033, Tokens per sec:   3426
2023-03-15 08:27:28,839 - INFO - __main__ - Epoch 111, Step:  240900, Batch Loss:     2.642853, Lr: 0.000033, Tokens per sec:   3433
2023-03-15 08:27:44,215 - INFO - __main__ - Epoch 111, Step:  241000, Batch Loss:     1.959577, Lr: 0.000033, Tokens per sec:   3527
2023-03-15 08:27:59,713 - INFO - __main__ - Epoch 111, Step:  241100, Batch Loss:     2.632967, Lr: 0.000033, Tokens per sec:   3460
2023-03-15 08:28:15,069 - INFO - __main__ - Epoch 111, Step:  241200, Batch Loss:     1.701977, Lr: 0.000033, Tokens per sec:   3531
2023-03-15 08:28:30,496 - INFO - __main__ - Epoch 111, Step:  241300, Batch Loss:     2.206305, Lr: 0.000033, Tokens per sec:   3505
2023-03-15 08:28:46,062 - INFO - __main__ - Epoch 111, Step:  241400, Batch Loss:     2.176795, Lr: 0.000033, Tokens per sec:   3474
2023-03-15 08:29:01,423 - INFO - __main__ - Epoch 111, Step:  241500, Batch Loss:     2.907300, Lr: 0.000033, Tokens per sec:   3573
2023-03-15 08:29:16,915 - INFO - __main__ - Epoch 111, Step:  241600, Batch Loss:     2.552468, Lr: 0.000033, Tokens per sec:   3455
2023-03-15 08:29:32,300 - INFO - __main__ - Epoch 111, Step:  241700, Batch Loss:     1.767725, Lr: 0.000033, Tokens per sec:   3466
2023-03-15 08:29:47,989 - INFO - __main__ - Epoch 111, Step:  241800, Batch Loss:     2.780055, Lr: 0.000033, Tokens per sec:   3469
2023-03-15 08:29:58,781 - INFO - __main__ - Epoch 111: total training loss 4441.85
2023-03-15 08:29:58,782 - INFO - __main__ - Epoch 112
2023-03-15 08:30:04,017 - INFO - __main__ - Epoch 112, Step:  241900, Batch Loss:     1.635705, Lr: 0.000033, Tokens per sec:   3173
2023-03-15 08:30:19,502 - INFO - __main__ - Epoch 112, Step:  242000, Batch Loss:     1.429088, Lr: 0.000033, Tokens per sec:   3512
2023-03-15 08:30:35,101 - INFO - __main__ - Epoch 112, Step:  242100, Batch Loss:     1.387733, Lr: 0.000033, Tokens per sec:   3480
2023-03-15 08:30:50,815 - INFO - __main__ - Epoch 112, Step:  242200, Batch Loss:     1.971210, Lr: 0.000033, Tokens per sec:   3484
2023-03-15 08:31:06,261 - INFO - __main__ - Epoch 112, Step:  242300, Batch Loss:     1.588040, Lr: 0.000033, Tokens per sec:   3536
2023-03-15 08:31:21,558 - INFO - __main__ - Epoch 112, Step:  242400, Batch Loss:     2.091410, Lr: 0.000033, Tokens per sec:   3545
2023-03-15 08:31:37,029 - INFO - __main__ - Epoch 112, Step:  242500, Batch Loss:     2.511742, Lr: 0.000033, Tokens per sec:   3450
2023-03-15 08:31:52,207 - INFO - __main__ - Epoch 112, Step:  242600, Batch Loss:     1.893461, Lr: 0.000033, Tokens per sec:   3486
2023-03-15 08:32:07,806 - INFO - __main__ - Epoch 112, Step:  242700, Batch Loss:     1.429467, Lr: 0.000033, Tokens per sec:   3428
2023-03-15 08:32:23,386 - INFO - __main__ - Epoch 112, Step:  242800, Batch Loss:     1.873424, Lr: 0.000033, Tokens per sec:   3411
2023-03-15 08:32:39,027 - INFO - __main__ - Epoch 112, Step:  242900, Batch Loss:     1.435048, Lr: 0.000033, Tokens per sec:   3439
2023-03-15 08:32:54,430 - INFO - __main__ - Epoch 112, Step:  243000, Batch Loss:     1.555013, Lr: 0.000033, Tokens per sec:   3434
2023-03-15 08:33:09,966 - INFO - __main__ - Epoch 112, Step:  243100, Batch Loss:     1.974099, Lr: 0.000033, Tokens per sec:   3457
2023-03-15 08:33:25,043 - INFO - __main__ - Epoch 112, Step:  243200, Batch Loss:     1.534521, Lr: 0.000033, Tokens per sec:   3628
2023-03-15 08:33:40,580 - INFO - __main__ - Epoch 112, Step:  243300, Batch Loss:     1.954042, Lr: 0.000033, Tokens per sec:   3408
2023-03-15 08:33:56,043 - INFO - __main__ - Epoch 112, Step:  243400, Batch Loss:     2.626113, Lr: 0.000033, Tokens per sec:   3534
2023-03-15 08:34:11,449 - INFO - __main__ - Epoch 112, Step:  243500, Batch Loss:     1.421321, Lr: 0.000033, Tokens per sec:   3478
2023-03-15 08:34:26,811 - INFO - __main__ - Epoch 112, Step:  243600, Batch Loss:     2.127086, Lr: 0.000033, Tokens per sec:   3470
2023-03-15 08:34:41,918 - INFO - __main__ - Epoch 112, Step:  243700, Batch Loss:     1.780874, Lr: 0.000033, Tokens per sec:   3577
2023-03-15 08:34:57,189 - INFO - __main__ - Epoch 112, Step:  243800, Batch Loss:     1.995727, Lr: 0.000033, Tokens per sec:   3513
2023-03-15 08:35:12,725 - INFO - __main__ - Epoch 112, Step:  243900, Batch Loss:     2.715693, Lr: 0.000033, Tokens per sec:   3495
2023-03-15 08:35:28,116 - INFO - __main__ - Epoch 112, Step:  244000, Batch Loss:     1.803141, Lr: 0.000033, Tokens per sec:   3531
2023-03-15 08:35:35,639 - INFO - __main__ - Epoch 112: total training loss 4416.35
2023-03-15 08:35:35,640 - INFO - __main__ - Epoch 113
2023-03-15 08:35:44,092 - INFO - __main__ - Epoch 113, Step:  244100, Batch Loss:     2.732576, Lr: 0.000032, Tokens per sec:   3378
2023-03-15 08:35:59,742 - INFO - __main__ - Epoch 113, Step:  244200, Batch Loss:     1.778623, Lr: 0.000032, Tokens per sec:   3460
2023-03-15 08:36:15,083 - INFO - __main__ - Epoch 113, Step:  244300, Batch Loss:     1.553621, Lr: 0.000032, Tokens per sec:   3447
2023-03-15 08:36:30,613 - INFO - __main__ - Epoch 113, Step:  244400, Batch Loss:     1.972386, Lr: 0.000032, Tokens per sec:   3470
2023-03-15 08:36:46,036 - INFO - __main__ - Epoch 113, Step:  244500, Batch Loss:     1.453110, Lr: 0.000032, Tokens per sec:   3543
2023-03-15 08:37:01,435 - INFO - __main__ - Epoch 113, Step:  244600, Batch Loss:     1.385635, Lr: 0.000032, Tokens per sec:   3497
2023-03-15 08:37:16,750 - INFO - __main__ - Epoch 113, Step:  244700, Batch Loss:     1.883869, Lr: 0.000032, Tokens per sec:   3584
2023-03-15 08:37:32,210 - INFO - __main__ - Epoch 113, Step:  244800, Batch Loss:     2.559379, Lr: 0.000032, Tokens per sec:   3501
2023-03-15 08:37:47,558 - INFO - __main__ - Epoch 113, Step:  244900, Batch Loss:     1.875721, Lr: 0.000032, Tokens per sec:   3437
2023-03-15 08:38:02,954 - INFO - __main__ - Epoch 113, Step:  245000, Batch Loss:     2.543271, Lr: 0.000032, Tokens per sec:   3460
2023-03-15 08:38:18,553 - INFO - __main__ - Epoch 113, Step:  245100, Batch Loss:     2.113737, Lr: 0.000032, Tokens per sec:   3450
2023-03-15 08:38:33,910 - INFO - __main__ - Epoch 113, Step:  245200, Batch Loss:     1.738278, Lr: 0.000032, Tokens per sec:   3539
2023-03-15 08:38:49,229 - INFO - __main__ - Epoch 113, Step:  245300, Batch Loss:     1.371720, Lr: 0.000032, Tokens per sec:   3539
2023-03-15 08:39:04,601 - INFO - __main__ - Epoch 113, Step:  245400, Batch Loss:     2.363920, Lr: 0.000032, Tokens per sec:   3476
2023-03-15 08:39:19,622 - INFO - __main__ - Epoch 113, Step:  245500, Batch Loss:     1.629665, Lr: 0.000032, Tokens per sec:   3592
2023-03-15 08:39:34,965 - INFO - __main__ - Epoch 113, Step:  245600, Batch Loss:     1.437886, Lr: 0.000032, Tokens per sec:   3533
2023-03-15 08:39:50,224 - INFO - __main__ - Epoch 113, Step:  245700, Batch Loss:     1.984036, Lr: 0.000032, Tokens per sec:   3482
2023-03-15 08:40:05,796 - INFO - __main__ - Epoch 113, Step:  245800, Batch Loss:     1.651406, Lr: 0.000032, Tokens per sec:   3472
2023-03-15 08:40:20,810 - INFO - __main__ - Epoch 113, Step:  245900, Batch Loss:     0.892090, Lr: 0.000032, Tokens per sec:   3618
2023-03-15 08:40:35,984 - INFO - __main__ - Epoch 113, Step:  246000, Batch Loss:     2.716840, Lr: 0.000032, Tokens per sec:   3491
2023-03-15 08:40:51,443 - INFO - __main__ - Epoch 113, Step:  246100, Batch Loss:     1.915135, Lr: 0.000032, Tokens per sec:   3495
2023-03-15 08:41:07,041 - INFO - __main__ - Epoch 113, Step:  246200, Batch Loss:     2.962073, Lr: 0.000032, Tokens per sec:   3447
2023-03-15 08:41:11,373 - INFO - __main__ - Epoch 113: total training loss 4317.30
2023-03-15 08:41:11,374 - INFO - __main__ - Epoch 114
2023-03-15 08:41:23,115 - INFO - __main__ - Epoch 114, Step:  246300, Batch Loss:     2.122689, Lr: 0.000032, Tokens per sec:   3398
2023-03-15 08:41:38,609 - INFO - __main__ - Epoch 114, Step:  246400, Batch Loss:     1.966440, Lr: 0.000032, Tokens per sec:   3453
2023-03-15 08:41:53,804 - INFO - __main__ - Epoch 114, Step:  246500, Batch Loss:     2.027708, Lr: 0.000032, Tokens per sec:   3548
2023-03-15 08:42:09,143 - INFO - __main__ - Epoch 114, Step:  246600, Batch Loss:     2.624378, Lr: 0.000032, Tokens per sec:   3433
2023-03-15 08:42:24,675 - INFO - __main__ - Epoch 114, Step:  246700, Batch Loss:     1.588939, Lr: 0.000032, Tokens per sec:   3494
2023-03-15 08:42:40,039 - INFO - __main__ - Epoch 114, Step:  246800, Batch Loss:     2.064817, Lr: 0.000032, Tokens per sec:   3476
2023-03-15 08:42:55,422 - INFO - __main__ - Epoch 114, Step:  246900, Batch Loss:     1.686748, Lr: 0.000032, Tokens per sec:   3480
2023-03-15 08:43:10,950 - INFO - __main__ - Epoch 114, Step:  247000, Batch Loss:     1.659383, Lr: 0.000032, Tokens per sec:   3497
2023-03-15 08:43:26,240 - INFO - __main__ - Epoch 114, Step:  247100, Batch Loss:     2.481172, Lr: 0.000032, Tokens per sec:   3545
2023-03-15 08:43:41,885 - INFO - __main__ - Epoch 114, Step:  247200, Batch Loss:     1.811681, Lr: 0.000032, Tokens per sec:   3461
2023-03-15 08:43:57,540 - INFO - __main__ - Epoch 114, Step:  247300, Batch Loss:     1.882434, Lr: 0.000032, Tokens per sec:   3454
2023-03-15 08:44:12,994 - INFO - __main__ - Epoch 114, Step:  247400, Batch Loss:     2.937243, Lr: 0.000032, Tokens per sec:   3434
2023-03-15 08:44:28,446 - INFO - __main__ - Epoch 114, Step:  247500, Batch Loss:     1.972157, Lr: 0.000032, Tokens per sec:   3482
2023-03-15 08:44:44,027 - INFO - __main__ - Epoch 114, Step:  247600, Batch Loss:     2.424469, Lr: 0.000032, Tokens per sec:   3455
2023-03-15 08:44:59,693 - INFO - __main__ - Epoch 114, Step:  247700, Batch Loss:     1.786685, Lr: 0.000032, Tokens per sec:   3476
2023-03-15 08:45:15,157 - INFO - __main__ - Epoch 114, Step:  247800, Batch Loss:     1.853110, Lr: 0.000032, Tokens per sec:   3488
2023-03-15 08:45:30,754 - INFO - __main__ - Epoch 114, Step:  247900, Batch Loss:     1.892114, Lr: 0.000032, Tokens per sec:   3408
2023-03-15 08:45:46,303 - INFO - __main__ - Epoch 114, Step:  248000, Batch Loss:     2.118281, Lr: 0.000032, Tokens per sec:   3509
2023-03-15 08:46:02,041 - INFO - __main__ - Epoch 114, Step:  248100, Batch Loss:     2.144288, Lr: 0.000032, Tokens per sec:   3382
2023-03-15 08:46:17,738 - INFO - __main__ - Epoch 114, Step:  248200, Batch Loss:     2.098153, Lr: 0.000032, Tokens per sec:   3462
2023-03-15 08:46:33,260 - INFO - __main__ - Epoch 114, Step:  248300, Batch Loss:     2.189549, Lr: 0.000032, Tokens per sec:   3494
2023-03-15 08:46:48,585 - INFO - __main__ - Epoch 114, Step:  248400, Batch Loss:     2.331509, Lr: 0.000032, Tokens per sec:   3483
2023-03-15 08:46:49,462 - INFO - __main__ - Epoch 114: total training loss 4278.59
2023-03-15 08:46:49,463 - INFO - __main__ - Epoch 115
2023-03-15 08:47:03,879 - INFO - __main__ - Epoch 115, Step:  248500, Batch Loss:     1.609724, Lr: 0.000032, Tokens per sec:   3468
2023-03-15 08:47:19,447 - INFO - __main__ - Epoch 115, Step:  248600, Batch Loss:     2.189287, Lr: 0.000032, Tokens per sec:   3455
2023-03-15 08:47:34,977 - INFO - __main__ - Epoch 115, Step:  248700, Batch Loss:     1.800547, Lr: 0.000032, Tokens per sec:   3433
2023-03-15 08:47:50,423 - INFO - __main__ - Epoch 115, Step:  248800, Batch Loss:     1.573839, Lr: 0.000032, Tokens per sec:   3515
2023-03-15 08:48:05,742 - INFO - __main__ - Epoch 115, Step:  248900, Batch Loss:     2.206013, Lr: 0.000032, Tokens per sec:   3441
2023-03-15 08:48:21,130 - INFO - __main__ - Epoch 115, Step:  249000, Batch Loss:     2.766799, Lr: 0.000032, Tokens per sec:   3536
2023-03-15 08:48:36,825 - INFO - __main__ - Epoch 115, Step:  249100, Batch Loss:     2.986311, Lr: 0.000032, Tokens per sec:   3430
2023-03-15 08:48:52,300 - INFO - __main__ - Epoch 115, Step:  249200, Batch Loss:     1.984889, Lr: 0.000032, Tokens per sec:   3426
2023-03-15 08:49:07,460 - INFO - __main__ - Epoch 115, Step:  249300, Batch Loss:     2.208671, Lr: 0.000032, Tokens per sec:   3587
2023-03-15 08:49:23,051 - INFO - __main__ - Epoch 115, Step:  249400, Batch Loss:     1.313002, Lr: 0.000032, Tokens per sec:   3481
2023-03-15 08:49:38,617 - INFO - __main__ - Epoch 115, Step:  249500, Batch Loss:     2.232243, Lr: 0.000032, Tokens per sec:   3455
2023-03-15 08:49:54,061 - INFO - __main__ - Epoch 115, Step:  249600, Batch Loss:     2.363191, Lr: 0.000032, Tokens per sec:   3564
2023-03-15 08:50:09,458 - INFO - __main__ - Epoch 115, Step:  249700, Batch Loss:     1.610892, Lr: 0.000032, Tokens per sec:   3510
2023-03-15 08:50:24,941 - INFO - __main__ - Epoch 115, Step:  249800, Batch Loss:     2.053401, Lr: 0.000032, Tokens per sec:   3488
2023-03-15 08:50:40,850 - INFO - __main__ - Epoch 115, Step:  249900, Batch Loss:     1.745631, Lr: 0.000032, Tokens per sec:   3403
2023-03-15 08:50:56,378 - INFO - __main__ - Epoch 115, Step:  250000, Batch Loss:     1.844868, Lr: 0.000032, Tokens per sec:   3453
2023-03-15 08:51:11,878 - INFO - __main__ - Epoch 115, Step:  250100, Batch Loss:     2.642990, Lr: 0.000032, Tokens per sec:   3433
2023-03-15 08:51:26,995 - INFO - __main__ - Epoch 115, Step:  250200, Batch Loss:     2.479928, Lr: 0.000032, Tokens per sec:   3566
2023-03-15 08:51:42,553 - INFO - __main__ - Epoch 115, Step:  250300, Batch Loss:     1.655968, Lr: 0.000032, Tokens per sec:   3465
2023-03-15 08:51:57,748 - INFO - __main__ - Epoch 115, Step:  250400, Batch Loss:     1.195170, Lr: 0.000032, Tokens per sec:   3564
2023-03-15 08:52:13,151 - INFO - __main__ - Epoch 115, Step:  250500, Batch Loss:     2.019436, Lr: 0.000032, Tokens per sec:   3484
2023-03-15 08:52:26,403 - INFO - __main__ - Epoch 115: total training loss 4221.24
2023-03-15 08:52:26,404 - INFO - __main__ - Epoch 116
2023-03-15 08:52:29,073 - INFO - __main__ - Epoch 116, Step:  250600, Batch Loss:     1.798114, Lr: 0.000031, Tokens per sec:   3086
2023-03-15 08:52:44,231 - INFO - __main__ - Epoch 116, Step:  250700, Batch Loss:     2.114678, Lr: 0.000031, Tokens per sec:   3567
2023-03-15 08:52:59,620 - INFO - __main__ - Epoch 116, Step:  250800, Batch Loss:     1.611045, Lr: 0.000031, Tokens per sec:   3540
2023-03-15 08:53:14,976 - INFO - __main__ - Epoch 116, Step:  250900, Batch Loss:     1.446746, Lr: 0.000031, Tokens per sec:   3556
2023-03-15 08:53:30,268 - INFO - __main__ - Epoch 116, Step:  251000, Batch Loss:     2.598234, Lr: 0.000031, Tokens per sec:   3494
2023-03-15 08:53:45,818 - INFO - __main__ - Epoch 116, Step:  251100, Batch Loss:     1.757357, Lr: 0.000031, Tokens per sec:   3413
2023-03-15 08:54:01,075 - INFO - __main__ - Epoch 116, Step:  251200, Batch Loss:     1.086283, Lr: 0.000031, Tokens per sec:   3545
2023-03-15 08:54:16,245 - INFO - __main__ - Epoch 116, Step:  251300, Batch Loss:     1.867773, Lr: 0.000031, Tokens per sec:   3607
2023-03-15 08:54:31,318 - INFO - __main__ - Epoch 116, Step:  251400, Batch Loss:     1.802543, Lr: 0.000031, Tokens per sec:   3626
2023-03-15 08:54:46,539 - INFO - __main__ - Epoch 116, Step:  251500, Batch Loss:     1.892748, Lr: 0.000031, Tokens per sec:   3531
2023-03-15 08:55:01,760 - INFO - __main__ - Epoch 116, Step:  251600, Batch Loss:     2.234338, Lr: 0.000031, Tokens per sec:   3508
2023-03-15 08:55:17,021 - INFO - __main__ - Epoch 116, Step:  251700, Batch Loss:     2.650253, Lr: 0.000031, Tokens per sec:   3520
2023-03-15 08:55:32,248 - INFO - __main__ - Epoch 116, Step:  251800, Batch Loss:     1.549281, Lr: 0.000031, Tokens per sec:   3477
2023-03-15 08:55:47,575 - INFO - __main__ - Epoch 116, Step:  251900, Batch Loss:     1.835629, Lr: 0.000031, Tokens per sec:   3552
2023-03-15 08:56:02,691 - INFO - __main__ - Epoch 116, Step:  252000, Batch Loss:     2.098515, Lr: 0.000031, Tokens per sec:   3503
2023-03-15 08:56:17,862 - INFO - __main__ - Epoch 116, Step:  252100, Batch Loss:     1.722318, Lr: 0.000031, Tokens per sec:   3520
2023-03-15 08:56:33,182 - INFO - __main__ - Epoch 116, Step:  252200, Batch Loss:     2.401687, Lr: 0.000031, Tokens per sec:   3508
2023-03-15 08:56:48,513 - INFO - __main__ - Epoch 116, Step:  252300, Batch Loss:     1.724842, Lr: 0.000031, Tokens per sec:   3519
2023-03-15 08:57:03,929 - INFO - __main__ - Epoch 116, Step:  252400, Batch Loss:     1.747028, Lr: 0.000031, Tokens per sec:   3434
2023-03-15 08:57:19,111 - INFO - __main__ - Epoch 116, Step:  252500, Batch Loss:     1.415183, Lr: 0.000031, Tokens per sec:   3544
2023-03-15 08:57:34,134 - INFO - __main__ - Epoch 116, Step:  252600, Batch Loss:     2.476552, Lr: 0.000031, Tokens per sec:   3576
2023-03-15 08:57:49,529 - INFO - __main__ - Epoch 116, Step:  252700, Batch Loss:     2.313165, Lr: 0.000031, Tokens per sec:   3515
2023-03-15 08:57:59,319 - INFO - __main__ - Epoch 116: total training loss 4200.33
2023-03-15 08:57:59,321 - INFO - __main__ - Epoch 117
2023-03-15 08:58:05,217 - INFO - __main__ - Epoch 117, Step:  252800, Batch Loss:     1.946606, Lr: 0.000031, Tokens per sec:   3339
2023-03-15 08:58:20,605 - INFO - __main__ - Epoch 117, Step:  252900, Batch Loss:     1.668918, Lr: 0.000031, Tokens per sec:   3506
2023-03-15 08:58:36,079 - INFO - __main__ - Epoch 117, Step:  253000, Batch Loss:     1.143855, Lr: 0.000031, Tokens per sec:   3478
2023-03-15 08:58:51,550 - INFO - __main__ - Epoch 117, Step:  253100, Batch Loss:     1.548114, Lr: 0.000031, Tokens per sec:   3492
2023-03-15 08:59:06,961 - INFO - __main__ - Epoch 117, Step:  253200, Batch Loss:     1.982098, Lr: 0.000031, Tokens per sec:   3518
2023-03-15 08:59:22,534 - INFO - __main__ - Epoch 117, Step:  253300, Batch Loss:     0.915585, Lr: 0.000031, Tokens per sec:   3436
2023-03-15 08:59:38,058 - INFO - __main__ - Epoch 117, Step:  253400, Batch Loss:     1.563592, Lr: 0.000031, Tokens per sec:   3462
2023-03-15 08:59:53,533 - INFO - __main__ - Epoch 117, Step:  253500, Batch Loss:     1.015679, Lr: 0.000031, Tokens per sec:   3423
2023-03-15 09:00:09,047 - INFO - __main__ - Epoch 117, Step:  253600, Batch Loss:     2.209913, Lr: 0.000031, Tokens per sec:   3463
2023-03-15 09:00:24,660 - INFO - __main__ - Epoch 117, Step:  253700, Batch Loss:     1.676728, Lr: 0.000031, Tokens per sec:   3387
2023-03-15 09:00:40,226 - INFO - __main__ - Epoch 117, Step:  253800, Batch Loss:     1.705966, Lr: 0.000031, Tokens per sec:   3501
2023-03-15 09:00:55,414 - INFO - __main__ - Epoch 117, Step:  253900, Batch Loss:     2.459455, Lr: 0.000031, Tokens per sec:   3508
2023-03-15 09:01:10,687 - INFO - __main__ - Epoch 117, Step:  254000, Batch Loss:     1.843965, Lr: 0.000031, Tokens per sec:   3546
2023-03-15 09:01:26,034 - INFO - __main__ - Epoch 117, Step:  254100, Batch Loss:     2.496725, Lr: 0.000031, Tokens per sec:   3457
2023-03-15 09:01:41,552 - INFO - __main__ - Epoch 117, Step:  254200, Batch Loss:     2.290314, Lr: 0.000031, Tokens per sec:   3451
2023-03-15 09:01:57,254 - INFO - __main__ - Epoch 117, Step:  254300, Batch Loss:     1.723571, Lr: 0.000031, Tokens per sec:   3476
2023-03-15 09:02:12,702 - INFO - __main__ - Epoch 117, Step:  254400, Batch Loss:     1.416278, Lr: 0.000031, Tokens per sec:   3503
2023-03-15 09:02:28,221 - INFO - __main__ - Epoch 117, Step:  254500, Batch Loss:     1.573759, Lr: 0.000031, Tokens per sec:   3450
2023-03-15 09:02:43,874 - INFO - __main__ - Epoch 117, Step:  254600, Batch Loss:     1.364847, Lr: 0.000031, Tokens per sec:   3446
2023-03-15 09:02:59,197 - INFO - __main__ - Epoch 117, Step:  254700, Batch Loss:     1.766705, Lr: 0.000031, Tokens per sec:   3558
2023-03-15 09:03:14,843 - INFO - __main__ - Epoch 117, Step:  254800, Batch Loss:     2.141830, Lr: 0.000031, Tokens per sec:   3427
2023-03-15 09:03:30,206 - INFO - __main__ - Epoch 117, Step:  254900, Batch Loss:     1.460494, Lr: 0.000031, Tokens per sec:   3533
2023-03-15 09:03:36,828 - INFO - __main__ - Epoch 117: total training loss 4142.24
2023-03-15 09:03:36,829 - INFO - __main__ - Epoch 118
2023-03-15 09:03:46,086 - INFO - __main__ - Epoch 118, Step:  255000, Batch Loss:     1.889418, Lr: 0.000031, Tokens per sec:   3339
2023-03-15 09:04:01,488 - INFO - __main__ - Epoch 118, Step:  255100, Batch Loss:     2.874420, Lr: 0.000031, Tokens per sec:   3523
2023-03-15 09:04:16,885 - INFO - __main__ - Epoch 118, Step:  255200, Batch Loss:     1.783296, Lr: 0.000031, Tokens per sec:   3526
2023-03-15 09:04:32,485 - INFO - __main__ - Epoch 118, Step:  255300, Batch Loss:     2.050285, Lr: 0.000031, Tokens per sec:   3463
2023-03-15 09:04:47,955 - INFO - __main__ - Epoch 118, Step:  255400, Batch Loss:     2.359263, Lr: 0.000031, Tokens per sec:   3447
2023-03-15 09:05:03,247 - INFO - __main__ - Epoch 118, Step:  255500, Batch Loss:     2.138385, Lr: 0.000031, Tokens per sec:   3523
2023-03-15 09:05:18,721 - INFO - __main__ - Epoch 118, Step:  255600, Batch Loss:     2.036580, Lr: 0.000031, Tokens per sec:   3528
2023-03-15 09:05:33,993 - INFO - __main__ - Epoch 118, Step:  255700, Batch Loss:     2.061092, Lr: 0.000031, Tokens per sec:   3529
2023-03-15 09:05:49,486 - INFO - __main__ - Epoch 118, Step:  255800, Batch Loss:     2.119424, Lr: 0.000031, Tokens per sec:   3471
2023-03-15 09:06:04,850 - INFO - __main__ - Epoch 118, Step:  255900, Batch Loss:     2.560861, Lr: 0.000031, Tokens per sec:   3465
2023-03-15 09:06:20,446 - INFO - __main__ - Epoch 118, Step:  256000, Batch Loss:     1.400207, Lr: 0.000031, Tokens per sec:   3454
2023-03-15 09:06:35,800 - INFO - __main__ - Epoch 118, Step:  256100, Batch Loss:     1.518406, Lr: 0.000031, Tokens per sec:   3475
2023-03-15 09:06:51,275 - INFO - __main__ - Epoch 118, Step:  256200, Batch Loss:     2.406916, Lr: 0.000031, Tokens per sec:   3498
2023-03-15 09:07:06,629 - INFO - __main__ - Epoch 118, Step:  256300, Batch Loss:     1.743167, Lr: 0.000031, Tokens per sec:   3454
2023-03-15 09:07:21,868 - INFO - __main__ - Epoch 118, Step:  256400, Batch Loss:     1.849287, Lr: 0.000031, Tokens per sec:   3552
2023-03-15 09:07:37,303 - INFO - __main__ - Epoch 118, Step:  256500, Batch Loss:     2.111172, Lr: 0.000031, Tokens per sec:   3473
2023-03-15 09:07:52,422 - INFO - __main__ - Epoch 118, Step:  256600, Batch Loss:     2.456554, Lr: 0.000031, Tokens per sec:   3596
2023-03-15 09:08:06,873 - INFO - __main__ - Epoch 118, Step:  256700, Batch Loss:     2.508026, Lr: 0.000031, Tokens per sec:   3739
2023-03-15 09:08:22,295 - INFO - __main__ - Epoch 118, Step:  256800, Batch Loss:     1.762125, Lr: 0.000031, Tokens per sec:   3478
2023-03-15 09:08:36,973 - INFO - __main__ - Epoch 118, Step:  256900, Batch Loss:     2.113662, Lr: 0.000031, Tokens per sec:   3600
2023-03-15 09:08:52,470 - INFO - __main__ - Epoch 118, Step:  257000, Batch Loss:     1.715048, Lr: 0.000031, Tokens per sec:   3492
2023-03-15 09:09:07,917 - INFO - __main__ - Epoch 118, Step:  257100, Batch Loss:     3.530441, Lr: 0.000031, Tokens per sec:   3520
2023-03-15 09:09:11,018 - INFO - __main__ - Epoch 118: total training loss 4076.90
2023-03-15 09:09:11,019 - INFO - __main__ - Epoch 119
2023-03-15 09:09:23,573 - INFO - __main__ - Epoch 119, Step:  257200, Batch Loss:     1.602057, Lr: 0.000031, Tokens per sec:   3340
2023-03-15 09:09:38,981 - INFO - __main__ - Epoch 119, Step:  257300, Batch Loss:     1.791364, Lr: 0.000031, Tokens per sec:   3498
2023-03-15 09:09:54,620 - INFO - __main__ - Epoch 119, Step:  257400, Batch Loss:     2.011432, Lr: 0.000031, Tokens per sec:   3471
2023-03-15 09:10:10,142 - INFO - __main__ - Epoch 119, Step:  257500, Batch Loss:     1.896779, Lr: 0.000031, Tokens per sec:   3488
2023-03-15 09:10:25,535 - INFO - __main__ - Epoch 119, Step:  257600, Batch Loss:     1.522547, Lr: 0.000031, Tokens per sec:   3479
2023-03-15 09:10:41,058 - INFO - __main__ - Epoch 119, Step:  257700, Batch Loss:     2.056667, Lr: 0.000031, Tokens per sec:   3465
2023-03-15 09:10:56,516 - INFO - __main__ - Epoch 119, Step:  257800, Batch Loss:     1.620970, Lr: 0.000031, Tokens per sec:   3449
2023-03-15 09:11:11,827 - INFO - __main__ - Epoch 119, Step:  257900, Batch Loss:     0.907090, Lr: 0.000031, Tokens per sec:   3497
2023-03-15 09:11:27,028 - INFO - __main__ - Epoch 119, Step:  258000, Batch Loss:     1.904814, Lr: 0.000031, Tokens per sec:   3527
2023-03-15 09:11:42,332 - INFO - __main__ - Epoch 119, Step:  258100, Batch Loss:     1.247646, Lr: 0.000031, Tokens per sec:   3568
2023-03-15 09:11:57,847 - INFO - __main__ - Epoch 119, Step:  258200, Batch Loss:     2.047102, Lr: 0.000031, Tokens per sec:   3447
2023-03-15 09:12:13,308 - INFO - __main__ - Epoch 119, Step:  258300, Batch Loss:     2.274997, Lr: 0.000031, Tokens per sec:   3436
2023-03-15 09:12:28,404 - INFO - __main__ - Epoch 119, Step:  258400, Batch Loss:     1.466787, Lr: 0.000031, Tokens per sec:   3526
2023-03-15 09:12:43,528 - INFO - __main__ - Epoch 119, Step:  258500, Batch Loss:     2.734022, Lr: 0.000031, Tokens per sec:   3451
2023-03-15 09:12:59,020 - INFO - __main__ - Epoch 119, Step:  258600, Batch Loss:     1.527280, Lr: 0.000031, Tokens per sec:   3480
2023-03-15 09:13:14,509 - INFO - __main__ - Epoch 119, Step:  258700, Batch Loss:     2.136533, Lr: 0.000031, Tokens per sec:   3484
2023-03-15 09:13:29,720 - INFO - __main__ - Epoch 119, Step:  258800, Batch Loss:     2.353377, Lr: 0.000031, Tokens per sec:   3522
2023-03-15 09:13:45,277 - INFO - __main__ - Epoch 119, Step:  258900, Batch Loss:     1.553867, Lr: 0.000031, Tokens per sec:   3484
2023-03-15 09:14:00,661 - INFO - __main__ - Epoch 119, Step:  259000, Batch Loss:     2.909701, Lr: 0.000031, Tokens per sec:   3577
2023-03-15 09:14:15,937 - INFO - __main__ - Epoch 119, Step:  259100, Batch Loss:     2.369030, Lr: 0.000031, Tokens per sec:   3591
2023-03-15 09:14:31,313 - INFO - __main__ - Epoch 119, Step:  259200, Batch Loss:     2.650266, Lr: 0.000031, Tokens per sec:   3539
2023-03-15 09:14:46,901 - INFO - __main__ - Epoch 119, Step:  259300, Batch Loss:     1.776274, Lr: 0.000031, Tokens per sec:   3461
2023-03-15 09:14:47,160 - INFO - __main__ - Epoch 119: total training loss 4027.21
2023-03-15 09:14:47,161 - INFO - __main__ - Epoch 120
2023-03-15 09:15:02,698 - INFO - __main__ - Epoch 120, Step:  259400, Batch Loss:     2.009198, Lr: 0.000030, Tokens per sec:   3367
2023-03-15 09:15:18,225 - INFO - __main__ - Epoch 120, Step:  259500, Batch Loss:     2.042675, Lr: 0.000030, Tokens per sec:   3478
2023-03-15 09:15:33,731 - INFO - __main__ - Epoch 120, Step:  259600, Batch Loss:     1.662219, Lr: 0.000030, Tokens per sec:   3506
2023-03-15 09:15:49,188 - INFO - __main__ - Epoch 120, Step:  259700, Batch Loss:     1.682293, Lr: 0.000030, Tokens per sec:   3542
2023-03-15 09:16:04,655 - INFO - __main__ - Epoch 120, Step:  259800, Batch Loss:     1.766771, Lr: 0.000030, Tokens per sec:   3470
2023-03-15 09:16:20,045 - INFO - __main__ - Epoch 120, Step:  259900, Batch Loss:     1.884432, Lr: 0.000030, Tokens per sec:   3527
2023-03-15 09:16:35,535 - INFO - __main__ - Epoch 120, Step:  260000, Batch Loss:     1.387006, Lr: 0.000030, Tokens per sec:   3398
2023-03-15 09:16:50,969 - INFO - __main__ - Epoch 120, Step:  260100, Batch Loss:     1.457131, Lr: 0.000030, Tokens per sec:   3415
2023-03-15 09:17:06,298 - INFO - __main__ - Epoch 120, Step:  260200, Batch Loss:     1.649268, Lr: 0.000030, Tokens per sec:   3541
2023-03-15 09:17:21,680 - INFO - __main__ - Epoch 120, Step:  260300, Batch Loss:     3.150648, Lr: 0.000030, Tokens per sec:   3517
2023-03-15 09:17:37,236 - INFO - __main__ - Epoch 120, Step:  260400, Batch Loss:     1.804149, Lr: 0.000030, Tokens per sec:   3460
2023-03-15 09:17:52,827 - INFO - __main__ - Epoch 120, Step:  260500, Batch Loss:     1.393117, Lr: 0.000030, Tokens per sec:   3469
2023-03-15 09:18:08,112 - INFO - __main__ - Epoch 120, Step:  260600, Batch Loss:     2.038339, Lr: 0.000030, Tokens per sec:   3517
2023-03-15 09:18:23,339 - INFO - __main__ - Epoch 120, Step:  260700, Batch Loss:     1.420062, Lr: 0.000030, Tokens per sec:   3595
2023-03-15 09:18:38,828 - INFO - __main__ - Epoch 120, Step:  260800, Batch Loss:     1.404423, Lr: 0.000030, Tokens per sec:   3421
2023-03-15 09:18:54,228 - INFO - __main__ - Epoch 120, Step:  260900, Batch Loss:     1.411104, Lr: 0.000030, Tokens per sec:   3496
2023-03-15 09:19:09,463 - INFO - __main__ - Epoch 120, Step:  261000, Batch Loss:     1.350024, Lr: 0.000030, Tokens per sec:   3571
2023-03-15 09:19:24,847 - INFO - __main__ - Epoch 120, Step:  261100, Batch Loss:     2.770972, Lr: 0.000030, Tokens per sec:   3516
2023-03-15 09:19:40,408 - INFO - __main__ - Epoch 120, Step:  261200, Batch Loss:     2.279568, Lr: 0.000030, Tokens per sec:   3423
2023-03-15 09:19:55,777 - INFO - __main__ - Epoch 120, Step:  261300, Batch Loss:     2.894525, Lr: 0.000030, Tokens per sec:   3463
2023-03-15 09:20:11,145 - INFO - __main__ - Epoch 120, Step:  261400, Batch Loss:     2.035865, Lr: 0.000030, Tokens per sec:   3521
2023-03-15 09:20:23,701 - INFO - __main__ - Epoch 120: total training loss 3968.97
2023-03-15 09:20:23,702 - INFO - __main__ - Epoch 121
2023-03-15 09:20:27,220 - INFO - __main__ - Epoch 121, Step:  261500, Batch Loss:     2.605061, Lr: 0.000030, Tokens per sec:   3104
2023-03-15 09:20:42,745 - INFO - __main__ - Epoch 121, Step:  261600, Batch Loss:     1.553940, Lr: 0.000030, Tokens per sec:   3531
2023-03-15 09:20:58,486 - INFO - __main__ - Epoch 121, Step:  261700, Batch Loss:     1.783894, Lr: 0.000030, Tokens per sec:   3424
2023-03-15 09:21:13,910 - INFO - __main__ - Epoch 121, Step:  261800, Batch Loss:     1.953528, Lr: 0.000030, Tokens per sec:   3487
2023-03-15 09:21:29,286 - INFO - __main__ - Epoch 121, Step:  261900, Batch Loss:     2.322310, Lr: 0.000030, Tokens per sec:   3505
2023-03-15 09:21:44,854 - INFO - __main__ - Epoch 121, Step:  262000, Batch Loss:     1.488071, Lr: 0.000030, Tokens per sec:   3456
2023-03-15 09:22:00,442 - INFO - __main__ - Epoch 121, Step:  262100, Batch Loss:     1.288103, Lr: 0.000030, Tokens per sec:   3379
2023-03-15 09:22:15,557 - INFO - __main__ - Epoch 121, Step:  262200, Batch Loss:     1.857813, Lr: 0.000030, Tokens per sec:   3548
2023-03-15 09:22:30,997 - INFO - __main__ - Epoch 121, Step:  262300, Batch Loss:     1.421666, Lr: 0.000030, Tokens per sec:   3473
2023-03-15 09:22:46,566 - INFO - __main__ - Epoch 121, Step:  262400, Batch Loss:     1.527889, Lr: 0.000030, Tokens per sec:   3450
2023-03-15 09:23:02,077 - INFO - __main__ - Epoch 121, Step:  262500, Batch Loss:     1.679149, Lr: 0.000030, Tokens per sec:   3513
2023-03-15 09:23:17,687 - INFO - __main__ - Epoch 121, Step:  262600, Batch Loss:     0.953604, Lr: 0.000030, Tokens per sec:   3471
2023-03-15 09:23:33,095 - INFO - __main__ - Epoch 121, Step:  262700, Batch Loss:     2.443779, Lr: 0.000030, Tokens per sec:   3516
2023-03-15 09:23:48,739 - INFO - __main__ - Epoch 121, Step:  262800, Batch Loss:     1.176266, Lr: 0.000030, Tokens per sec:   3475
2023-03-15 09:24:04,095 - INFO - __main__ - Epoch 121, Step:  262900, Batch Loss:     2.396100, Lr: 0.000030, Tokens per sec:   3497
2023-03-15 09:24:19,495 - INFO - __main__ - Epoch 121, Step:  263000, Batch Loss:     1.534642, Lr: 0.000030, Tokens per sec:   3485
2023-03-15 09:24:34,917 - INFO - __main__ - Epoch 121, Step:  263100, Batch Loss:     0.979214, Lr: 0.000030, Tokens per sec:   3452
2023-03-15 09:24:50,508 - INFO - __main__ - Epoch 121, Step:  263200, Batch Loss:     1.644110, Lr: 0.000030, Tokens per sec:   3456
2023-03-15 09:25:05,919 - INFO - __main__ - Epoch 121, Step:  263300, Batch Loss:     1.847378, Lr: 0.000030, Tokens per sec:   3421
2023-03-15 09:25:21,468 - INFO - __main__ - Epoch 121, Step:  263400, Batch Loss:     2.147187, Lr: 0.000030, Tokens per sec:   3481
2023-03-15 09:25:36,919 - INFO - __main__ - Epoch 121, Step:  263500, Batch Loss:     1.606201, Lr: 0.000030, Tokens per sec:   3457
2023-03-15 09:25:52,422 - INFO - __main__ - Epoch 121, Step:  263600, Batch Loss:     1.217665, Lr: 0.000030, Tokens per sec:   3526
2023-03-15 09:26:01,582 - INFO - __main__ - Epoch 121: total training loss 3925.11
2023-03-15 09:26:01,583 - INFO - __main__ - Epoch 122
2023-03-15 09:26:08,365 - INFO - __main__ - Epoch 122, Step:  263700, Batch Loss:     1.992916, Lr: 0.000030, Tokens per sec:   3287
2023-03-15 09:26:23,756 - INFO - __main__ - Epoch 122, Step:  263800, Batch Loss:     1.379652, Lr: 0.000030, Tokens per sec:   3422
2023-03-15 09:26:39,140 - INFO - __main__ - Epoch 122, Step:  263900, Batch Loss:     1.234206, Lr: 0.000030, Tokens per sec:   3542
2023-03-15 09:26:54,553 - INFO - __main__ - Epoch 122, Step:  264000, Batch Loss:     1.476309, Lr: 0.000030, Tokens per sec:   3510
2023-03-15 09:27:09,966 - INFO - __main__ - Epoch 122, Step:  264100, Batch Loss:     1.633153, Lr: 0.000030, Tokens per sec:   3402
2023-03-15 09:27:25,550 - INFO - __main__ - Epoch 122, Step:  264200, Batch Loss:     1.705291, Lr: 0.000030, Tokens per sec:   3458
2023-03-15 09:27:40,824 - INFO - __main__ - Epoch 122, Step:  264300, Batch Loss:     1.981200, Lr: 0.000030, Tokens per sec:   3560
2023-03-15 09:27:55,540 - INFO - __main__ - Epoch 122, Step:  264400, Batch Loss:     2.132800, Lr: 0.000030, Tokens per sec:   3686
2023-03-15 09:28:11,006 - INFO - __main__ - Epoch 122, Step:  264500, Batch Loss:     2.667356, Lr: 0.000030, Tokens per sec:   3468
2023-03-15 09:28:26,553 - INFO - __main__ - Epoch 122, Step:  264600, Batch Loss:     1.738294, Lr: 0.000030, Tokens per sec:   3438
2023-03-15 09:28:42,012 - INFO - __main__ - Epoch 122, Step:  264700, Batch Loss:     2.335513, Lr: 0.000030, Tokens per sec:   3467
2023-03-15 09:28:57,475 - INFO - __main__ - Epoch 122, Step:  264800, Batch Loss:     1.983773, Lr: 0.000030, Tokens per sec:   3486
2023-03-15 09:29:12,952 - INFO - __main__ - Epoch 122, Step:  264900, Batch Loss:     1.959010, Lr: 0.000030, Tokens per sec:   3475
2023-03-15 09:29:28,305 - INFO - __main__ - Epoch 122, Step:  265000, Batch Loss:     2.111511, Lr: 0.000030, Tokens per sec:   3526
2023-03-15 09:29:43,557 - INFO - __main__ - Epoch 122, Step:  265100, Batch Loss:     1.912416, Lr: 0.000030, Tokens per sec:   3560
2023-03-15 09:29:58,753 - INFO - __main__ - Epoch 122, Step:  265200, Batch Loss:     0.714437, Lr: 0.000030, Tokens per sec:   3564
2023-03-15 09:30:14,251 - INFO - __main__ - Epoch 122, Step:  265300, Batch Loss:     2.338853, Lr: 0.000030, Tokens per sec:   3511
2023-03-15 09:30:29,836 - INFO - __main__ - Epoch 122, Step:  265400, Batch Loss:     1.550229, Lr: 0.000030, Tokens per sec:   3491
2023-03-15 09:30:45,161 - INFO - __main__ - Epoch 122, Step:  265500, Batch Loss:     1.556121, Lr: 0.000030, Tokens per sec:   3548
2023-03-15 09:31:00,743 - INFO - __main__ - Epoch 122, Step:  265600, Batch Loss:     2.221461, Lr: 0.000030, Tokens per sec:   3430
2023-03-15 09:31:16,178 - INFO - __main__ - Epoch 122, Step:  265700, Batch Loss:     1.707198, Lr: 0.000030, Tokens per sec:   3454
2023-03-15 09:31:31,241 - INFO - __main__ - Epoch 122, Step:  265800, Batch Loss:     1.466505, Lr: 0.000030, Tokens per sec:   3499
2023-03-15 09:31:37,223 - INFO - __main__ - Epoch 122: total training loss 3883.36
2023-03-15 09:31:37,224 - INFO - __main__ - Epoch 123
2023-03-15 09:31:47,148 - INFO - __main__ - Epoch 123, Step:  265900, Batch Loss:     1.471799, Lr: 0.000029, Tokens per sec:   3388
2023-03-15 09:32:02,599 - INFO - __main__ - Epoch 123, Step:  266000, Batch Loss:     1.610133, Lr: 0.000029, Tokens per sec:   3481
2023-03-15 09:32:18,001 - INFO - __main__ - Epoch 123, Step:  266100, Batch Loss:     1.929078, Lr: 0.000029, Tokens per sec:   3539
2023-03-15 09:32:33,511 - INFO - __main__ - Epoch 123, Step:  266200, Batch Loss:     0.818195, Lr: 0.000029, Tokens per sec:   3433
2023-03-15 09:32:48,662 - INFO - __main__ - Epoch 123, Step:  266300, Batch Loss:     1.993615, Lr: 0.000029, Tokens per sec:   3525
2023-03-15 09:33:03,924 - INFO - __main__ - Epoch 123, Step:  266400, Batch Loss:     1.796849, Lr: 0.000029, Tokens per sec:   3500
2023-03-15 09:33:19,477 - INFO - __main__ - Epoch 123, Step:  266500, Batch Loss:     1.314319, Lr: 0.000029, Tokens per sec:   3449
2023-03-15 09:33:34,577 - INFO - __main__ - Epoch 123, Step:  266600, Batch Loss:     1.656085, Lr: 0.000029, Tokens per sec:   3525
2023-03-15 09:33:50,008 - INFO - __main__ - Epoch 123, Step:  266700, Batch Loss:     0.818067, Lr: 0.000029, Tokens per sec:   3526
2023-03-15 09:34:05,477 - INFO - __main__ - Epoch 123, Step:  266800, Batch Loss:     1.599961, Lr: 0.000029, Tokens per sec:   3499
2023-03-15 09:34:20,555 - INFO - __main__ - Epoch 123, Step:  266900, Batch Loss:     2.372905, Lr: 0.000029, Tokens per sec:   3572
2023-03-15 09:34:35,993 - INFO - __main__ - Epoch 123, Step:  267000, Batch Loss:     1.794415, Lr: 0.000029, Tokens per sec:   3514
2023-03-15 09:34:51,545 - INFO - __main__ - Epoch 123, Step:  267100, Batch Loss:     1.180701, Lr: 0.000029, Tokens per sec:   3418
2023-03-15 09:35:06,983 - INFO - __main__ - Epoch 123, Step:  267200, Batch Loss:     1.263385, Lr: 0.000029, Tokens per sec:   3447
2023-03-15 09:35:22,513 - INFO - __main__ - Epoch 123, Step:  267300, Batch Loss:     2.344240, Lr: 0.000029, Tokens per sec:   3501
2023-03-15 09:35:38,097 - INFO - __main__ - Epoch 123, Step:  267400, Batch Loss:     1.582073, Lr: 0.000029, Tokens per sec:   3401
2023-03-15 09:35:53,507 - INFO - __main__ - Epoch 123, Step:  267500, Batch Loss:     2.057348, Lr: 0.000029, Tokens per sec:   3502
2023-03-15 09:36:09,128 - INFO - __main__ - Epoch 123, Step:  267600, Batch Loss:     1.985693, Lr: 0.000029, Tokens per sec:   3476
2023-03-15 09:36:24,644 - INFO - __main__ - Epoch 123, Step:  267700, Batch Loss:     1.806014, Lr: 0.000029, Tokens per sec:   3512
2023-03-15 09:36:39,931 - INFO - __main__ - Epoch 123, Step:  267800, Batch Loss:     2.419917, Lr: 0.000029, Tokens per sec:   3536
2023-03-15 09:36:55,584 - INFO - __main__ - Epoch 123, Step:  267900, Batch Loss:     1.478461, Lr: 0.000029, Tokens per sec:   3431
2023-03-15 09:37:11,200 - INFO - __main__ - Epoch 123, Step:  268000, Batch Loss:     1.558730, Lr: 0.000029, Tokens per sec:   3482
2023-03-15 09:37:13,942 - INFO - __main__ - Epoch 123: total training loss 3859.52
2023-03-15 09:37:13,943 - INFO - __main__ - Epoch 124
2023-03-15 09:37:27,119 - INFO - __main__ - Epoch 124, Step:  268100, Batch Loss:     0.488682, Lr: 0.000029, Tokens per sec:   3393
2023-03-15 09:37:42,262 - INFO - __main__ - Epoch 124, Step:  268200, Batch Loss:     1.601623, Lr: 0.000029, Tokens per sec:   3549
2023-03-15 09:37:57,631 - INFO - __main__ - Epoch 124, Step:  268300, Batch Loss:     2.267503, Lr: 0.000029, Tokens per sec:   3577
2023-03-15 09:38:13,104 - INFO - __main__ - Epoch 124, Step:  268400, Batch Loss:     1.615196, Lr: 0.000029, Tokens per sec:   3493
2023-03-15 09:38:28,420 - INFO - __main__ - Epoch 124, Step:  268500, Batch Loss:     3.003383, Lr: 0.000029, Tokens per sec:   3494
2023-03-15 09:38:43,906 - INFO - __main__ - Epoch 124, Step:  268600, Batch Loss:     2.066681, Lr: 0.000029, Tokens per sec:   3453
2023-03-15 09:38:59,034 - INFO - __main__ - Epoch 124, Step:  268700, Batch Loss:     2.126095, Lr: 0.000029, Tokens per sec:   3606
2023-03-15 09:39:14,629 - INFO - __main__ - Epoch 124, Step:  268800, Batch Loss:     2.208431, Lr: 0.000029, Tokens per sec:   3454
2023-03-15 09:39:30,092 - INFO - __main__ - Epoch 124, Step:  268900, Batch Loss:     1.627273, Lr: 0.000029, Tokens per sec:   3515
2023-03-15 09:39:45,552 - INFO - __main__ - Epoch 124, Step:  269000, Batch Loss:     1.928947, Lr: 0.000029, Tokens per sec:   3478
2023-03-15 09:40:01,159 - INFO - __main__ - Epoch 124, Step:  269100, Batch Loss:     1.342110, Lr: 0.000029, Tokens per sec:   3464
2023-03-15 09:40:16,578 - INFO - __main__ - Epoch 124, Step:  269200, Batch Loss:     1.206005, Lr: 0.000029, Tokens per sec:   3503
2023-03-15 09:40:32,027 - INFO - __main__ - Epoch 124, Step:  269300, Batch Loss:     1.943705, Lr: 0.000029, Tokens per sec:   3416
2023-03-15 09:40:47,354 - INFO - __main__ - Epoch 124, Step:  269400, Batch Loss:     1.553370, Lr: 0.000029, Tokens per sec:   3537
2023-03-15 09:41:03,114 - INFO - __main__ - Epoch 124, Step:  269500, Batch Loss:     1.702886, Lr: 0.000029, Tokens per sec:   3422
2023-03-15 09:41:18,279 - INFO - __main__ - Epoch 124, Step:  269600, Batch Loss:     2.538778, Lr: 0.000029, Tokens per sec:   3542
2023-03-15 09:41:33,227 - INFO - __main__ - Epoch 124, Step:  269700, Batch Loss:     1.992994, Lr: 0.000029, Tokens per sec:   3599
2023-03-15 09:41:48,632 - INFO - __main__ - Epoch 124, Step:  269800, Batch Loss:     1.272176, Lr: 0.000029, Tokens per sec:   3466
2023-03-15 09:42:04,071 - INFO - __main__ - Epoch 124, Step:  269900, Batch Loss:     1.313504, Lr: 0.000029, Tokens per sec:   3490
2023-03-15 09:42:19,086 - INFO - __main__ - Epoch 124, Step:  270000, Batch Loss:     1.789506, Lr: 0.000029, Tokens per sec:   3543
2023-03-15 09:42:34,468 - INFO - __main__ - Epoch 124, Step:  270100, Batch Loss:     1.264262, Lr: 0.000029, Tokens per sec:   3494
2023-03-15 09:42:49,139 - INFO - __main__ - Epoch 124: total training loss 3808.47
2023-03-15 09:42:49,140 - INFO - __main__ - Epoch 125
2023-03-15 09:42:50,126 - INFO - __main__ - Epoch 125, Step:  270200, Batch Loss:     1.286669, Lr: 0.000029, Tokens per sec:   2185
2023-03-15 09:43:05,722 - INFO - __main__ - Epoch 125, Step:  270300, Batch Loss:     1.208724, Lr: 0.000029, Tokens per sec:   3433
2023-03-15 09:43:21,135 - INFO - __main__ - Epoch 125, Step:  270400, Batch Loss:     0.914759, Lr: 0.000029, Tokens per sec:   3508
2023-03-15 09:43:36,662 - INFO - __main__ - Epoch 125, Step:  270500, Batch Loss:     1.751888, Lr: 0.000029, Tokens per sec:   3410
2023-03-15 09:43:52,202 - INFO - __main__ - Epoch 125, Step:  270600, Batch Loss:     1.385664, Lr: 0.000029, Tokens per sec:   3543
2023-03-15 09:44:07,399 - INFO - __main__ - Epoch 125, Step:  270700, Batch Loss:     2.477919, Lr: 0.000029, Tokens per sec:   3564
2023-03-15 09:44:22,156 - INFO - __main__ - Epoch 125, Step:  270800, Batch Loss:     1.485914, Lr: 0.000029, Tokens per sec:   3663
2023-03-15 09:44:37,733 - INFO - __main__ - Epoch 125, Step:  270900, Batch Loss:     1.182008, Lr: 0.000029, Tokens per sec:   3478
2023-03-15 09:44:52,648 - INFO - __main__ - Epoch 125, Step:  271000, Batch Loss:     2.100455, Lr: 0.000029, Tokens per sec:   3590
2023-03-15 09:45:08,051 - INFO - __main__ - Epoch 125, Step:  271100, Batch Loss:     1.023973, Lr: 0.000029, Tokens per sec:   3576
2023-03-15 09:45:23,539 - INFO - __main__ - Epoch 125, Step:  271200, Batch Loss:     2.041487, Lr: 0.000029, Tokens per sec:   3472
2023-03-15 09:45:38,955 - INFO - __main__ - Epoch 125, Step:  271300, Batch Loss:     0.995923, Lr: 0.000029, Tokens per sec:   3496
2023-03-15 09:45:54,378 - INFO - __main__ - Epoch 125, Step:  271400, Batch Loss:     1.930866, Lr: 0.000029, Tokens per sec:   3445
2023-03-15 09:46:09,725 - INFO - __main__ - Epoch 125, Step:  271500, Batch Loss:     2.031067, Lr: 0.000029, Tokens per sec:   3497
2023-03-15 09:46:25,193 - INFO - __main__ - Epoch 125, Step:  271600, Batch Loss:     1.989460, Lr: 0.000029, Tokens per sec:   3462
2023-03-15 09:46:40,671 - INFO - __main__ - Epoch 125, Step:  271700, Batch Loss:     1.680656, Lr: 0.000029, Tokens per sec:   3487
2023-03-15 09:46:56,253 - INFO - __main__ - Epoch 125, Step:  271800, Batch Loss:     1.675808, Lr: 0.000029, Tokens per sec:   3443
2023-03-15 09:47:11,443 - INFO - __main__ - Epoch 125, Step:  271900, Batch Loss:     2.268515, Lr: 0.000029, Tokens per sec:   3588
2023-03-15 09:47:26,799 - INFO - __main__ - Epoch 125, Step:  272000, Batch Loss:     1.403296, Lr: 0.000029, Tokens per sec:   3471
2023-03-15 09:47:42,131 - INFO - __main__ - Epoch 125, Step:  272100, Batch Loss:     1.315099, Lr: 0.000029, Tokens per sec:   3450
2023-03-15 09:47:57,730 - INFO - __main__ - Epoch 125, Step:  272200, Batch Loss:     2.017583, Lr: 0.000029, Tokens per sec:   3449
2023-03-15 09:48:13,258 - INFO - __main__ - Epoch 125, Step:  272300, Batch Loss:     2.091693, Lr: 0.000029, Tokens per sec:   3454
2023-03-15 09:48:24,791 - INFO - __main__ - Epoch 125: total training loss 3764.94
2023-03-15 09:48:24,791 - INFO - __main__ - Epoch 126
2023-03-15 09:48:29,104 - INFO - __main__ - Epoch 126, Step:  272400, Batch Loss:     1.492581, Lr: 0.000028, Tokens per sec:   3118
2023-03-15 09:48:44,549 - INFO - __main__ - Epoch 126, Step:  272500, Batch Loss:     1.385987, Lr: 0.000028, Tokens per sec:   3494
2023-03-15 09:49:00,020 - INFO - __main__ - Epoch 126, Step:  272600, Batch Loss:     1.500591, Lr: 0.000028, Tokens per sec:   3489
2023-03-15 09:49:15,534 - INFO - __main__ - Epoch 126, Step:  272700, Batch Loss:     0.943933, Lr: 0.000028, Tokens per sec:   3511
2023-03-15 09:49:30,878 - INFO - __main__ - Epoch 126, Step:  272800, Batch Loss:     1.524529, Lr: 0.000028, Tokens per sec:   3474
2023-03-15 09:49:45,972 - INFO - __main__ - Epoch 126, Step:  272900, Batch Loss:     1.620099, Lr: 0.000028, Tokens per sec:   3562
2023-03-15 09:50:01,216 - INFO - __main__ - Epoch 126, Step:  273000, Batch Loss:     1.336495, Lr: 0.000028, Tokens per sec:   3537
2023-03-15 09:50:16,518 - INFO - __main__ - Epoch 126, Step:  273100, Batch Loss:     1.861248, Lr: 0.000028, Tokens per sec:   3528
2023-03-15 09:50:31,840 - INFO - __main__ - Epoch 126, Step:  273200, Batch Loss:     2.001863, Lr: 0.000028, Tokens per sec:   3499
2023-03-15 09:50:47,221 - INFO - __main__ - Epoch 126, Step:  273300, Batch Loss:     1.494415, Lr: 0.000028, Tokens per sec:   3480
2023-03-15 09:51:02,711 - INFO - __main__ - Epoch 126, Step:  273400, Batch Loss:     1.587783, Lr: 0.000028, Tokens per sec:   3522
2023-03-15 09:51:18,149 - INFO - __main__ - Epoch 126, Step:  273500, Batch Loss:     1.804949, Lr: 0.000028, Tokens per sec:   3439
2023-03-15 09:51:33,668 - INFO - __main__ - Epoch 126, Step:  273600, Batch Loss:     1.915705, Lr: 0.000028, Tokens per sec:   3442
2023-03-15 09:51:49,271 - INFO - __main__ - Epoch 126, Step:  273700, Batch Loss:     2.300289, Lr: 0.000028, Tokens per sec:   3462
2023-03-15 09:52:04,906 - INFO - __main__ - Epoch 126, Step:  273800, Batch Loss:     0.891866, Lr: 0.000028, Tokens per sec:   3440
2023-03-15 09:52:19,971 - INFO - __main__ - Epoch 126, Step:  273900, Batch Loss:     1.787488, Lr: 0.000028, Tokens per sec:   3607
2023-03-15 09:52:35,446 - INFO - __main__ - Epoch 126, Step:  274000, Batch Loss:     1.446554, Lr: 0.000028, Tokens per sec:   3483
2023-03-15 09:52:50,669 - INFO - __main__ - Epoch 126, Step:  274100, Batch Loss:     2.018058, Lr: 0.000028, Tokens per sec:   3539
2023-03-15 09:53:06,103 - INFO - __main__ - Epoch 126, Step:  274200, Batch Loss:     0.687755, Lr: 0.000028, Tokens per sec:   3516
2023-03-15 09:53:21,508 - INFO - __main__ - Epoch 126, Step:  274300, Batch Loss:     1.743461, Lr: 0.000028, Tokens per sec:   3451
2023-03-15 09:53:36,953 - INFO - __main__ - Epoch 126, Step:  274400, Batch Loss:     1.669654, Lr: 0.000028, Tokens per sec:   3448
2023-03-15 09:53:52,475 - INFO - __main__ - Epoch 126, Step:  274500, Batch Loss:     1.583127, Lr: 0.000028, Tokens per sec:   3487
2023-03-15 09:54:00,884 - INFO - __main__ - Epoch 126: total training loss 3715.58
2023-03-15 09:54:00,885 - INFO - __main__ - Epoch 127
2023-03-15 09:54:08,313 - INFO - __main__ - Epoch 127, Step:  274600, Batch Loss:     1.739441, Lr: 0.000028, Tokens per sec:   3404
2023-03-15 09:54:24,081 - INFO - __main__ - Epoch 127, Step:  274700, Batch Loss:     1.243113, Lr: 0.000028, Tokens per sec:   3394
2023-03-15 09:54:39,770 - INFO - __main__ - Epoch 127, Step:  274800, Batch Loss:     1.890694, Lr: 0.000028, Tokens per sec:   3420
2023-03-15 09:54:55,258 - INFO - __main__ - Epoch 127, Step:  274900, Batch Loss:     1.527082, Lr: 0.000028, Tokens per sec:   3530
2023-03-15 09:55:10,769 - INFO - __main__ - Epoch 127, Step:  275000, Batch Loss:     1.448277, Lr: 0.000028, Tokens per sec:   3357
2023-03-15 09:55:26,490 - INFO - __main__ - Epoch 127, Step:  275100, Batch Loss:     1.221637, Lr: 0.000028, Tokens per sec:   3460
2023-03-15 09:55:41,610 - INFO - __main__ - Epoch 127, Step:  275200, Batch Loss:     1.347295, Lr: 0.000028, Tokens per sec:   3606
2023-03-15 09:55:57,016 - INFO - __main__ - Epoch 127, Step:  275300, Batch Loss:     0.870442, Lr: 0.000028, Tokens per sec:   3487
2023-03-15 09:56:12,114 - INFO - __main__ - Epoch 127, Step:  275400, Batch Loss:     1.797282, Lr: 0.000028, Tokens per sec:   3528
2023-03-15 09:56:27,570 - INFO - __main__ - Epoch 127, Step:  275500, Batch Loss:     1.689559, Lr: 0.000028, Tokens per sec:   3516
2023-03-15 09:56:42,979 - INFO - __main__ - Epoch 127, Step:  275600, Batch Loss:     2.261289, Lr: 0.000028, Tokens per sec:   3531
2023-03-15 09:56:58,379 - INFO - __main__ - Epoch 127, Step:  275700, Batch Loss:     2.059817, Lr: 0.000028, Tokens per sec:   3510
2023-03-15 09:57:13,423 - INFO - __main__ - Epoch 127, Step:  275800, Batch Loss:     1.291020, Lr: 0.000028, Tokens per sec:   3601
2023-03-15 09:57:28,746 - INFO - __main__ - Epoch 127, Step:  275900, Batch Loss:     2.096400, Lr: 0.000028, Tokens per sec:   3551
2023-03-15 09:57:43,804 - INFO - __main__ - Epoch 127, Step:  276000, Batch Loss:     1.681869, Lr: 0.000028, Tokens per sec:   3569
2023-03-15 09:57:58,204 - INFO - __main__ - Epoch 127, Step:  276100, Batch Loss:     1.696734, Lr: 0.000028, Tokens per sec:   3776
2023-03-15 09:58:13,511 - INFO - __main__ - Epoch 127, Step:  276200, Batch Loss:     2.105276, Lr: 0.000028, Tokens per sec:   3519
2023-03-15 09:58:27,711 - INFO - __main__ - Epoch 127, Step:  276300, Batch Loss:     2.045316, Lr: 0.000028, Tokens per sec:   3770
2023-03-15 09:58:41,943 - INFO - __main__ - Epoch 127, Step:  276400, Batch Loss:     1.608623, Lr: 0.000028, Tokens per sec:   3810
2023-03-15 09:58:57,359 - INFO - __main__ - Epoch 127, Step:  276500, Batch Loss:     2.137449, Lr: 0.000028, Tokens per sec:   3439
2023-03-15 09:59:12,682 - INFO - __main__ - Epoch 127, Step:  276600, Batch Loss:     1.319197, Lr: 0.000028, Tokens per sec:   3484
2023-03-15 09:59:28,046 - INFO - __main__ - Epoch 127, Step:  276700, Batch Loss:     2.348473, Lr: 0.000028, Tokens per sec:   3449
2023-03-15 09:59:33,166 - INFO - __main__ - Epoch 127: total training loss 3673.61
2023-03-15 09:59:33,167 - INFO - __main__ - Epoch 128
2023-03-15 09:59:43,861 - INFO - __main__ - Epoch 128, Step:  276800, Batch Loss:     1.665788, Lr: 0.000028, Tokens per sec:   3363
2023-03-15 09:59:59,249 - INFO - __main__ - Epoch 128, Step:  276900, Batch Loss:     1.543578, Lr: 0.000028, Tokens per sec:   3558
2023-03-15 10:00:14,519 - INFO - __main__ - Epoch 128, Step:  277000, Batch Loss:     1.331095, Lr: 0.000028, Tokens per sec:   3526
2023-03-15 10:00:29,435 - INFO - __main__ - Epoch 128, Step:  277100, Batch Loss:     1.295078, Lr: 0.000028, Tokens per sec:   3609
2023-03-15 10:00:44,653 - INFO - __main__ - Epoch 128, Step:  277200, Batch Loss:     1.385358, Lr: 0.000028, Tokens per sec:   3533
2023-03-15 10:00:59,922 - INFO - __main__ - Epoch 128, Step:  277300, Batch Loss:     1.474518, Lr: 0.000028, Tokens per sec:   3540
2023-03-15 10:01:15,252 - INFO - __main__ - Epoch 128, Step:  277400, Batch Loss:     1.037365, Lr: 0.000028, Tokens per sec:   3506
2023-03-15 10:01:30,588 - INFO - __main__ - Epoch 128, Step:  277500, Batch Loss:     1.401605, Lr: 0.000028, Tokens per sec:   3526
2023-03-15 10:01:45,437 - INFO - __main__ - Epoch 128, Step:  277600, Batch Loss:     2.332409, Lr: 0.000028, Tokens per sec:   3630
2023-03-15 10:01:59,421 - INFO - __main__ - Epoch 128, Step:  277700, Batch Loss:     1.822783, Lr: 0.000028, Tokens per sec:   3906
2023-03-15 10:02:14,700 - INFO - __main__ - Epoch 128, Step:  277800, Batch Loss:     2.442968, Lr: 0.000028, Tokens per sec:   3549
2023-03-15 10:02:29,975 - INFO - __main__ - Epoch 128, Step:  277900, Batch Loss:     1.958846, Lr: 0.000028, Tokens per sec:   3501
2023-03-15 10:02:45,543 - INFO - __main__ - Epoch 128, Step:  278000, Batch Loss:     2.135340, Lr: 0.000028, Tokens per sec:   3487
2023-03-15 10:03:00,999 - INFO - __main__ - Epoch 128, Step:  278100, Batch Loss:     2.474355, Lr: 0.000028, Tokens per sec:   3482
2023-03-15 10:03:16,330 - INFO - __main__ - Epoch 128, Step:  278200, Batch Loss:     1.476082, Lr: 0.000028, Tokens per sec:   3443
2023-03-15 10:03:31,688 - INFO - __main__ - Epoch 128, Step:  278300, Batch Loss:     2.283962, Lr: 0.000028, Tokens per sec:   3471
2023-03-15 10:03:47,042 - INFO - __main__ - Epoch 128, Step:  278400, Batch Loss:     1.815939, Lr: 0.000028, Tokens per sec:   3514
2023-03-15 10:04:02,518 - INFO - __main__ - Epoch 128, Step:  278500, Batch Loss:     1.228437, Lr: 0.000028, Tokens per sec:   3514
2023-03-15 10:04:18,249 - INFO - __main__ - Epoch 128, Step:  278600, Batch Loss:     1.621736, Lr: 0.000028, Tokens per sec:   3450
2023-03-15 10:04:33,761 - INFO - __main__ - Epoch 128, Step:  278700, Batch Loss:     1.427586, Lr: 0.000028, Tokens per sec:   3439
2023-03-15 10:04:49,359 - INFO - __main__ - Epoch 128, Step:  278800, Batch Loss:     1.992611, Lr: 0.000028, Tokens per sec:   3471
2023-03-15 10:05:04,885 - INFO - __main__ - Epoch 128, Step:  278900, Batch Loss:     1.874800, Lr: 0.000028, Tokens per sec:   3342
2023-03-15 10:05:06,713 - INFO - __main__ - Epoch 128: total training loss 3640.96
2023-03-15 10:05:06,714 - INFO - __main__ - Epoch 129
2023-03-15 10:05:20,853 - INFO - __main__ - Epoch 129, Step:  279000, Batch Loss:     2.364155, Lr: 0.000028, Tokens per sec:   3367
2023-03-15 10:05:36,186 - INFO - __main__ - Epoch 129, Step:  279100, Batch Loss:     2.541583, Lr: 0.000028, Tokens per sec:   3505
2023-03-15 10:05:51,627 - INFO - __main__ - Epoch 129, Step:  279200, Batch Loss:     1.405890, Lr: 0.000028, Tokens per sec:   3499
2023-03-15 10:06:07,119 - INFO - __main__ - Epoch 129, Step:  279300, Batch Loss:     1.614848, Lr: 0.000028, Tokens per sec:   3495
2023-03-15 10:06:22,699 - INFO - __main__ - Epoch 129, Step:  279400, Batch Loss:     0.816282, Lr: 0.000028, Tokens per sec:   3447
2023-03-15 10:06:38,271 - INFO - __main__ - Epoch 129, Step:  279500, Batch Loss:     2.314239, Lr: 0.000028, Tokens per sec:   3451
2023-03-15 10:06:53,811 - INFO - __main__ - Epoch 129, Step:  279600, Batch Loss:     0.935843, Lr: 0.000028, Tokens per sec:   3526
2023-03-15 10:07:09,230 - INFO - __main__ - Epoch 129, Step:  279700, Batch Loss:     1.978095, Lr: 0.000028, Tokens per sec:   3518
2023-03-15 10:07:24,612 - INFO - __main__ - Epoch 129, Step:  279800, Batch Loss:     1.445357, Lr: 0.000028, Tokens per sec:   3541
2023-03-15 10:07:39,927 - INFO - __main__ - Epoch 129, Step:  279900, Batch Loss:     1.452017, Lr: 0.000028, Tokens per sec:   3475
2023-03-15 10:07:55,306 - INFO - __main__ - Epoch 129, Step:  280000, Batch Loss:     1.036808, Lr: 0.000028, Tokens per sec:   3475
2023-03-15 10:08:10,736 - INFO - __main__ - Epoch 129, Step:  280100, Batch Loss:     2.186821, Lr: 0.000028, Tokens per sec:   3469
2023-03-15 10:08:26,129 - INFO - __main__ - Epoch 129, Step:  280200, Batch Loss:     1.742195, Lr: 0.000028, Tokens per sec:   3483
2023-03-15 10:08:41,476 - INFO - __main__ - Epoch 129, Step:  280300, Batch Loss:     2.861607, Lr: 0.000028, Tokens per sec:   3478
2023-03-15 10:08:56,543 - INFO - __main__ - Epoch 129, Step:  280400, Batch Loss:     1.726572, Lr: 0.000028, Tokens per sec:   3546
2023-03-15 10:09:11,476 - INFO - __main__ - Epoch 129, Step:  280500, Batch Loss:     1.249351, Lr: 0.000028, Tokens per sec:   3590
2023-03-15 10:09:26,975 - INFO - __main__ - Epoch 129, Step:  280600, Batch Loss:     2.094615, Lr: 0.000028, Tokens per sec:   3438
2023-03-15 10:09:42,656 - INFO - __main__ - Epoch 129, Step:  280700, Batch Loss:     2.084903, Lr: 0.000028, Tokens per sec:   3452
2023-03-15 10:09:58,285 - INFO - __main__ - Epoch 129, Step:  280800, Batch Loss:     1.625587, Lr: 0.000028, Tokens per sec:   3441
2023-03-15 10:10:13,705 - INFO - __main__ - Epoch 129, Step:  280900, Batch Loss:     1.061565, Lr: 0.000028, Tokens per sec:   3496
2023-03-15 10:10:29,153 - INFO - __main__ - Epoch 129, Step:  281000, Batch Loss:     2.271626, Lr: 0.000028, Tokens per sec:   3521
2023-03-15 10:10:43,303 - INFO - __main__ - Epoch 129: total training loss 3622.12
2023-03-15 10:10:43,304 - INFO - __main__ - Epoch 130
2023-03-15 10:10:45,055 - INFO - __main__ - Epoch 130, Step:  281100, Batch Loss:     0.710216, Lr: 0.000027, Tokens per sec:   2717
2023-03-15 10:11:00,550 - INFO - __main__ - Epoch 130, Step:  281200, Batch Loss:     1.927252, Lr: 0.000027, Tokens per sec:   3451
2023-03-15 10:11:15,922 - INFO - __main__ - Epoch 130, Step:  281300, Batch Loss:     1.123620, Lr: 0.000027, Tokens per sec:   3521
2023-03-15 10:11:31,436 - INFO - __main__ - Epoch 130, Step:  281400, Batch Loss:     1.786565, Lr: 0.000027, Tokens per sec:   3477
2023-03-15 10:11:46,960 - INFO - __main__ - Epoch 130, Step:  281500, Batch Loss:     1.517749, Lr: 0.000027, Tokens per sec:   3421
2023-03-15 10:12:02,329 - INFO - __main__ - Epoch 130, Step:  281600, Batch Loss:     1.693083, Lr: 0.000027, Tokens per sec:   3508
2023-03-15 10:12:17,911 - INFO - __main__ - Epoch 130, Step:  281700, Batch Loss:     1.063676, Lr: 0.000027, Tokens per sec:   3412
2023-03-15 10:12:33,299 - INFO - __main__ - Epoch 130, Step:  281800, Batch Loss:     2.186273, Lr: 0.000027, Tokens per sec:   3517
2023-03-15 10:12:48,642 - INFO - __main__ - Epoch 130, Step:  281900, Batch Loss:     1.245940, Lr: 0.000027, Tokens per sec:   3534
2023-03-15 10:13:04,122 - INFO - __main__ - Epoch 130, Step:  282000, Batch Loss:     1.920948, Lr: 0.000027, Tokens per sec:   3429
2023-03-15 10:13:19,548 - INFO - __main__ - Epoch 130, Step:  282100, Batch Loss:     2.328865, Lr: 0.000027, Tokens per sec:   3492
2023-03-15 10:13:35,191 - INFO - __main__ - Epoch 130, Step:  282200, Batch Loss:     0.939587, Lr: 0.000027, Tokens per sec:   3462
2023-03-15 10:13:50,599 - INFO - __main__ - Epoch 130, Step:  282300, Batch Loss:     1.490721, Lr: 0.000027, Tokens per sec:   3543
2023-03-15 10:14:06,088 - INFO - __main__ - Epoch 130, Step:  282400, Batch Loss:     1.100044, Lr: 0.000027, Tokens per sec:   3454
2023-03-15 10:14:21,740 - INFO - __main__ - Epoch 130, Step:  282500, Batch Loss:     1.582905, Lr: 0.000027, Tokens per sec:   3476
2023-03-15 10:14:37,342 - INFO - __main__ - Epoch 130, Step:  282600, Batch Loss:     2.164857, Lr: 0.000027, Tokens per sec:   3467
2023-03-15 10:14:52,836 - INFO - __main__ - Epoch 130, Step:  282700, Batch Loss:     1.796570, Lr: 0.000027, Tokens per sec:   3498
2023-03-15 10:15:07,833 - INFO - __main__ - Epoch 130, Step:  282800, Batch Loss:     1.427747, Lr: 0.000027, Tokens per sec:   3599
2023-03-15 10:15:24,260 - INFO - __main__ - Epoch 130, Step:  282900, Batch Loss:     1.510342, Lr: 0.000027, Tokens per sec:   3259
2023-03-15 10:15:40,974 - INFO - __main__ - Epoch 130, Step:  283000, Batch Loss:     1.344076, Lr: 0.000027, Tokens per sec:   3170
2023-03-15 10:15:56,838 - INFO - __main__ - Epoch 130, Step:  283100, Batch Loss:     1.411808, Lr: 0.000027, Tokens per sec:   3433
2023-03-15 10:16:13,123 - INFO - __main__ - Epoch 130, Step:  283200, Batch Loss:     2.081323, Lr: 0.000027, Tokens per sec:   3290
2023-03-15 10:16:24,681 - INFO - __main__ - Epoch 130: total training loss 3571.53
2023-03-15 10:16:24,682 - INFO - __main__ - Epoch 131
2023-03-15 10:16:29,670 - INFO - __main__ - Epoch 131, Step:  283300, Batch Loss:     1.385177, Lr: 0.000027, Tokens per sec:   3149
2023-03-15 10:16:46,091 - INFO - __main__ - Epoch 131, Step:  283400, Batch Loss:     1.187819, Lr: 0.000027, Tokens per sec:   3282
2023-03-15 10:17:02,938 - INFO - __main__ - Epoch 131, Step:  283500, Batch Loss:     1.157297, Lr: 0.000027, Tokens per sec:   3149
2023-03-15 10:17:18,999 - INFO - __main__ - Epoch 131, Step:  283600, Batch Loss:     1.185025, Lr: 0.000027, Tokens per sec:   3297
2023-03-15 10:17:35,763 - INFO - __main__ - Epoch 131, Step:  283700, Batch Loss:     0.982152, Lr: 0.000027, Tokens per sec:   3227
2023-03-15 10:17:51,749 - INFO - __main__ - Epoch 131, Step:  283800, Batch Loss:     1.394636, Lr: 0.000027, Tokens per sec:   3407
2023-03-15 10:18:08,260 - INFO - __main__ - Epoch 131, Step:  283900, Batch Loss:     0.961562, Lr: 0.000027, Tokens per sec:   3230
2023-03-15 10:18:24,081 - INFO - __main__ - Epoch 131, Step:  284000, Batch Loss:     1.316637, Lr: 0.000027, Tokens per sec:   3441
2023-03-15 10:18:39,977 - INFO - __main__ - Epoch 131, Step:  284100, Batch Loss:     1.205034, Lr: 0.000027, Tokens per sec:   3378
2023-03-15 10:18:55,904 - INFO - __main__ - Epoch 131, Step:  284200, Batch Loss:     1.678158, Lr: 0.000027, Tokens per sec:   3357
2023-03-15 10:19:12,042 - INFO - __main__ - Epoch 131, Step:  284300, Batch Loss:     2.694276, Lr: 0.000027, Tokens per sec:   3334
2023-03-15 10:19:28,040 - INFO - __main__ - Epoch 131, Step:  284400, Batch Loss:     1.758454, Lr: 0.000027, Tokens per sec:   3412
2023-03-15 10:19:44,383 - INFO - __main__ - Epoch 131, Step:  284500, Batch Loss:     1.960495, Lr: 0.000027, Tokens per sec:   3322
2023-03-15 10:20:00,459 - INFO - __main__ - Epoch 131, Step:  284600, Batch Loss:     1.423471, Lr: 0.000027, Tokens per sec:   3357
2023-03-15 10:20:15,896 - INFO - __main__ - Epoch 131, Step:  284700, Batch Loss:     1.221309, Lr: 0.000027, Tokens per sec:   3529
2023-03-15 10:20:31,381 - INFO - __main__ - Epoch 131, Step:  284800, Batch Loss:     0.776209, Lr: 0.000027, Tokens per sec:   3451
2023-03-15 10:20:47,217 - INFO - __main__ - Epoch 131, Step:  284900, Batch Loss:     2.171532, Lr: 0.000027, Tokens per sec:   3346
2023-03-15 10:21:03,107 - INFO - __main__ - Epoch 131, Step:  285000, Batch Loss:     1.896866, Lr: 0.000027, Tokens per sec:   3382
2023-03-15 10:21:19,023 - INFO - __main__ - Epoch 131, Step:  285100, Batch Loss:     1.810322, Lr: 0.000027, Tokens per sec:   3402
2023-03-15 10:21:34,804 - INFO - __main__ - Epoch 131, Step:  285200, Batch Loss:     1.381765, Lr: 0.000027, Tokens per sec:   3416
2023-03-15 10:21:50,491 - INFO - __main__ - Epoch 131, Step:  285300, Batch Loss:     1.309759, Lr: 0.000027, Tokens per sec:   3454
2023-03-15 10:22:05,681 - INFO - __main__ - Epoch 131, Step:  285400, Batch Loss:     2.273461, Lr: 0.000027, Tokens per sec:   3576
2023-03-15 10:22:13,371 - INFO - __main__ - Epoch 131: total training loss 3506.20
2023-03-15 10:22:13,372 - INFO - __main__ - Epoch 132
2023-03-15 10:22:22,407 - INFO - __main__ - Epoch 132, Step:  285500, Batch Loss:     2.038068, Lr: 0.000027, Tokens per sec:   3079
2023-03-15 10:22:37,950 - INFO - __main__ - Epoch 132, Step:  285600, Batch Loss:     1.872698, Lr: 0.000027, Tokens per sec:   3432
2023-03-15 10:22:53,598 - INFO - __main__ - Epoch 132, Step:  285700, Batch Loss:     1.474615, Lr: 0.000027, Tokens per sec:   3430
2023-03-15 10:23:09,010 - INFO - __main__ - Epoch 132, Step:  285800, Batch Loss:     1.979190, Lr: 0.000027, Tokens per sec:   3506
2023-03-15 10:23:24,483 - INFO - __main__ - Epoch 132, Step:  285900, Batch Loss:     0.559356, Lr: 0.000027, Tokens per sec:   3517
2023-03-15 10:23:40,431 - INFO - __main__ - Epoch 132, Step:  286000, Batch Loss:     1.010940, Lr: 0.000027, Tokens per sec:   3362
2023-03-15 10:23:56,723 - INFO - __main__ - Epoch 132, Step:  286100, Batch Loss:     1.839766, Lr: 0.000027, Tokens per sec:   3330
2023-03-15 10:24:12,752 - INFO - __main__ - Epoch 132, Step:  286200, Batch Loss:     1.381436, Lr: 0.000027, Tokens per sec:   3368
2023-03-15 10:24:28,550 - INFO - __main__ - Epoch 132, Step:  286300, Batch Loss:     1.668226, Lr: 0.000027, Tokens per sec:   3363
2023-03-15 10:24:44,834 - INFO - __main__ - Epoch 132, Step:  286400, Batch Loss:     1.602009, Lr: 0.000027, Tokens per sec:   3373
2023-03-15 10:25:00,625 - INFO - __main__ - Epoch 132, Step:  286500, Batch Loss:     2.347825, Lr: 0.000027, Tokens per sec:   3435
2023-03-15 10:25:16,887 - INFO - __main__ - Epoch 132, Step:  286600, Batch Loss:     1.641221, Lr: 0.000027, Tokens per sec:   3342
2023-03-15 10:25:32,820 - INFO - __main__ - Epoch 132, Step:  286700, Batch Loss:     0.966102, Lr: 0.000027, Tokens per sec:   3369
2023-03-15 10:25:49,131 - INFO - __main__ - Epoch 132, Step:  286800, Batch Loss:     1.114532, Lr: 0.000027, Tokens per sec:   3312
2023-03-15 10:26:05,227 - INFO - __main__ - Epoch 132, Step:  286900, Batch Loss:     1.092735, Lr: 0.000027, Tokens per sec:   3333
2023-03-15 10:26:21,184 - INFO - __main__ - Epoch 132, Step:  287000, Batch Loss:     1.995970, Lr: 0.000027, Tokens per sec:   3366
2023-03-15 10:26:37,242 - INFO - __main__ - Epoch 132, Step:  287100, Batch Loss:     1.582950, Lr: 0.000027, Tokens per sec:   3340
2023-03-15 10:26:53,287 - INFO - __main__ - Epoch 132, Step:  287200, Batch Loss:     1.679675, Lr: 0.000027, Tokens per sec:   3282
2023-03-15 10:27:09,245 - INFO - __main__ - Epoch 132, Step:  287300, Batch Loss:     1.651095, Lr: 0.000027, Tokens per sec:   3384
2023-03-15 10:27:25,147 - INFO - __main__ - Epoch 132, Step:  287400, Batch Loss:     1.318056, Lr: 0.000027, Tokens per sec:   3384
2023-03-15 10:27:41,728 - INFO - __main__ - Epoch 132, Step:  287500, Batch Loss:     1.712904, Lr: 0.000027, Tokens per sec:   3252
2023-03-15 10:27:58,310 - INFO - __main__ - Epoch 132, Step:  287600, Batch Loss:     0.994512, Lr: 0.000027, Tokens per sec:   3183
2023-03-15 10:28:02,817 - INFO - __main__ - Epoch 132: total training loss 3499.80
2023-03-15 10:28:02,818 - INFO - __main__ - Epoch 133
2023-03-15 10:28:14,604 - INFO - __main__ - Epoch 133, Step:  287700, Batch Loss:     1.206382, Lr: 0.000027, Tokens per sec:   3279
2023-03-15 10:28:30,558 - INFO - __main__ - Epoch 133, Step:  287800, Batch Loss:     1.754970, Lr: 0.000027, Tokens per sec:   3381
2023-03-15 10:28:46,520 - INFO - __main__ - Epoch 133, Step:  287900, Batch Loss:     1.160058, Lr: 0.000027, Tokens per sec:   3377
2023-03-15 10:29:02,262 - INFO - __main__ - Epoch 133, Step:  288000, Batch Loss:     1.719838, Lr: 0.000027, Tokens per sec:   3460
2023-03-15 10:29:17,672 - INFO - __main__ - Epoch 133, Step:  288100, Batch Loss:     1.415857, Lr: 0.000027, Tokens per sec:   3536
2023-03-15 10:29:33,532 - INFO - __main__ - Epoch 133, Step:  288200, Batch Loss:     1.958794, Lr: 0.000027, Tokens per sec:   3399
2023-03-15 10:29:49,128 - INFO - __main__ - Epoch 133, Step:  288300, Batch Loss:     1.454372, Lr: 0.000027, Tokens per sec:   3414
2023-03-15 10:30:04,986 - INFO - __main__ - Epoch 133, Step:  288400, Batch Loss:     1.787137, Lr: 0.000027, Tokens per sec:   3377
2023-03-15 10:30:20,942 - INFO - __main__ - Epoch 133, Step:  288500, Batch Loss:     1.952994, Lr: 0.000027, Tokens per sec:   3390
2023-03-15 10:30:36,924 - INFO - __main__ - Epoch 133, Step:  288600, Batch Loss:     1.264368, Lr: 0.000027, Tokens per sec:   3375
2023-03-15 10:30:52,895 - INFO - __main__ - Epoch 133, Step:  288700, Batch Loss:     1.580468, Lr: 0.000027, Tokens per sec:   3373
2023-03-15 10:31:08,644 - INFO - __main__ - Epoch 133, Step:  288800, Batch Loss:     1.951905, Lr: 0.000027, Tokens per sec:   3381
2023-03-15 10:31:24,301 - INFO - __main__ - Epoch 133, Step:  288900, Batch Loss:     1.794999, Lr: 0.000027, Tokens per sec:   3459
2023-03-15 10:31:40,582 - INFO - __main__ - Epoch 133, Step:  289000, Batch Loss:     1.841538, Lr: 0.000027, Tokens per sec:   3294
2023-03-15 10:31:56,471 - INFO - __main__ - Epoch 133, Step:  289100, Batch Loss:     2.736463, Lr: 0.000027, Tokens per sec:   3367
2023-03-15 10:32:12,254 - INFO - __main__ - Epoch 133, Step:  289200, Batch Loss:     1.493120, Lr: 0.000027, Tokens per sec:   3373
2023-03-15 10:32:28,055 - INFO - __main__ - Epoch 133, Step:  289300, Batch Loss:     1.798647, Lr: 0.000027, Tokens per sec:   3404
2023-03-15 10:32:43,772 - INFO - __main__ - Epoch 133, Step:  289400, Batch Loss:     1.454684, Lr: 0.000027, Tokens per sec:   3486
2023-03-15 10:32:59,588 - INFO - __main__ - Epoch 133, Step:  289500, Batch Loss:     1.040500, Lr: 0.000027, Tokens per sec:   3400
2023-03-15 10:33:15,345 - INFO - __main__ - Epoch 133, Step:  289600, Batch Loss:     1.252117, Lr: 0.000027, Tokens per sec:   3428
2023-03-15 10:33:30,961 - INFO - __main__ - Epoch 133, Step:  289700, Batch Loss:     1.585777, Lr: 0.000027, Tokens per sec:   3421
2023-03-15 10:33:47,191 - INFO - __main__ - Epoch 133, Step:  289800, Batch Loss:     1.401587, Lr: 0.000027, Tokens per sec:   3302
2023-03-15 10:33:48,384 - INFO - __main__ - Epoch 133: total training loss 3433.12
2023-03-15 10:33:48,385 - INFO - __main__ - Epoch 134
2023-03-15 10:34:03,566 - INFO - __main__ - Epoch 134, Step:  289900, Batch Loss:     1.995996, Lr: 0.000026, Tokens per sec:   3231
2023-03-15 10:34:19,283 - INFO - __main__ - Epoch 134, Step:  290000, Batch Loss:     1.947604, Lr: 0.000026, Tokens per sec:   3399
2023-03-15 10:34:35,241 - INFO - __main__ - Epoch 134, Step:  290100, Batch Loss:     1.661498, Lr: 0.000026, Tokens per sec:   3356
2023-03-15 10:34:50,852 - INFO - __main__ - Epoch 134, Step:  290200, Batch Loss:     2.056894, Lr: 0.000026, Tokens per sec:   3437
2023-03-15 10:35:06,769 - INFO - __main__ - Epoch 134, Step:  290300, Batch Loss:     1.379516, Lr: 0.000026, Tokens per sec:   3457
2023-03-15 10:35:22,445 - INFO - __main__ - Epoch 134, Step:  290400, Batch Loss:     1.771554, Lr: 0.000026, Tokens per sec:   3447
2023-03-15 10:35:38,038 - INFO - __main__ - Epoch 134, Step:  290500, Batch Loss:     1.219861, Lr: 0.000026, Tokens per sec:   3468
2023-03-15 10:35:53,513 - INFO - __main__ - Epoch 134, Step:  290600, Batch Loss:     1.840115, Lr: 0.000026, Tokens per sec:   3490
2023-03-15 10:36:08,962 - INFO - __main__ - Epoch 134, Step:  290700, Batch Loss:     1.236822, Lr: 0.000026, Tokens per sec:   3530
2023-03-15 10:36:24,450 - INFO - __main__ - Epoch 134, Step:  290800, Batch Loss:     1.680317, Lr: 0.000026, Tokens per sec:   3390
2023-03-15 10:36:40,051 - INFO - __main__ - Epoch 134, Step:  290900, Batch Loss:     1.786960, Lr: 0.000026, Tokens per sec:   3459
2023-03-15 10:36:55,448 - INFO - __main__ - Epoch 134, Step:  291000, Batch Loss:     1.825865, Lr: 0.000026, Tokens per sec:   3501
2023-03-15 10:37:11,020 - INFO - __main__ - Epoch 134, Step:  291100, Batch Loss:     2.423481, Lr: 0.000026, Tokens per sec:   3470
2023-03-15 10:37:26,080 - INFO - __main__ - Epoch 134, Step:  291200, Batch Loss:     1.583547, Lr: 0.000026, Tokens per sec:   3560
2023-03-15 10:37:41,655 - INFO - __main__ - Epoch 134, Step:  291300, Batch Loss:     2.389842, Lr: 0.000026, Tokens per sec:   3452
2023-03-15 10:37:57,316 - INFO - __main__ - Epoch 134, Step:  291400, Batch Loss:     1.654561, Lr: 0.000026, Tokens per sec:   3417
2023-03-15 10:38:13,213 - INFO - __main__ - Epoch 134, Step:  291500, Batch Loss:     0.934247, Lr: 0.000026, Tokens per sec:   3412
2023-03-15 10:38:28,729 - INFO - __main__ - Epoch 134, Step:  291600, Batch Loss:     1.418275, Lr: 0.000026, Tokens per sec:   3470
2023-03-15 10:38:44,652 - INFO - __main__ - Epoch 134, Step:  291700, Batch Loss:     0.940641, Lr: 0.000026, Tokens per sec:   3386
2023-03-15 10:39:00,507 - INFO - __main__ - Epoch 134, Step:  291800, Batch Loss:     2.085122, Lr: 0.000026, Tokens per sec:   3412
2023-03-15 10:39:16,423 - INFO - __main__ - Epoch 134, Step:  291900, Batch Loss:     1.442430, Lr: 0.000026, Tokens per sec:   3356
2023-03-15 10:39:29,881 - INFO - __main__ - Epoch 134: total training loss 3419.51
2023-03-15 10:39:29,882 - INFO - __main__ - Epoch 135
2023-03-15 10:39:32,567 - INFO - __main__ - Epoch 135, Step:  292000, Batch Loss:     1.137035, Lr: 0.000026, Tokens per sec:   2774
2023-03-15 10:39:48,240 - INFO - __main__ - Epoch 135, Step:  292100, Batch Loss:     1.326836, Lr: 0.000026, Tokens per sec:   3420
2023-03-15 10:40:04,215 - INFO - __main__ - Epoch 135, Step:  292200, Batch Loss:     1.930205, Lr: 0.000026, Tokens per sec:   3364
2023-03-15 10:40:20,013 - INFO - __main__ - Epoch 135, Step:  292300, Batch Loss:     1.432142, Lr: 0.000026, Tokens per sec:   3425
2023-03-15 10:40:35,702 - INFO - __main__ - Epoch 135, Step:  292400, Batch Loss:     1.194618, Lr: 0.000026, Tokens per sec:   3463
2023-03-15 10:40:51,315 - INFO - __main__ - Epoch 135, Step:  292500, Batch Loss:     1.376036, Lr: 0.000026, Tokens per sec:   3442
2023-03-15 10:41:06,911 - INFO - __main__ - Epoch 135, Step:  292600, Batch Loss:     1.199062, Lr: 0.000026, Tokens per sec:   3428
2023-03-15 10:41:22,584 - INFO - __main__ - Epoch 135, Step:  292700, Batch Loss:     1.729770, Lr: 0.000026, Tokens per sec:   3415
2023-03-15 10:41:38,347 - INFO - __main__ - Epoch 135, Step:  292800, Batch Loss:     1.911240, Lr: 0.000026, Tokens per sec:   3416
2023-03-15 10:41:54,268 - INFO - __main__ - Epoch 135, Step:  292900, Batch Loss:     1.700383, Lr: 0.000026, Tokens per sec:   3368
2023-03-15 10:42:10,073 - INFO - __main__ - Epoch 135, Step:  293000, Batch Loss:     1.713948, Lr: 0.000026, Tokens per sec:   3422
2023-03-15 10:42:25,632 - INFO - __main__ - Epoch 135, Step:  293100, Batch Loss:     1.798739, Lr: 0.000026, Tokens per sec:   3462
2023-03-15 10:42:41,339 - INFO - __main__ - Epoch 135, Step:  293200, Batch Loss:     0.934674, Lr: 0.000026, Tokens per sec:   3468
2023-03-15 10:42:57,232 - INFO - __main__ - Epoch 135, Step:  293300, Batch Loss:     1.808649, Lr: 0.000026, Tokens per sec:   3416
2023-03-15 10:43:13,086 - INFO - __main__ - Epoch 135, Step:  293400, Batch Loss:     1.419873, Lr: 0.000026, Tokens per sec:   3441
2023-03-15 10:43:29,197 - INFO - __main__ - Epoch 135, Step:  293500, Batch Loss:     1.659861, Lr: 0.000026, Tokens per sec:   3341
2023-03-15 10:43:44,849 - INFO - __main__ - Epoch 135, Step:  293600, Batch Loss:     1.843551, Lr: 0.000026, Tokens per sec:   3501
2023-03-15 10:44:00,627 - INFO - __main__ - Epoch 135, Step:  293700, Batch Loss:     1.296789, Lr: 0.000026, Tokens per sec:   3363
2023-03-15 10:44:16,392 - INFO - __main__ - Epoch 135, Step:  293800, Batch Loss:     1.314663, Lr: 0.000026, Tokens per sec:   3404
2023-03-15 10:44:32,096 - INFO - __main__ - Epoch 135, Step:  293900, Batch Loss:     1.809600, Lr: 0.000026, Tokens per sec:   3392
2023-03-15 10:44:47,881 - INFO - __main__ - Epoch 135, Step:  294000, Batch Loss:     0.811560, Lr: 0.000026, Tokens per sec:   3411
2023-03-15 10:45:03,447 - INFO - __main__ - Epoch 135, Step:  294100, Batch Loss:     1.644274, Lr: 0.000026, Tokens per sec:   3409
2023-03-15 10:45:13,537 - INFO - __main__ - Epoch 135: total training loss 3402.03
2023-03-15 10:45:13,537 - INFO - __main__ - Epoch 136
2023-03-15 10:45:19,552 - INFO - __main__ - Epoch 136, Step:  294200, Batch Loss:     2.007352, Lr: 0.000026, Tokens per sec:   3109
2023-03-15 10:45:35,016 - INFO - __main__ - Epoch 136, Step:  294300, Batch Loss:     1.556048, Lr: 0.000026, Tokens per sec:   3469
2023-03-15 10:45:51,132 - INFO - __main__ - Epoch 136, Step:  294400, Batch Loss:     1.421458, Lr: 0.000026, Tokens per sec:   3321
2023-03-15 10:46:06,795 - INFO - __main__ - Epoch 136, Step:  294500, Batch Loss:     1.393431, Lr: 0.000026, Tokens per sec:   3497
2023-03-15 10:46:22,894 - INFO - __main__ - Epoch 136, Step:  294600, Batch Loss:     1.052950, Lr: 0.000026, Tokens per sec:   3314
2023-03-15 10:46:38,949 - INFO - __main__ - Epoch 136, Step:  294700, Batch Loss:     1.633824, Lr: 0.000026, Tokens per sec:   3327
2023-03-15 10:46:54,748 - INFO - __main__ - Epoch 136, Step:  294800, Batch Loss:     1.882075, Lr: 0.000026, Tokens per sec:   3467
2023-03-15 10:47:10,275 - INFO - __main__ - Epoch 136, Step:  294900, Batch Loss:     1.647172, Lr: 0.000026, Tokens per sec:   3420
2023-03-15 10:47:25,532 - INFO - __main__ - Epoch 136, Step:  295000, Batch Loss:     1.266044, Lr: 0.000026, Tokens per sec:   3564
2023-03-15 10:47:40,784 - INFO - __main__ - Epoch 136, Step:  295100, Batch Loss:     1.236927, Lr: 0.000026, Tokens per sec:   3544
2023-03-15 10:47:56,038 - INFO - __main__ - Epoch 136, Step:  295200, Batch Loss:     2.369412, Lr: 0.000026, Tokens per sec:   3531
2023-03-15 10:48:11,545 - INFO - __main__ - Epoch 136, Step:  295300, Batch Loss:     1.395489, Lr: 0.000026, Tokens per sec:   3479
2023-03-15 10:48:27,113 - INFO - __main__ - Epoch 136, Step:  295400, Batch Loss:     1.609611, Lr: 0.000026, Tokens per sec:   3451
2023-03-15 10:48:42,408 - INFO - __main__ - Epoch 136, Step:  295500, Batch Loss:     2.025008, Lr: 0.000026, Tokens per sec:   3516
2023-03-15 10:48:57,818 - INFO - __main__ - Epoch 136, Step:  295600, Batch Loss:     2.164461, Lr: 0.000026, Tokens per sec:   3511
2023-03-15 10:49:13,204 - INFO - __main__ - Epoch 136, Step:  295700, Batch Loss:     1.926263, Lr: 0.000026, Tokens per sec:   3510
2023-03-15 10:49:28,691 - INFO - __main__ - Epoch 136, Step:  295800, Batch Loss:     1.597613, Lr: 0.000026, Tokens per sec:   3440
2023-03-15 10:49:43,920 - INFO - __main__ - Epoch 136, Step:  295900, Batch Loss:     1.581568, Lr: 0.000026, Tokens per sec:   3507
2023-03-15 10:49:59,513 - INFO - __main__ - Epoch 136, Step:  296000, Batch Loss:     1.664393, Lr: 0.000026, Tokens per sec:   3456
2023-03-15 10:50:14,906 - INFO - __main__ - Epoch 136, Step:  296100, Batch Loss:     0.897671, Lr: 0.000026, Tokens per sec:   3556
2023-03-15 10:50:30,402 - INFO - __main__ - Epoch 136, Step:  296200, Batch Loss:     0.990315, Lr: 0.000026, Tokens per sec:   3474
2023-03-15 10:50:46,349 - INFO - __main__ - Epoch 136, Step:  296300, Batch Loss:     1.855893, Lr: 0.000026, Tokens per sec:   3331
2023-03-15 10:50:53,466 - INFO - __main__ - Epoch 136: total training loss 3356.83
2023-03-15 10:50:53,466 - INFO - __main__ - Epoch 137
2023-03-15 10:51:02,709 - INFO - __main__ - Epoch 137, Step:  296400, Batch Loss:     1.500882, Lr: 0.000025, Tokens per sec:   3267
2023-03-15 10:51:18,068 - INFO - __main__ - Epoch 137, Step:  296500, Batch Loss:     1.005445, Lr: 0.000025, Tokens per sec:   3496
2023-03-15 10:51:33,184 - INFO - __main__ - Epoch 137, Step:  296600, Batch Loss:     1.842465, Lr: 0.000025, Tokens per sec:   3559
2023-03-15 10:51:48,943 - INFO - __main__ - Epoch 137, Step:  296700, Batch Loss:     2.290178, Lr: 0.000025, Tokens per sec:   3455
2023-03-15 10:52:04,783 - INFO - __main__ - Epoch 137, Step:  296800, Batch Loss:     1.408461, Lr: 0.000025, Tokens per sec:   3388
2023-03-15 10:52:20,801 - INFO - __main__ - Epoch 137, Step:  296900, Batch Loss:     1.187908, Lr: 0.000025, Tokens per sec:   3317
2023-03-15 10:52:36,151 - INFO - __main__ - Epoch 137, Step:  297000, Batch Loss:     0.955448, Lr: 0.000025, Tokens per sec:   3549
2023-03-15 10:52:51,542 - INFO - __main__ - Epoch 137, Step:  297100, Batch Loss:     0.729135, Lr: 0.000025, Tokens per sec:   3534
2023-03-15 10:53:06,771 - INFO - __main__ - Epoch 137, Step:  297200, Batch Loss:     1.892017, Lr: 0.000025, Tokens per sec:   3509
2023-03-15 10:53:22,481 - INFO - __main__ - Epoch 137, Step:  297300, Batch Loss:     1.214224, Lr: 0.000025, Tokens per sec:   3450
2023-03-15 10:53:37,672 - INFO - __main__ - Epoch 137, Step:  297400, Batch Loss:     1.426703, Lr: 0.000025, Tokens per sec:   3600
2023-03-15 10:53:53,130 - INFO - __main__ - Epoch 137, Step:  297500, Batch Loss:     1.680580, Lr: 0.000025, Tokens per sec:   3465
2023-03-15 10:54:08,670 - INFO - __main__ - Epoch 137, Step:  297600, Batch Loss:     1.846933, Lr: 0.000025, Tokens per sec:   3483
2023-03-15 10:54:23,944 - INFO - __main__ - Epoch 137, Step:  297700, Batch Loss:     1.326686, Lr: 0.000025, Tokens per sec:   3600
2023-03-15 10:54:39,577 - INFO - __main__ - Epoch 137, Step:  297800, Batch Loss:     1.915001, Lr: 0.000025, Tokens per sec:   3417
2023-03-15 10:54:54,847 - INFO - __main__ - Epoch 137, Step:  297900, Batch Loss:     2.080402, Lr: 0.000025, Tokens per sec:   3559
2023-03-15 10:55:10,521 - INFO - __main__ - Epoch 137, Step:  298000, Batch Loss:     1.391599, Lr: 0.000025, Tokens per sec:   3369
2023-03-15 10:55:26,143 - INFO - __main__ - Epoch 137, Step:  298100, Batch Loss:     1.903319, Lr: 0.000025, Tokens per sec:   3404
2023-03-15 10:55:41,940 - INFO - __main__ - Epoch 137, Step:  298200, Batch Loss:     1.544811, Lr: 0.000025, Tokens per sec:   3378
2023-03-15 10:55:57,416 - INFO - __main__ - Epoch 137, Step:  298300, Batch Loss:     2.067017, Lr: 0.000025, Tokens per sec:   3440
2023-03-15 10:56:12,451 - INFO - __main__ - Epoch 137, Step:  298400, Batch Loss:     1.579623, Lr: 0.000025, Tokens per sec:   3627
2023-03-15 10:56:28,160 - INFO - __main__ - Epoch 137, Step:  298500, Batch Loss:     0.887994, Lr: 0.000025, Tokens per sec:   3390
2023-03-15 10:56:31,879 - INFO - __main__ - Epoch 137: total training loss 3330.71
2023-03-15 10:56:31,880 - INFO - __main__ - Epoch 138
2023-03-15 10:56:44,616 - INFO - __main__ - Epoch 138, Step:  298600, Batch Loss:     0.984507, Lr: 0.000025, Tokens per sec:   3236
2023-03-15 10:57:00,513 - INFO - __main__ - Epoch 138, Step:  298700, Batch Loss:     1.721274, Lr: 0.000025, Tokens per sec:   3412
2023-03-15 10:57:16,832 - INFO - __main__ - Epoch 138, Step:  298800, Batch Loss:     1.399668, Lr: 0.000025, Tokens per sec:   3335
2023-03-15 10:57:32,894 - INFO - __main__ - Epoch 138, Step:  298900, Batch Loss:     1.103299, Lr: 0.000025, Tokens per sec:   3373
2023-03-15 10:57:48,233 - INFO - __main__ - Epoch 138, Step:  299000, Batch Loss:     1.063045, Lr: 0.000025, Tokens per sec:   3488
2023-03-15 10:58:03,724 - INFO - __main__ - Epoch 138, Step:  299100, Batch Loss:     1.064135, Lr: 0.000025, Tokens per sec:   3497
2023-03-15 10:58:19,184 - INFO - __main__ - Epoch 138, Step:  299200, Batch Loss:     1.019549, Lr: 0.000025, Tokens per sec:   3458
2023-03-15 10:58:34,547 - INFO - __main__ - Epoch 138, Step:  299300, Batch Loss:     1.117224, Lr: 0.000025, Tokens per sec:   3532
2023-03-15 10:58:50,099 - INFO - __main__ - Epoch 138, Step:  299400, Batch Loss:     2.036936, Lr: 0.000025, Tokens per sec:   3409
2023-03-15 10:59:05,801 - INFO - __main__ - Epoch 138, Step:  299500, Batch Loss:     1.824751, Lr: 0.000025, Tokens per sec:   3332
2023-03-15 10:59:21,668 - INFO - __main__ - Epoch 138, Step:  299600, Batch Loss:     1.337756, Lr: 0.000025, Tokens per sec:   3323
2023-03-15 10:59:37,151 - INFO - __main__ - Epoch 138, Step:  299700, Batch Loss:     1.271602, Lr: 0.000025, Tokens per sec:   3510
2023-03-15 10:59:52,850 - INFO - __main__ - Epoch 138, Step:  299800, Batch Loss:     1.908039, Lr: 0.000025, Tokens per sec:   3418
2023-03-15 11:00:08,545 - INFO - __main__ - Epoch 138, Step:  299900, Batch Loss:     1.514263, Lr: 0.000025, Tokens per sec:   3432
2023-03-15 11:00:24,416 - INFO - __main__ - Epoch 138, Step:  300000, Batch Loss:     1.559387, Lr: 0.000025, Tokens per sec:   3425
2023-03-15 11:00:39,832 - INFO - __main__ - Epoch 138, Step:  300100, Batch Loss:     1.042255, Lr: 0.000025, Tokens per sec:   3523
2023-03-15 11:00:55,232 - INFO - __main__ - Epoch 138, Step:  300200, Batch Loss:     1.503729, Lr: 0.000025, Tokens per sec:   3455
2023-03-15 11:01:10,449 - INFO - __main__ - Epoch 138, Step:  300300, Batch Loss:     1.071772, Lr: 0.000025, Tokens per sec:   3587
2023-03-15 11:01:26,293 - INFO - __main__ - Epoch 138, Step:  300400, Batch Loss:     1.032679, Lr: 0.000025, Tokens per sec:   3449
2023-03-15 11:01:42,275 - INFO - __main__ - Epoch 138, Step:  300500, Batch Loss:     1.906818, Lr: 0.000025, Tokens per sec:   3399
2023-03-15 11:01:57,566 - INFO - __main__ - Epoch 138, Step:  300600, Batch Loss:     1.651214, Lr: 0.000025, Tokens per sec:   3530
2023-03-15 11:02:13,040 - INFO - __main__ - Epoch 138, Step:  300700, Batch Loss:     1.718878, Lr: 0.000025, Tokens per sec:   3447
2023-03-15 11:02:13,434 - INFO - __main__ - Epoch 138: total training loss 3288.66
2023-03-15 11:02:13,434 - INFO - __main__ - Epoch 139
2023-03-15 11:02:29,710 - INFO - __main__ - Epoch 139, Step:  300800, Batch Loss:     0.823089, Lr: 0.000025, Tokens per sec:   3186
2023-03-15 11:02:45,316 - INFO - __main__ - Epoch 139, Step:  300900, Batch Loss:     0.944696, Lr: 0.000025, Tokens per sec:   3476
2023-03-15 11:03:00,764 - INFO - __main__ - Epoch 139, Step:  301000, Batch Loss:     1.729309, Lr: 0.000025, Tokens per sec:   3483
2023-03-15 11:03:16,313 - INFO - __main__ - Epoch 139, Step:  301100, Batch Loss:     1.760233, Lr: 0.000025, Tokens per sec:   3488
2023-03-15 11:03:32,081 - INFO - __main__ - Epoch 139, Step:  301200, Batch Loss:     1.101880, Lr: 0.000025, Tokens per sec:   3408
2023-03-15 11:03:47,717 - INFO - __main__ - Epoch 139, Step:  301300, Batch Loss:     1.592162, Lr: 0.000025, Tokens per sec:   3485
2023-03-15 11:04:03,346 - INFO - __main__ - Epoch 139, Step:  301400, Batch Loss:     1.890377, Lr: 0.000025, Tokens per sec:   3412
2023-03-15 11:04:19,223 - INFO - __main__ - Epoch 139, Step:  301500, Batch Loss:     1.776300, Lr: 0.000025, Tokens per sec:   3414
2023-03-15 11:04:34,785 - INFO - __main__ - Epoch 139, Step:  301600, Batch Loss:     1.022862, Lr: 0.000025, Tokens per sec:   3452
2023-03-15 11:04:49,877 - INFO - __main__ - Epoch 139, Step:  301700, Batch Loss:     1.434161, Lr: 0.000025, Tokens per sec:   3587
2023-03-15 11:05:05,577 - INFO - __main__ - Epoch 139, Step:  301800, Batch Loss:     1.539183, Lr: 0.000025, Tokens per sec:   3409
2023-03-15 11:05:21,937 - INFO - __main__ - Epoch 139, Step:  301900, Batch Loss:     1.388837, Lr: 0.000025, Tokens per sec:   3263
2023-03-15 11:05:38,039 - INFO - __main__ - Epoch 139, Step:  302000, Batch Loss:     1.298956, Lr: 0.000025, Tokens per sec:   3320
2023-03-15 11:05:54,123 - INFO - __main__ - Epoch 139, Step:  302100, Batch Loss:     2.073200, Lr: 0.000025, Tokens per sec:   3306
2023-03-15 11:06:10,043 - INFO - __main__ - Epoch 139, Step:  302200, Batch Loss:     2.360382, Lr: 0.000025, Tokens per sec:   3372
2023-03-15 11:06:26,127 - INFO - __main__ - Epoch 139, Step:  302300, Batch Loss:     2.063176, Lr: 0.000025, Tokens per sec:   3332
2023-03-15 11:06:42,039 - INFO - __main__ - Epoch 139, Step:  302400, Batch Loss:     1.145416, Lr: 0.000025, Tokens per sec:   3367
2023-03-15 11:06:58,508 - INFO - __main__ - Epoch 139, Step:  302500, Batch Loss:     1.241912, Lr: 0.000025, Tokens per sec:   3273
2023-03-15 11:07:14,579 - INFO - __main__ - Epoch 139, Step:  302600, Batch Loss:     1.876075, Lr: 0.000025, Tokens per sec:   3388
2023-03-15 11:07:30,613 - INFO - __main__ - Epoch 139, Step:  302700, Batch Loss:     1.753253, Lr: 0.000025, Tokens per sec:   3402
2023-03-15 11:07:46,739 - INFO - __main__ - Epoch 139, Step:  302800, Batch Loss:     1.477966, Lr: 0.000025, Tokens per sec:   3391
2023-03-15 11:07:59,911 - INFO - __main__ - Epoch 139: total training loss 3241.31
2023-03-15 11:07:59,912 - INFO - __main__ - Epoch 140
2023-03-15 11:08:03,301 - INFO - __main__ - Epoch 140, Step:  302900, Batch Loss:     1.329169, Lr: 0.000025, Tokens per sec:   2976
2023-03-15 11:08:19,217 - INFO - __main__ - Epoch 140, Step:  303000, Batch Loss:     1.388135, Lr: 0.000025, Tokens per sec:   3341
2023-03-15 11:08:35,123 - INFO - __main__ - Epoch 140, Step:  303100, Batch Loss:     1.114012, Lr: 0.000025, Tokens per sec:   3383
2023-03-15 11:08:51,211 - INFO - __main__ - Epoch 140, Step:  303200, Batch Loss:     1.532222, Lr: 0.000025, Tokens per sec:   3328
2023-03-15 11:09:07,105 - INFO - __main__ - Epoch 140, Step:  303300, Batch Loss:     0.983074, Lr: 0.000025, Tokens per sec:   3401
2023-03-15 11:09:23,574 - INFO - __main__ - Epoch 140, Step:  303400, Batch Loss:     1.633172, Lr: 0.000025, Tokens per sec:   3334
2023-03-15 11:09:40,087 - INFO - __main__ - Epoch 140, Step:  303500, Batch Loss:     1.745256, Lr: 0.000025, Tokens per sec:   3310
2023-03-15 11:09:56,041 - INFO - __main__ - Epoch 140, Step:  303600, Batch Loss:     1.755833, Lr: 0.000025, Tokens per sec:   3390
2023-03-15 11:10:13,086 - INFO - __main__ - Epoch 140, Step:  303700, Batch Loss:     1.734780, Lr: 0.000025, Tokens per sec:   3143
2023-03-15 11:10:29,001 - INFO - __main__ - Epoch 140, Step:  303800, Batch Loss:     1.487519, Lr: 0.000025, Tokens per sec:   3425
2023-03-15 11:10:44,907 - INFO - __main__ - Epoch 140, Step:  303900, Batch Loss:     2.889169, Lr: 0.000025, Tokens per sec:   3352
2023-03-15 11:11:00,962 - INFO - __main__ - Epoch 140, Step:  304000, Batch Loss:     1.702910, Lr: 0.000025, Tokens per sec:   3299
2023-03-15 11:11:17,058 - INFO - __main__ - Epoch 140, Step:  304100, Batch Loss:     1.341296, Lr: 0.000025, Tokens per sec:   3346
2023-03-15 11:11:32,905 - INFO - __main__ - Epoch 140, Step:  304200, Batch Loss:     1.583653, Lr: 0.000025, Tokens per sec:   3444
2023-03-15 11:11:49,054 - INFO - __main__ - Epoch 140, Step:  304300, Batch Loss:     1.331816, Lr: 0.000025, Tokens per sec:   3276
2023-03-15 11:12:05,182 - INFO - __main__ - Epoch 140, Step:  304400, Batch Loss:     1.397083, Lr: 0.000025, Tokens per sec:   3254
2023-03-15 11:12:20,759 - INFO - __main__ - Epoch 140, Step:  304500, Batch Loss:     1.946228, Lr: 0.000025, Tokens per sec:   3436
2023-03-15 11:12:36,857 - INFO - __main__ - Epoch 140, Step:  304600, Batch Loss:     0.960951, Lr: 0.000025, Tokens per sec:   3406
2023-03-15 11:12:52,454 - INFO - __main__ - Epoch 140, Step:  304700, Batch Loss:     1.497758, Lr: 0.000025, Tokens per sec:   3429
2023-03-15 11:13:07,964 - INFO - __main__ - Epoch 140, Step:  304800, Batch Loss:     2.190490, Lr: 0.000025, Tokens per sec:   3496
2023-03-15 11:13:23,959 - INFO - __main__ - Epoch 140, Step:  304900, Batch Loss:     1.613838, Lr: 0.000025, Tokens per sec:   3372
2023-03-15 11:13:40,170 - INFO - __main__ - Epoch 140, Step:  305000, Batch Loss:     1.160291, Lr: 0.000025, Tokens per sec:   3341
2023-03-15 11:13:49,626 - INFO - __main__ - Epoch 140: total training loss 3239.77
2023-03-15 11:13:49,627 - INFO - __main__ - Epoch 141
2023-03-15 11:13:56,358 - INFO - __main__ - Epoch 141, Step:  305100, Batch Loss:     0.932422, Lr: 0.000024, Tokens per sec:   3113
2023-03-15 11:14:12,130 - INFO - __main__ - Epoch 141, Step:  305200, Batch Loss:     1.022786, Lr: 0.000024, Tokens per sec:   3453
2023-03-15 11:14:28,061 - INFO - __main__ - Epoch 141, Step:  305300, Batch Loss:     2.200786, Lr: 0.000024, Tokens per sec:   3401
2023-03-15 11:14:43,859 - INFO - __main__ - Epoch 141, Step:  305400, Batch Loss:     1.363970, Lr: 0.000024, Tokens per sec:   3455
2023-03-15 11:15:01,175 - INFO - __main__ - Epoch 141, Step:  305500, Batch Loss:     1.801786, Lr: 0.000024, Tokens per sec:   3091
2023-03-15 11:15:17,279 - INFO - __main__ - Epoch 141, Step:  305600, Batch Loss:     1.305239, Lr: 0.000024, Tokens per sec:   3346
2023-03-15 11:15:33,089 - INFO - __main__ - Epoch 141, Step:  305700, Batch Loss:     0.535949, Lr: 0.000024, Tokens per sec:   3451
2023-03-15 11:15:48,754 - INFO - __main__ - Epoch 141, Step:  305800, Batch Loss:     1.251932, Lr: 0.000024, Tokens per sec:   3419
2023-03-15 11:16:05,243 - INFO - __main__ - Epoch 141, Step:  305900, Batch Loss:     2.467387, Lr: 0.000024, Tokens per sec:   3233
2023-03-15 11:16:21,479 - INFO - __main__ - Epoch 141, Step:  306000, Batch Loss:     1.728154, Lr: 0.000024, Tokens per sec:   3343
2023-03-15 11:16:37,563 - INFO - __main__ - Epoch 141, Step:  306100, Batch Loss:     2.092285, Lr: 0.000024, Tokens per sec:   3340
2023-03-15 11:16:53,571 - INFO - __main__ - Epoch 141, Step:  306200, Batch Loss:     1.469884, Lr: 0.000024, Tokens per sec:   3371
2023-03-15 11:17:09,877 - INFO - __main__ - Epoch 141, Step:  306300, Batch Loss:     1.460897, Lr: 0.000024, Tokens per sec:   3301
2023-03-15 11:17:26,032 - INFO - __main__ - Epoch 141, Step:  306400, Batch Loss:     1.158610, Lr: 0.000024, Tokens per sec:   3313
2023-03-15 11:17:42,254 - INFO - __main__ - Epoch 141, Step:  306500, Batch Loss:     0.742409, Lr: 0.000024, Tokens per sec:   3379
2023-03-15 11:17:57,932 - INFO - __main__ - Epoch 141, Step:  306600, Batch Loss:     1.643858, Lr: 0.000024, Tokens per sec:   3403
2023-03-15 11:18:14,161 - INFO - __main__ - Epoch 141, Step:  306700, Batch Loss:     1.020527, Lr: 0.000024, Tokens per sec:   3375
2023-03-15 11:18:30,542 - INFO - __main__ - Epoch 141, Step:  306800, Batch Loss:     1.225840, Lr: 0.000024, Tokens per sec:   3234
2023-03-15 11:18:46,718 - INFO - __main__ - Epoch 141, Step:  306900, Batch Loss:     1.916866, Lr: 0.000024, Tokens per sec:   3285
2023-03-15 11:19:02,845 - INFO - __main__ - Epoch 141, Step:  307000, Batch Loss:     2.049162, Lr: 0.000024, Tokens per sec:   3315
2023-03-15 11:19:18,898 - INFO - __main__ - Epoch 141, Step:  307100, Batch Loss:     1.516478, Lr: 0.000024, Tokens per sec:   3306
2023-03-15 11:19:34,578 - INFO - __main__ - Epoch 141, Step:  307200, Batch Loss:     1.227096, Lr: 0.000024, Tokens per sec:   3455
2023-03-15 11:19:41,007 - INFO - __main__ - Epoch 141: total training loss 3165.30
2023-03-15 11:19:41,007 - INFO - __main__ - Epoch 142
2023-03-15 11:19:51,479 - INFO - __main__ - Epoch 142, Step:  307300, Batch Loss:     1.339928, Lr: 0.000024, Tokens per sec:   3199
2023-03-15 11:20:07,810 - INFO - __main__ - Epoch 142, Step:  307400, Batch Loss:     1.216408, Lr: 0.000024, Tokens per sec:   3291
2023-03-15 11:20:23,904 - INFO - __main__ - Epoch 142, Step:  307500, Batch Loss:     1.180064, Lr: 0.000024, Tokens per sec:   3342
2023-03-15 11:20:39,928 - INFO - __main__ - Epoch 142, Step:  307600, Batch Loss:     1.534078, Lr: 0.000024, Tokens per sec:   3404
2023-03-15 11:20:56,200 - INFO - __main__ - Epoch 142, Step:  307700, Batch Loss:     0.910555, Lr: 0.000024, Tokens per sec:   3296
2023-03-15 11:21:12,266 - INFO - __main__ - Epoch 142, Step:  307800, Batch Loss:     1.246870, Lr: 0.000024, Tokens per sec:   3348
2023-03-15 11:21:28,689 - INFO - __main__ - Epoch 142, Step:  307900, Batch Loss:     1.777323, Lr: 0.000024, Tokens per sec:   3279
2023-03-15 11:21:44,488 - INFO - __main__ - Epoch 142, Step:  308000, Batch Loss:     0.817366, Lr: 0.000024, Tokens per sec:   3387
2023-03-15 11:22:00,514 - INFO - __main__ - Epoch 142, Step:  308100, Batch Loss:     1.197831, Lr: 0.000024, Tokens per sec:   3374
2023-03-15 11:22:16,434 - INFO - __main__ - Epoch 142, Step:  308200, Batch Loss:     1.551918, Lr: 0.000024, Tokens per sec:   3368
2023-03-15 11:22:32,479 - INFO - __main__ - Epoch 142, Step:  308300, Batch Loss:     1.023837, Lr: 0.000024, Tokens per sec:   3309
2023-03-15 11:22:48,061 - INFO - __main__ - Epoch 142, Step:  308400, Batch Loss:     1.381063, Lr: 0.000024, Tokens per sec:   3477
2023-03-15 11:23:03,847 - INFO - __main__ - Epoch 142, Step:  308500, Batch Loss:     1.666386, Lr: 0.000024, Tokens per sec:   3369
2023-03-15 11:23:19,841 - INFO - __main__ - Epoch 142, Step:  308600, Batch Loss:     1.696362, Lr: 0.000024, Tokens per sec:   3358
2023-03-15 11:23:35,732 - INFO - __main__ - Epoch 142, Step:  308700, Batch Loss:     1.061764, Lr: 0.000024, Tokens per sec:   3403
2023-03-15 11:23:51,368 - INFO - __main__ - Epoch 142, Step:  308800, Batch Loss:     1.476296, Lr: 0.000024, Tokens per sec:   3463
2023-03-15 11:24:06,827 - INFO - __main__ - Epoch 142, Step:  308900, Batch Loss:     1.024764, Lr: 0.000024, Tokens per sec:   3478
2023-03-15 11:24:22,240 - INFO - __main__ - Epoch 142, Step:  309000, Batch Loss:     1.361467, Lr: 0.000024, Tokens per sec:   3522
2023-03-15 11:24:38,574 - INFO - __main__ - Epoch 142, Step:  309100, Batch Loss:     1.746501, Lr: 0.000024, Tokens per sec:   3318
2023-03-15 11:24:54,790 - INFO - __main__ - Epoch 142, Step:  309200, Batch Loss:     1.002702, Lr: 0.000024, Tokens per sec:   3373
2023-03-15 11:25:10,428 - INFO - __main__ - Epoch 142, Step:  309300, Batch Loss:     1.548185, Lr: 0.000024, Tokens per sec:   3360
2023-03-15 11:25:25,611 - INFO - __main__ - Epoch 142, Step:  309400, Batch Loss:     0.892957, Lr: 0.000024, Tokens per sec:   3527
2023-03-15 11:25:28,475 - INFO - __main__ - Epoch 142: total training loss 3137.66
2023-03-15 11:25:28,477 - INFO - __main__ - Epoch 143
2023-03-15 11:25:42,024 - INFO - __main__ - Epoch 143, Step:  309500, Batch Loss:     1.861012, Lr: 0.000024, Tokens per sec:   3213
2023-03-15 11:25:57,301 - INFO - __main__ - Epoch 143, Step:  309600, Batch Loss:     1.158314, Lr: 0.000024, Tokens per sec:   3520
2023-03-15 11:26:14,672 - INFO - __main__ - Epoch 143, Step:  309700, Batch Loss:     1.278286, Lr: 0.000024, Tokens per sec:   3124
2023-03-15 11:26:31,044 - INFO - __main__ - Epoch 143, Step:  309800, Batch Loss:     2.087638, Lr: 0.000024, Tokens per sec:   3263
2023-03-15 11:26:48,177 - INFO - __main__ - Epoch 143, Step:  309900, Batch Loss:     1.795111, Lr: 0.000024, Tokens per sec:   3155
2023-03-15 11:27:03,868 - INFO - __main__ - Epoch 143, Step:  310000, Batch Loss:     1.530913, Lr: 0.000024, Tokens per sec:   3406
2023-03-15 11:27:19,225 - INFO - __main__ - Epoch 143, Step:  310100, Batch Loss:     1.265919, Lr: 0.000024, Tokens per sec:   3489
2023-03-15 11:27:34,886 - INFO - __main__ - Epoch 143, Step:  310200, Batch Loss:     1.129556, Lr: 0.000024, Tokens per sec:   3457
2023-03-15 11:27:50,713 - INFO - __main__ - Epoch 143, Step:  310300, Batch Loss:     1.336944, Lr: 0.000024, Tokens per sec:   3371
2023-03-15 11:28:06,988 - INFO - __main__ - Epoch 143, Step:  310400, Batch Loss:     1.278793, Lr: 0.000024, Tokens per sec:   3364
2023-03-15 11:28:23,003 - INFO - __main__ - Epoch 143, Step:  310500, Batch Loss:     1.557147, Lr: 0.000024, Tokens per sec:   3350
2023-03-15 11:28:38,737 - INFO - __main__ - Epoch 143, Step:  310600, Batch Loss:     1.862424, Lr: 0.000024, Tokens per sec:   3388
2023-03-15 11:28:54,617 - INFO - __main__ - Epoch 143, Step:  310700, Batch Loss:     1.114883, Lr: 0.000024, Tokens per sec:   3397
2023-03-15 11:29:10,455 - INFO - __main__ - Epoch 143, Step:  310800, Batch Loss:     1.401478, Lr: 0.000024, Tokens per sec:   3374
2023-03-15 11:29:26,150 - INFO - __main__ - Epoch 143, Step:  310900, Batch Loss:     1.554897, Lr: 0.000024, Tokens per sec:   3516
2023-03-15 11:29:41,887 - INFO - __main__ - Epoch 143, Step:  311000, Batch Loss:     1.052606, Lr: 0.000024, Tokens per sec:   3369
2023-03-15 11:29:57,376 - INFO - __main__ - Epoch 143, Step:  311100, Batch Loss:     1.519336, Lr: 0.000024, Tokens per sec:   3524
2023-03-15 11:30:12,977 - INFO - __main__ - Epoch 143, Step:  311200, Batch Loss:     1.575109, Lr: 0.000024, Tokens per sec:   3440
2023-03-15 11:30:28,642 - INFO - __main__ - Epoch 143, Step:  311300, Batch Loss:     0.930330, Lr: 0.000024, Tokens per sec:   3386
2023-03-15 11:30:44,154 - INFO - __main__ - Epoch 143, Step:  311400, Batch Loss:     1.426084, Lr: 0.000024, Tokens per sec:   3467
2023-03-15 11:30:59,860 - INFO - __main__ - Epoch 143, Step:  311500, Batch Loss:     0.762894, Lr: 0.000024, Tokens per sec:   3429
2023-03-15 11:31:14,730 - INFO - __main__ - Epoch 143: total training loss 3117.22
2023-03-15 11:31:14,731 - INFO - __main__ - Epoch 144
2023-03-15 11:31:15,655 - INFO - __main__ - Epoch 144, Step:  311600, Batch Loss:     1.915398, Lr: 0.000024, Tokens per sec:   1744
2023-03-15 11:31:31,254 - INFO - __main__ - Epoch 144, Step:  311700, Batch Loss:     0.908586, Lr: 0.000024, Tokens per sec:   3428
2023-03-15 11:31:46,760 - INFO - __main__ - Epoch 144, Step:  311800, Batch Loss:     1.436314, Lr: 0.000024, Tokens per sec:   3515
2023-03-15 11:32:02,197 - INFO - __main__ - Epoch 144, Step:  311900, Batch Loss:     1.663239, Lr: 0.000024, Tokens per sec:   3449
2023-03-15 11:32:17,566 - INFO - __main__ - Epoch 144, Step:  312000, Batch Loss:     1.385766, Lr: 0.000024, Tokens per sec:   3528
2023-03-15 11:32:32,806 - INFO - __main__ - Epoch 144, Step:  312100, Batch Loss:     1.178587, Lr: 0.000024, Tokens per sec:   3611
2023-03-15 11:32:48,471 - INFO - __main__ - Epoch 144, Step:  312200, Batch Loss:     1.560455, Lr: 0.000024, Tokens per sec:   3413
2023-03-15 11:33:04,132 - INFO - __main__ - Epoch 144, Step:  312300, Batch Loss:     1.665991, Lr: 0.000024, Tokens per sec:   3410
2023-03-15 11:33:20,022 - INFO - __main__ - Epoch 144, Step:  312400, Batch Loss:     2.253486, Lr: 0.000024, Tokens per sec:   3385
2023-03-15 11:33:35,804 - INFO - __main__ - Epoch 144, Step:  312500, Batch Loss:     1.111914, Lr: 0.000024, Tokens per sec:   3346
2023-03-15 11:33:51,730 - INFO - __main__ - Epoch 144, Step:  312600, Batch Loss:     1.652833, Lr: 0.000024, Tokens per sec:   3392
2023-03-15 11:34:07,978 - INFO - __main__ - Epoch 144, Step:  312700, Batch Loss:     2.228878, Lr: 0.000024, Tokens per sec:   3298
2023-03-15 11:34:23,355 - INFO - __main__ - Epoch 144, Step:  312800, Batch Loss:     1.809561, Lr: 0.000024, Tokens per sec:   3525
2023-03-15 11:34:38,700 - INFO - __main__ - Epoch 144, Step:  312900, Batch Loss:     1.449130, Lr: 0.000024, Tokens per sec:   3564
2023-03-15 11:34:55,029 - INFO - __main__ - Epoch 144, Step:  313000, Batch Loss:     1.520866, Lr: 0.000024, Tokens per sec:   3293
2023-03-15 11:35:11,710 - INFO - __main__ - Epoch 144, Step:  313100, Batch Loss:     1.517073, Lr: 0.000024, Tokens per sec:   3194
2023-03-15 11:35:27,606 - INFO - __main__ - Epoch 144, Step:  313200, Batch Loss:     1.023895, Lr: 0.000024, Tokens per sec:   3381
2023-03-15 11:35:43,064 - INFO - __main__ - Epoch 144, Step:  313300, Batch Loss:     2.015771, Lr: 0.000024, Tokens per sec:   3516
2023-03-15 11:35:58,758 - INFO - __main__ - Epoch 144, Step:  313400, Batch Loss:     1.443149, Lr: 0.000024, Tokens per sec:   3472
2023-03-15 11:36:15,581 - INFO - __main__ - Epoch 144, Step:  313500, Batch Loss:     1.021374, Lr: 0.000024, Tokens per sec:   3176
2023-03-15 11:36:31,099 - INFO - __main__ - Epoch 144, Step:  313600, Batch Loss:     1.031819, Lr: 0.000024, Tokens per sec:   3462
2023-03-15 11:36:46,672 - INFO - __main__ - Epoch 144, Step:  313700, Batch Loss:     1.444119, Lr: 0.000024, Tokens per sec:   3414
2023-03-15 11:36:59,381 - INFO - __main__ - Epoch 144: total training loss 3099.31
2023-03-15 11:36:59,382 - INFO - __main__ - Epoch 145
2023-03-15 11:37:03,940 - INFO - __main__ - Epoch 145, Step:  313800, Batch Loss:     1.780169, Lr: 0.000024, Tokens per sec:   2804
2023-03-15 11:37:19,365 - INFO - __main__ - Epoch 145, Step:  313900, Batch Loss:     1.406770, Lr: 0.000024, Tokens per sec:   3503
2023-03-15 11:37:35,581 - INFO - __main__ - Epoch 145, Step:  314000, Batch Loss:     1.462544, Lr: 0.000024, Tokens per sec:   3316
2023-03-15 11:37:51,005 - INFO - __main__ - Epoch 145, Step:  314100, Batch Loss:     1.080015, Lr: 0.000024, Tokens per sec:   3498
2023-03-15 11:38:06,544 - INFO - __main__ - Epoch 145, Step:  314200, Batch Loss:     0.884271, Lr: 0.000024, Tokens per sec:   3452
2023-03-15 11:38:23,256 - INFO - __main__ - Epoch 145, Step:  314300, Batch Loss:     2.140482, Lr: 0.000024, Tokens per sec:   3179
2023-03-15 11:38:38,952 - INFO - __main__ - Epoch 145, Step:  314400, Batch Loss:     1.295810, Lr: 0.000024, Tokens per sec:   3362
2023-03-15 11:38:54,807 - INFO - __main__ - Epoch 145, Step:  314500, Batch Loss:     1.373361, Lr: 0.000024, Tokens per sec:   3407
2023-03-15 11:39:10,074 - INFO - __main__ - Epoch 145, Step:  314600, Batch Loss:     1.031307, Lr: 0.000024, Tokens per sec:   3475
2023-03-15 11:39:25,519 - INFO - __main__ - Epoch 145, Step:  314700, Batch Loss:     1.579528, Lr: 0.000024, Tokens per sec:   3556
2023-03-15 11:39:40,442 - INFO - __main__ - Epoch 145, Step:  314800, Batch Loss:     1.745401, Lr: 0.000024, Tokens per sec:   3603
2023-03-15 11:39:55,549 - INFO - __main__ - Epoch 145, Step:  314900, Batch Loss:     0.955427, Lr: 0.000024, Tokens per sec:   3572
2023-03-15 11:40:10,878 - INFO - __main__ - Epoch 145, Step:  315000, Batch Loss:     1.395123, Lr: 0.000024, Tokens per sec:   3499
2023-03-15 11:40:25,995 - INFO - __main__ - Epoch 145, Step:  315100, Batch Loss:     1.209909, Lr: 0.000024, Tokens per sec:   3557
2023-03-15 11:40:40,886 - INFO - __main__ - Epoch 145, Step:  315200, Batch Loss:     1.313346, Lr: 0.000024, Tokens per sec:   3641
2023-03-15 11:40:56,124 - INFO - __main__ - Epoch 145, Step:  315300, Batch Loss:     1.285172, Lr: 0.000024, Tokens per sec:   3592
2023-03-15 11:41:11,257 - INFO - __main__ - Epoch 145, Step:  315400, Batch Loss:     1.583614, Lr: 0.000024, Tokens per sec:   3599
2023-03-15 11:41:26,459 - INFO - __main__ - Epoch 145, Step:  315500, Batch Loss:     2.870796, Lr: 0.000024, Tokens per sec:   3531
2023-03-15 11:41:41,882 - INFO - __main__ - Epoch 145, Step:  315600, Batch Loss:     0.634669, Lr: 0.000024, Tokens per sec:   3466
2023-03-15 11:41:57,380 - INFO - __main__ - Epoch 145, Step:  315700, Batch Loss:     2.454087, Lr: 0.000024, Tokens per sec:   3442
2023-03-15 11:42:12,619 - INFO - __main__ - Epoch 145, Step:  315800, Batch Loss:     1.550848, Lr: 0.000024, Tokens per sec:   3546
2023-03-15 11:42:28,624 - INFO - __main__ - Epoch 145, Step:  315900, Batch Loss:     1.746512, Lr: 0.000024, Tokens per sec:   3369
2023-03-15 11:42:37,537 - INFO - __main__ - Epoch 145: total training loss 3075.64
2023-03-15 11:42:37,537 - INFO - __main__ - Epoch 146
2023-03-15 11:42:45,035 - INFO - __main__ - Epoch 146, Step:  316000, Batch Loss:     1.469207, Lr: 0.000023, Tokens per sec:   3323
2023-03-15 11:43:00,378 - INFO - __main__ - Epoch 146, Step:  316100, Batch Loss:     1.084687, Lr: 0.000023, Tokens per sec:   3528
2023-03-15 11:43:15,575 - INFO - __main__ - Epoch 146, Step:  316200, Batch Loss:     0.911267, Lr: 0.000023, Tokens per sec:   3550
2023-03-15 11:43:31,103 - INFO - __main__ - Epoch 146, Step:  316300, Batch Loss:     1.495936, Lr: 0.000023, Tokens per sec:   3482
2023-03-15 11:43:46,335 - INFO - __main__ - Epoch 146, Step:  316400, Batch Loss:     1.331291, Lr: 0.000023, Tokens per sec:   3492
2023-03-15 11:44:01,865 - INFO - __main__ - Epoch 146, Step:  316500, Batch Loss:     1.675661, Lr: 0.000023, Tokens per sec:   3379
2023-03-15 11:44:17,423 - INFO - __main__ - Epoch 146, Step:  316600, Batch Loss:     1.654296, Lr: 0.000023, Tokens per sec:   3411
2023-03-15 11:44:32,767 - INFO - __main__ - Epoch 146, Step:  316700, Batch Loss:     1.540488, Lr: 0.000023, Tokens per sec:   3467
2023-03-15 11:44:48,186 - INFO - __main__ - Epoch 146, Step:  316800, Batch Loss:     1.200212, Lr: 0.000023, Tokens per sec:   3515
2023-03-15 11:45:03,521 - INFO - __main__ - Epoch 146, Step:  316900, Batch Loss:     1.319875, Lr: 0.000023, Tokens per sec:   3519
2023-03-15 11:45:18,805 - INFO - __main__ - Epoch 146, Step:  317000, Batch Loss:     1.460315, Lr: 0.000023, Tokens per sec:   3556
2023-03-15 11:45:34,168 - INFO - __main__ - Epoch 146, Step:  317100, Batch Loss:     1.212178, Lr: 0.000023, Tokens per sec:   3466
2023-03-15 11:45:49,372 - INFO - __main__ - Epoch 146, Step:  317200, Batch Loss:     1.238723, Lr: 0.000023, Tokens per sec:   3563
2023-03-15 11:46:04,629 - INFO - __main__ - Epoch 146, Step:  317300, Batch Loss:     1.873543, Lr: 0.000023, Tokens per sec:   3579
2023-03-15 11:46:20,275 - INFO - __main__ - Epoch 146, Step:  317400, Batch Loss:     1.856969, Lr: 0.000023, Tokens per sec:   3429
2023-03-15 11:46:35,691 - INFO - __main__ - Epoch 146, Step:  317500, Batch Loss:     0.801443, Lr: 0.000023, Tokens per sec:   3462
2023-03-15 11:46:51,080 - INFO - __main__ - Epoch 146, Step:  317600, Batch Loss:     0.864161, Lr: 0.000023, Tokens per sec:   3517
2023-03-15 11:47:05,941 - INFO - __main__ - Epoch 146, Step:  317700, Batch Loss:     1.495970, Lr: 0.000023, Tokens per sec:   3619
2023-03-15 11:47:21,432 - INFO - __main__ - Epoch 146, Step:  317800, Batch Loss:     1.623269, Lr: 0.000023, Tokens per sec:   3486
2023-03-15 11:47:36,845 - INFO - __main__ - Epoch 146, Step:  317900, Batch Loss:     1.171114, Lr: 0.000023, Tokens per sec:   3478
2023-03-15 11:47:51,904 - INFO - __main__ - Epoch 146, Step:  318000, Batch Loss:     1.267939, Lr: 0.000023, Tokens per sec:   3597
2023-03-15 11:48:07,255 - INFO - __main__ - Epoch 146, Step:  318100, Batch Loss:     1.879710, Lr: 0.000023, Tokens per sec:   3545
2023-03-15 11:48:12,347 - INFO - __main__ - Epoch 146: total training loss 3028.19
2023-03-15 11:48:12,348 - INFO - __main__ - Epoch 147
2023-03-15 11:48:22,650 - INFO - __main__ - Epoch 147, Step:  318200, Batch Loss:     1.929653, Lr: 0.000023, Tokens per sec:   3421
2023-03-15 11:48:37,933 - INFO - __main__ - Epoch 147, Step:  318300, Batch Loss:     1.154476, Lr: 0.000023, Tokens per sec:   3493
2023-03-15 11:48:53,181 - INFO - __main__ - Epoch 147, Step:  318400, Batch Loss:     0.754761, Lr: 0.000023, Tokens per sec:   3508
2023-03-15 11:49:07,982 - INFO - __main__ - Epoch 147, Step:  318500, Batch Loss:     0.818043, Lr: 0.000023, Tokens per sec:   3578
2023-03-15 11:49:23,024 - INFO - __main__ - Epoch 147, Step:  318600, Batch Loss:     1.614330, Lr: 0.000023, Tokens per sec:   3586
2023-03-15 11:49:38,418 - INFO - __main__ - Epoch 147, Step:  318700, Batch Loss:     0.914378, Lr: 0.000023, Tokens per sec:   3522
2023-03-15 11:49:53,750 - INFO - __main__ - Epoch 147, Step:  318800, Batch Loss:     1.434145, Lr: 0.000023, Tokens per sec:   3465
2023-03-15 11:50:09,130 - INFO - __main__ - Epoch 147, Step:  318900, Batch Loss:     1.682538, Lr: 0.000023, Tokens per sec:   3484
2023-03-15 11:50:24,453 - INFO - __main__ - Epoch 147, Step:  319000, Batch Loss:     1.109532, Lr: 0.000023, Tokens per sec:   3517
2023-03-15 11:50:39,558 - INFO - __main__ - Epoch 147, Step:  319100, Batch Loss:     1.396590, Lr: 0.000023, Tokens per sec:   3594
2023-03-15 11:50:54,831 - INFO - __main__ - Epoch 147, Step:  319200, Batch Loss:     2.019991, Lr: 0.000023, Tokens per sec:   3518
2023-03-15 11:51:10,280 - INFO - __main__ - Epoch 147, Step:  319300, Batch Loss:     1.565540, Lr: 0.000023, Tokens per sec:   3498
2023-03-15 11:51:25,680 - INFO - __main__ - Epoch 147, Step:  319400, Batch Loss:     1.713976, Lr: 0.000023, Tokens per sec:   3521
2023-03-15 11:51:41,043 - INFO - __main__ - Epoch 147, Step:  319500, Batch Loss:     1.138232, Lr: 0.000023, Tokens per sec:   3516
2023-03-15 11:51:56,707 - INFO - __main__ - Epoch 147, Step:  319600, Batch Loss:     1.566635, Lr: 0.000023, Tokens per sec:   3440
2023-03-15 11:52:11,657 - INFO - __main__ - Epoch 147, Step:  319700, Batch Loss:     1.655692, Lr: 0.000023, Tokens per sec:   3608
2023-03-15 11:52:27,169 - INFO - __main__ - Epoch 147, Step:  319800, Batch Loss:     1.020129, Lr: 0.000023, Tokens per sec:   3485
2023-03-15 11:52:42,495 - INFO - __main__ - Epoch 147, Step:  319900, Batch Loss:     0.889410, Lr: 0.000023, Tokens per sec:   3518
2023-03-15 11:52:57,637 - INFO - __main__ - Epoch 147, Step:  320000, Batch Loss:     1.538641, Lr: 0.000023, Tokens per sec:   3525
2023-03-15 11:53:12,892 - INFO - __main__ - Epoch 147, Step:  320100, Batch Loss:     1.791253, Lr: 0.000023, Tokens per sec:   3539
2023-03-15 11:53:28,141 - INFO - __main__ - Epoch 147, Step:  320200, Batch Loss:     1.148734, Lr: 0.000023, Tokens per sec:   3547
2023-03-15 11:53:43,303 - INFO - __main__ - Epoch 147, Step:  320300, Batch Loss:     1.724905, Lr: 0.000023, Tokens per sec:   3575
2023-03-15 11:53:45,218 - INFO - __main__ - Epoch 147: total training loss 2999.23
2023-03-15 11:53:45,219 - INFO - __main__ - Epoch 148
2023-03-15 11:53:58,847 - INFO - __main__ - Epoch 148, Step:  320400, Batch Loss:     2.327458, Lr: 0.000023, Tokens per sec:   3488
2023-03-15 11:54:14,161 - INFO - __main__ - Epoch 148, Step:  320500, Batch Loss:     1.877558, Lr: 0.000023, Tokens per sec:   3514
2023-03-15 11:54:29,500 - INFO - __main__ - Epoch 148, Step:  320600, Batch Loss:     0.944281, Lr: 0.000023, Tokens per sec:   3471
2023-03-15 11:54:44,903 - INFO - __main__ - Epoch 148, Step:  320700, Batch Loss:     1.403802, Lr: 0.000023, Tokens per sec:   3453
2023-03-15 11:55:00,266 - INFO - __main__ - Epoch 148, Step:  320800, Batch Loss:     1.424250, Lr: 0.000023, Tokens per sec:   3483
2023-03-15 11:55:15,046 - INFO - __main__ - Epoch 148, Step:  320900, Batch Loss:     1.026632, Lr: 0.000023, Tokens per sec:   3666
2023-03-15 11:55:30,400 - INFO - __main__ - Epoch 148, Step:  321000, Batch Loss:     1.468498, Lr: 0.000023, Tokens per sec:   3496
2023-03-15 11:55:45,754 - INFO - __main__ - Epoch 148, Step:  321100, Batch Loss:     1.446162, Lr: 0.000023, Tokens per sec:   3562
2023-03-15 11:56:01,084 - INFO - __main__ - Epoch 148, Step:  321200, Batch Loss:     0.797461, Lr: 0.000023, Tokens per sec:   3467
2023-03-15 11:56:16,417 - INFO - __main__ - Epoch 148, Step:  321300, Batch Loss:     0.793412, Lr: 0.000023, Tokens per sec:   3515
2023-03-15 11:56:31,760 - INFO - __main__ - Epoch 148, Step:  321400, Batch Loss:     2.117269, Lr: 0.000023, Tokens per sec:   3522
2023-03-15 11:56:47,061 - INFO - __main__ - Epoch 148, Step:  321500, Batch Loss:     0.959744, Lr: 0.000023, Tokens per sec:   3486
2023-03-15 11:57:02,404 - INFO - __main__ - Epoch 148, Step:  321600, Batch Loss:     1.628858, Lr: 0.000023, Tokens per sec:   3436
2023-03-15 11:57:17,640 - INFO - __main__ - Epoch 148, Step:  321700, Batch Loss:     1.176505, Lr: 0.000023, Tokens per sec:   3573
2023-03-15 11:57:32,908 - INFO - __main__ - Epoch 148, Step:  321800, Batch Loss:     1.992815, Lr: 0.000023, Tokens per sec:   3482
2023-03-15 11:57:48,320 - INFO - __main__ - Epoch 148, Step:  321900, Batch Loss:     1.776344, Lr: 0.000023, Tokens per sec:   3519
2023-03-15 11:58:03,742 - INFO - __main__ - Epoch 148, Step:  322000, Batch Loss:     2.316033, Lr: 0.000023, Tokens per sec:   3521
2023-03-15 11:58:19,147 - INFO - __main__ - Epoch 148, Step:  322100, Batch Loss:     1.462875, Lr: 0.000023, Tokens per sec:   3543
2023-03-15 11:58:34,621 - INFO - __main__ - Epoch 148, Step:  322200, Batch Loss:     1.048783, Lr: 0.000023, Tokens per sec:   3423
2023-03-15 11:58:50,033 - INFO - __main__ - Epoch 148, Step:  322300, Batch Loss:     1.566357, Lr: 0.000023, Tokens per sec:   3521
2023-03-15 11:59:05,331 - INFO - __main__ - Epoch 148, Step:  322400, Batch Loss:     1.027178, Lr: 0.000023, Tokens per sec:   3520
2023-03-15 11:59:19,599 - INFO - __main__ - Epoch 148: total training loss 3002.42
2023-03-15 11:59:19,599 - INFO - __main__ - Epoch 149
2023-03-15 11:59:21,217 - INFO - __main__ - Epoch 149, Step:  322500, Batch Loss:     1.315109, Lr: 0.000023, Tokens per sec:   2690
2023-03-15 11:59:36,490 - INFO - __main__ - Epoch 149, Step:  322600, Batch Loss:     1.028501, Lr: 0.000023, Tokens per sec:   3517
2023-03-15 11:59:51,836 - INFO - __main__ - Epoch 149, Step:  322700, Batch Loss:     1.981850, Lr: 0.000023, Tokens per sec:   3513
2023-03-15 12:00:07,160 - INFO - __main__ - Epoch 149, Step:  322800, Batch Loss:     1.296626, Lr: 0.000023, Tokens per sec:   3572
2023-03-15 12:00:22,489 - INFO - __main__ - Epoch 149, Step:  322900, Batch Loss:     2.115452, Lr: 0.000023, Tokens per sec:   3490
2023-03-15 12:00:37,797 - INFO - __main__ - Epoch 149, Step:  323000, Batch Loss:     1.928378, Lr: 0.000023, Tokens per sec:   3533
2023-03-15 12:00:53,136 - INFO - __main__ - Epoch 149, Step:  323100, Batch Loss:     1.961291, Lr: 0.000023, Tokens per sec:   3513
2023-03-15 12:01:08,465 - INFO - __main__ - Epoch 149, Step:  323200, Batch Loss:     1.334953, Lr: 0.000023, Tokens per sec:   3545
2023-03-15 12:01:23,526 - INFO - __main__ - Epoch 149, Step:  323300, Batch Loss:     1.010210, Lr: 0.000023, Tokens per sec:   3597
2023-03-15 12:01:37,516 - INFO - __main__ - Epoch 149, Step:  323400, Batch Loss:     1.035673, Lr: 0.000023, Tokens per sec:   3843
2023-03-15 12:01:50,984 - INFO - __main__ - Epoch 149, Step:  323500, Batch Loss:     1.290460, Lr: 0.000023, Tokens per sec:   3987
2023-03-15 12:02:05,625 - INFO - __main__ - Epoch 149, Step:  323600, Batch Loss:     1.064943, Lr: 0.000023, Tokens per sec:   3739
2023-03-15 12:02:20,982 - INFO - __main__ - Epoch 149, Step:  323700, Batch Loss:     1.336538, Lr: 0.000023, Tokens per sec:   3506
2023-03-15 12:02:36,317 - INFO - __main__ - Epoch 149, Step:  323800, Batch Loss:     1.291296, Lr: 0.000023, Tokens per sec:   3493
2023-03-15 12:02:51,605 - INFO - __main__ - Epoch 149, Step:  323900, Batch Loss:     1.608624, Lr: 0.000023, Tokens per sec:   3464
2023-03-15 12:03:06,995 - INFO - __main__ - Epoch 149, Step:  324000, Batch Loss:     1.687144, Lr: 0.000023, Tokens per sec:   3494
2023-03-15 12:03:22,214 - INFO - __main__ - Epoch 149, Step:  324100, Batch Loss:     1.537019, Lr: 0.000023, Tokens per sec:   3492
2023-03-15 12:03:37,549 - INFO - __main__ - Epoch 149, Step:  324200, Batch Loss:     0.989875, Lr: 0.000023, Tokens per sec:   3534
2023-03-15 12:03:52,810 - INFO - __main__ - Epoch 149, Step:  324300, Batch Loss:     2.002005, Lr: 0.000023, Tokens per sec:   3534
2023-03-15 12:04:08,088 - INFO - __main__ - Epoch 149, Step:  324400, Batch Loss:     0.618902, Lr: 0.000023, Tokens per sec:   3515
2023-03-15 12:04:23,396 - INFO - __main__ - Epoch 149, Step:  324500, Batch Loss:     1.511422, Lr: 0.000023, Tokens per sec:   3514
2023-03-15 12:04:38,762 - INFO - __main__ - Epoch 149, Step:  324600, Batch Loss:     1.002194, Lr: 0.000023, Tokens per sec:   3485
2023-03-15 12:04:49,358 - INFO - __main__ - Epoch 149: total training loss 2945.54
2023-03-15 12:04:49,358 - INFO - __main__ - Epoch 150
2023-03-15 12:04:54,134 - INFO - __main__ - Epoch 150, Step:  324700, Batch Loss:     1.453716, Lr: 0.000022, Tokens per sec:   3272
2023-03-15 12:05:09,445 - INFO - __main__ - Epoch 150, Step:  324800, Batch Loss:     1.353936, Lr: 0.000022, Tokens per sec:   3497
2023-03-15 12:05:24,759 - INFO - __main__ - Epoch 150, Step:  324900, Batch Loss:     1.267072, Lr: 0.000022, Tokens per sec:   3557
2023-03-15 12:05:40,075 - INFO - __main__ - Epoch 150, Step:  325000, Batch Loss:     0.946641, Lr: 0.000022, Tokens per sec:   3455
2023-03-15 12:05:55,422 - INFO - __main__ - Epoch 150, Step:  325100, Batch Loss:     1.881295, Lr: 0.000022, Tokens per sec:   3520
2023-03-15 12:06:10,742 - INFO - __main__ - Epoch 150, Step:  325200, Batch Loss:     1.069547, Lr: 0.000022, Tokens per sec:   3539
2023-03-15 12:06:26,072 - INFO - __main__ - Epoch 150, Step:  325300, Batch Loss:     0.854881, Lr: 0.000022, Tokens per sec:   3496
2023-03-15 12:06:41,451 - INFO - __main__ - Epoch 150, Step:  325400, Batch Loss:     0.835227, Lr: 0.000022, Tokens per sec:   3440
2023-03-15 12:06:56,479 - INFO - __main__ - Epoch 150, Step:  325500, Batch Loss:     1.069834, Lr: 0.000022, Tokens per sec:   3642
2023-03-15 12:07:11,661 - INFO - __main__ - Epoch 150, Step:  325600, Batch Loss:     1.722697, Lr: 0.000022, Tokens per sec:   3568
2023-03-15 12:07:26,961 - INFO - __main__ - Epoch 150, Step:  325700, Batch Loss:     1.636428, Lr: 0.000022, Tokens per sec:   3544
2023-03-15 12:07:42,217 - INFO - __main__ - Epoch 150, Step:  325800, Batch Loss:     0.999391, Lr: 0.000022, Tokens per sec:   3499
2023-03-15 12:07:57,515 - INFO - __main__ - Epoch 150, Step:  325900, Batch Loss:     1.314606, Lr: 0.000022, Tokens per sec:   3519
2023-03-15 12:08:12,856 - INFO - __main__ - Epoch 150, Step:  326000, Batch Loss:     0.716315, Lr: 0.000022, Tokens per sec:   3564
2023-03-15 12:08:28,236 - INFO - __main__ - Epoch 150, Step:  326100, Batch Loss:     1.401682, Lr: 0.000022, Tokens per sec:   3504
2023-03-15 12:08:43,538 - INFO - __main__ - Epoch 150, Step:  326200, Batch Loss:     0.970218, Lr: 0.000022, Tokens per sec:   3525
2023-03-15 12:08:58,885 - INFO - __main__ - Epoch 150, Step:  326300, Batch Loss:     1.615984, Lr: 0.000022, Tokens per sec:   3525
2023-03-15 12:09:14,214 - INFO - __main__ - Epoch 150, Step:  326400, Batch Loss:     1.899374, Lr: 0.000022, Tokens per sec:   3491
2023-03-15 12:09:29,498 - INFO - __main__ - Epoch 150, Step:  326500, Batch Loss:     1.269547, Lr: 0.000022, Tokens per sec:   3545
2023-03-15 12:09:44,733 - INFO - __main__ - Epoch 150, Step:  326600, Batch Loss:     1.243133, Lr: 0.000022, Tokens per sec:   3526
2023-03-15 12:10:00,014 - INFO - __main__ - Epoch 150, Step:  326700, Batch Loss:     1.270816, Lr: 0.000022, Tokens per sec:   3478
2023-03-15 12:10:15,369 - INFO - __main__ - Epoch 150, Step:  326800, Batch Loss:     1.571463, Lr: 0.000022, Tokens per sec:   3525
2023-03-15 12:10:23,109 - INFO - __main__ - Epoch 150: total training loss 2942.74
2023-03-15 12:10:23,110 - INFO - __main__ - Epoch 151
2023-03-15 12:10:31,141 - INFO - __main__ - Epoch 151, Step:  326900, Batch Loss:     0.487836, Lr: 0.000022, Tokens per sec:   3290
2023-03-15 12:10:46,405 - INFO - __main__ - Epoch 151, Step:  327000, Batch Loss:     1.502863, Lr: 0.000022, Tokens per sec:   3471
2023-03-15 12:11:01,844 - INFO - __main__ - Epoch 151, Step:  327100, Batch Loss:     0.553721, Lr: 0.000022, Tokens per sec:   3430
2023-03-15 12:11:17,117 - INFO - __main__ - Epoch 151, Step:  327200, Batch Loss:     1.304125, Lr: 0.000022, Tokens per sec:   3435
2023-03-15 12:11:32,477 - INFO - __main__ - Epoch 151, Step:  327300, Batch Loss:     1.191865, Lr: 0.000022, Tokens per sec:   3425
2023-03-15 12:11:47,801 - INFO - __main__ - Epoch 151, Step:  327400, Batch Loss:     0.917645, Lr: 0.000022, Tokens per sec:   3499
2023-03-15 12:12:03,220 - INFO - __main__ - Epoch 151, Step:  327500, Batch Loss:     1.397914, Lr: 0.000022, Tokens per sec:   3497
2023-03-15 12:12:18,615 - INFO - __main__ - Epoch 151, Step:  327600, Batch Loss:     1.774102, Lr: 0.000022, Tokens per sec:   3543
2023-03-15 12:12:33,973 - INFO - __main__ - Epoch 151, Step:  327700, Batch Loss:     1.340327, Lr: 0.000022, Tokens per sec:   3543
2023-03-15 12:12:49,280 - INFO - __main__ - Epoch 151, Step:  327800, Batch Loss:     1.060763, Lr: 0.000022, Tokens per sec:   3543
2023-03-15 12:13:04,607 - INFO - __main__ - Epoch 151, Step:  327900, Batch Loss:     2.022434, Lr: 0.000022, Tokens per sec:   3556
2023-03-15 12:13:20,327 - INFO - __main__ - Epoch 151, Step:  328000, Batch Loss:     1.044446, Lr: 0.000022, Tokens per sec:   3433
2023-03-15 12:13:35,744 - INFO - __main__ - Epoch 151, Step:  328100, Batch Loss:     1.385990, Lr: 0.000022, Tokens per sec:   3510
2023-03-15 12:13:51,120 - INFO - __main__ - Epoch 151, Step:  328200, Batch Loss:     1.292553, Lr: 0.000022, Tokens per sec:   3492
2023-03-15 12:14:06,515 - INFO - __main__ - Epoch 151, Step:  328300, Batch Loss:     1.280743, Lr: 0.000022, Tokens per sec:   3539
2023-03-15 12:14:21,848 - INFO - __main__ - Epoch 151, Step:  328400, Batch Loss:     1.461293, Lr: 0.000022, Tokens per sec:   3531
2023-03-15 12:14:37,204 - INFO - __main__ - Epoch 151, Step:  328500, Batch Loss:     1.289668, Lr: 0.000022, Tokens per sec:   3520
2023-03-15 12:14:52,469 - INFO - __main__ - Epoch 151, Step:  328600, Batch Loss:     1.265452, Lr: 0.000022, Tokens per sec:   3525
2023-03-15 12:15:07,810 - INFO - __main__ - Epoch 151, Step:  328700, Batch Loss:     1.055369, Lr: 0.000022, Tokens per sec:   3523
2023-03-15 12:15:23,188 - INFO - __main__ - Epoch 151, Step:  328800, Batch Loss:     1.258185, Lr: 0.000022, Tokens per sec:   3546
2023-03-15 12:15:38,437 - INFO - __main__ - Epoch 151, Step:  328900, Batch Loss:     1.410169, Lr: 0.000022, Tokens per sec:   3534
2023-03-15 12:15:53,746 - INFO - __main__ - Epoch 151, Step:  329000, Batch Loss:     1.412332, Lr: 0.000022, Tokens per sec:   3487
2023-03-15 12:15:58,303 - INFO - __main__ - Epoch 151: total training loss 2896.11
2023-03-15 12:15:58,304 - INFO - __main__ - Epoch 152
2023-03-15 12:16:09,165 - INFO - __main__ - Epoch 152, Step:  329100, Batch Loss:     1.116915, Lr: 0.000022, Tokens per sec:   3538
2023-03-15 12:16:24,414 - INFO - __main__ - Epoch 152, Step:  329200, Batch Loss:     1.190481, Lr: 0.000022, Tokens per sec:   3429
2023-03-15 12:16:39,691 - INFO - __main__ - Epoch 152, Step:  329300, Batch Loss:     1.292002, Lr: 0.000022, Tokens per sec:   3532
2023-03-15 12:16:54,966 - INFO - __main__ - Epoch 152, Step:  329400, Batch Loss:     1.452972, Lr: 0.000022, Tokens per sec:   3495
2023-03-15 12:17:10,227 - INFO - __main__ - Epoch 152, Step:  329500, Batch Loss:     1.387191, Lr: 0.000022, Tokens per sec:   3554
2023-03-15 12:17:25,477 - INFO - __main__ - Epoch 152, Step:  329600, Batch Loss:     1.020638, Lr: 0.000022, Tokens per sec:   3576
2023-03-15 12:17:40,737 - INFO - __main__ - Epoch 152, Step:  329700, Batch Loss:     1.991601, Lr: 0.000022, Tokens per sec:   3514
2023-03-15 12:17:56,073 - INFO - __main__ - Epoch 152, Step:  329800, Batch Loss:     1.304680, Lr: 0.000022, Tokens per sec:   3524
2023-03-15 12:18:11,392 - INFO - __main__ - Epoch 152, Step:  329900, Batch Loss:     1.644851, Lr: 0.000022, Tokens per sec:   3521
2023-03-15 12:18:26,654 - INFO - __main__ - Epoch 152, Step:  330000, Batch Loss:     1.866314, Lr: 0.000022, Tokens per sec:   3513
2023-03-15 12:18:41,740 - INFO - __main__ - Epoch 152, Step:  330100, Batch Loss:     1.497791, Lr: 0.000022, Tokens per sec:   3663
2023-03-15 12:18:57,126 - INFO - __main__ - Epoch 152, Step:  330200, Batch Loss:     0.920255, Lr: 0.000022, Tokens per sec:   3461
2023-03-15 12:19:12,469 - INFO - __main__ - Epoch 152, Step:  330300, Batch Loss:     1.093464, Lr: 0.000022, Tokens per sec:   3496
2023-03-15 12:19:27,764 - INFO - __main__ - Epoch 152, Step:  330400, Batch Loss:     1.044248, Lr: 0.000022, Tokens per sec:   3556
2023-03-15 12:19:42,998 - INFO - __main__ - Epoch 152, Step:  330500, Batch Loss:     2.009583, Lr: 0.000022, Tokens per sec:   3488
2023-03-15 12:19:57,896 - INFO - __main__ - Epoch 152, Step:  330600, Batch Loss:     0.972689, Lr: 0.000022, Tokens per sec:   3584
2023-03-15 12:20:13,155 - INFO - __main__ - Epoch 152, Step:  330700, Batch Loss:     1.372570, Lr: 0.000022, Tokens per sec:   3551
2023-03-15 12:20:28,464 - INFO - __main__ - Epoch 152, Step:  330800, Batch Loss:     1.598378, Lr: 0.000022, Tokens per sec:   3516
2023-03-15 12:20:43,761 - INFO - __main__ - Epoch 152, Step:  330900, Batch Loss:     1.578061, Lr: 0.000022, Tokens per sec:   3578
2023-03-15 12:20:59,081 - INFO - __main__ - Epoch 152, Step:  331000, Batch Loss:     2.208136, Lr: 0.000022, Tokens per sec:   3489
2023-03-15 12:21:14,113 - INFO - __main__ - Epoch 152, Step:  331100, Batch Loss:     0.404436, Lr: 0.000022, Tokens per sec:   3526
2023-03-15 12:21:29,409 - INFO - __main__ - Epoch 152, Step:  331200, Batch Loss:     1.434690, Lr: 0.000022, Tokens per sec:   3568
2023-03-15 12:21:30,733 - INFO - __main__ - Epoch 152: total training loss 2876.01
2023-03-15 12:21:30,734 - INFO - __main__ - Epoch 153
2023-03-15 12:21:45,238 - INFO - __main__ - Epoch 153, Step:  331300, Batch Loss:     1.567783, Lr: 0.000022, Tokens per sec:   3411
2023-03-15 12:22:00,539 - INFO - __main__ - Epoch 153, Step:  331400, Batch Loss:     1.265635, Lr: 0.000022, Tokens per sec:   3505
2023-03-15 12:22:15,800 - INFO - __main__ - Epoch 153, Step:  331500, Batch Loss:     1.286647, Lr: 0.000022, Tokens per sec:   3521
2023-03-15 12:22:31,071 - INFO - __main__ - Epoch 153, Step:  331600, Batch Loss:     1.409243, Lr: 0.000022, Tokens per sec:   3555
2023-03-15 12:22:46,402 - INFO - __main__ - Epoch 153, Step:  331700, Batch Loss:     1.381037, Lr: 0.000022, Tokens per sec:   3542
2023-03-15 12:23:01,642 - INFO - __main__ - Epoch 153, Step:  331800, Batch Loss:     1.237670, Lr: 0.000022, Tokens per sec:   3479
2023-03-15 12:23:16,971 - INFO - __main__ - Epoch 153, Step:  331900, Batch Loss:     1.542052, Lr: 0.000022, Tokens per sec:   3455
2023-03-15 12:23:32,224 - INFO - __main__ - Epoch 153, Step:  332000, Batch Loss:     1.886249, Lr: 0.000022, Tokens per sec:   3563
2023-03-15 12:23:47,544 - INFO - __main__ - Epoch 153, Step:  332100, Batch Loss:     1.556461, Lr: 0.000022, Tokens per sec:   3584
2023-03-15 12:24:02,883 - INFO - __main__ - Epoch 153, Step:  332200, Batch Loss:     1.151052, Lr: 0.000022, Tokens per sec:   3527
2023-03-15 12:24:18,106 - INFO - __main__ - Epoch 153, Step:  332300, Batch Loss:     1.349694, Lr: 0.000022, Tokens per sec:   3569
2023-03-15 12:24:33,120 - INFO - __main__ - Epoch 153, Step:  332400, Batch Loss:     1.517477, Lr: 0.000022, Tokens per sec:   3663
2023-03-15 12:24:47,961 - INFO - __main__ - Epoch 153, Step:  332500, Batch Loss:     2.002014, Lr: 0.000022, Tokens per sec:   3651
2023-03-15 12:25:03,201 - INFO - __main__ - Epoch 153, Step:  332600, Batch Loss:     1.612208, Lr: 0.000022, Tokens per sec:   3497
2023-03-15 12:25:17,874 - INFO - __main__ - Epoch 153, Step:  332700, Batch Loss:     0.791689, Lr: 0.000022, Tokens per sec:   3647
2023-03-15 12:25:32,963 - INFO - __main__ - Epoch 153, Step:  332800, Batch Loss:     0.708236, Lr: 0.000022, Tokens per sec:   3497
2023-03-15 12:25:48,300 - INFO - __main__ - Epoch 153, Step:  332900, Batch Loss:     1.815992, Lr: 0.000022, Tokens per sec:   3555
2023-03-15 12:26:03,631 - INFO - __main__ - Epoch 153, Step:  333000, Batch Loss:     1.221337, Lr: 0.000022, Tokens per sec:   3471
2023-03-15 12:26:19,032 - INFO - __main__ - Epoch 153, Step:  333100, Batch Loss:     1.571751, Lr: 0.000022, Tokens per sec:   3499
2023-03-15 12:26:33,628 - INFO - __main__ - Epoch 153, Step:  333200, Batch Loss:     1.307943, Lr: 0.000022, Tokens per sec:   3753
2023-03-15 12:26:48,476 - INFO - __main__ - Epoch 153, Step:  333300, Batch Loss:     1.229909, Lr: 0.000022, Tokens per sec:   3553
2023-03-15 12:27:01,880 - INFO - __main__ - Epoch 153: total training loss 2875.80
2023-03-15 12:27:01,881 - INFO - __main__ - Epoch 154
2023-03-15 12:27:04,241 - INFO - __main__ - Epoch 154, Step:  333400, Batch Loss:     1.379197, Lr: 0.000021, Tokens per sec:   3033
2023-03-15 12:27:19,515 - INFO - __main__ - Epoch 154, Step:  333500, Batch Loss:     1.307237, Lr: 0.000021, Tokens per sec:   3583
2023-03-15 12:27:34,815 - INFO - __main__ - Epoch 154, Step:  333600, Batch Loss:     1.473716, Lr: 0.000021, Tokens per sec:   3523
2023-03-15 12:27:50,126 - INFO - __main__ - Epoch 154, Step:  333700, Batch Loss:     0.788745, Lr: 0.000021, Tokens per sec:   3500
2023-03-15 12:28:05,433 - INFO - __main__ - Epoch 154, Step:  333800, Batch Loss:     1.672114, Lr: 0.000021, Tokens per sec:   3506
2023-03-15 12:28:20,728 - INFO - __main__ - Epoch 154, Step:  333900, Batch Loss:     0.993292, Lr: 0.000021, Tokens per sec:   3497
2023-03-15 12:28:35,972 - INFO - __main__ - Epoch 154, Step:  334000, Batch Loss:     1.603243, Lr: 0.000021, Tokens per sec:   3576
2023-03-15 12:28:51,277 - INFO - __main__ - Epoch 154, Step:  334100, Batch Loss:     1.318125, Lr: 0.000021, Tokens per sec:   3490
2023-03-15 12:29:06,575 - INFO - __main__ - Epoch 154, Step:  334200, Batch Loss:     0.797512, Lr: 0.000021, Tokens per sec:   3526
2023-03-15 12:29:21,829 - INFO - __main__ - Epoch 154, Step:  334300, Batch Loss:     1.949551, Lr: 0.000021, Tokens per sec:   3508
2023-03-15 12:29:37,126 - INFO - __main__ - Epoch 154, Step:  334400, Batch Loss:     1.376440, Lr: 0.000021, Tokens per sec:   3584
2023-03-15 12:29:52,380 - INFO - __main__ - Epoch 154, Step:  334500, Batch Loss:     1.770130, Lr: 0.000021, Tokens per sec:   3523
2023-03-15 12:30:07,698 - INFO - __main__ - Epoch 154, Step:  334600, Batch Loss:     1.043078, Lr: 0.000021, Tokens per sec:   3484
2023-03-15 12:30:22,968 - INFO - __main__ - Epoch 154, Step:  334700, Batch Loss:     0.778845, Lr: 0.000021, Tokens per sec:   3516
2023-03-15 12:30:38,301 - INFO - __main__ - Epoch 154, Step:  334800, Batch Loss:     1.544821, Lr: 0.000021, Tokens per sec:   3552
2023-03-15 12:30:53,627 - INFO - __main__ - Epoch 154, Step:  334900, Batch Loss:     1.750012, Lr: 0.000021, Tokens per sec:   3514
2023-03-15 12:31:08,979 - INFO - __main__ - Epoch 154, Step:  335000, Batch Loss:     1.136173, Lr: 0.000021, Tokens per sec:   3476
2023-03-15 12:31:24,384 - INFO - __main__ - Epoch 154, Step:  335100, Batch Loss:     1.247778, Lr: 0.000021, Tokens per sec:   3439
2023-03-15 12:31:39,712 - INFO - __main__ - Epoch 154, Step:  335200, Batch Loss:     0.985458, Lr: 0.000021, Tokens per sec:   3535
2023-03-15 12:31:55,113 - INFO - __main__ - Epoch 154, Step:  335300, Batch Loss:     1.332737, Lr: 0.000021, Tokens per sec:   3485
2023-03-15 12:32:10,489 - INFO - __main__ - Epoch 154, Step:  335400, Batch Loss:     1.575928, Lr: 0.000021, Tokens per sec:   3513
2023-03-15 12:32:25,777 - INFO - __main__ - Epoch 154, Step:  335500, Batch Loss:     1.859040, Lr: 0.000021, Tokens per sec:   3491
2023-03-15 12:32:36,031 - INFO - __main__ - Epoch 154: total training loss 2823.88
2023-03-15 12:32:36,032 - INFO - __main__ - Epoch 155
2023-03-15 12:32:41,628 - INFO - __main__ - Epoch 155, Step:  335600, Batch Loss:     1.456358, Lr: 0.000021, Tokens per sec:   3217
2023-03-15 12:32:56,930 - INFO - __main__ - Epoch 155, Step:  335700, Batch Loss:     1.674955, Lr: 0.000021, Tokens per sec:   3513
2023-03-15 12:33:12,197 - INFO - __main__ - Epoch 155, Step:  335800, Batch Loss:     1.567060, Lr: 0.000021, Tokens per sec:   3546
2023-03-15 12:33:27,473 - INFO - __main__ - Epoch 155, Step:  335900, Batch Loss:     0.674382, Lr: 0.000021, Tokens per sec:   3529
2023-03-15 12:33:42,738 - INFO - __main__ - Epoch 155, Step:  336000, Batch Loss:     1.147787, Lr: 0.000021, Tokens per sec:   3482
2023-03-15 12:33:58,011 - INFO - __main__ - Epoch 155, Step:  336100, Batch Loss:     0.732861, Lr: 0.000021, Tokens per sec:   3519
2023-03-15 12:34:13,374 - INFO - __main__ - Epoch 155, Step:  336200, Batch Loss:     1.217677, Lr: 0.000021, Tokens per sec:   3541
2023-03-15 12:34:28,716 - INFO - __main__ - Epoch 155, Step:  336300, Batch Loss:     1.103514, Lr: 0.000021, Tokens per sec:   3515
2023-03-15 12:34:43,946 - INFO - __main__ - Epoch 155, Step:  336400, Batch Loss:     1.533309, Lr: 0.000021, Tokens per sec:   3463
2023-03-15 12:34:59,170 - INFO - __main__ - Epoch 155, Step:  336500, Batch Loss:     1.008220, Lr: 0.000021, Tokens per sec:   3507
2023-03-15 12:35:14,394 - INFO - __main__ - Epoch 155, Step:  336600, Batch Loss:     1.004254, Lr: 0.000021, Tokens per sec:   3484
2023-03-15 12:35:29,512 - INFO - __main__ - Epoch 155, Step:  336700, Batch Loss:     1.961627, Lr: 0.000021, Tokens per sec:   3583
2023-03-15 12:35:44,665 - INFO - __main__ - Epoch 155, Step:  336800, Batch Loss:     1.390628, Lr: 0.000021, Tokens per sec:   3632
2023-03-15 12:35:59,814 - INFO - __main__ - Epoch 155, Step:  336900, Batch Loss:     1.205420, Lr: 0.000021, Tokens per sec:   3495
2023-03-15 12:36:14,286 - INFO - __main__ - Epoch 155, Step:  337000, Batch Loss:     1.343837, Lr: 0.000021, Tokens per sec:   3686
2023-03-15 12:36:29,115 - INFO - __main__ - Epoch 155, Step:  337100, Batch Loss:     1.317626, Lr: 0.000021, Tokens per sec:   3679
2023-03-15 12:36:44,171 - INFO - __main__ - Epoch 155, Step:  337200, Batch Loss:     0.888836, Lr: 0.000021, Tokens per sec:   3646
2023-03-15 12:36:59,253 - INFO - __main__ - Epoch 155, Step:  337300, Batch Loss:     0.993342, Lr: 0.000021, Tokens per sec:   3595
2023-03-15 12:37:14,328 - INFO - __main__ - Epoch 155, Step:  337400, Batch Loss:     1.182482, Lr: 0.000021, Tokens per sec:   3553
2023-03-15 12:37:29,344 - INFO - __main__ - Epoch 155, Step:  337500, Batch Loss:     1.744348, Lr: 0.000021, Tokens per sec:   3568
2023-03-15 12:37:44,522 - INFO - __main__ - Epoch 155, Step:  337600, Batch Loss:     1.115813, Lr: 0.000021, Tokens per sec:   3587
2023-03-15 12:37:59,672 - INFO - __main__ - Epoch 155, Step:  337700, Batch Loss:     1.239956, Lr: 0.000021, Tokens per sec:   3579
2023-03-15 12:38:06,575 - INFO - __main__ - Epoch 155: total training loss 2806.84
2023-03-15 12:38:06,576 - INFO - __main__ - Epoch 156
2023-03-15 12:38:15,304 - INFO - __main__ - Epoch 156, Step:  337800, Batch Loss:     0.556073, Lr: 0.000021, Tokens per sec:   3436
2023-03-15 12:38:30,379 - INFO - __main__ - Epoch 156, Step:  337900, Batch Loss:     1.042157, Lr: 0.000021, Tokens per sec:   3513
2023-03-15 12:38:45,520 - INFO - __main__ - Epoch 156, Step:  338000, Batch Loss:     1.683304, Lr: 0.000021, Tokens per sec:   3585
2023-03-15 12:39:00,646 - INFO - __main__ - Epoch 156, Step:  338100, Batch Loss:     1.219693, Lr: 0.000021, Tokens per sec:   3506
2023-03-15 12:39:15,749 - INFO - __main__ - Epoch 156, Step:  338200, Batch Loss:     1.599471, Lr: 0.000021, Tokens per sec:   3522
2023-03-15 12:39:30,897 - INFO - __main__ - Epoch 156, Step:  338300, Batch Loss:     1.005382, Lr: 0.000021, Tokens per sec:   3529
2023-03-15 12:39:46,105 - INFO - __main__ - Epoch 156, Step:  338400, Batch Loss:     0.947051, Lr: 0.000021, Tokens per sec:   3510
2023-03-15 12:40:01,201 - INFO - __main__ - Epoch 156, Step:  338500, Batch Loss:     1.287570, Lr: 0.000021, Tokens per sec:   3490
2023-03-15 12:40:16,295 - INFO - __main__ - Epoch 156, Step:  338600, Batch Loss:     1.012509, Lr: 0.000021, Tokens per sec:   3536
2023-03-15 12:40:31,378 - INFO - __main__ - Epoch 156, Step:  338700, Batch Loss:     1.085473, Lr: 0.000021, Tokens per sec:   3567
2023-03-15 12:40:46,434 - INFO - __main__ - Epoch 156, Step:  338800, Batch Loss:     1.587552, Lr: 0.000021, Tokens per sec:   3641
2023-03-15 12:41:01,482 - INFO - __main__ - Epoch 156, Step:  338900, Batch Loss:     1.834914, Lr: 0.000021, Tokens per sec:   3553
2023-03-15 12:41:16,618 - INFO - __main__ - Epoch 156, Step:  339000, Batch Loss:     1.036201, Lr: 0.000021, Tokens per sec:   3561
2023-03-15 12:41:31,803 - INFO - __main__ - Epoch 156, Step:  339100, Batch Loss:     1.219395, Lr: 0.000021, Tokens per sec:   3533
2023-03-15 12:41:46,949 - INFO - __main__ - Epoch 156, Step:  339200, Batch Loss:     1.352184, Lr: 0.000021, Tokens per sec:   3574
2023-03-15 12:42:02,014 - INFO - __main__ - Epoch 156, Step:  339300, Batch Loss:     1.281792, Lr: 0.000021, Tokens per sec:   3550
2023-03-15 12:42:17,181 - INFO - __main__ - Epoch 156, Step:  339400, Batch Loss:     1.247644, Lr: 0.000021, Tokens per sec:   3592
2023-03-15 12:42:32,235 - INFO - __main__ - Epoch 156, Step:  339500, Batch Loss:     0.909823, Lr: 0.000021, Tokens per sec:   3589
2023-03-15 12:42:47,308 - INFO - __main__ - Epoch 156, Step:  339600, Batch Loss:     1.011252, Lr: 0.000021, Tokens per sec:   3643
2023-03-15 12:43:02,458 - INFO - __main__ - Epoch 156, Step:  339700, Batch Loss:     1.509815, Lr: 0.000021, Tokens per sec:   3547
2023-03-15 12:43:17,561 - INFO - __main__ - Epoch 156, Step:  339800, Batch Loss:     1.142229, Lr: 0.000021, Tokens per sec:   3614
2023-03-15 12:43:32,634 - INFO - __main__ - Epoch 156, Step:  339900, Batch Loss:     1.181304, Lr: 0.000021, Tokens per sec:   3604
2023-03-15 12:43:36,352 - INFO - __main__ - Epoch 156: total training loss 2786.84
2023-03-15 12:43:36,353 - INFO - __main__ - Epoch 157
2023-03-15 12:43:48,161 - INFO - __main__ - Epoch 157, Step:  340000, Batch Loss:     1.286388, Lr: 0.000021, Tokens per sec:   3441
2023-03-15 12:44:03,230 - INFO - __main__ - Epoch 157, Step:  340100, Batch Loss:     0.919535, Lr: 0.000021, Tokens per sec:   3554
2023-03-15 12:44:18,330 - INFO - __main__ - Epoch 157, Step:  340200, Batch Loss:     1.483022, Lr: 0.000021, Tokens per sec:   3525
2023-03-15 12:44:33,473 - INFO - __main__ - Epoch 157, Step:  340300, Batch Loss:     1.004802, Lr: 0.000021, Tokens per sec:   3580
2023-03-15 12:44:48,583 - INFO - __main__ - Epoch 157, Step:  340400, Batch Loss:     1.211208, Lr: 0.000021, Tokens per sec:   3611
2023-03-15 12:45:03,632 - INFO - __main__ - Epoch 157, Step:  340500, Batch Loss:     1.015983, Lr: 0.000021, Tokens per sec:   3561
2023-03-15 12:45:18,717 - INFO - __main__ - Epoch 157, Step:  340600, Batch Loss:     0.722127, Lr: 0.000021, Tokens per sec:   3620
2023-03-15 12:45:33,770 - INFO - __main__ - Epoch 157, Step:  340700, Batch Loss:     1.075424, Lr: 0.000021, Tokens per sec:   3524
2023-03-15 12:45:48,842 - INFO - __main__ - Epoch 157, Step:  340800, Batch Loss:     1.202793, Lr: 0.000021, Tokens per sec:   3619
2023-03-15 12:46:03,935 - INFO - __main__ - Epoch 157, Step:  340900, Batch Loss:     1.227572, Lr: 0.000021, Tokens per sec:   3580
2023-03-15 12:46:18,955 - INFO - __main__ - Epoch 157, Step:  341000, Batch Loss:     1.804194, Lr: 0.000021, Tokens per sec:   3540
2023-03-15 12:46:34,015 - INFO - __main__ - Epoch 157, Step:  341100, Batch Loss:     1.400682, Lr: 0.000021, Tokens per sec:   3608
2023-03-15 12:46:49,070 - INFO - __main__ - Epoch 157, Step:  341200, Batch Loss:     1.011552, Lr: 0.000021, Tokens per sec:   3524
2023-03-15 12:47:04,186 - INFO - __main__ - Epoch 157, Step:  341300, Batch Loss:     0.970768, Lr: 0.000021, Tokens per sec:   3627
2023-03-15 12:47:19,330 - INFO - __main__ - Epoch 157, Step:  341400, Batch Loss:     0.952418, Lr: 0.000021, Tokens per sec:   3508
2023-03-15 12:47:34,436 - INFO - __main__ - Epoch 157, Step:  341500, Batch Loss:     1.529283, Lr: 0.000021, Tokens per sec:   3591
2023-03-15 12:47:49,493 - INFO - __main__ - Epoch 157, Step:  341600, Batch Loss:     1.065569, Lr: 0.000021, Tokens per sec:   3592
2023-03-15 12:48:04,579 - INFO - __main__ - Epoch 157, Step:  341700, Batch Loss:     0.745176, Lr: 0.000021, Tokens per sec:   3609
2023-03-15 12:48:19,682 - INFO - __main__ - Epoch 157, Step:  341800, Batch Loss:     1.319144, Lr: 0.000021, Tokens per sec:   3556
2023-03-15 12:48:34,800 - INFO - __main__ - Epoch 157, Step:  341900, Batch Loss:     1.546064, Lr: 0.000021, Tokens per sec:   3596
2023-03-15 12:48:49,902 - INFO - __main__ - Epoch 157, Step:  342000, Batch Loss:     1.040304, Lr: 0.000021, Tokens per sec:   3436
2023-03-15 12:49:05,041 - INFO - __main__ - Epoch 157, Step:  342100, Batch Loss:     1.702949, Lr: 0.000021, Tokens per sec:   3582
2023-03-15 12:49:05,592 - INFO - __main__ - Epoch 157: total training loss 2753.99
2023-03-15 12:49:05,593 - INFO - __main__ - Epoch 158
2023-03-15 12:49:20,512 - INFO - __main__ - Epoch 158, Step:  342200, Batch Loss:     1.180830, Lr: 0.000021, Tokens per sec:   3526
2023-03-15 12:49:35,579 - INFO - __main__ - Epoch 158, Step:  342300, Batch Loss:     1.422586, Lr: 0.000021, Tokens per sec:   3629
2023-03-15 12:49:50,698 - INFO - __main__ - Epoch 158, Step:  342400, Batch Loss:     0.809451, Lr: 0.000021, Tokens per sec:   3558
2023-03-15 12:50:05,854 - INFO - __main__ - Epoch 158, Step:  342500, Batch Loss:     1.386154, Lr: 0.000021, Tokens per sec:   3620
2023-03-15 12:50:20,892 - INFO - __main__ - Epoch 158, Step:  342600, Batch Loss:     1.261340, Lr: 0.000021, Tokens per sec:   3566
2023-03-15 12:50:36,009 - INFO - __main__ - Epoch 158, Step:  342700, Batch Loss:     1.157262, Lr: 0.000021, Tokens per sec:   3532
2023-03-15 12:50:51,067 - INFO - __main__ - Epoch 158, Step:  342800, Batch Loss:     1.494391, Lr: 0.000021, Tokens per sec:   3534
2023-03-15 12:51:06,189 - INFO - __main__ - Epoch 158, Step:  342900, Batch Loss:     1.858718, Lr: 0.000021, Tokens per sec:   3609
2023-03-15 12:51:21,297 - INFO - __main__ - Epoch 158, Step:  343000, Batch Loss:     1.137690, Lr: 0.000021, Tokens per sec:   3551
2023-03-15 12:51:36,400 - INFO - __main__ - Epoch 158, Step:  343100, Batch Loss:     1.038303, Lr: 0.000021, Tokens per sec:   3602
2023-03-15 12:51:51,467 - INFO - __main__ - Epoch 158, Step:  343200, Batch Loss:     1.791317, Lr: 0.000021, Tokens per sec:   3567
2023-03-15 12:52:06,538 - INFO - __main__ - Epoch 158, Step:  343300, Batch Loss:     0.857255, Lr: 0.000021, Tokens per sec:   3599
2023-03-15 12:52:21,799 - INFO - __main__ - Epoch 158, Step:  343400, Batch Loss:     1.647705, Lr: 0.000021, Tokens per sec:   3505
2023-03-15 12:52:36,910 - INFO - __main__ - Epoch 158, Step:  343500, Batch Loss:     1.293687, Lr: 0.000021, Tokens per sec:   3580
2023-03-15 12:52:51,966 - INFO - __main__ - Epoch 158, Step:  343600, Batch Loss:     1.716226, Lr: 0.000021, Tokens per sec:   3502
2023-03-15 12:53:07,066 - INFO - __main__ - Epoch 158, Step:  343700, Batch Loss:     0.532236, Lr: 0.000021, Tokens per sec:   3526
2023-03-15 12:53:22,173 - INFO - __main__ - Epoch 158, Step:  343800, Batch Loss:     1.481735, Lr: 0.000021, Tokens per sec:   3559
2023-03-15 12:53:37,272 - INFO - __main__ - Epoch 158, Step:  343900, Batch Loss:     1.095181, Lr: 0.000021, Tokens per sec:   3545
2023-03-15 12:53:52,268 - INFO - __main__ - Epoch 158, Step:  344000, Batch Loss:     1.681059, Lr: 0.000021, Tokens per sec:   3588
2023-03-15 12:54:07,324 - INFO - __main__ - Epoch 158, Step:  344100, Batch Loss:     0.522968, Lr: 0.000021, Tokens per sec:   3582
2023-03-15 12:54:22,422 - INFO - __main__ - Epoch 158, Step:  344200, Batch Loss:     1.036402, Lr: 0.000021, Tokens per sec:   3593
2023-03-15 12:54:34,849 - INFO - __main__ - Epoch 158: total training loss 2769.14
2023-03-15 12:54:34,850 - INFO - __main__ - Epoch 159
2023-03-15 12:54:37,887 - INFO - __main__ - Epoch 159, Step:  344300, Batch Loss:     1.297352, Lr: 0.000020, Tokens per sec:   3266
2023-03-15 12:54:52,928 - INFO - __main__ - Epoch 159, Step:  344400, Batch Loss:     1.142558, Lr: 0.000020, Tokens per sec:   3599
2023-03-15 12:55:08,026 - INFO - __main__ - Epoch 159, Step:  344500, Batch Loss:     1.313742, Lr: 0.000020, Tokens per sec:   3563
2023-03-15 12:55:23,113 - INFO - __main__ - Epoch 159, Step:  344600, Batch Loss:     1.857784, Lr: 0.000020, Tokens per sec:   3529
2023-03-15 12:55:38,224 - INFO - __main__ - Epoch 159, Step:  344700, Batch Loss:     0.968052, Lr: 0.000020, Tokens per sec:   3620
2023-03-15 12:55:53,244 - INFO - __main__ - Epoch 159, Step:  344800, Batch Loss:     1.707026, Lr: 0.000020, Tokens per sec:   3591
2023-03-15 12:56:08,313 - INFO - __main__ - Epoch 159, Step:  344900, Batch Loss:     1.624902, Lr: 0.000020, Tokens per sec:   3572
2023-03-15 12:56:23,354 - INFO - __main__ - Epoch 159, Step:  345000, Batch Loss:     1.034784, Lr: 0.000020, Tokens per sec:   3544
2023-03-15 12:56:38,506 - INFO - __main__ - Epoch 159, Step:  345100, Batch Loss:     1.311444, Lr: 0.000020, Tokens per sec:   3531
2023-03-15 12:56:53,565 - INFO - __main__ - Epoch 159, Step:  345200, Batch Loss:     1.391539, Lr: 0.000020, Tokens per sec:   3576
2023-03-15 12:57:08,694 - INFO - __main__ - Epoch 159, Step:  345300, Batch Loss:     1.342918, Lr: 0.000020, Tokens per sec:   3590
2023-03-15 12:57:23,729 - INFO - __main__ - Epoch 159, Step:  345400, Batch Loss:     1.169440, Lr: 0.000020, Tokens per sec:   3581
2023-03-15 12:57:38,817 - INFO - __main__ - Epoch 159, Step:  345500, Batch Loss:     1.161633, Lr: 0.000020, Tokens per sec:   3621
2023-03-15 12:57:53,915 - INFO - __main__ - Epoch 159, Step:  345600, Batch Loss:     1.498096, Lr: 0.000020, Tokens per sec:   3535
2023-03-15 12:58:08,971 - INFO - __main__ - Epoch 159, Step:  345700, Batch Loss:     1.563159, Lr: 0.000020, Tokens per sec:   3565
2023-03-15 12:58:24,052 - INFO - __main__ - Epoch 159, Step:  345800, Batch Loss:     1.445360, Lr: 0.000020, Tokens per sec:   3583
2023-03-15 12:58:39,127 - INFO - __main__ - Epoch 159, Step:  345900, Batch Loss:     1.242824, Lr: 0.000020, Tokens per sec:   3508
2023-03-15 12:58:54,235 - INFO - __main__ - Epoch 159, Step:  346000, Batch Loss:     0.892431, Lr: 0.000020, Tokens per sec:   3508
2023-03-15 12:59:09,315 - INFO - __main__ - Epoch 159, Step:  346100, Batch Loss:     1.414352, Lr: 0.000020, Tokens per sec:   3615
2023-03-15 12:59:24,392 - INFO - __main__ - Epoch 159, Step:  346200, Batch Loss:     1.850552, Lr: 0.000020, Tokens per sec:   3595
2023-03-15 12:59:39,470 - INFO - __main__ - Epoch 159, Step:  346300, Batch Loss:     0.623785, Lr: 0.000020, Tokens per sec:   3550
2023-03-15 12:59:54,570 - INFO - __main__ - Epoch 159, Step:  346400, Batch Loss:     0.844918, Lr: 0.000020, Tokens per sec:   3541
2023-03-15 13:00:03,864 - INFO - __main__ - Epoch 159: total training loss 2712.98
2023-03-15 13:00:03,865 - INFO - __main__ - Epoch 160
2023-03-15 13:00:10,135 - INFO - __main__ - Epoch 160, Step:  346500, Batch Loss:     1.393026, Lr: 0.000020, Tokens per sec:   3399
2023-03-15 13:00:25,215 - INFO - __main__ - Epoch 160, Step:  346600, Batch Loss:     1.300825, Lr: 0.000020, Tokens per sec:   3531
2023-03-15 13:00:40,285 - INFO - __main__ - Epoch 160, Step:  346700, Batch Loss:     1.154377, Lr: 0.000020, Tokens per sec:   3571
2023-03-15 13:00:55,418 - INFO - __main__ - Epoch 160, Step:  346800, Batch Loss:     1.524655, Lr: 0.000020, Tokens per sec:   3472
2023-03-15 13:01:10,564 - INFO - __main__ - Epoch 160, Step:  346900, Batch Loss:     1.019641, Lr: 0.000020, Tokens per sec:   3502
2023-03-15 13:01:25,663 - INFO - __main__ - Epoch 160, Step:  347000, Batch Loss:     0.973784, Lr: 0.000020, Tokens per sec:   3552
2023-03-15 13:01:40,725 - INFO - __main__ - Epoch 160, Step:  347100, Batch Loss:     1.024199, Lr: 0.000020, Tokens per sec:   3601
2023-03-15 13:01:55,806 - INFO - __main__ - Epoch 160, Step:  347200, Batch Loss:     1.203369, Lr: 0.000020, Tokens per sec:   3611
2023-03-15 13:02:10,945 - INFO - __main__ - Epoch 160, Step:  347300, Batch Loss:     1.667102, Lr: 0.000020, Tokens per sec:   3578
2023-03-15 13:02:26,016 - INFO - __main__ - Epoch 160, Step:  347400, Batch Loss:     1.145201, Lr: 0.000020, Tokens per sec:   3619
2023-03-15 13:02:41,171 - INFO - __main__ - Epoch 160, Step:  347500, Batch Loss:     1.363165, Lr: 0.000020, Tokens per sec:   3557
2023-03-15 13:02:56,214 - INFO - __main__ - Epoch 160, Step:  347600, Batch Loss:     0.902822, Lr: 0.000020, Tokens per sec:   3610
2023-03-15 13:03:11,242 - INFO - __main__ - Epoch 160, Step:  347700, Batch Loss:     1.274377, Lr: 0.000020, Tokens per sec:   3620
2023-03-15 13:03:26,283 - INFO - __main__ - Epoch 160, Step:  347800, Batch Loss:     1.801211, Lr: 0.000020, Tokens per sec:   3501
2023-03-15 13:03:41,380 - INFO - __main__ - Epoch 160, Step:  347900, Batch Loss:     1.807734, Lr: 0.000020, Tokens per sec:   3656
2023-03-15 13:03:56,481 - INFO - __main__ - Epoch 160, Step:  348000, Batch Loss:     1.524364, Lr: 0.000020, Tokens per sec:   3554
2023-03-15 13:04:11,619 - INFO - __main__ - Epoch 160, Step:  348100, Batch Loss:     1.625782, Lr: 0.000020, Tokens per sec:   3515
2023-03-15 13:04:26,848 - INFO - __main__ - Epoch 160, Step:  348200, Batch Loss:     1.428886, Lr: 0.000020, Tokens per sec:   3564
2023-03-15 13:04:41,961 - INFO - __main__ - Epoch 160, Step:  348300, Batch Loss:     1.042820, Lr: 0.000020, Tokens per sec:   3595
2023-03-15 13:04:57,100 - INFO - __main__ - Epoch 160, Step:  348400, Batch Loss:     0.971576, Lr: 0.000020, Tokens per sec:   3515
2023-03-15 13:05:12,141 - INFO - __main__ - Epoch 160, Step:  348500, Batch Loss:     1.649827, Lr: 0.000020, Tokens per sec:   3596
2023-03-15 13:05:27,221 - INFO - __main__ - Epoch 160, Step:  348600, Batch Loss:     0.921541, Lr: 0.000020, Tokens per sec:   3554
2023-03-15 13:05:33,404 - INFO - __main__ - Epoch 160: total training loss 2694.14
2023-03-15 13:05:33,405 - INFO - __main__ - Epoch 161
2023-03-15 13:05:42,790 - INFO - __main__ - Epoch 161, Step:  348700, Batch Loss:     1.186036, Lr: 0.000020, Tokens per sec:   3441
2023-03-15 13:05:57,886 - INFO - __main__ - Epoch 161, Step:  348800, Batch Loss:     1.130641, Lr: 0.000020, Tokens per sec:   3624
2023-03-15 13:06:13,017 - INFO - __main__ - Epoch 161, Step:  348900, Batch Loss:     1.307392, Lr: 0.000020, Tokens per sec:   3552
2023-03-15 13:06:28,112 - INFO - __main__ - Epoch 161, Step:  349000, Batch Loss:     1.467920, Lr: 0.000020, Tokens per sec:   3564
2023-03-15 13:06:43,237 - INFO - __main__ - Epoch 161, Step:  349100, Batch Loss:     1.211909, Lr: 0.000020, Tokens per sec:   3539
2023-03-15 13:06:58,376 - INFO - __main__ - Epoch 161, Step:  349200, Batch Loss:     1.626497, Lr: 0.000020, Tokens per sec:   3557
2023-03-15 13:07:13,480 - INFO - __main__ - Epoch 161, Step:  349300, Batch Loss:     0.728449, Lr: 0.000020, Tokens per sec:   3503
2023-03-15 13:07:28,659 - INFO - __main__ - Epoch 161, Step:  349400, Batch Loss:     1.163752, Lr: 0.000020, Tokens per sec:   3560
2023-03-15 13:07:43,769 - INFO - __main__ - Epoch 161, Step:  349500, Batch Loss:     1.018433, Lr: 0.000020, Tokens per sec:   3553
2023-03-15 13:07:58,788 - INFO - __main__ - Epoch 161, Step:  349600, Batch Loss:     1.201622, Lr: 0.000020, Tokens per sec:   3569
2023-03-15 13:08:13,882 - INFO - __main__ - Epoch 161, Step:  349700, Batch Loss:     0.897171, Lr: 0.000020, Tokens per sec:   3563
2023-03-15 13:08:28,938 - INFO - __main__ - Epoch 161, Step:  349800, Batch Loss:     0.790045, Lr: 0.000020, Tokens per sec:   3568
2023-03-15 13:08:44,052 - INFO - __main__ - Epoch 161, Step:  349900, Batch Loss:     2.125388, Lr: 0.000020, Tokens per sec:   3498
2023-03-15 13:08:59,182 - INFO - __main__ - Epoch 161, Step:  350000, Batch Loss:     0.940014, Lr: 0.000020, Tokens per sec:   3543
2023-03-15 13:09:14,250 - INFO - __main__ - Epoch 161, Step:  350100, Batch Loss:     1.804896, Lr: 0.000020, Tokens per sec:   3602
2023-03-15 13:09:29,351 - INFO - __main__ - Epoch 161, Step:  350200, Batch Loss:     0.643316, Lr: 0.000020, Tokens per sec:   3627
2023-03-15 13:09:44,424 - INFO - __main__ - Epoch 161, Step:  350300, Batch Loss:     1.158301, Lr: 0.000020, Tokens per sec:   3559
2023-03-15 13:09:59,551 - INFO - __main__ - Epoch 161, Step:  350400, Batch Loss:     0.895274, Lr: 0.000020, Tokens per sec:   3609
2023-03-15 13:10:14,636 - INFO - __main__ - Epoch 161, Step:  350500, Batch Loss:     1.581523, Lr: 0.000020, Tokens per sec:   3590
2023-03-15 13:10:29,722 - INFO - __main__ - Epoch 161, Step:  350600, Batch Loss:     1.246473, Lr: 0.000020, Tokens per sec:   3621
2023-03-15 13:10:44,799 - INFO - __main__ - Epoch 161, Step:  350700, Batch Loss:     1.171452, Lr: 0.000020, Tokens per sec:   3523
2023-03-15 13:10:59,891 - INFO - __main__ - Epoch 161, Step:  350800, Batch Loss:     1.437295, Lr: 0.000020, Tokens per sec:   3536
2023-03-15 13:11:02,806 - INFO - __main__ - Epoch 161: total training loss 2670.86
2023-03-15 13:11:02,807 - INFO - __main__ - Epoch 162
2023-03-15 13:11:15,471 - INFO - __main__ - Epoch 162, Step:  350900, Batch Loss:     1.129342, Lr: 0.000020, Tokens per sec:   3440
2023-03-15 13:11:30,549 - INFO - __main__ - Epoch 162, Step:  351000, Batch Loss:     1.326472, Lr: 0.000020, Tokens per sec:   3568
2023-03-15 13:11:45,662 - INFO - __main__ - Epoch 162, Step:  351100, Batch Loss:     1.063079, Lr: 0.000020, Tokens per sec:   3574
2023-03-15 13:12:00,719 - INFO - __main__ - Epoch 162, Step:  351200, Batch Loss:     1.322305, Lr: 0.000020, Tokens per sec:   3571
2023-03-15 13:12:15,775 - INFO - __main__ - Epoch 162, Step:  351300, Batch Loss:     1.448342, Lr: 0.000020, Tokens per sec:   3515
2023-03-15 13:12:30,870 - INFO - __main__ - Epoch 162, Step:  351400, Batch Loss:     1.337788, Lr: 0.000020, Tokens per sec:   3595
2023-03-15 13:12:46,034 - INFO - __main__ - Epoch 162, Step:  351500, Batch Loss:     1.520503, Lr: 0.000020, Tokens per sec:   3580
2023-03-15 13:13:01,142 - INFO - __main__ - Epoch 162, Step:  351600, Batch Loss:     0.732839, Lr: 0.000020, Tokens per sec:   3544
2023-03-15 13:13:16,225 - INFO - __main__ - Epoch 162, Step:  351700, Batch Loss:     0.801365, Lr: 0.000020, Tokens per sec:   3521
2023-03-15 13:13:31,306 - INFO - __main__ - Epoch 162, Step:  351800, Batch Loss:     1.331669, Lr: 0.000020, Tokens per sec:   3627
2023-03-15 13:13:46,379 - INFO - __main__ - Epoch 162, Step:  351900, Batch Loss:     1.336064, Lr: 0.000020, Tokens per sec:   3591
2023-03-15 13:14:01,412 - INFO - __main__ - Epoch 162, Step:  352000, Batch Loss:     1.386986, Lr: 0.000020, Tokens per sec:   3560
2023-03-15 13:14:16,560 - INFO - __main__ - Epoch 162, Step:  352100, Batch Loss:     2.399300, Lr: 0.000020, Tokens per sec:   3569
2023-03-15 13:14:31,665 - INFO - __main__ - Epoch 162, Step:  352200, Batch Loss:     1.449890, Lr: 0.000020, Tokens per sec:   3563
2023-03-15 13:14:46,753 - INFO - __main__ - Epoch 162, Step:  352300, Batch Loss:     1.999576, Lr: 0.000020, Tokens per sec:   3597
2023-03-15 13:15:01,889 - INFO - __main__ - Epoch 162, Step:  352400, Batch Loss:     1.098296, Lr: 0.000020, Tokens per sec:   3515
2023-03-15 13:15:16,970 - INFO - __main__ - Epoch 162, Step:  352500, Batch Loss:     1.368006, Lr: 0.000020, Tokens per sec:   3645
2023-03-15 13:15:32,046 - INFO - __main__ - Epoch 162, Step:  352600, Batch Loss:     1.584160, Lr: 0.000020, Tokens per sec:   3556
2023-03-15 13:15:47,142 - INFO - __main__ - Epoch 162, Step:  352700, Batch Loss:     0.723321, Lr: 0.000020, Tokens per sec:   3536
2023-03-15 13:16:02,219 - INFO - __main__ - Epoch 162, Step:  352800, Batch Loss:     1.328315, Lr: 0.000020, Tokens per sec:   3605
2023-03-15 13:16:17,429 - INFO - __main__ - Epoch 162, Step:  352900, Batch Loss:     1.592463, Lr: 0.000020, Tokens per sec:   3539
2023-03-15 13:16:32,306 - INFO - __main__ - Epoch 162: total training loss 2647.52
2023-03-15 13:16:32,307 - INFO - __main__ - Epoch 163
2023-03-15 13:16:32,931 - INFO - __main__ - Epoch 163, Step:  353000, Batch Loss:     0.815346, Lr: 0.000020, Tokens per sec:   1873
2023-03-15 13:16:47,950 - INFO - __main__ - Epoch 163, Step:  353100, Batch Loss:     1.015802, Lr: 0.000020, Tokens per sec:   3572
2023-03-15 13:17:02,996 - INFO - __main__ - Epoch 163, Step:  353200, Batch Loss:     1.306478, Lr: 0.000020, Tokens per sec:   3612
2023-03-15 13:17:18,205 - INFO - __main__ - Epoch 163, Step:  353300, Batch Loss:     1.672437, Lr: 0.000020, Tokens per sec:   3496
2023-03-15 13:17:33,333 - INFO - __main__ - Epoch 163, Step:  353400, Batch Loss:     0.349314, Lr: 0.000020, Tokens per sec:   3567
2023-03-15 13:17:48,428 - INFO - __main__ - Epoch 163, Step:  353500, Batch Loss:     1.464418, Lr: 0.000020, Tokens per sec:   3579
2023-03-15 13:18:03,487 - INFO - __main__ - Epoch 163, Step:  353600, Batch Loss:     1.029907, Lr: 0.000020, Tokens per sec:   3600
2023-03-15 13:18:18,545 - INFO - __main__ - Epoch 163, Step:  353700, Batch Loss:     1.285517, Lr: 0.000020, Tokens per sec:   3559
2023-03-15 13:18:33,677 - INFO - __main__ - Epoch 163, Step:  353800, Batch Loss:     1.009534, Lr: 0.000020, Tokens per sec:   3529
2023-03-15 13:18:48,761 - INFO - __main__ - Epoch 163, Step:  353900, Batch Loss:     1.413110, Lr: 0.000020, Tokens per sec:   3576
2023-03-15 13:19:03,857 - INFO - __main__ - Epoch 163, Step:  354000, Batch Loss:     1.081010, Lr: 0.000020, Tokens per sec:   3590
2023-03-15 13:19:18,904 - INFO - __main__ - Epoch 163, Step:  354100, Batch Loss:     1.467730, Lr: 0.000020, Tokens per sec:   3586
2023-03-15 13:19:34,024 - INFO - __main__ - Epoch 163, Step:  354200, Batch Loss:     1.064538, Lr: 0.000020, Tokens per sec:   3535
2023-03-15 13:19:49,103 - INFO - __main__ - Epoch 163, Step:  354300, Batch Loss:     1.613919, Lr: 0.000020, Tokens per sec:   3564
2023-03-15 13:20:04,116 - INFO - __main__ - Epoch 163, Step:  354400, Batch Loss:     1.145550, Lr: 0.000020, Tokens per sec:   3547
2023-03-15 13:20:19,132 - INFO - __main__ - Epoch 163, Step:  354500, Batch Loss:     0.641428, Lr: 0.000020, Tokens per sec:   3530
2023-03-15 13:20:34,170 - INFO - __main__ - Epoch 163, Step:  354600, Batch Loss:     0.859703, Lr: 0.000020, Tokens per sec:   3564
2023-03-15 13:20:49,199 - INFO - __main__ - Epoch 163, Step:  354700, Batch Loss:     0.787970, Lr: 0.000020, Tokens per sec:   3552
2023-03-15 13:21:04,254 - INFO - __main__ - Epoch 163, Step:  354800, Batch Loss:     1.134480, Lr: 0.000020, Tokens per sec:   3571
2023-03-15 13:21:19,321 - INFO - __main__ - Epoch 163, Step:  354900, Batch Loss:     0.783066, Lr: 0.000020, Tokens per sec:   3657
2023-03-15 13:21:34,360 - INFO - __main__ - Epoch 163, Step:  355000, Batch Loss:     1.147325, Lr: 0.000020, Tokens per sec:   3620
2023-03-15 13:21:49,414 - INFO - __main__ - Epoch 163, Step:  355100, Batch Loss:     1.207522, Lr: 0.000020, Tokens per sec:   3605
2023-03-15 13:22:01,124 - INFO - __main__ - Epoch 163: total training loss 2639.13
2023-03-15 13:22:01,125 - INFO - __main__ - Epoch 164
2023-03-15 13:22:04,921 - INFO - __main__ - Epoch 164, Step:  355200, Batch Loss:     0.981963, Lr: 0.000019, Tokens per sec:   3171
2023-03-15 13:22:20,026 - INFO - __main__ - Epoch 164, Step:  355300, Batch Loss:     1.600916, Lr: 0.000019, Tokens per sec:   3522
2023-03-15 13:22:35,103 - INFO - __main__ - Epoch 164, Step:  355400, Batch Loss:     0.843635, Lr: 0.000019, Tokens per sec:   3554
2023-03-15 13:22:50,210 - INFO - __main__ - Epoch 164, Step:  355500, Batch Loss:     1.248723, Lr: 0.000019, Tokens per sec:   3590
2023-03-15 13:23:05,319 - INFO - __main__ - Epoch 164, Step:  355600, Batch Loss:     1.014359, Lr: 0.000019, Tokens per sec:   3596
2023-03-15 13:23:20,390 - INFO - __main__ - Epoch 164, Step:  355700, Batch Loss:     0.980866, Lr: 0.000019, Tokens per sec:   3598
2023-03-15 13:23:35,433 - INFO - __main__ - Epoch 164, Step:  355800, Batch Loss:     0.838865, Lr: 0.000019, Tokens per sec:   3551
2023-03-15 13:23:50,522 - INFO - __main__ - Epoch 164, Step:  355900, Batch Loss:     1.439104, Lr: 0.000019, Tokens per sec:   3601
2023-03-15 13:24:05,550 - INFO - __main__ - Epoch 164, Step:  356000, Batch Loss:     0.977645, Lr: 0.000019, Tokens per sec:   3541
2023-03-15 13:24:20,548 - INFO - __main__ - Epoch 164, Step:  356100, Batch Loss:     1.782365, Lr: 0.000019, Tokens per sec:   3632
2023-03-15 13:24:35,598 - INFO - __main__ - Epoch 164, Step:  356200, Batch Loss:     1.151033, Lr: 0.000019, Tokens per sec:   3589
2023-03-15 13:24:50,785 - INFO - __main__ - Epoch 164, Step:  356300, Batch Loss:     0.963404, Lr: 0.000019, Tokens per sec:   3569
2023-03-15 13:25:05,855 - INFO - __main__ - Epoch 164, Step:  356400, Batch Loss:     1.160672, Lr: 0.000019, Tokens per sec:   3586
2023-03-15 13:25:20,977 - INFO - __main__ - Epoch 164, Step:  356500, Batch Loss:     1.126178, Lr: 0.000019, Tokens per sec:   3573
2023-03-15 13:25:36,134 - INFO - __main__ - Epoch 164, Step:  356600, Batch Loss:     1.014421, Lr: 0.000019, Tokens per sec:   3536
2023-03-15 13:25:51,262 - INFO - __main__ - Epoch 164, Step:  356700, Batch Loss:     1.877685, Lr: 0.000019, Tokens per sec:   3526
2023-03-15 13:26:06,274 - INFO - __main__ - Epoch 164, Step:  356800, Batch Loss:     1.674943, Lr: 0.000019, Tokens per sec:   3581
2023-03-15 13:26:21,335 - INFO - __main__ - Epoch 164, Step:  356900, Batch Loss:     1.056497, Lr: 0.000019, Tokens per sec:   3593
2023-03-15 13:26:36,404 - INFO - __main__ - Epoch 164, Step:  357000, Batch Loss:     1.733914, Lr: 0.000019, Tokens per sec:   3572
2023-03-15 13:26:51,511 - INFO - __main__ - Epoch 164, Step:  357100, Batch Loss:     2.092107, Lr: 0.000019, Tokens per sec:   3606
2023-03-15 13:27:06,575 - INFO - __main__ - Epoch 164, Step:  357200, Batch Loss:     1.105243, Lr: 0.000019, Tokens per sec:   3554
2023-03-15 13:27:21,602 - INFO - __main__ - Epoch 164, Step:  357300, Batch Loss:     0.820103, Lr: 0.000019, Tokens per sec:   3559
2023-03-15 13:27:30,117 - INFO - __main__ - Epoch 164: total training loss 2597.64
2023-03-15 13:27:30,118 - INFO - __main__ - Epoch 165
2023-03-15 13:27:37,068 - INFO - __main__ - Epoch 165, Step:  357400, Batch Loss:     1.025875, Lr: 0.000019, Tokens per sec:   3490
2023-03-15 13:27:52,082 - INFO - __main__ - Epoch 165, Step:  357500, Batch Loss:     1.240928, Lr: 0.000019, Tokens per sec:   3574
2023-03-15 13:28:07,267 - INFO - __main__ - Epoch 165, Step:  357600, Batch Loss:     1.357908, Lr: 0.000019, Tokens per sec:   3526
2023-03-15 13:28:22,366 - INFO - __main__ - Epoch 165, Step:  357700, Batch Loss:     1.172710, Lr: 0.000019, Tokens per sec:   3522
2023-03-15 13:28:37,438 - INFO - __main__ - Epoch 165, Step:  357800, Batch Loss:     1.694629, Lr: 0.000019, Tokens per sec:   3523
2023-03-15 13:28:52,537 - INFO - __main__ - Epoch 165, Step:  357900, Batch Loss:     1.546893, Lr: 0.000019, Tokens per sec:   3529
2023-03-15 13:29:07,582 - INFO - __main__ - Epoch 165, Step:  358000, Batch Loss:     1.148808, Lr: 0.000019, Tokens per sec:   3569
2023-03-15 13:29:22,647 - INFO - __main__ - Epoch 165, Step:  358100, Batch Loss:     0.910389, Lr: 0.000019, Tokens per sec:   3574
2023-03-15 13:29:37,762 - INFO - __main__ - Epoch 165, Step:  358200, Batch Loss:     1.070193, Lr: 0.000019, Tokens per sec:   3576
2023-03-15 13:29:52,824 - INFO - __main__ - Epoch 165, Step:  358300, Batch Loss:     1.163872, Lr: 0.000019, Tokens per sec:   3618
2023-03-15 13:30:07,888 - INFO - __main__ - Epoch 165, Step:  358400, Batch Loss:     1.533287, Lr: 0.000019, Tokens per sec:   3521
2023-03-15 13:30:22,956 - INFO - __main__ - Epoch 165, Step:  358500, Batch Loss:     0.852657, Lr: 0.000019, Tokens per sec:   3575
2023-03-15 13:30:38,023 - INFO - __main__ - Epoch 165, Step:  358600, Batch Loss:     1.055955, Lr: 0.000019, Tokens per sec:   3580
2023-03-15 13:30:53,061 - INFO - __main__ - Epoch 165, Step:  358700, Batch Loss:     1.288952, Lr: 0.000019, Tokens per sec:   3532
2023-03-15 13:31:08,055 - INFO - __main__ - Epoch 165, Step:  358800, Batch Loss:     1.341795, Lr: 0.000019, Tokens per sec:   3649
2023-03-15 13:31:23,143 - INFO - __main__ - Epoch 165, Step:  358900, Batch Loss:     1.441128, Lr: 0.000019, Tokens per sec:   3618
2023-03-15 13:31:38,180 - INFO - __main__ - Epoch 165, Step:  359000, Batch Loss:     0.731069, Lr: 0.000019, Tokens per sec:   3583
2023-03-15 13:31:53,311 - INFO - __main__ - Epoch 165, Step:  359100, Batch Loss:     0.939336, Lr: 0.000019, Tokens per sec:   3562
2023-03-15 13:32:08,352 - INFO - __main__ - Epoch 165, Step:  359200, Batch Loss:     1.783339, Lr: 0.000019, Tokens per sec:   3505
2023-03-15 13:32:23,400 - INFO - __main__ - Epoch 165, Step:  359300, Batch Loss:     1.683614, Lr: 0.000019, Tokens per sec:   3616
2023-03-15 13:32:38,491 - INFO - __main__ - Epoch 165, Step:  359400, Batch Loss:     1.046084, Lr: 0.000019, Tokens per sec:   3541
2023-03-15 13:32:53,571 - INFO - __main__ - Epoch 165, Step:  359500, Batch Loss:     1.443213, Lr: 0.000019, Tokens per sec:   3650
2023-03-15 13:32:58,962 - INFO - __main__ - Epoch 165: total training loss 2591.01
2023-03-15 13:32:58,963 - INFO - __main__ - Epoch 166
2023-03-15 13:33:09,090 - INFO - __main__ - Epoch 166, Step:  359600, Batch Loss:     0.854531, Lr: 0.000019, Tokens per sec:   3421
2023-03-15 13:33:24,169 - INFO - __main__ - Epoch 166, Step:  359700, Batch Loss:     1.467926, Lr: 0.000019, Tokens per sec:   3521
2023-03-15 13:33:39,185 - INFO - __main__ - Epoch 166, Step:  359800, Batch Loss:     1.290176, Lr: 0.000019, Tokens per sec:   3622
2023-03-15 13:33:54,242 - INFO - __main__ - Epoch 166, Step:  359900, Batch Loss:     0.945410, Lr: 0.000019, Tokens per sec:   3625
2023-03-15 13:34:09,338 - INFO - __main__ - Epoch 166, Step:  360000, Batch Loss:     0.641940, Lr: 0.000019, Tokens per sec:   3628
2023-03-15 13:34:24,395 - INFO - __main__ - Epoch 166, Step:  360100, Batch Loss:     1.199408, Lr: 0.000019, Tokens per sec:   3558
2023-03-15 13:34:39,467 - INFO - __main__ - Epoch 166, Step:  360200, Batch Loss:     1.341783, Lr: 0.000019, Tokens per sec:   3589
2023-03-15 13:34:54,610 - INFO - __main__ - Epoch 166, Step:  360300, Batch Loss:     1.342147, Lr: 0.000019, Tokens per sec:   3613
2023-03-15 13:35:09,612 - INFO - __main__ - Epoch 166, Step:  360400, Batch Loss:     0.991353, Lr: 0.000019, Tokens per sec:   3612
2023-03-15 13:35:24,636 - INFO - __main__ - Epoch 166, Step:  360500, Batch Loss:     0.614588, Lr: 0.000019, Tokens per sec:   3566
2023-03-15 13:35:39,687 - INFO - __main__ - Epoch 166, Step:  360600, Batch Loss:     1.254962, Lr: 0.000019, Tokens per sec:   3493
2023-03-15 13:35:54,853 - INFO - __main__ - Epoch 166, Step:  360700, Batch Loss:     1.407585, Lr: 0.000019, Tokens per sec:   3530
2023-03-15 13:36:09,879 - INFO - __main__ - Epoch 166, Step:  360800, Batch Loss:     1.407308, Lr: 0.000019, Tokens per sec:   3558
2023-03-15 13:36:25,053 - INFO - __main__ - Epoch 166, Step:  360900, Batch Loss:     0.987291, Lr: 0.000019, Tokens per sec:   3560
2023-03-15 13:36:40,190 - INFO - __main__ - Epoch 166, Step:  361000, Batch Loss:     0.708532, Lr: 0.000019, Tokens per sec:   3562
2023-03-15 13:36:55,315 - INFO - __main__ - Epoch 166, Step:  361100, Batch Loss:     0.851849, Lr: 0.000019, Tokens per sec:   3522
2023-03-15 13:37:10,445 - INFO - __main__ - Epoch 166, Step:  361200, Batch Loss:     0.755586, Lr: 0.000019, Tokens per sec:   3594
2023-03-15 13:37:25,527 - INFO - __main__ - Epoch 166, Step:  361300, Batch Loss:     2.018548, Lr: 0.000019, Tokens per sec:   3533
2023-03-15 13:37:40,625 - INFO - __main__ - Epoch 166, Step:  361400, Batch Loss:     1.353312, Lr: 0.000019, Tokens per sec:   3640
2023-03-15 13:37:55,873 - INFO - __main__ - Epoch 166, Step:  361500, Batch Loss:     1.273709, Lr: 0.000019, Tokens per sec:   3550
2023-03-15 13:38:11,161 - INFO - __main__ - Epoch 166, Step:  361600, Batch Loss:     1.445220, Lr: 0.000019, Tokens per sec:   3467
2023-03-15 13:38:26,603 - INFO - __main__ - Epoch 166, Step:  361700, Batch Loss:     1.809498, Lr: 0.000019, Tokens per sec:   3452
2023-03-15 13:38:28,895 - INFO - __main__ - Epoch 166: total training loss 2567.72
2023-03-15 13:38:28,896 - INFO - __main__ - Epoch 167
2023-03-15 13:38:42,417 - INFO - __main__ - Epoch 167, Step:  361800, Batch Loss:     0.804989, Lr: 0.000019, Tokens per sec:   3391
2023-03-15 13:38:57,949 - INFO - __main__ - Epoch 167, Step:  361900, Batch Loss:     0.922148, Lr: 0.000019, Tokens per sec:   3453
2023-03-15 13:39:13,857 - INFO - __main__ - Epoch 167, Step:  362000, Batch Loss:     0.543222, Lr: 0.000019, Tokens per sec:   3393
2023-03-15 13:39:29,366 - INFO - __main__ - Epoch 167, Step:  362100, Batch Loss:     1.648250, Lr: 0.000019, Tokens per sec:   3541
2023-03-15 13:39:44,874 - INFO - __main__ - Epoch 167, Step:  362200, Batch Loss:     0.767517, Lr: 0.000019, Tokens per sec:   3491
2023-03-15 13:40:00,474 - INFO - __main__ - Epoch 167, Step:  362300, Batch Loss:     1.574230, Lr: 0.000019, Tokens per sec:   3466
2023-03-15 13:40:16,173 - INFO - __main__ - Epoch 167, Step:  362400, Batch Loss:     0.930783, Lr: 0.000019, Tokens per sec:   3430
2023-03-15 13:40:31,755 - INFO - __main__ - Epoch 167, Step:  362500, Batch Loss:     0.866900, Lr: 0.000019, Tokens per sec:   3468
2023-03-15 13:40:47,304 - INFO - __main__ - Epoch 167, Step:  362600, Batch Loss:     1.221267, Lr: 0.000019, Tokens per sec:   3419
2023-03-15 13:41:02,811 - INFO - __main__ - Epoch 167, Step:  362700, Batch Loss:     0.719755, Lr: 0.000019, Tokens per sec:   3436
2023-03-15 13:41:18,373 - INFO - __main__ - Epoch 167, Step:  362800, Batch Loss:     0.768088, Lr: 0.000019, Tokens per sec:   3492
2023-03-15 13:41:33,741 - INFO - __main__ - Epoch 167, Step:  362900, Batch Loss:     1.053588, Lr: 0.000019, Tokens per sec:   3555
2023-03-15 13:41:49,042 - INFO - __main__ - Epoch 167, Step:  363000, Batch Loss:     1.084597, Lr: 0.000019, Tokens per sec:   3482
2023-03-15 13:42:04,458 - INFO - __main__ - Epoch 167, Step:  363100, Batch Loss:     0.526522, Lr: 0.000019, Tokens per sec:   3472
2023-03-15 13:42:20,021 - INFO - __main__ - Epoch 167, Step:  363200, Batch Loss:     1.181182, Lr: 0.000019, Tokens per sec:   3482
2023-03-15 13:42:35,772 - INFO - __main__ - Epoch 167, Step:  363300, Batch Loss:     0.879917, Lr: 0.000019, Tokens per sec:   3358
2023-03-15 13:42:51,366 - INFO - __main__ - Epoch 167, Step:  363400, Batch Loss:     0.936851, Lr: 0.000019, Tokens per sec:   3478
2023-03-15 13:43:06,959 - INFO - __main__ - Epoch 167, Step:  363500, Batch Loss:     1.111859, Lr: 0.000019, Tokens per sec:   3402
2023-03-15 13:43:22,324 - INFO - __main__ - Epoch 167, Step:  363600, Batch Loss:     1.015878, Lr: 0.000019, Tokens per sec:   3485
2023-03-15 13:43:37,779 - INFO - __main__ - Epoch 167, Step:  363700, Batch Loss:     1.467278, Lr: 0.000019, Tokens per sec:   3459
2023-03-15 13:43:53,214 - INFO - __main__ - Epoch 167, Step:  363800, Batch Loss:     1.678008, Lr: 0.000019, Tokens per sec:   3494
2023-03-15 13:44:07,567 - INFO - __main__ - Epoch 167: total training loss 2522.43
2023-03-15 13:44:07,568 - INFO - __main__ - Epoch 168
2023-03-15 13:44:09,009 - INFO - __main__ - Epoch 168, Step:  363900, Batch Loss:     0.952586, Lr: 0.000019, Tokens per sec:   2643
2023-03-15 13:44:24,393 - INFO - __main__ - Epoch 168, Step:  364000, Batch Loss:     1.098144, Lr: 0.000019, Tokens per sec:   3543
2023-03-15 13:44:40,080 - INFO - __main__ - Epoch 168, Step:  364100, Batch Loss:     1.306370, Lr: 0.000019, Tokens per sec:   3424
2023-03-15 13:44:55,360 - INFO - __main__ - Epoch 168, Step:  364200, Batch Loss:     0.864704, Lr: 0.000019, Tokens per sec:   3524
2023-03-15 13:45:10,948 - INFO - __main__ - Epoch 168, Step:  364300, Batch Loss:     0.861714, Lr: 0.000019, Tokens per sec:   3403
2023-03-15 13:45:26,381 - INFO - __main__ - Epoch 168, Step:  364400, Batch Loss:     1.261901, Lr: 0.000019, Tokens per sec:   3493
2023-03-15 13:45:41,918 - INFO - __main__ - Epoch 168, Step:  364500, Batch Loss:     0.873159, Lr: 0.000019, Tokens per sec:   3503
2023-03-15 13:45:57,434 - INFO - __main__ - Epoch 168, Step:  364600, Batch Loss:     1.135329, Lr: 0.000019, Tokens per sec:   3497
2023-03-15 13:46:13,100 - INFO - __main__ - Epoch 168, Step:  364700, Batch Loss:     0.661658, Lr: 0.000019, Tokens per sec:   3427
2023-03-15 13:46:28,819 - INFO - __main__ - Epoch 168, Step:  364800, Batch Loss:     1.204951, Lr: 0.000019, Tokens per sec:   3428
2023-03-15 13:46:44,357 - INFO - __main__ - Epoch 168, Step:  364900, Batch Loss:     0.850993, Lr: 0.000019, Tokens per sec:   3453
2023-03-15 13:47:02,124 - INFO - __main__ - Epoch 168, Step:  365000, Batch Loss:     1.112168, Lr: 0.000019, Tokens per sec:   3011
2023-03-15 13:47:18,511 - INFO - __main__ - Epoch 168, Step:  365100, Batch Loss:     1.249206, Lr: 0.000019, Tokens per sec:   3318
2023-03-15 13:47:35,567 - INFO - __main__ - Epoch 168, Step:  365200, Batch Loss:     0.708731, Lr: 0.000019, Tokens per sec:   3136
2023-03-15 13:47:51,254 - INFO - __main__ - Epoch 168, Step:  365300, Batch Loss:     1.421879, Lr: 0.000019, Tokens per sec:   3481
2023-03-15 13:48:06,932 - INFO - __main__ - Epoch 168, Step:  365400, Batch Loss:     0.720150, Lr: 0.000019, Tokens per sec:   3399
2023-03-15 13:48:22,412 - INFO - __main__ - Epoch 168, Step:  365500, Batch Loss:     1.563930, Lr: 0.000019, Tokens per sec:   3431
2023-03-15 13:48:38,414 - INFO - __main__ - Epoch 168, Step:  365600, Batch Loss:     0.868850, Lr: 0.000019, Tokens per sec:   3350
2023-03-15 13:48:56,176 - INFO - __main__ - Epoch 168, Step:  365700, Batch Loss:     1.384309, Lr: 0.000019, Tokens per sec:   3052
2023-03-15 13:49:12,739 - INFO - __main__ - Epoch 168, Step:  365800, Batch Loss:     1.289464, Lr: 0.000019, Tokens per sec:   3249
2023-03-15 13:49:28,886 - INFO - __main__ - Epoch 168, Step:  365900, Batch Loss:     0.688677, Lr: 0.000019, Tokens per sec:   3363
2023-03-15 13:49:44,400 - INFO - __main__ - Epoch 168, Step:  366000, Batch Loss:     1.362999, Lr: 0.000019, Tokens per sec:   3435
2023-03-15 13:49:55,664 - INFO - __main__ - Epoch 168: total training loss 2515.92
2023-03-15 13:49:55,665 - INFO - __main__ - Epoch 169
2023-03-15 13:50:00,302 - INFO - __main__ - Epoch 169, Step:  366100, Batch Loss:     1.058732, Lr: 0.000018, Tokens per sec:   3297
2023-03-15 13:50:15,970 - INFO - __main__ - Epoch 169, Step:  366200, Batch Loss:     1.051222, Lr: 0.000018, Tokens per sec:   3414
2023-03-15 13:50:31,263 - INFO - __main__ - Epoch 169, Step:  366300, Batch Loss:     1.019135, Lr: 0.000018, Tokens per sec:   3607
2023-03-15 13:50:46,744 - INFO - __main__ - Epoch 169, Step:  366400, Batch Loss:     1.227405, Lr: 0.000018, Tokens per sec:   3433
2023-03-15 13:51:02,386 - INFO - __main__ - Epoch 169, Step:  366500, Batch Loss:     1.025511, Lr: 0.000018, Tokens per sec:   3484
2023-03-15 13:51:17,748 - INFO - __main__ - Epoch 169, Step:  366600, Batch Loss:     1.134070, Lr: 0.000018, Tokens per sec:   3539
2023-03-15 13:51:33,179 - INFO - __main__ - Epoch 169, Step:  366700, Batch Loss:     1.128993, Lr: 0.000018, Tokens per sec:   3506
2023-03-15 13:51:48,427 - INFO - __main__ - Epoch 169, Step:  366800, Batch Loss:     1.296997, Lr: 0.000018, Tokens per sec:   3507
2023-03-15 13:52:04,027 - INFO - __main__ - Epoch 169, Step:  366900, Batch Loss:     1.636980, Lr: 0.000018, Tokens per sec:   3448
2023-03-15 13:52:19,618 - INFO - __main__ - Epoch 169, Step:  367000, Batch Loss:     0.651745, Lr: 0.000018, Tokens per sec:   3457
2023-03-15 13:52:35,184 - INFO - __main__ - Epoch 169, Step:  367100, Batch Loss:     1.440337, Lr: 0.000018, Tokens per sec:   3444
2023-03-15 13:52:50,750 - INFO - __main__ - Epoch 169, Step:  367200, Batch Loss:     1.446739, Lr: 0.000018, Tokens per sec:   3391
2023-03-15 13:53:06,243 - INFO - __main__ - Epoch 169, Step:  367300, Batch Loss:     1.186466, Lr: 0.000018, Tokens per sec:   3512
2023-03-15 13:53:21,728 - INFO - __main__ - Epoch 169, Step:  367400, Batch Loss:     0.909399, Lr: 0.000018, Tokens per sec:   3439
2023-03-15 13:53:37,445 - INFO - __main__ - Epoch 169, Step:  367500, Batch Loss:     0.713527, Lr: 0.000018, Tokens per sec:   3410
2023-03-15 13:53:52,878 - INFO - __main__ - Epoch 169, Step:  367600, Batch Loss:     1.000594, Lr: 0.000018, Tokens per sec:   3501
2023-03-15 13:54:08,298 - INFO - __main__ - Epoch 169, Step:  367700, Batch Loss:     0.964045, Lr: 0.000018, Tokens per sec:   3516
2023-03-15 13:54:23,665 - INFO - __main__ - Epoch 169, Step:  367800, Batch Loss:     0.495998, Lr: 0.000018, Tokens per sec:   3477
2023-03-15 13:54:39,235 - INFO - __main__ - Epoch 169, Step:  367900, Batch Loss:     1.085557, Lr: 0.000018, Tokens per sec:   3464
2023-03-15 13:54:54,554 - INFO - __main__ - Epoch 169, Step:  368000, Batch Loss:     1.246997, Lr: 0.000018, Tokens per sec:   3478
2023-03-15 13:55:09,915 - INFO - __main__ - Epoch 169, Step:  368100, Batch Loss:     1.765061, Lr: 0.000018, Tokens per sec:   3519
2023-03-15 13:55:25,494 - INFO - __main__ - Epoch 169, Step:  368200, Batch Loss:     1.407595, Lr: 0.000018, Tokens per sec:   3461
2023-03-15 13:55:33,461 - INFO - __main__ - Epoch 169: total training loss 2497.70
2023-03-15 13:55:33,461 - INFO - __main__ - Epoch 170
2023-03-15 13:55:41,163 - INFO - __main__ - Epoch 170, Step:  368300, Batch Loss:     1.475298, Lr: 0.000018, Tokens per sec:   3359
2023-03-15 13:55:56,672 - INFO - __main__ - Epoch 170, Step:  368400, Batch Loss:     0.859564, Lr: 0.000018, Tokens per sec:   3470
2023-03-15 13:56:12,229 - INFO - __main__ - Epoch 170, Step:  368500, Batch Loss:     1.345989, Lr: 0.000018, Tokens per sec:   3415
2023-03-15 13:56:27,477 - INFO - __main__ - Epoch 170, Step:  368600, Batch Loss:     0.916799, Lr: 0.000018, Tokens per sec:   3584
2023-03-15 13:56:43,024 - INFO - __main__ - Epoch 170, Step:  368700, Batch Loss:     1.391718, Lr: 0.000018, Tokens per sec:   3409
2023-03-15 13:56:58,210 - INFO - __main__ - Epoch 170, Step:  368800, Batch Loss:     1.198542, Lr: 0.000018, Tokens per sec:   3500
2023-03-15 13:57:13,892 - INFO - __main__ - Epoch 170, Step:  368900, Batch Loss:     0.777503, Lr: 0.000018, Tokens per sec:   3470
2023-03-15 13:57:29,299 - INFO - __main__ - Epoch 170, Step:  369000, Batch Loss:     0.977665, Lr: 0.000018, Tokens per sec:   3486
2023-03-15 13:57:44,318 - INFO - __main__ - Epoch 170, Step:  369100, Batch Loss:     0.736786, Lr: 0.000018, Tokens per sec:   3643
2023-03-15 13:57:59,820 - INFO - __main__ - Epoch 170, Step:  369200, Batch Loss:     1.487286, Lr: 0.000018, Tokens per sec:   3481
2023-03-15 13:58:15,146 - INFO - __main__ - Epoch 170, Step:  369300, Batch Loss:     0.533128, Lr: 0.000018, Tokens per sec:   3507
2023-03-15 13:58:30,315 - INFO - __main__ - Epoch 170, Step:  369400, Batch Loss:     0.855123, Lr: 0.000018, Tokens per sec:   3588
2023-03-15 13:58:45,552 - INFO - __main__ - Epoch 170, Step:  369500, Batch Loss:     0.990019, Lr: 0.000018, Tokens per sec:   3534
2023-03-15 13:59:01,097 - INFO - __main__ - Epoch 170, Step:  369600, Batch Loss:     1.638879, Lr: 0.000018, Tokens per sec:   3466
2023-03-15 13:59:16,578 - INFO - __main__ - Epoch 170, Step:  369700, Batch Loss:     1.110331, Lr: 0.000018, Tokens per sec:   3500
2023-03-15 13:59:31,993 - INFO - __main__ - Epoch 170, Step:  369800, Batch Loss:     1.218193, Lr: 0.000018, Tokens per sec:   3534
2023-03-15 13:59:47,479 - INFO - __main__ - Epoch 170, Step:  369900, Batch Loss:     0.823715, Lr: 0.000018, Tokens per sec:   3482
2023-03-15 14:00:02,706 - INFO - __main__ - Epoch 170, Step:  370000, Batch Loss:     1.129905, Lr: 0.000018, Tokens per sec:   3535
2023-03-15 14:00:17,986 - INFO - __main__ - Epoch 170, Step:  370100, Batch Loss:     0.848164, Lr: 0.000018, Tokens per sec:   3489
2023-03-15 14:00:33,408 - INFO - __main__ - Epoch 170, Step:  370200, Batch Loss:     1.449776, Lr: 0.000018, Tokens per sec:   3487
2023-03-15 14:00:48,257 - INFO - __main__ - Epoch 170, Step:  370300, Batch Loss:     0.714891, Lr: 0.000018, Tokens per sec:   3577
2023-03-15 14:01:03,862 - INFO - __main__ - Epoch 170, Step:  370400, Batch Loss:     1.494256, Lr: 0.000018, Tokens per sec:   3483
2023-03-15 14:01:08,648 - INFO - __main__ - Epoch 170: total training loss 2481.94
2023-03-15 14:01:08,649 - INFO - __main__ - Epoch 171
2023-03-15 14:01:19,634 - INFO - __main__ - Epoch 171, Step:  370500, Batch Loss:     0.608322, Lr: 0.000018, Tokens per sec:   3452
2023-03-15 14:01:35,164 - INFO - __main__ - Epoch 171, Step:  370600, Batch Loss:     1.335579, Lr: 0.000018, Tokens per sec:   3460
2023-03-15 14:01:50,659 - INFO - __main__ - Epoch 171, Step:  370700, Batch Loss:     1.126469, Lr: 0.000018, Tokens per sec:   3407
2023-03-15 14:02:05,763 - INFO - __main__ - Epoch 171, Step:  370800, Batch Loss:     0.823056, Lr: 0.000018, Tokens per sec:   3508
2023-03-15 14:02:21,137 - INFO - __main__ - Epoch 171, Step:  370900, Batch Loss:     1.030714, Lr: 0.000018, Tokens per sec:   3557
2023-03-15 14:02:36,787 - INFO - __main__ - Epoch 171, Step:  371000, Batch Loss:     1.091648, Lr: 0.000018, Tokens per sec:   3533
2023-03-15 14:02:52,277 - INFO - __main__ - Epoch 171, Step:  371100, Batch Loss:     0.968118, Lr: 0.000018, Tokens per sec:   3451
2023-03-15 14:03:07,975 - INFO - __main__ - Epoch 171, Step:  371200, Batch Loss:     2.107935, Lr: 0.000018, Tokens per sec:   3462
2023-03-15 14:03:23,571 - INFO - __main__ - Epoch 171, Step:  371300, Batch Loss:     0.802653, Lr: 0.000018, Tokens per sec:   3488
2023-03-15 14:03:39,074 - INFO - __main__ - Epoch 171, Step:  371400, Batch Loss:     0.985578, Lr: 0.000018, Tokens per sec:   3529
2023-03-15 14:03:54,512 - INFO - __main__ - Epoch 171, Step:  371500, Batch Loss:     1.209691, Lr: 0.000018, Tokens per sec:   3534
2023-03-15 14:04:09,748 - INFO - __main__ - Epoch 171, Step:  371600, Batch Loss:     0.984571, Lr: 0.000018, Tokens per sec:   3556
2023-03-15 14:04:25,192 - INFO - __main__ - Epoch 171, Step:  371700, Batch Loss:     1.704224, Lr: 0.000018, Tokens per sec:   3433
2023-03-15 14:04:40,643 - INFO - __main__ - Epoch 171, Step:  371800, Batch Loss:     1.228491, Lr: 0.000018, Tokens per sec:   3412
2023-03-15 14:04:56,304 - INFO - __main__ - Epoch 171, Step:  371900, Batch Loss:     0.929615, Lr: 0.000018, Tokens per sec:   3431
2023-03-15 14:05:11,670 - INFO - __main__ - Epoch 171, Step:  372000, Batch Loss:     0.938230, Lr: 0.000018, Tokens per sec:   3561
2023-03-15 14:05:27,180 - INFO - __main__ - Epoch 171, Step:  372100, Batch Loss:     0.942461, Lr: 0.000018, Tokens per sec:   3440
2023-03-15 14:05:42,789 - INFO - __main__ - Epoch 171, Step:  372200, Batch Loss:     1.568439, Lr: 0.000018, Tokens per sec:   3418
2023-03-15 14:05:58,389 - INFO - __main__ - Epoch 171, Step:  372300, Batch Loss:     1.545591, Lr: 0.000018, Tokens per sec:   3419
2023-03-15 14:06:13,982 - INFO - __main__ - Epoch 171, Step:  372400, Batch Loss:     1.721468, Lr: 0.000018, Tokens per sec:   3425
2023-03-15 14:06:29,658 - INFO - __main__ - Epoch 171, Step:  372500, Batch Loss:     1.122109, Lr: 0.000018, Tokens per sec:   3413
2023-03-15 14:06:45,162 - INFO - __main__ - Epoch 171, Step:  372600, Batch Loss:     0.688754, Lr: 0.000018, Tokens per sec:   3503
2023-03-15 14:06:46,668 - INFO - __main__ - Epoch 171: total training loss 2442.82
2023-03-15 14:06:46,669 - INFO - __main__ - Epoch 172
2023-03-15 14:07:01,073 - INFO - __main__ - Epoch 172, Step:  372700, Batch Loss:     2.214263, Lr: 0.000018, Tokens per sec:   3357
2023-03-15 14:07:16,628 - INFO - __main__ - Epoch 172, Step:  372800, Batch Loss:     1.445009, Lr: 0.000018, Tokens per sec:   3464
2023-03-15 14:07:32,138 - INFO - __main__ - Epoch 172, Step:  372900, Batch Loss:     1.006704, Lr: 0.000018, Tokens per sec:   3483
2023-03-15 14:07:47,772 - INFO - __main__ - Epoch 172, Step:  373000, Batch Loss:     1.146919, Lr: 0.000018, Tokens per sec:   3363
2023-03-15 14:08:03,169 - INFO - __main__ - Epoch 172, Step:  373100, Batch Loss:     1.535341, Lr: 0.000018, Tokens per sec:   3511
2023-03-15 14:08:18,537 - INFO - __main__ - Epoch 172, Step:  373200, Batch Loss:     1.095954, Lr: 0.000018, Tokens per sec:   3522
2023-03-15 14:08:33,522 - INFO - __main__ - Epoch 172, Step:  373300, Batch Loss:     1.310511, Lr: 0.000018, Tokens per sec:   3592
2023-03-15 14:08:49,174 - INFO - __main__ - Epoch 172, Step:  373400, Batch Loss:     0.982508, Lr: 0.000018, Tokens per sec:   3386
2023-03-15 14:09:04,751 - INFO - __main__ - Epoch 172, Step:  373500, Batch Loss:     0.891273, Lr: 0.000018, Tokens per sec:   3517
2023-03-15 14:09:20,221 - INFO - __main__ - Epoch 172, Step:  373600, Batch Loss:     0.908121, Lr: 0.000018, Tokens per sec:   3441
2023-03-15 14:09:35,507 - INFO - __main__ - Epoch 172, Step:  373700, Batch Loss:     0.669618, Lr: 0.000018, Tokens per sec:   3516
2023-03-15 14:09:51,106 - INFO - __main__ - Epoch 172, Step:  373800, Batch Loss:     0.972998, Lr: 0.000018, Tokens per sec:   3430
2023-03-15 14:10:06,560 - INFO - __main__ - Epoch 172, Step:  373900, Batch Loss:     1.145797, Lr: 0.000018, Tokens per sec:   3521
2023-03-15 14:10:21,885 - INFO - __main__ - Epoch 172, Step:  374000, Batch Loss:     2.020850, Lr: 0.000018, Tokens per sec:   3474
2023-03-15 14:10:37,305 - INFO - __main__ - Epoch 172, Step:  374100, Batch Loss:     1.211634, Lr: 0.000018, Tokens per sec:   3509
2023-03-15 14:10:52,647 - INFO - __main__ - Epoch 172, Step:  374200, Batch Loss:     1.373614, Lr: 0.000018, Tokens per sec:   3565
2023-03-15 14:11:08,246 - INFO - __main__ - Epoch 172, Step:  374300, Batch Loss:     1.873997, Lr: 0.000018, Tokens per sec:   3374
2023-03-15 14:11:23,733 - INFO - __main__ - Epoch 172, Step:  374400, Batch Loss:     0.727688, Lr: 0.000018, Tokens per sec:   3526
2023-03-15 14:11:39,177 - INFO - __main__ - Epoch 172, Step:  374500, Batch Loss:     1.513965, Lr: 0.000018, Tokens per sec:   3485
2023-03-15 14:11:54,283 - INFO - __main__ - Epoch 172, Step:  374600, Batch Loss:     1.384765, Lr: 0.000018, Tokens per sec:   3614
2023-03-15 14:12:09,671 - INFO - __main__ - Epoch 172, Step:  374700, Batch Loss:     1.174638, Lr: 0.000018, Tokens per sec:   3572
2023-03-15 14:12:23,471 - INFO - __main__ - Epoch 172: total training loss 2445.06
2023-03-15 14:12:23,472 - INFO - __main__ - Epoch 173
2023-03-15 14:12:25,733 - INFO - __main__ - Epoch 173, Step:  374800, Batch Loss:     0.844622, Lr: 0.000018, Tokens per sec:   2806
2023-03-15 14:12:41,211 - INFO - __main__ - Epoch 173, Step:  374900, Batch Loss:     1.328223, Lr: 0.000018, Tokens per sec:   3509
2023-03-15 14:12:56,753 - INFO - __main__ - Epoch 173, Step:  375000, Batch Loss:     1.038117, Lr: 0.000018, Tokens per sec:   3433
2023-03-15 14:13:12,316 - INFO - __main__ - Epoch 173, Step:  375100, Batch Loss:     0.988747, Lr: 0.000018, Tokens per sec:   3508
2023-03-15 14:13:27,961 - INFO - __main__ - Epoch 173, Step:  375200, Batch Loss:     0.635615, Lr: 0.000018, Tokens per sec:   3445
2023-03-15 14:13:43,481 - INFO - __main__ - Epoch 173, Step:  375300, Batch Loss:     1.909562, Lr: 0.000018, Tokens per sec:   3474
2023-03-15 14:13:58,741 - INFO - __main__ - Epoch 173, Step:  375400, Batch Loss:     0.666440, Lr: 0.000018, Tokens per sec:   3493
2023-03-15 14:14:13,989 - INFO - __main__ - Epoch 173, Step:  375500, Batch Loss:     0.728871, Lr: 0.000018, Tokens per sec:   3545
2023-03-15 14:14:29,574 - INFO - __main__ - Epoch 173, Step:  375600, Batch Loss:     1.628392, Lr: 0.000018, Tokens per sec:   3444
2023-03-15 14:14:45,183 - INFO - __main__ - Epoch 173, Step:  375700, Batch Loss:     1.145243, Lr: 0.000018, Tokens per sec:   3432
2023-03-15 14:15:00,498 - INFO - __main__ - Epoch 173, Step:  375800, Batch Loss:     0.700525, Lr: 0.000018, Tokens per sec:   3516
2023-03-15 14:15:16,125 - INFO - __main__ - Epoch 173, Step:  375900, Batch Loss:     1.046455, Lr: 0.000018, Tokens per sec:   3442
2023-03-15 14:15:31,568 - INFO - __main__ - Epoch 173, Step:  376000, Batch Loss:     1.043044, Lr: 0.000018, Tokens per sec:   3511
2023-03-15 14:15:46,972 - INFO - __main__ - Epoch 173, Step:  376100, Batch Loss:     1.521170, Lr: 0.000018, Tokens per sec:   3497
2023-03-15 14:16:02,537 - INFO - __main__ - Epoch 173, Step:  376200, Batch Loss:     1.425223, Lr: 0.000018, Tokens per sec:   3520
2023-03-15 14:16:18,006 - INFO - __main__ - Epoch 173, Step:  376300, Batch Loss:     1.659648, Lr: 0.000018, Tokens per sec:   3520
2023-03-15 14:16:33,473 - INFO - __main__ - Epoch 173, Step:  376400, Batch Loss:     0.757664, Lr: 0.000018, Tokens per sec:   3425
2023-03-15 14:16:48,954 - INFO - __main__ - Epoch 173, Step:  376500, Batch Loss:     1.189038, Lr: 0.000018, Tokens per sec:   3411
2023-03-15 14:17:04,442 - INFO - __main__ - Epoch 173, Step:  376600, Batch Loss:     1.181697, Lr: 0.000018, Tokens per sec:   3527
2023-03-15 14:17:19,889 - INFO - __main__ - Epoch 173, Step:  376700, Batch Loss:     0.875216, Lr: 0.000018, Tokens per sec:   3481
2023-03-15 14:17:35,280 - INFO - __main__ - Epoch 173, Step:  376800, Batch Loss:     1.401610, Lr: 0.000018, Tokens per sec:   3470
2023-03-15 14:17:50,598 - INFO - __main__ - Epoch 173, Step:  376900, Batch Loss:     1.129061, Lr: 0.000018, Tokens per sec:   3505
2023-03-15 14:18:01,275 - INFO - __main__ - Epoch 173: total training loss 2395.69
2023-03-15 14:18:01,276 - INFO - __main__ - Epoch 174
2023-03-15 14:18:06,882 - INFO - __main__ - Epoch 174, Step:  377000, Batch Loss:     1.587604, Lr: 0.000018, Tokens per sec:   3270
2023-03-15 14:18:22,383 - INFO - __main__ - Epoch 174, Step:  377100, Batch Loss:     1.025528, Lr: 0.000018, Tokens per sec:   3445
2023-03-15 14:18:37,819 - INFO - __main__ - Epoch 174, Step:  377200, Batch Loss:     0.567157, Lr: 0.000018, Tokens per sec:   3512
2023-03-15 14:18:53,438 - INFO - __main__ - Epoch 174, Step:  377300, Batch Loss:     0.743115, Lr: 0.000018, Tokens per sec:   3529
2023-03-15 14:19:08,767 - INFO - __main__ - Epoch 174, Step:  377400, Batch Loss:     1.890411, Lr: 0.000018, Tokens per sec:   3553
2023-03-15 14:19:24,346 - INFO - __main__ - Epoch 174, Step:  377500, Batch Loss:     1.021180, Lr: 0.000018, Tokens per sec:   3472
2023-03-15 14:19:39,837 - INFO - __main__ - Epoch 174, Step:  377600, Batch Loss:     1.455354, Lr: 0.000018, Tokens per sec:   3503
2023-03-15 14:19:55,142 - INFO - __main__ - Epoch 174, Step:  377700, Batch Loss:     0.458096, Lr: 0.000018, Tokens per sec:   3434
2023-03-15 14:20:10,829 - INFO - __main__ - Epoch 174, Step:  377800, Batch Loss:     1.022929, Lr: 0.000018, Tokens per sec:   3449
2023-03-15 14:20:26,396 - INFO - __main__ - Epoch 174, Step:  377900, Batch Loss:     1.468366, Lr: 0.000018, Tokens per sec:   3474
2023-03-15 14:20:41,943 - INFO - __main__ - Epoch 174, Step:  378000, Batch Loss:     1.446558, Lr: 0.000018, Tokens per sec:   3463
2023-03-15 14:20:57,664 - INFO - __main__ - Epoch 174, Step:  378100, Batch Loss:     1.063676, Lr: 0.000018, Tokens per sec:   3400
2023-03-15 14:21:13,180 - INFO - __main__ - Epoch 174, Step:  378200, Batch Loss:     1.326681, Lr: 0.000018, Tokens per sec:   3502
2023-03-15 14:21:28,376 - INFO - __main__ - Epoch 174, Step:  378300, Batch Loss:     0.470009, Lr: 0.000018, Tokens per sec:   3581
2023-03-15 14:21:43,828 - INFO - __main__ - Epoch 174, Step:  378400, Batch Loss:     1.110199, Lr: 0.000018, Tokens per sec:   3492
2023-03-15 14:21:59,098 - INFO - __main__ - Epoch 174, Step:  378500, Batch Loss:     0.834426, Lr: 0.000018, Tokens per sec:   3484
2023-03-15 14:22:14,403 - INFO - __main__ - Epoch 174, Step:  378600, Batch Loss:     1.202285, Lr: 0.000018, Tokens per sec:   3451
2023-03-15 14:22:30,083 - INFO - __main__ - Epoch 174, Step:  378700, Batch Loss:     0.464720, Lr: 0.000018, Tokens per sec:   3403
2023-03-15 14:22:45,683 - INFO - __main__ - Epoch 174, Step:  378800, Batch Loss:     1.209418, Lr: 0.000018, Tokens per sec:   3455
2023-03-15 14:23:00,767 - INFO - __main__ - Epoch 174, Step:  378900, Batch Loss:     0.973422, Lr: 0.000018, Tokens per sec:   3571
2023-03-15 14:23:16,134 - INFO - __main__ - Epoch 174, Step:  379000, Batch Loss:     1.435603, Lr: 0.000018, Tokens per sec:   3445
2023-03-15 14:23:31,820 - INFO - __main__ - Epoch 174, Step:  379100, Batch Loss:     1.149887, Lr: 0.000018, Tokens per sec:   3412
2023-03-15 14:23:39,049 - INFO - __main__ - Epoch 174: total training loss 2407.76
2023-03-15 14:23:39,050 - INFO - __main__ - Epoch 175
2023-03-15 14:23:47,881 - INFO - __main__ - Epoch 175, Step:  379200, Batch Loss:     1.396743, Lr: 0.000017, Tokens per sec:   3353
2023-03-15 14:24:03,215 - INFO - __main__ - Epoch 175, Step:  379300, Batch Loss:     0.877907, Lr: 0.000017, Tokens per sec:   3500
2023-03-15 14:24:18,785 - INFO - __main__ - Epoch 175, Step:  379400, Batch Loss:     1.467995, Lr: 0.000017, Tokens per sec:   3486
2023-03-15 14:24:34,328 - INFO - __main__ - Epoch 175, Step:  379500, Batch Loss:     0.929994, Lr: 0.000017, Tokens per sec:   3483
2023-03-15 14:24:49,834 - INFO - __main__ - Epoch 175, Step:  379600, Batch Loss:     0.649432, Lr: 0.000017, Tokens per sec:   3442
2023-03-15 14:25:05,103 - INFO - __main__ - Epoch 175, Step:  379700, Batch Loss:     0.777934, Lr: 0.000017, Tokens per sec:   3515
2023-03-15 14:25:20,619 - INFO - __main__ - Epoch 175, Step:  379800, Batch Loss:     1.572678, Lr: 0.000017, Tokens per sec:   3477
2023-03-15 14:25:36,232 - INFO - __main__ - Epoch 175, Step:  379900, Batch Loss:     1.256215, Lr: 0.000017, Tokens per sec:   3394
2023-03-15 14:25:51,672 - INFO - __main__ - Epoch 175, Step:  380000, Batch Loss:     0.643201, Lr: 0.000017, Tokens per sec:   3402
2023-03-15 14:26:06,897 - INFO - __main__ - Epoch 175, Step:  380100, Batch Loss:     0.679740, Lr: 0.000017, Tokens per sec:   3561
2023-03-15 14:26:22,398 - INFO - __main__ - Epoch 175, Step:  380200, Batch Loss:     0.800641, Lr: 0.000017, Tokens per sec:   3529
2023-03-15 14:26:37,897 - INFO - __main__ - Epoch 175, Step:  380300, Batch Loss:     0.852494, Lr: 0.000017, Tokens per sec:   3435
2023-03-15 14:26:53,370 - INFO - __main__ - Epoch 175, Step:  380400, Batch Loss:     1.411173, Lr: 0.000017, Tokens per sec:   3495
2023-03-15 14:27:08,699 - INFO - __main__ - Epoch 175, Step:  380500, Batch Loss:     0.850953, Lr: 0.000017, Tokens per sec:   3500
2023-03-15 14:27:24,292 - INFO - __main__ - Epoch 175, Step:  380600, Batch Loss:     1.202900, Lr: 0.000017, Tokens per sec:   3457
2023-03-15 14:27:39,881 - INFO - __main__ - Epoch 175, Step:  380700, Batch Loss:     0.975603, Lr: 0.000017, Tokens per sec:   3495
2023-03-15 14:27:55,211 - INFO - __main__ - Epoch 175, Step:  380800, Batch Loss:     1.619790, Lr: 0.000017, Tokens per sec:   3501
2023-03-15 14:28:10,697 - INFO - __main__ - Epoch 175, Step:  380900, Batch Loss:     1.215479, Lr: 0.000017, Tokens per sec:   3442
2023-03-15 14:28:26,377 - INFO - __main__ - Epoch 175, Step:  381000, Batch Loss:     1.459246, Lr: 0.000017, Tokens per sec:   3401
2023-03-15 14:28:42,035 - INFO - __main__ - Epoch 175, Step:  381100, Batch Loss:     1.162795, Lr: 0.000017, Tokens per sec:   3497
2023-03-15 14:28:57,601 - INFO - __main__ - Epoch 175, Step:  381200, Batch Loss:     1.472914, Lr: 0.000017, Tokens per sec:   3482
2023-03-15 14:29:12,932 - INFO - __main__ - Epoch 175, Step:  381300, Batch Loss:     1.621936, Lr: 0.000017, Tokens per sec:   3515
2023-03-15 14:29:16,918 - INFO - __main__ - Epoch 175: total training loss 2387.44
2023-03-15 14:29:16,919 - INFO - __main__ - Epoch 176
2023-03-15 14:29:28,940 - INFO - __main__ - Epoch 176, Step:  381400, Batch Loss:     1.108457, Lr: 0.000017, Tokens per sec:   3387
2023-03-15 14:29:44,550 - INFO - __main__ - Epoch 176, Step:  381500, Batch Loss:     1.739510, Lr: 0.000017, Tokens per sec:   3394
2023-03-15 14:30:00,094 - INFO - __main__ - Epoch 176, Step:  381600, Batch Loss:     1.265889, Lr: 0.000017, Tokens per sec:   3445
2023-03-15 14:30:15,637 - INFO - __main__ - Epoch 176, Step:  381700, Batch Loss:     0.817571, Lr: 0.000017, Tokens per sec:   3464
2023-03-15 14:30:31,259 - INFO - __main__ - Epoch 176, Step:  381800, Batch Loss:     1.625005, Lr: 0.000017, Tokens per sec:   3402
2023-03-15 14:30:46,427 - INFO - __main__ - Epoch 176, Step:  381900, Batch Loss:     1.480548, Lr: 0.000017, Tokens per sec:   3553
2023-03-15 14:31:01,988 - INFO - __main__ - Epoch 176, Step:  382000, Batch Loss:     1.210313, Lr: 0.000017, Tokens per sec:   3477
2023-03-15 14:31:17,630 - INFO - __main__ - Epoch 176, Step:  382100, Batch Loss:     1.216536, Lr: 0.000017, Tokens per sec:   3424
2023-03-15 14:31:32,775 - INFO - __main__ - Epoch 176, Step:  382200, Batch Loss:     1.413576, Lr: 0.000017, Tokens per sec:   3595
2023-03-15 14:31:48,354 - INFO - __main__ - Epoch 176, Step:  382300, Batch Loss:     1.485037, Lr: 0.000017, Tokens per sec:   3491
2023-03-15 14:32:03,930 - INFO - __main__ - Epoch 176, Step:  382400, Batch Loss:     1.548770, Lr: 0.000017, Tokens per sec:   3443
2023-03-15 14:32:19,061 - INFO - __main__ - Epoch 176, Step:  382500, Batch Loss:     1.650030, Lr: 0.000017, Tokens per sec:   3571
2023-03-15 14:32:34,581 - INFO - __main__ - Epoch 176, Step:  382600, Batch Loss:     1.811030, Lr: 0.000017, Tokens per sec:   3511
2023-03-15 14:32:50,053 - INFO - __main__ - Epoch 176, Step:  382700, Batch Loss:     0.784211, Lr: 0.000017, Tokens per sec:   3502
2023-03-15 14:33:05,528 - INFO - __main__ - Epoch 176, Step:  382800, Batch Loss:     0.734140, Lr: 0.000017, Tokens per sec:   3463
2023-03-15 14:33:21,082 - INFO - __main__ - Epoch 176, Step:  382900, Batch Loss:     1.543554, Lr: 0.000017, Tokens per sec:   3498
2023-03-15 14:33:36,767 - INFO - __main__ - Epoch 176, Step:  383000, Batch Loss:     1.220665, Lr: 0.000017, Tokens per sec:   3455
2023-03-15 14:33:52,233 - INFO - __main__ - Epoch 176, Step:  383100, Batch Loss:     1.802812, Lr: 0.000017, Tokens per sec:   3446
2023-03-15 14:34:07,446 - INFO - __main__ - Epoch 176, Step:  383200, Batch Loss:     1.279042, Lr: 0.000017, Tokens per sec:   3578
2023-03-15 14:34:23,090 - INFO - __main__ - Epoch 176, Step:  383300, Batch Loss:     0.549625, Lr: 0.000017, Tokens per sec:   3458
2023-03-15 14:34:38,632 - INFO - __main__ - Epoch 176, Step:  383400, Batch Loss:     0.930347, Lr: 0.000017, Tokens per sec:   3451
2023-03-15 14:34:54,217 - INFO - __main__ - Epoch 176, Step:  383500, Batch Loss:     0.841876, Lr: 0.000017, Tokens per sec:   3363
2023-03-15 14:34:54,952 - INFO - __main__ - Epoch 176: total training loss 2373.20
2023-03-15 14:34:54,953 - INFO - __main__ - Epoch 177
2023-03-15 14:35:10,093 - INFO - __main__ - Epoch 177, Step:  383600, Batch Loss:     1.105130, Lr: 0.000017, Tokens per sec:   3431
2023-03-15 14:35:25,793 - INFO - __main__ - Epoch 177, Step:  383700, Batch Loss:     1.256590, Lr: 0.000017, Tokens per sec:   3414
2023-03-15 14:35:41,116 - INFO - __main__ - Epoch 177, Step:  383800, Batch Loss:     1.442922, Lr: 0.000017, Tokens per sec:   3519
2023-03-15 14:35:56,794 - INFO - __main__ - Epoch 177, Step:  383900, Batch Loss:     0.829343, Lr: 0.000017, Tokens per sec:   3450
2023-03-15 14:36:12,261 - INFO - __main__ - Epoch 177, Step:  384000, Batch Loss:     1.156713, Lr: 0.000017, Tokens per sec:   3523
2023-03-15 14:36:27,865 - INFO - __main__ - Epoch 177, Step:  384100, Batch Loss:     1.122134, Lr: 0.000017, Tokens per sec:   3482
2023-03-15 14:36:43,512 - INFO - __main__ - Epoch 177, Step:  384200, Batch Loss:     1.258133, Lr: 0.000017, Tokens per sec:   3424
2023-03-15 14:36:59,033 - INFO - __main__ - Epoch 177, Step:  384300, Batch Loss:     0.680711, Lr: 0.000017, Tokens per sec:   3421
2023-03-15 14:37:14,333 - INFO - __main__ - Epoch 177, Step:  384400, Batch Loss:     0.809110, Lr: 0.000017, Tokens per sec:   3601
2023-03-15 14:37:29,536 - INFO - __main__ - Epoch 177, Step:  384500, Batch Loss:     1.211319, Lr: 0.000017, Tokens per sec:   3585
2023-03-15 14:37:45,097 - INFO - __main__ - Epoch 177, Step:  384600, Batch Loss:     0.881602, Lr: 0.000017, Tokens per sec:   3457
2023-03-15 14:38:00,507 - INFO - __main__ - Epoch 177, Step:  384700, Batch Loss:     1.264600, Lr: 0.000017, Tokens per sec:   3527
2023-03-15 14:38:15,993 - INFO - __main__ - Epoch 177, Step:  384800, Batch Loss:     1.452015, Lr: 0.000017, Tokens per sec:   3479
2023-03-15 14:38:31,319 - INFO - __main__ - Epoch 177, Step:  384900, Batch Loss:     1.098118, Lr: 0.000017, Tokens per sec:   3525
2023-03-15 14:38:46,690 - INFO - __main__ - Epoch 177, Step:  385000, Batch Loss:     0.849314, Lr: 0.000017, Tokens per sec:   3484
2023-03-15 14:39:01,974 - INFO - __main__ - Epoch 177, Step:  385100, Batch Loss:     1.169242, Lr: 0.000017, Tokens per sec:   3476
2023-03-15 14:39:17,636 - INFO - __main__ - Epoch 177, Step:  385200, Batch Loss:     0.950161, Lr: 0.000017, Tokens per sec:   3389
2023-03-15 14:39:32,997 - INFO - __main__ - Epoch 177, Step:  385300, Batch Loss:     1.322190, Lr: 0.000017, Tokens per sec:   3503
2023-03-15 14:39:48,544 - INFO - __main__ - Epoch 177, Step:  385400, Batch Loss:     1.662239, Lr: 0.000017, Tokens per sec:   3480
2023-03-15 14:40:04,120 - INFO - __main__ - Epoch 177, Step:  385500, Batch Loss:     1.039415, Lr: 0.000017, Tokens per sec:   3396
2023-03-15 14:40:19,591 - INFO - __main__ - Epoch 177, Step:  385600, Batch Loss:     0.804948, Lr: 0.000017, Tokens per sec:   3470
2023-03-15 14:40:32,107 - INFO - __main__ - Epoch 177: total training loss 2350.80
2023-03-15 14:40:32,108 - INFO - __main__ - Epoch 178
2023-03-15 14:40:35,123 - INFO - __main__ - Epoch 178, Step:  385700, Batch Loss:     1.104965, Lr: 0.000017, Tokens per sec:   3031
2023-03-15 14:40:50,509 - INFO - __main__ - Epoch 178, Step:  385800, Batch Loss:     1.216989, Lr: 0.000017, Tokens per sec:   3526
2023-03-15 14:41:06,002 - INFO - __main__ - Epoch 178, Step:  385900, Batch Loss:     1.249933, Lr: 0.000017, Tokens per sec:   3449
2023-03-15 14:41:21,794 - INFO - __main__ - Epoch 178, Step:  386000, Batch Loss:     0.948245, Lr: 0.000017, Tokens per sec:   3398
2023-03-15 14:41:37,592 - INFO - __main__ - Epoch 178, Step:  386100, Batch Loss:     0.965603, Lr: 0.000017, Tokens per sec:   3426
2023-03-15 14:41:52,998 - INFO - __main__ - Epoch 178, Step:  386200, Batch Loss:     0.957109, Lr: 0.000017, Tokens per sec:   3559
2023-03-15 14:42:08,748 - INFO - __main__ - Epoch 178, Step:  386300, Batch Loss:     1.075196, Lr: 0.000017, Tokens per sec:   3396
2023-03-15 14:42:24,317 - INFO - __main__ - Epoch 178, Step:  386400, Batch Loss:     0.704465, Lr: 0.000017, Tokens per sec:   3472
2023-03-15 14:42:39,886 - INFO - __main__ - Epoch 178, Step:  386500, Batch Loss:     0.676884, Lr: 0.000017, Tokens per sec:   3454
2023-03-15 14:42:55,385 - INFO - __main__ - Epoch 178, Step:  386600, Batch Loss:     1.224833, Lr: 0.000017, Tokens per sec:   3459
2023-03-15 14:43:10,867 - INFO - __main__ - Epoch 178, Step:  386700, Batch Loss:     1.332310, Lr: 0.000017, Tokens per sec:   3505
2023-03-15 14:43:26,571 - INFO - __main__ - Epoch 178, Step:  386800, Batch Loss:     1.346186, Lr: 0.000017, Tokens per sec:   3442
2023-03-15 14:43:41,854 - INFO - __main__ - Epoch 178, Step:  386900, Batch Loss:     1.413666, Lr: 0.000017, Tokens per sec:   3490
2023-03-15 14:43:57,223 - INFO - __main__ - Epoch 178, Step:  387000, Batch Loss:     1.377660, Lr: 0.000017, Tokens per sec:   3522
2023-03-15 14:44:12,491 - INFO - __main__ - Epoch 178, Step:  387100, Batch Loss:     0.731887, Lr: 0.000017, Tokens per sec:   3478
2023-03-15 14:44:27,976 - INFO - __main__ - Epoch 178, Step:  387200, Batch Loss:     1.359232, Lr: 0.000017, Tokens per sec:   3463
2023-03-15 14:44:43,420 - INFO - __main__ - Epoch 178, Step:  387300, Batch Loss:     0.883758, Lr: 0.000017, Tokens per sec:   3488
2023-03-15 14:44:58,910 - INFO - __main__ - Epoch 178, Step:  387400, Batch Loss:     1.037752, Lr: 0.000017, Tokens per sec:   3455
2023-03-15 14:45:14,419 - INFO - __main__ - Epoch 178, Step:  387500, Batch Loss:     0.996819, Lr: 0.000017, Tokens per sec:   3486
2023-03-15 14:45:29,844 - INFO - __main__ - Epoch 178, Step:  387600, Batch Loss:     0.723292, Lr: 0.000017, Tokens per sec:   3487
2023-03-15 14:45:45,319 - INFO - __main__ - Epoch 178, Step:  387700, Batch Loss:     1.393849, Lr: 0.000017, Tokens per sec:   3458
2023-03-15 14:46:00,617 - INFO - __main__ - Epoch 178, Step:  387800, Batch Loss:     1.310662, Lr: 0.000017, Tokens per sec:   3491
2023-03-15 14:46:10,305 - INFO - __main__ - Epoch 178: total training loss 2339.41
2023-03-15 14:46:10,306 - INFO - __main__ - Epoch 179
2023-03-15 14:46:16,422 - INFO - __main__ - Epoch 179, Step:  387900, Batch Loss:     0.996226, Lr: 0.000017, Tokens per sec:   3332
2023-03-15 14:46:31,950 - INFO - __main__ - Epoch 179, Step:  388000, Batch Loss:     1.160768, Lr: 0.000017, Tokens per sec:   3438
2023-03-15 14:46:47,519 - INFO - __main__ - Epoch 179, Step:  388100, Batch Loss:     0.759853, Lr: 0.000017, Tokens per sec:   3414
2023-03-15 14:47:03,007 - INFO - __main__ - Epoch 179, Step:  388200, Batch Loss:     1.300686, Lr: 0.000017, Tokens per sec:   3493
2023-03-15 14:47:18,448 - INFO - __main__ - Epoch 179, Step:  388300, Batch Loss:     1.482130, Lr: 0.000017, Tokens per sec:   3489
2023-03-15 14:47:34,085 - INFO - __main__ - Epoch 179, Step:  388400, Batch Loss:     1.079563, Lr: 0.000017, Tokens per sec:   3472
2023-03-15 14:47:49,490 - INFO - __main__ - Epoch 179, Step:  388500, Batch Loss:     1.186563, Lr: 0.000017, Tokens per sec:   3487
2023-03-15 14:48:04,917 - INFO - __main__ - Epoch 179, Step:  388600, Batch Loss:     1.549707, Lr: 0.000017, Tokens per sec:   3480
2023-03-15 14:48:19,840 - INFO - __main__ - Epoch 179, Step:  388700, Batch Loss:     0.978123, Lr: 0.000017, Tokens per sec:   3593
2023-03-15 14:48:35,600 - INFO - __main__ - Epoch 179, Step:  388800, Batch Loss:     0.765859, Lr: 0.000017, Tokens per sec:   3403
2023-03-15 14:48:51,231 - INFO - __main__ - Epoch 179, Step:  388900, Batch Loss:     1.207890, Lr: 0.000017, Tokens per sec:   3434
2023-03-15 14:49:06,767 - INFO - __main__ - Epoch 179, Step:  389000, Batch Loss:     1.573575, Lr: 0.000017, Tokens per sec:   3477
2023-03-15 14:49:22,130 - INFO - __main__ - Epoch 179, Step:  389100, Batch Loss:     0.892009, Lr: 0.000017, Tokens per sec:   3475
2023-03-15 14:49:37,551 - INFO - __main__ - Epoch 179, Step:  389200, Batch Loss:     1.670390, Lr: 0.000017, Tokens per sec:   3569
2023-03-15 14:49:52,689 - INFO - __main__ - Epoch 179, Step:  389300, Batch Loss:     0.952597, Lr: 0.000017, Tokens per sec:   3549
2023-03-15 14:50:08,244 - INFO - __main__ - Epoch 179, Step:  389400, Batch Loss:     1.372839, Lr: 0.000017, Tokens per sec:   3474
2023-03-15 14:50:23,797 - INFO - __main__ - Epoch 179, Step:  389500, Batch Loss:     1.339889, Lr: 0.000017, Tokens per sec:   3504
2023-03-15 14:50:39,180 - INFO - __main__ - Epoch 179, Step:  389600, Batch Loss:     0.706569, Lr: 0.000017, Tokens per sec:   3506
2023-03-15 14:50:54,689 - INFO - __main__ - Epoch 179, Step:  389700, Batch Loss:     1.336652, Lr: 0.000017, Tokens per sec:   3435
2023-03-15 14:51:09,939 - INFO - __main__ - Epoch 179, Step:  389800, Batch Loss:     1.354143, Lr: 0.000017, Tokens per sec:   3533
2023-03-15 14:51:25,380 - INFO - __main__ - Epoch 179, Step:  389900, Batch Loss:     1.176249, Lr: 0.000017, Tokens per sec:   3462
2023-03-15 14:51:40,761 - INFO - __main__ - Epoch 179, Step:  390000, Batch Loss:     1.138469, Lr: 0.000017, Tokens per sec:   3544
2023-03-15 14:51:47,068 - INFO - __main__ - Epoch 179: total training loss 2321.42
2023-03-15 14:51:47,069 - INFO - __main__ - Epoch 180
2023-03-15 14:51:56,343 - INFO - __main__ - Epoch 180, Step:  390100, Batch Loss:     0.695676, Lr: 0.000017, Tokens per sec:   3368
2023-03-15 14:52:11,576 - INFO - __main__ - Epoch 180, Step:  390200, Batch Loss:     1.409706, Lr: 0.000017, Tokens per sec:   3599
2023-03-15 14:52:26,886 - INFO - __main__ - Epoch 180, Step:  390300, Batch Loss:     0.452111, Lr: 0.000017, Tokens per sec:   3492
2023-03-15 14:52:42,121 - INFO - __main__ - Epoch 180, Step:  390400, Batch Loss:     0.860007, Lr: 0.000017, Tokens per sec:   3572
2023-03-15 14:52:57,497 - INFO - __main__ - Epoch 180, Step:  390500, Batch Loss:     0.655136, Lr: 0.000017, Tokens per sec:   3415
2023-03-15 14:53:12,787 - INFO - __main__ - Epoch 180, Step:  390600, Batch Loss:     0.657328, Lr: 0.000017, Tokens per sec:   3531
2023-03-15 14:53:28,153 - INFO - __main__ - Epoch 180, Step:  390700, Batch Loss:     0.885345, Lr: 0.000017, Tokens per sec:   3477
2023-03-15 14:53:43,247 - INFO - __main__ - Epoch 180, Step:  390800, Batch Loss:     1.079484, Lr: 0.000017, Tokens per sec:   3522
2023-03-15 14:53:58,753 - INFO - __main__ - Epoch 180, Step:  390900, Batch Loss:     0.644013, Lr: 0.000017, Tokens per sec:   3445
2023-03-15 14:54:14,053 - INFO - __main__ - Epoch 180, Step:  391000, Batch Loss:     0.753238, Lr: 0.000017, Tokens per sec:   3471
2023-03-15 14:54:29,365 - INFO - __main__ - Epoch 180, Step:  391100, Batch Loss:     1.127667, Lr: 0.000017, Tokens per sec:   3502
2023-03-15 14:54:44,465 - INFO - __main__ - Epoch 180, Step:  391200, Batch Loss:     1.775952, Lr: 0.000017, Tokens per sec:   3575
2023-03-15 14:54:59,773 - INFO - __main__ - Epoch 180, Step:  391300, Batch Loss:     1.258009, Lr: 0.000017, Tokens per sec:   3551
2023-03-15 14:55:15,133 - INFO - __main__ - Epoch 180, Step:  391400, Batch Loss:     1.268215, Lr: 0.000017, Tokens per sec:   3549
2023-03-15 14:55:30,394 - INFO - __main__ - Epoch 180, Step:  391500, Batch Loss:     0.930743, Lr: 0.000017, Tokens per sec:   3518
2023-03-15 14:55:45,633 - INFO - __main__ - Epoch 180, Step:  391600, Batch Loss:     1.532490, Lr: 0.000017, Tokens per sec:   3548
2023-03-15 14:56:00,888 - INFO - __main__ - Epoch 180, Step:  391700, Batch Loss:     1.264691, Lr: 0.000017, Tokens per sec:   3564
2023-03-15 14:56:16,074 - INFO - __main__ - Epoch 180, Step:  391800, Batch Loss:     0.718470, Lr: 0.000017, Tokens per sec:   3566
2023-03-15 14:56:31,521 - INFO - __main__ - Epoch 180, Step:  391900, Batch Loss:     0.708763, Lr: 0.000017, Tokens per sec:   3548
2023-03-15 14:56:46,705 - INFO - __main__ - Epoch 180, Step:  392000, Batch Loss:     1.376088, Lr: 0.000017, Tokens per sec:   3504
2023-03-15 14:57:02,126 - INFO - __main__ - Epoch 180, Step:  392100, Batch Loss:     1.666866, Lr: 0.000017, Tokens per sec:   3502
2023-03-15 14:57:17,318 - INFO - __main__ - Epoch 180, Step:  392200, Batch Loss:     1.469877, Lr: 0.000017, Tokens per sec:   3567
2023-03-15 14:57:20,526 - INFO - __main__ - Epoch 180: total training loss 2291.50
2023-03-15 14:57:20,527 - INFO - __main__ - Epoch 181
2023-03-15 14:57:33,277 - INFO - __main__ - Epoch 181, Step:  392300, Batch Loss:     0.877882, Lr: 0.000016, Tokens per sec:   3346
2023-03-15 14:57:48,689 - INFO - __main__ - Epoch 181, Step:  392400, Batch Loss:     1.128789, Lr: 0.000016, Tokens per sec:   3535
2023-03-15 14:58:04,346 - INFO - __main__ - Epoch 181, Step:  392500, Batch Loss:     1.202372, Lr: 0.000016, Tokens per sec:   3445
2023-03-15 14:58:19,845 - INFO - __main__ - Epoch 181, Step:  392600, Batch Loss:     0.685995, Lr: 0.000016, Tokens per sec:   3549
2023-03-15 14:58:35,350 - INFO - __main__ - Epoch 181, Step:  392700, Batch Loss:     0.879818, Lr: 0.000016, Tokens per sec:   3515
2023-03-15 14:58:50,945 - INFO - __main__ - Epoch 181, Step:  392800, Batch Loss:     1.029933, Lr: 0.000016, Tokens per sec:   3500
2023-03-15 14:59:06,702 - INFO - __main__ - Epoch 181, Step:  392900, Batch Loss:     0.885376, Lr: 0.000016, Tokens per sec:   3416
2023-03-15 14:59:22,111 - INFO - __main__ - Epoch 181, Step:  393000, Batch Loss:     1.488934, Lr: 0.000016, Tokens per sec:   3484
2023-03-15 14:59:37,627 - INFO - __main__ - Epoch 181, Step:  393100, Batch Loss:     0.709958, Lr: 0.000016, Tokens per sec:   3408
2023-03-15 14:59:53,047 - INFO - __main__ - Epoch 181, Step:  393200, Batch Loss:     0.934856, Lr: 0.000016, Tokens per sec:   3448
2023-03-15 15:00:08,458 - INFO - __main__ - Epoch 181, Step:  393300, Batch Loss:     0.883753, Lr: 0.000016, Tokens per sec:   3483
2023-03-15 15:00:24,044 - INFO - __main__ - Epoch 181, Step:  393400, Batch Loss:     0.972751, Lr: 0.000016, Tokens per sec:   3477
2023-03-15 15:00:39,541 - INFO - __main__ - Epoch 181, Step:  393500, Batch Loss:     1.059181, Lr: 0.000016, Tokens per sec:   3497
2023-03-15 15:00:55,130 - INFO - __main__ - Epoch 181, Step:  393600, Batch Loss:     1.116735, Lr: 0.000016, Tokens per sec:   3420
2023-03-15 15:01:10,640 - INFO - __main__ - Epoch 181, Step:  393700, Batch Loss:     0.885932, Lr: 0.000016, Tokens per sec:   3434
2023-03-15 15:01:26,091 - INFO - __main__ - Epoch 181, Step:  393800, Batch Loss:     1.333865, Lr: 0.000016, Tokens per sec:   3532
2023-03-15 15:01:41,699 - INFO - __main__ - Epoch 181, Step:  393900, Batch Loss:     1.310475, Lr: 0.000016, Tokens per sec:   3457
2023-03-15 15:01:57,276 - INFO - __main__ - Epoch 181, Step:  394000, Batch Loss:     0.640916, Lr: 0.000016, Tokens per sec:   3433
2023-03-15 15:02:12,650 - INFO - __main__ - Epoch 181, Step:  394100, Batch Loss:     0.626693, Lr: 0.000016, Tokens per sec:   3517
2023-03-15 15:02:28,443 - INFO - __main__ - Epoch 181, Step:  394200, Batch Loss:     0.993689, Lr: 0.000016, Tokens per sec:   3378
2023-03-15 15:02:44,080 - INFO - __main__ - Epoch 181, Step:  394300, Batch Loss:     1.107559, Lr: 0.000016, Tokens per sec:   3422
2023-03-15 15:02:59,703 - INFO - __main__ - Epoch 181: total training loss 2275.71
2023-03-15 15:02:59,704 - INFO - __main__ - Epoch 182
2023-03-15 15:03:00,311 - INFO - __main__ - Epoch 182, Step:  394400, Batch Loss:     1.327493, Lr: 0.000016, Tokens per sec:    942
2023-03-15 15:03:15,779 - INFO - __main__ - Epoch 182, Step:  394500, Batch Loss:     0.745235, Lr: 0.000016, Tokens per sec:   3466
2023-03-15 15:03:31,305 - INFO - __main__ - Epoch 182, Step:  394600, Batch Loss:     1.245963, Lr: 0.000016, Tokens per sec:   3492
2023-03-15 15:03:46,872 - INFO - __main__ - Epoch 182, Step:  394700, Batch Loss:     1.076765, Lr: 0.000016, Tokens per sec:   3547
2023-03-15 15:04:02,219 - INFO - __main__ - Epoch 182, Step:  394800, Batch Loss:     1.014672, Lr: 0.000016, Tokens per sec:   3441
2023-03-15 15:04:17,447 - INFO - __main__ - Epoch 182, Step:  394900, Batch Loss:     1.789149, Lr: 0.000016, Tokens per sec:   3519
2023-03-15 15:04:32,802 - INFO - __main__ - Epoch 182, Step:  395000, Batch Loss:     1.019160, Lr: 0.000016, Tokens per sec:   3541
2023-03-15 15:04:48,401 - INFO - __main__ - Epoch 182, Step:  395100, Batch Loss:     1.085839, Lr: 0.000016, Tokens per sec:   3482
2023-03-15 15:05:04,016 - INFO - __main__ - Epoch 182, Step:  395200, Batch Loss:     1.188522, Lr: 0.000016, Tokens per sec:   3445
2023-03-15 15:05:19,572 - INFO - __main__ - Epoch 182, Step:  395300, Batch Loss:     0.643696, Lr: 0.000016, Tokens per sec:   3433
2023-03-15 15:05:35,218 - INFO - __main__ - Epoch 182, Step:  395400, Batch Loss:     1.495504, Lr: 0.000016, Tokens per sec:   3435
2023-03-15 15:05:50,971 - INFO - __main__ - Epoch 182, Step:  395500, Batch Loss:     0.807005, Lr: 0.000016, Tokens per sec:   3401
2023-03-15 15:06:06,557 - INFO - __main__ - Epoch 182, Step:  395600, Batch Loss:     1.217123, Lr: 0.000016, Tokens per sec:   3476
2023-03-15 15:06:22,132 - INFO - __main__ - Epoch 182, Step:  395700, Batch Loss:     0.807067, Lr: 0.000016, Tokens per sec:   3467
2023-03-15 15:06:37,700 - INFO - __main__ - Epoch 182, Step:  395800, Batch Loss:     1.743097, Lr: 0.000016, Tokens per sec:   3395
2023-03-15 15:06:53,124 - INFO - __main__ - Epoch 182, Step:  395900, Batch Loss:     1.001804, Lr: 0.000016, Tokens per sec:   3441
2023-03-15 15:07:08,634 - INFO - __main__ - Epoch 182, Step:  396000, Batch Loss:     0.910373, Lr: 0.000016, Tokens per sec:   3458
2023-03-15 15:07:23,902 - INFO - __main__ - Epoch 182, Step:  396100, Batch Loss:     0.837639, Lr: 0.000016, Tokens per sec:   3540
2023-03-15 15:07:39,507 - INFO - __main__ - Epoch 182, Step:  396200, Batch Loss:     1.116798, Lr: 0.000016, Tokens per sec:   3485
2023-03-15 15:07:55,198 - INFO - __main__ - Epoch 182, Step:  396300, Batch Loss:     0.941801, Lr: 0.000016, Tokens per sec:   3494
2023-03-15 15:08:10,643 - INFO - __main__ - Epoch 182, Step:  396400, Batch Loss:     0.960933, Lr: 0.000016, Tokens per sec:   3475
2023-03-15 15:08:26,012 - INFO - __main__ - Epoch 182, Step:  396500, Batch Loss:     1.091958, Lr: 0.000016, Tokens per sec:   3483
2023-03-15 15:08:38,033 - INFO - __main__ - Epoch 182: total training loss 2272.89
2023-03-15 15:08:38,034 - INFO - __main__ - Epoch 183
2023-03-15 15:08:41,778 - INFO - __main__ - Epoch 183, Step:  396600, Batch Loss:     0.868178, Lr: 0.000016, Tokens per sec:   3289
2023-03-15 15:08:57,228 - INFO - __main__ - Epoch 183, Step:  396700, Batch Loss:     0.860263, Lr: 0.000016, Tokens per sec:   3512
2023-03-15 15:09:12,773 - INFO - __main__ - Epoch 183, Step:  396800, Batch Loss:     0.546336, Lr: 0.000016, Tokens per sec:   3455
2023-03-15 15:09:28,382 - INFO - __main__ - Epoch 183, Step:  396900, Batch Loss:     1.292563, Lr: 0.000016, Tokens per sec:   3447
2023-03-15 15:09:43,849 - INFO - __main__ - Epoch 183, Step:  397000, Batch Loss:     1.145685, Lr: 0.000016, Tokens per sec:   3509
2023-03-15 15:09:59,372 - INFO - __main__ - Epoch 183, Step:  397100, Batch Loss:     1.531406, Lr: 0.000016, Tokens per sec:   3478
2023-03-15 15:10:14,681 - INFO - __main__ - Epoch 183, Step:  397200, Batch Loss:     1.102672, Lr: 0.000016, Tokens per sec:   3510
2023-03-15 15:10:30,074 - INFO - __main__ - Epoch 183, Step:  397300, Batch Loss:     1.016287, Lr: 0.000016, Tokens per sec:   3490
2023-03-15 15:10:45,683 - INFO - __main__ - Epoch 183, Step:  397400, Batch Loss:     1.143283, Lr: 0.000016, Tokens per sec:   3428
2023-03-15 15:11:01,161 - INFO - __main__ - Epoch 183, Step:  397500, Batch Loss:     1.134002, Lr: 0.000016, Tokens per sec:   3473
2023-03-15 15:11:16,890 - INFO - __main__ - Epoch 183, Step:  397600, Batch Loss:     0.906510, Lr: 0.000016, Tokens per sec:   3419
2023-03-15 15:11:32,432 - INFO - __main__ - Epoch 183, Step:  397700, Batch Loss:     0.782373, Lr: 0.000016, Tokens per sec:   3512
2023-03-15 15:11:47,927 - INFO - __main__ - Epoch 183, Step:  397800, Batch Loss:     1.291184, Lr: 0.000016, Tokens per sec:   3482
2023-03-15 15:12:03,262 - INFO - __main__ - Epoch 183, Step:  397900, Batch Loss:     1.139534, Lr: 0.000016, Tokens per sec:   3475
2023-03-15 15:12:18,697 - INFO - __main__ - Epoch 183, Step:  398000, Batch Loss:     1.098668, Lr: 0.000016, Tokens per sec:   3503
2023-03-15 15:12:34,322 - INFO - __main__ - Epoch 183, Step:  398100, Batch Loss:     0.487310, Lr: 0.000016, Tokens per sec:   3417
2023-03-15 15:12:49,703 - INFO - __main__ - Epoch 183, Step:  398200, Batch Loss:     1.161832, Lr: 0.000016, Tokens per sec:   3454
2023-03-15 15:13:05,177 - INFO - __main__ - Epoch 183, Step:  398300, Batch Loss:     0.870686, Lr: 0.000016, Tokens per sec:   3497
2023-03-15 15:13:20,642 - INFO - __main__ - Epoch 183, Step:  398400, Batch Loss:     1.256610, Lr: 0.000016, Tokens per sec:   3476
2023-03-15 15:13:36,174 - INFO - __main__ - Epoch 183, Step:  398500, Batch Loss:     0.593040, Lr: 0.000016, Tokens per sec:   3441
2023-03-15 15:13:51,316 - INFO - __main__ - Epoch 183, Step:  398600, Batch Loss:     1.377495, Lr: 0.000016, Tokens per sec:   3542
2023-03-15 15:14:06,526 - INFO - __main__ - Epoch 183, Step:  398700, Batch Loss:     0.990288, Lr: 0.000016, Tokens per sec:   3537
2023-03-15 15:14:15,542 - INFO - __main__ - Epoch 183: total training loss 2254.81
2023-03-15 15:14:15,543 - INFO - __main__ - Epoch 184
2023-03-15 15:14:22,616 - INFO - __main__ - Epoch 184, Step:  398800, Batch Loss:     1.030500, Lr: 0.000016, Tokens per sec:   3334
2023-03-15 15:14:37,922 - INFO - __main__ - Epoch 184, Step:  398900, Batch Loss:     1.192525, Lr: 0.000016, Tokens per sec:   3543
2023-03-15 15:14:53,205 - INFO - __main__ - Epoch 184, Step:  399000, Batch Loss:     0.955408, Lr: 0.000016, Tokens per sec:   3516
2023-03-15 15:15:08,476 - INFO - __main__ - Epoch 184, Step:  399100, Batch Loss:     1.314480, Lr: 0.000016, Tokens per sec:   3523
2023-03-15 15:15:23,826 - INFO - __main__ - Epoch 184, Step:  399200, Batch Loss:     1.022165, Lr: 0.000016, Tokens per sec:   3515
2023-03-15 15:15:39,246 - INFO - __main__ - Epoch 184, Step:  399300, Batch Loss:     1.192549, Lr: 0.000016, Tokens per sec:   3507
2023-03-15 15:15:54,656 - INFO - __main__ - Epoch 184, Step:  399400, Batch Loss:     1.328573, Lr: 0.000016, Tokens per sec:   3507
2023-03-15 15:16:09,860 - INFO - __main__ - Epoch 184, Step:  399500, Batch Loss:     0.855418, Lr: 0.000016, Tokens per sec:   3524
2023-03-15 15:16:24,999 - INFO - __main__ - Epoch 184, Step:  399600, Batch Loss:     1.044837, Lr: 0.000016, Tokens per sec:   3588
2023-03-15 15:16:40,537 - INFO - __main__ - Epoch 184, Step:  399700, Batch Loss:     0.715090, Lr: 0.000016, Tokens per sec:   3517
2023-03-15 15:16:55,858 - INFO - __main__ - Epoch 184, Step:  399800, Batch Loss:     1.495417, Lr: 0.000016, Tokens per sec:   3535
2023-03-15 15:17:10,914 - INFO - __main__ - Epoch 184, Step:  399900, Batch Loss:     1.326636, Lr: 0.000016, Tokens per sec:   3553
2023-03-15 15:17:26,223 - INFO - __main__ - Epoch 184, Step:  400000, Batch Loss:     0.576203, Lr: 0.000016, Tokens per sec:   3507
2023-03-15 15:17:41,626 - INFO - __main__ - Epoch 184, Step:  400100, Batch Loss:     1.536664, Lr: 0.000016, Tokens per sec:   3523
2023-03-15 15:17:56,629 - INFO - __main__ - Epoch 184, Step:  400200, Batch Loss:     1.344541, Lr: 0.000016, Tokens per sec:   3509
2023-03-15 15:18:11,839 - INFO - __main__ - Epoch 184, Step:  400300, Batch Loss:     0.787955, Lr: 0.000016, Tokens per sec:   3471
2023-03-15 15:18:26,979 - INFO - __main__ - Epoch 184, Step:  400400, Batch Loss:     0.876121, Lr: 0.000016, Tokens per sec:   3511
2023-03-15 15:18:42,445 - INFO - __main__ - Epoch 184, Step:  400500, Batch Loss:     0.735009, Lr: 0.000016, Tokens per sec:   3486
2023-03-15 15:18:58,080 - INFO - __main__ - Epoch 184, Step:  400600, Batch Loss:     1.258724, Lr: 0.000016, Tokens per sec:   3427
2023-03-15 15:19:13,224 - INFO - __main__ - Epoch 184, Step:  400700, Batch Loss:     1.452356, Lr: 0.000016, Tokens per sec:   3579
2023-03-15 15:19:28,394 - INFO - __main__ - Epoch 184, Step:  400800, Batch Loss:     1.017269, Lr: 0.000016, Tokens per sec:   3520
2023-03-15 15:19:43,688 - INFO - __main__ - Epoch 184, Step:  400900, Batch Loss:     1.026216, Lr: 0.000016, Tokens per sec:   3581
2023-03-15 15:19:49,324 - INFO - __main__ - Epoch 184: total training loss 2226.39
2023-03-15 15:19:49,325 - INFO - __main__ - Epoch 185
2023-03-15 15:19:59,492 - INFO - __main__ - Epoch 185, Step:  401000, Batch Loss:     0.787256, Lr: 0.000016, Tokens per sec:   3477
2023-03-15 15:20:14,832 - INFO - __main__ - Epoch 185, Step:  401100, Batch Loss:     0.848843, Lr: 0.000016, Tokens per sec:   3547
2023-03-15 15:20:29,881 - INFO - __main__ - Epoch 185, Step:  401200, Batch Loss:     1.110930, Lr: 0.000016, Tokens per sec:   3548
2023-03-15 15:20:45,015 - INFO - __main__ - Epoch 185, Step:  401300, Batch Loss:     1.006672, Lr: 0.000016, Tokens per sec:   3582
2023-03-15 15:21:00,146 - INFO - __main__ - Epoch 185, Step:  401400, Batch Loss:     0.580672, Lr: 0.000016, Tokens per sec:   3502
2023-03-15 15:21:15,246 - INFO - __main__ - Epoch 185, Step:  401500, Batch Loss:     0.901757, Lr: 0.000016, Tokens per sec:   3527
2023-03-15 15:21:30,385 - INFO - __main__ - Epoch 185, Step:  401600, Batch Loss:     1.530424, Lr: 0.000016, Tokens per sec:   3588
2023-03-15 15:21:45,605 - INFO - __main__ - Epoch 185, Step:  401700, Batch Loss:     1.344487, Lr: 0.000016, Tokens per sec:   3608
2023-03-15 15:22:00,616 - INFO - __main__ - Epoch 185, Step:  401800, Batch Loss:     0.525599, Lr: 0.000016, Tokens per sec:   3572
2023-03-15 15:22:15,758 - INFO - __main__ - Epoch 185, Step:  401900, Batch Loss:     0.714498, Lr: 0.000016, Tokens per sec:   3610
2023-03-15 15:22:30,952 - INFO - __main__ - Epoch 185, Step:  402000, Batch Loss:     1.009704, Lr: 0.000016, Tokens per sec:   3518
2023-03-15 15:22:46,372 - INFO - __main__ - Epoch 185, Step:  402100, Batch Loss:     1.268035, Lr: 0.000016, Tokens per sec:   3433
2023-03-15 15:23:01,730 - INFO - __main__ - Epoch 185, Step:  402200, Batch Loss:     0.907393, Lr: 0.000016, Tokens per sec:   3486
2023-03-15 15:23:16,976 - INFO - __main__ - Epoch 185, Step:  402300, Batch Loss:     1.196632, Lr: 0.000016, Tokens per sec:   3530
2023-03-15 15:23:32,133 - INFO - __main__ - Epoch 185, Step:  402400, Batch Loss:     1.098667, Lr: 0.000016, Tokens per sec:   3542
2023-03-15 15:23:47,225 - INFO - __main__ - Epoch 185, Step:  402500, Batch Loss:     0.802807, Lr: 0.000016, Tokens per sec:   3567
2023-03-15 15:24:02,174 - INFO - __main__ - Epoch 185, Step:  402600, Batch Loss:     0.983830, Lr: 0.000016, Tokens per sec:   3594
2023-03-15 15:24:17,472 - INFO - __main__ - Epoch 185, Step:  402700, Batch Loss:     0.841391, Lr: 0.000016, Tokens per sec:   3532
2023-03-15 15:24:32,872 - INFO - __main__ - Epoch 185, Step:  402800, Batch Loss:     0.613820, Lr: 0.000016, Tokens per sec:   3528
2023-03-15 15:24:48,069 - INFO - __main__ - Epoch 185, Step:  402900, Batch Loss:     1.196688, Lr: 0.000016, Tokens per sec:   3512
2023-03-15 15:25:03,386 - INFO - __main__ - Epoch 185, Step:  403000, Batch Loss:     1.230214, Lr: 0.000016, Tokens per sec:   3503
2023-03-15 15:25:18,430 - INFO - __main__ - Epoch 185, Step:  403100, Batch Loss:     0.741953, Lr: 0.000016, Tokens per sec:   3568
2023-03-15 15:25:20,794 - INFO - __main__ - Epoch 185: total training loss 2202.58
2023-03-15 15:25:20,795 - INFO - __main__ - Epoch 186
2023-03-15 15:25:33,997 - INFO - __main__ - Epoch 186, Step:  403200, Batch Loss:     1.196646, Lr: 0.000016, Tokens per sec:   3401
2023-03-15 15:25:49,058 - INFO - __main__ - Epoch 186, Step:  403300, Batch Loss:     0.488054, Lr: 0.000016, Tokens per sec:   3556
2023-03-15 15:26:04,092 - INFO - __main__ - Epoch 186, Step:  403400, Batch Loss:     1.411479, Lr: 0.000016, Tokens per sec:   3598
2023-03-15 15:26:19,232 - INFO - __main__ - Epoch 186, Step:  403500, Batch Loss:     0.976534, Lr: 0.000016, Tokens per sec:   3600
2023-03-15 15:26:34,486 - INFO - __main__ - Epoch 186, Step:  403600, Batch Loss:     0.492218, Lr: 0.000016, Tokens per sec:   3550
2023-03-15 15:26:49,480 - INFO - __main__ - Epoch 186, Step:  403700, Batch Loss:     1.039284, Lr: 0.000016, Tokens per sec:   3589
2023-03-15 15:27:04,114 - INFO - __main__ - Epoch 186, Step:  403800, Batch Loss:     1.157067, Lr: 0.000016, Tokens per sec:   3700
2023-03-15 15:27:19,314 - INFO - __main__ - Epoch 186, Step:  403900, Batch Loss:     0.651357, Lr: 0.000016, Tokens per sec:   3475
2023-03-15 15:27:34,637 - INFO - __main__ - Epoch 186, Step:  404000, Batch Loss:     0.643054, Lr: 0.000016, Tokens per sec:   3538
2023-03-15 15:27:49,708 - INFO - __main__ - Epoch 186, Step:  404100, Batch Loss:     1.078712, Lr: 0.000016, Tokens per sec:   3573
2023-03-15 15:28:04,931 - INFO - __main__ - Epoch 186, Step:  404200, Batch Loss:     1.412399, Lr: 0.000016, Tokens per sec:   3502
2023-03-15 15:28:20,141 - INFO - __main__ - Epoch 186, Step:  404300, Batch Loss:     0.460927, Lr: 0.000016, Tokens per sec:   3496
2023-03-15 15:28:35,361 - INFO - __main__ - Epoch 186, Step:  404400, Batch Loss:     0.910020, Lr: 0.000016, Tokens per sec:   3580
2023-03-15 15:28:50,498 - INFO - __main__ - Epoch 186, Step:  404500, Batch Loss:     1.694313, Lr: 0.000016, Tokens per sec:   3589
2023-03-15 15:29:05,791 - INFO - __main__ - Epoch 186, Step:  404600, Batch Loss:     1.131919, Lr: 0.000016, Tokens per sec:   3557
2023-03-15 15:29:21,235 - INFO - __main__ - Epoch 186, Step:  404700, Batch Loss:     1.158370, Lr: 0.000016, Tokens per sec:   3539
2023-03-15 15:29:36,626 - INFO - __main__ - Epoch 186, Step:  404800, Batch Loss:     0.574381, Lr: 0.000016, Tokens per sec:   3509
2023-03-15 15:29:51,973 - INFO - __main__ - Epoch 186, Step:  404900, Batch Loss:     1.227066, Lr: 0.000016, Tokens per sec:   3419
2023-03-15 15:30:07,171 - INFO - __main__ - Epoch 186, Step:  405000, Batch Loss:     1.005702, Lr: 0.000016, Tokens per sec:   3553
2023-03-15 15:30:21,804 - INFO - __main__ - Epoch 186, Step:  405100, Batch Loss:     0.885951, Lr: 0.000016, Tokens per sec:   3644
2023-03-15 15:30:37,104 - INFO - __main__ - Epoch 186, Step:  405200, Batch Loss:     0.978923, Lr: 0.000016, Tokens per sec:   3555
2023-03-15 15:30:51,831 - INFO - __main__ - Epoch 186: total training loss 2184.88
2023-03-15 15:30:51,833 - INFO - __main__ - Epoch 187
2023-03-15 15:30:53,148 - INFO - __main__ - Epoch 187, Step:  405300, Batch Loss:     0.892771, Lr: 0.000015, Tokens per sec:   2492
2023-03-15 15:31:08,425 - INFO - __main__ - Epoch 187, Step:  405400, Batch Loss:     0.890704, Lr: 0.000015, Tokens per sec:   3463
2023-03-15 15:31:23,635 - INFO - __main__ - Epoch 187, Step:  405500, Batch Loss:     1.264342, Lr: 0.000015, Tokens per sec:   3566
2023-03-15 15:31:38,345 - INFO - __main__ - Epoch 187, Step:  405600, Batch Loss:     0.545649, Lr: 0.000015, Tokens per sec:   3598
2023-03-15 15:31:53,622 - INFO - __main__ - Epoch 187, Step:  405700, Batch Loss:     0.816881, Lr: 0.000015, Tokens per sec:   3571
2023-03-15 15:32:08,678 - INFO - __main__ - Epoch 187, Step:  405800, Batch Loss:     0.754960, Lr: 0.000015, Tokens per sec:   3591
2023-03-15 15:32:23,953 - INFO - __main__ - Epoch 187, Step:  405900, Batch Loss:     0.699899, Lr: 0.000015, Tokens per sec:   3486
2023-03-15 15:32:38,800 - INFO - __main__ - Epoch 187, Step:  406000, Batch Loss:     0.667440, Lr: 0.000015, Tokens per sec:   3599
2023-03-15 15:32:54,009 - INFO - __main__ - Epoch 187, Step:  406100, Batch Loss:     0.684945, Lr: 0.000015, Tokens per sec:   3509
2023-03-15 15:33:09,204 - INFO - __main__ - Epoch 187, Step:  406200, Batch Loss:     0.565639, Lr: 0.000015, Tokens per sec:   3585
2023-03-15 15:33:24,475 - INFO - __main__ - Epoch 187, Step:  406300, Batch Loss:     1.094986, Lr: 0.000015, Tokens per sec:   3524
2023-03-15 15:33:39,651 - INFO - __main__ - Epoch 187, Step:  406400, Batch Loss:     1.137059, Lr: 0.000015, Tokens per sec:   3619
2023-03-15 15:33:54,661 - INFO - __main__ - Epoch 187, Step:  406500, Batch Loss:     1.485837, Lr: 0.000015, Tokens per sec:   3502
2023-03-15 15:34:10,145 - INFO - __main__ - Epoch 187, Step:  406600, Batch Loss:     0.989693, Lr: 0.000015, Tokens per sec:   3451
2023-03-15 15:34:25,174 - INFO - __main__ - Epoch 187, Step:  406700, Batch Loss:     0.635413, Lr: 0.000015, Tokens per sec:   3503
2023-03-15 15:34:40,233 - INFO - __main__ - Epoch 187, Step:  406800, Batch Loss:     0.993418, Lr: 0.000015, Tokens per sec:   3652
2023-03-15 15:34:55,415 - INFO - __main__ - Epoch 187, Step:  406900, Batch Loss:     1.390006, Lr: 0.000015, Tokens per sec:   3557
2023-03-15 15:35:10,669 - INFO - __main__ - Epoch 187, Step:  407000, Batch Loss:     0.812681, Lr: 0.000015, Tokens per sec:   3544
2023-03-15 15:35:25,801 - INFO - __main__ - Epoch 187, Step:  407100, Batch Loss:     1.298323, Lr: 0.000015, Tokens per sec:   3574
2023-03-15 15:35:40,868 - INFO - __main__ - Epoch 187, Step:  407200, Batch Loss:     0.614564, Lr: 0.000015, Tokens per sec:   3585
2023-03-15 15:35:56,264 - INFO - __main__ - Epoch 187, Step:  407300, Batch Loss:     0.594961, Lr: 0.000015, Tokens per sec:   3593
2023-03-15 15:36:11,847 - INFO - __main__ - Epoch 187, Step:  407400, Batch Loss:     1.495507, Lr: 0.000015, Tokens per sec:   3455
2023-03-15 15:36:23,093 - INFO - __main__ - Epoch 187: total training loss 2178.76
2023-03-15 15:36:23,094 - INFO - __main__ - Epoch 188
2023-03-15 15:36:27,508 - INFO - __main__ - Epoch 188, Step:  407500, Batch Loss:     1.006132, Lr: 0.000015, Tokens per sec:   3322
2023-03-15 15:36:42,573 - INFO - __main__ - Epoch 188, Step:  407600, Batch Loss:     1.563831, Lr: 0.000015, Tokens per sec:   3551
2023-03-15 15:36:57,588 - INFO - __main__ - Epoch 188, Step:  407700, Batch Loss:     0.770929, Lr: 0.000015, Tokens per sec:   3575
2023-03-15 15:37:12,988 - INFO - __main__ - Epoch 188, Step:  407800, Batch Loss:     0.912065, Lr: 0.000015, Tokens per sec:   3483
2023-03-15 15:37:28,328 - INFO - __main__ - Epoch 188, Step:  407900, Batch Loss:     0.909060, Lr: 0.000015, Tokens per sec:   3491
2023-03-15 15:37:43,857 - INFO - __main__ - Epoch 188, Step:  408000, Batch Loss:     0.732744, Lr: 0.000015, Tokens per sec:   3452
2023-03-15 15:37:59,239 - INFO - __main__ - Epoch 188, Step:  408100, Batch Loss:     0.889164, Lr: 0.000015, Tokens per sec:   3563
2023-03-15 15:38:14,497 - INFO - __main__ - Epoch 188, Step:  408200, Batch Loss:     0.502603, Lr: 0.000015, Tokens per sec:   3493
2023-03-15 15:38:29,758 - INFO - __main__ - Epoch 188, Step:  408300, Batch Loss:     0.787115, Lr: 0.000015, Tokens per sec:   3452
2023-03-15 15:38:45,124 - INFO - __main__ - Epoch 188, Step:  408400, Batch Loss:     0.924796, Lr: 0.000015, Tokens per sec:   3463
2023-03-15 15:39:00,723 - INFO - __main__ - Epoch 188, Step:  408500, Batch Loss:     0.817154, Lr: 0.000015, Tokens per sec:   3439
2023-03-15 15:39:16,004 - INFO - __main__ - Epoch 188, Step:  408600, Batch Loss:     1.299683, Lr: 0.000015, Tokens per sec:   3601
2023-03-15 15:39:31,216 - INFO - __main__ - Epoch 188, Step:  408700, Batch Loss:     0.672316, Lr: 0.000015, Tokens per sec:   3468
2023-03-15 15:39:46,303 - INFO - __main__ - Epoch 188, Step:  408800, Batch Loss:     1.391588, Lr: 0.000015, Tokens per sec:   3613
2023-03-15 15:40:00,801 - INFO - __main__ - Epoch 188, Step:  408900, Batch Loss:     0.941171, Lr: 0.000015, Tokens per sec:   3748
2023-03-15 15:40:15,904 - INFO - __main__ - Epoch 188, Step:  409000, Batch Loss:     1.311045, Lr: 0.000015, Tokens per sec:   3549
2023-03-15 15:40:31,070 - INFO - __main__ - Epoch 188, Step:  409100, Batch Loss:     0.871830, Lr: 0.000015, Tokens per sec:   3609
2023-03-15 15:40:46,317 - INFO - __main__ - Epoch 188, Step:  409200, Batch Loss:     0.606369, Lr: 0.000015, Tokens per sec:   3531
2023-03-15 15:41:01,448 - INFO - __main__ - Epoch 188, Step:  409300, Batch Loss:     0.981505, Lr: 0.000015, Tokens per sec:   3549
2023-03-15 15:41:16,697 - INFO - __main__ - Epoch 188, Step:  409400, Batch Loss:     1.201627, Lr: 0.000015, Tokens per sec:   3582
2023-03-15 15:41:31,909 - INFO - __main__ - Epoch 188, Step:  409500, Batch Loss:     0.848378, Lr: 0.000015, Tokens per sec:   3515
2023-03-15 15:41:47,089 - INFO - __main__ - Epoch 188, Step:  409600, Batch Loss:     0.396881, Lr: 0.000015, Tokens per sec:   3539
2023-03-15 15:41:55,250 - INFO - __main__ - Epoch 188: total training loss 2159.92
2023-03-15 15:41:55,250 - INFO - __main__ - Epoch 189
2023-03-15 15:42:02,778 - INFO - __main__ - Epoch 189, Step:  409700, Batch Loss:     0.755816, Lr: 0.000015, Tokens per sec:   3511
2023-03-15 15:42:18,258 - INFO - __main__ - Epoch 189, Step:  409800, Batch Loss:     0.999431, Lr: 0.000015, Tokens per sec:   3502
2023-03-15 15:42:33,649 - INFO - __main__ - Epoch 189, Step:  409900, Batch Loss:     1.045052, Lr: 0.000015, Tokens per sec:   3508
2023-03-15 15:42:48,889 - INFO - __main__ - Epoch 189, Step:  410000, Batch Loss:     0.596425, Lr: 0.000015, Tokens per sec:   3528
2023-03-15 15:43:04,345 - INFO - __main__ - Epoch 189, Step:  410100, Batch Loss:     0.705892, Lr: 0.000015, Tokens per sec:   3466
2023-03-15 15:43:19,245 - INFO - __main__ - Epoch 189, Step:  410200, Batch Loss:     1.110154, Lr: 0.000015, Tokens per sec:   3643
2023-03-15 15:43:34,651 - INFO - __main__ - Epoch 189, Step:  410300, Batch Loss:     0.842327, Lr: 0.000015, Tokens per sec:   3478
2023-03-15 15:43:49,769 - INFO - __main__ - Epoch 189, Step:  410400, Batch Loss:     0.891012, Lr: 0.000015, Tokens per sec:   3545
2023-03-15 15:44:04,844 - INFO - __main__ - Epoch 189, Step:  410500, Batch Loss:     0.931333, Lr: 0.000015, Tokens per sec:   3515
2023-03-15 15:44:19,842 - INFO - __main__ - Epoch 189, Step:  410600, Batch Loss:     1.382556, Lr: 0.000015, Tokens per sec:   3642
2023-03-15 15:44:35,091 - INFO - __main__ - Epoch 189, Step:  410700, Batch Loss:     0.744464, Lr: 0.000015, Tokens per sec:   3504
2023-03-15 15:44:50,096 - INFO - __main__ - Epoch 189, Step:  410800, Batch Loss:     0.805622, Lr: 0.000015, Tokens per sec:   3625
2023-03-15 15:45:05,209 - INFO - __main__ - Epoch 189, Step:  410900, Batch Loss:     0.451190, Lr: 0.000015, Tokens per sec:   3532
2023-03-15 15:45:20,521 - INFO - __main__ - Epoch 189, Step:  411000, Batch Loss:     1.327454, Lr: 0.000015, Tokens per sec:   3490
2023-03-15 15:45:36,023 - INFO - __main__ - Epoch 189, Step:  411100, Batch Loss:     0.847925, Lr: 0.000015, Tokens per sec:   3467
2023-03-15 15:45:51,185 - INFO - __main__ - Epoch 189, Step:  411200, Batch Loss:     1.287016, Lr: 0.000015, Tokens per sec:   3582
2023-03-15 15:46:06,288 - INFO - __main__ - Epoch 189, Step:  411300, Batch Loss:     0.988004, Lr: 0.000015, Tokens per sec:   3575
2023-03-15 15:46:21,566 - INFO - __main__ - Epoch 189, Step:  411400, Batch Loss:     0.892407, Lr: 0.000015, Tokens per sec:   3525
2023-03-15 15:46:36,812 - INFO - __main__ - Epoch 189, Step:  411500, Batch Loss:     1.224639, Lr: 0.000015, Tokens per sec:   3524
2023-03-15 15:46:51,472 - INFO - __main__ - Epoch 189, Step:  411600, Batch Loss:     1.024257, Lr: 0.000015, Tokens per sec:   3677
2023-03-15 15:47:06,739 - INFO - __main__ - Epoch 189, Step:  411700, Batch Loss:     1.164641, Lr: 0.000015, Tokens per sec:   3539
2023-03-15 15:47:21,771 - INFO - __main__ - Epoch 189, Step:  411800, Batch Loss:     1.029271, Lr: 0.000015, Tokens per sec:   3531
2023-03-15 15:47:26,627 - INFO - __main__ - Epoch 189: total training loss 2146.57
2023-03-15 15:47:26,628 - INFO - __main__ - Epoch 190
2023-03-15 15:47:37,310 - INFO - __main__ - Epoch 190, Step:  411900, Batch Loss:     1.397723, Lr: 0.000015, Tokens per sec:   3556
2023-03-15 15:47:52,244 - INFO - __main__ - Epoch 190, Step:  412000, Batch Loss:     0.981685, Lr: 0.000015, Tokens per sec:   3606
2023-03-15 15:48:07,522 - INFO - __main__ - Epoch 190, Step:  412100, Batch Loss:     1.050957, Lr: 0.000015, Tokens per sec:   3552
2023-03-15 15:48:22,737 - INFO - __main__ - Epoch 190, Step:  412200, Batch Loss:     1.004895, Lr: 0.000015, Tokens per sec:   3599
2023-03-15 15:48:37,836 - INFO - __main__ - Epoch 190, Step:  412300, Batch Loss:     1.532441, Lr: 0.000015, Tokens per sec:   3560
2023-03-15 15:48:53,043 - INFO - __main__ - Epoch 190, Step:  412400, Batch Loss:     1.084127, Lr: 0.000015, Tokens per sec:   3455
2023-03-15 15:49:08,110 - INFO - __main__ - Epoch 190, Step:  412500, Batch Loss:     1.046430, Lr: 0.000015, Tokens per sec:   3514
2023-03-15 15:49:23,134 - INFO - __main__ - Epoch 190, Step:  412600, Batch Loss:     1.013227, Lr: 0.000015, Tokens per sec:   3600
2023-03-15 15:49:38,128 - INFO - __main__ - Epoch 190, Step:  412700, Batch Loss:     1.051447, Lr: 0.000015, Tokens per sec:   3525
2023-03-15 15:49:53,627 - INFO - __main__ - Epoch 190, Step:  412800, Batch Loss:     1.274479, Lr: 0.000015, Tokens per sec:   3458
2023-03-15 15:50:08,675 - INFO - __main__ - Epoch 190, Step:  412900, Batch Loss:     0.841251, Lr: 0.000015, Tokens per sec:   3616
2023-03-15 15:50:23,941 - INFO - __main__ - Epoch 190, Step:  413000, Batch Loss:     0.736378, Lr: 0.000015, Tokens per sec:   3572
2023-03-15 15:50:39,322 - INFO - __main__ - Epoch 190, Step:  413100, Batch Loss:     0.814163, Lr: 0.000015, Tokens per sec:   3511
2023-03-15 15:50:54,697 - INFO - __main__ - Epoch 190, Step:  413200, Batch Loss:     1.470523, Lr: 0.000015, Tokens per sec:   3528
2023-03-15 15:51:09,655 - INFO - __main__ - Epoch 190, Step:  413300, Batch Loss:     1.336362, Lr: 0.000015, Tokens per sec:   3558
2023-03-15 15:51:24,943 - INFO - __main__ - Epoch 190, Step:  413400, Batch Loss:     0.787002, Lr: 0.000015, Tokens per sec:   3498
2023-03-15 15:51:39,832 - INFO - __main__ - Epoch 190, Step:  413500, Batch Loss:     1.139829, Lr: 0.000015, Tokens per sec:   3633
2023-03-15 15:51:55,007 - INFO - __main__ - Epoch 190, Step:  413600, Batch Loss:     0.701026, Lr: 0.000015, Tokens per sec:   3503
2023-03-15 15:52:10,256 - INFO - __main__ - Epoch 190, Step:  413700, Batch Loss:     1.467019, Lr: 0.000015, Tokens per sec:   3535
2023-03-15 15:52:26,012 - INFO - __main__ - Epoch 190, Step:  413800, Batch Loss:     1.100409, Lr: 0.000015, Tokens per sec:   3402
2023-03-15 15:52:41,363 - INFO - __main__ - Epoch 190, Step:  413900, Batch Loss:     0.589104, Lr: 0.000015, Tokens per sec:   3532
2023-03-15 15:52:56,624 - INFO - __main__ - Epoch 190, Step:  414000, Batch Loss:     0.962987, Lr: 0.000015, Tokens per sec:   3565
2023-03-15 15:52:58,204 - INFO - __main__ - Epoch 190: total training loss 2153.81
2023-03-15 15:52:58,204 - INFO - __main__ - Epoch 191
2023-03-15 15:53:12,375 - INFO - __main__ - Epoch 191, Step:  414100, Batch Loss:     1.585827, Lr: 0.000015, Tokens per sec:   3379
2023-03-15 15:53:27,649 - INFO - __main__ - Epoch 191, Step:  414200, Batch Loss:     0.969712, Lr: 0.000015, Tokens per sec:   3517
2023-03-15 15:53:42,702 - INFO - __main__ - Epoch 191, Step:  414300, Batch Loss:     0.755238, Lr: 0.000015, Tokens per sec:   3564
2023-03-15 15:53:57,876 - INFO - __main__ - Epoch 191, Step:  414400, Batch Loss:     0.646471, Lr: 0.000015, Tokens per sec:   3491
2023-03-15 15:54:13,078 - INFO - __main__ - Epoch 191, Step:  414500, Batch Loss:     1.018227, Lr: 0.000015, Tokens per sec:   3539
2023-03-15 15:54:28,260 - INFO - __main__ - Epoch 191, Step:  414600, Batch Loss:     1.044075, Lr: 0.000015, Tokens per sec:   3583
2023-03-15 15:54:43,398 - INFO - __main__ - Epoch 191, Step:  414700, Batch Loss:     1.416421, Lr: 0.000015, Tokens per sec:   3515
2023-03-15 15:54:58,422 - INFO - __main__ - Epoch 191, Step:  414800, Batch Loss:     0.746510, Lr: 0.000015, Tokens per sec:   3605
2023-03-15 15:55:13,662 - INFO - __main__ - Epoch 191, Step:  414900, Batch Loss:     0.993318, Lr: 0.000015, Tokens per sec:   3517
2023-03-15 15:55:28,694 - INFO - __main__ - Epoch 191, Step:  415000, Batch Loss:     0.635153, Lr: 0.000015, Tokens per sec:   3569
2023-03-15 15:55:43,967 - INFO - __main__ - Epoch 191, Step:  415100, Batch Loss:     0.837902, Lr: 0.000015, Tokens per sec:   3572
2023-03-15 15:55:58,849 - INFO - __main__ - Epoch 191, Step:  415200, Batch Loss:     0.769467, Lr: 0.000015, Tokens per sec:   3654
2023-03-15 15:56:14,325 - INFO - __main__ - Epoch 191, Step:  415300, Batch Loss:     0.675871, Lr: 0.000015, Tokens per sec:   3515
2023-03-15 15:56:29,707 - INFO - __main__ - Epoch 191, Step:  415400, Batch Loss:     1.076038, Lr: 0.000015, Tokens per sec:   3548
2023-03-15 15:56:45,004 - INFO - __main__ - Epoch 191, Step:  415500, Batch Loss:     1.082424, Lr: 0.000015, Tokens per sec:   3493
2023-03-15 15:57:00,280 - INFO - __main__ - Epoch 191, Step:  415600, Batch Loss:     1.162521, Lr: 0.000015, Tokens per sec:   3573
2023-03-15 15:57:15,467 - INFO - __main__ - Epoch 191, Step:  415700, Batch Loss:     1.241630, Lr: 0.000015, Tokens per sec:   3504
2023-03-15 15:57:30,570 - INFO - __main__ - Epoch 191, Step:  415800, Batch Loss:     0.737000, Lr: 0.000015, Tokens per sec:   3515
2023-03-15 15:57:45,720 - INFO - __main__ - Epoch 191, Step:  415900, Batch Loss:     1.412797, Lr: 0.000015, Tokens per sec:   3584
2023-03-15 15:58:00,504 - INFO - __main__ - Epoch 191, Step:  416000, Batch Loss:     1.042472, Lr: 0.000015, Tokens per sec:   3601
2023-03-15 15:58:15,830 - INFO - __main__ - Epoch 191, Step:  416100, Batch Loss:     0.753401, Lr: 0.000015, Tokens per sec:   3509
2023-03-15 15:58:29,496 - INFO - __main__ - Epoch 191: total training loss 2138.26
2023-03-15 15:58:29,497 - INFO - __main__ - Epoch 192
2023-03-15 15:58:31,517 - INFO - __main__ - Epoch 192, Step:  416200, Batch Loss:     0.773065, Lr: 0.000015, Tokens per sec:   3086
2023-03-15 15:58:46,555 - INFO - __main__ - Epoch 192, Step:  416300, Batch Loss:     0.965844, Lr: 0.000015, Tokens per sec:   3595
2023-03-15 15:59:01,890 - INFO - __main__ - Epoch 192, Step:  416400, Batch Loss:     0.985015, Lr: 0.000015, Tokens per sec:   3542
2023-03-15 15:59:17,040 - INFO - __main__ - Epoch 192, Step:  416500, Batch Loss:     0.753023, Lr: 0.000015, Tokens per sec:   3541
