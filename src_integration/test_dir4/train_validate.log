2023-03-14 10:32:24,491 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-14 10:33:56,695 - INFO - data - average code tokens = 109.28999515442095
2023-03-14 10:33:56,695 - INFO - data - average ast tokens = 188.85505342888476
2023-03-14 10:33:56,695 - INFO - data - average text tokens = 15.993139680191783
2023-03-14 10:33:56,695 - INFO - data - average position tokens = 188.85505342888476
2023-03-14 10:33:56,695 - INFO - data - average ast edges = 375.7101068577695
2023-03-14 10:34:13,326 - INFO - data - code vocab length = 26684
2023-03-14 10:34:13,326 - INFO - data - text vocab length = 13207
2023-03-14 10:34:13,326 - INFO - data - position vocab length = 20587
2023-03-14 10:34:28,802 - INFO - model - Build Model...
2023-03-14 10:34:29,784 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (2): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (3): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (4): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (5): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-14 10:34:29,793 - INFO - model - Total parameters number: 91563520
2023-03-14 10:34:29,795 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-14 10:34:29,795 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,796 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-14 10:34:29,797 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.weight               [512]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.bias                 [512]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.weight [512, 512]
2023-03-14 10:34:29,798 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.bias [512]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.weight [512, 512]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.bias [512]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.weight [512, 512]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.bias [512]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.bias [512]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,799 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.weight               [512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.bias                 [512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.weight [512, 512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.bias [512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.weight [512, 512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.bias [512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.weight [512, 512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.bias [512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.bias [512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,800 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.weight               [512]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.bias                 [512]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.weight [512, 512]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.bias [512]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.weight [512, 512]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.bias [512]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.weight [512, 512]
2023-03-14 10:34:29,801 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.bias [512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.bias [512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.weight               [512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.bias                 [512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.weight [512, 512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.bias [512]
2023-03-14 10:34:29,802 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.weight [512, 512]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.bias [512]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.weight [512, 512]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.bias [512]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.weight [512, 512]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.bias [512]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,803 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-14 10:34:29,804 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-14 10:34:29,805 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,806 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,807 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,808 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,809 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,811 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,811 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,812 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,813 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,814 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,815 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-14 10:34:29,816 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-14 10:34:29,817 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-14 10:34:29,817 - INFO - model - The model is built.
2023-03-14 10:34:29,823 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-14 10:34:29,823 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-14 10:34:31,229 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-14 10:34:31,229 - INFO - __main__ - Scheduler = StepLR
2023-03-14 10:34:31,230 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-14 10:34:31,230 - INFO - __main__ - Epoch 1
2023-03-14 10:34:52,959 - INFO - __main__ - Epoch   1, Step:     100, Batch Loss:    90.195351, Lr: 0.000100, Tokens per sec:   2437
2023-03-14 10:35:12,980 - INFO - __main__ - Epoch   1, Step:     200, Batch Loss:    84.846024, Lr: 0.000100, Tokens per sec:   2653
2023-03-14 10:35:33,215 - INFO - __main__ - Epoch   1, Step:     300, Batch Loss:    74.624908, Lr: 0.000100, Tokens per sec:   2715
2023-03-14 10:35:53,529 - INFO - __main__ - Epoch   1, Step:     400, Batch Loss:    72.335991, Lr: 0.000100, Tokens per sec:   2673
2023-03-14 10:36:13,497 - INFO - __main__ - Epoch   1, Step:     500, Batch Loss:    68.081276, Lr: 0.000100, Tokens per sec:   2721
2023-03-14 10:36:33,433 - INFO - __main__ - Epoch   1, Step:     600, Batch Loss:    89.625786, Lr: 0.000100, Tokens per sec:   2703
2023-03-14 10:36:53,739 - INFO - __main__ - Epoch   1, Step:     700, Batch Loss:    78.360703, Lr: 0.000100, Tokens per sec:   2641
2023-03-14 10:37:14,405 - INFO - __main__ - Epoch   1, Step:     800, Batch Loss:    71.245308, Lr: 0.000100, Tokens per sec:   2608
2023-03-14 10:37:34,886 - INFO - __main__ - Epoch   1, Step:     900, Batch Loss:    73.362045, Lr: 0.000100, Tokens per sec:   2668
2023-03-14 10:37:54,835 - INFO - __main__ - Epoch   1, Step:    1000, Batch Loss:    82.151535, Lr: 0.000100, Tokens per sec:   2689
2023-03-14 10:38:15,726 - INFO - __main__ - Epoch   1, Step:    1100, Batch Loss:    80.555458, Lr: 0.000100, Tokens per sec:   2530
2023-03-14 10:38:49,064 - INFO - __main__ - Epoch   1, Step:    1200, Batch Loss:    69.340622, Lr: 0.000100, Tokens per sec:   1611
2023-03-14 10:39:26,267 - INFO - __main__ - Epoch   1, Step:    1300, Batch Loss:    73.405159, Lr: 0.000100, Tokens per sec:   1464
2023-03-14 10:40:03,743 - INFO - __main__ - Epoch   1, Step:    1400, Batch Loss:    75.890244, Lr: 0.000100, Tokens per sec:   1414
2023-03-14 10:40:40,378 - INFO - __main__ - Epoch   1, Step:    1500, Batch Loss:    88.948059, Lr: 0.000100, Tokens per sec:   1458
2023-03-14 10:41:17,235 - INFO - __main__ - Epoch   1, Step:    1600, Batch Loss:    97.008064, Lr: 0.000100, Tokens per sec:   1469
2023-03-14 10:41:54,224 - INFO - __main__ - Epoch   1, Step:    1700, Batch Loss:    86.817764, Lr: 0.000100, Tokens per sec:   1479
2023-03-14 10:42:31,188 - INFO - __main__ - Epoch   1, Step:    1800, Batch Loss:    85.760002, Lr: 0.000100, Tokens per sec:   1466
2023-03-14 10:43:07,392 - INFO - __main__ - Epoch   1, Step:    1900, Batch Loss:    90.264450, Lr: 0.000100, Tokens per sec:   1487
2023-03-14 10:43:44,409 - INFO - __main__ - Epoch   1, Step:    2000, Batch Loss:    70.567284, Lr: 0.000100, Tokens per sec:   1460
2023-03-14 10:44:21,435 - INFO - __main__ - Epoch   1, Step:    2100, Batch Loss:    72.245712, Lr: 0.000100, Tokens per sec:   1441
2023-03-14 10:44:50,802 - INFO - __main__ - Epoch   1: total training loss 171148.08
2023-03-14 10:44:50,802 - INFO - __main__ - Epoch 2
2023-03-14 10:44:59,261 - INFO - __main__ - Epoch   2, Step:    2200, Batch Loss:    70.701691, Lr: 0.000099, Tokens per sec:   1322
2023-03-14 10:45:36,542 - INFO - __main__ - Epoch   2, Step:    2300, Batch Loss:    64.074875, Lr: 0.000099, Tokens per sec:   1436
2023-03-14 10:46:13,030 - INFO - __main__ - Epoch   2, Step:    2400, Batch Loss:    60.971931, Lr: 0.000099, Tokens per sec:   1450
2023-03-14 10:46:50,140 - INFO - __main__ - Epoch   2, Step:    2500, Batch Loss:    49.797573, Lr: 0.000099, Tokens per sec:   1471
2023-03-14 10:47:27,866 - INFO - __main__ - Epoch   2, Step:    2600, Batch Loss:    62.306042, Lr: 0.000099, Tokens per sec:   1433
2023-03-14 10:48:04,122 - INFO - __main__ - Epoch   2, Step:    2700, Batch Loss:    58.502003, Lr: 0.000099, Tokens per sec:   1482
2023-03-14 10:48:41,576 - INFO - __main__ - Epoch   2, Step:    2800, Batch Loss:    71.761017, Lr: 0.000099, Tokens per sec:   1448
2023-03-14 10:49:17,835 - INFO - __main__ - Epoch   2, Step:    2900, Batch Loss:    70.494888, Lr: 0.000099, Tokens per sec:   1486
2023-03-14 10:49:54,028 - INFO - __main__ - Epoch   2, Step:    3000, Batch Loss:    67.346260, Lr: 0.000099, Tokens per sec:   1488
2023-03-14 10:50:30,359 - INFO - __main__ - Epoch   2, Step:    3100, Batch Loss:    57.134125, Lr: 0.000099, Tokens per sec:   1484
2023-03-14 10:51:06,608 - INFO - __main__ - Epoch   2, Step:    3200, Batch Loss:    53.911251, Lr: 0.000099, Tokens per sec:   1495
2023-03-14 10:51:44,401 - INFO - __main__ - Epoch   2, Step:    3300, Batch Loss:    60.399910, Lr: 0.000099, Tokens per sec:   1409
2023-03-14 10:52:21,086 - INFO - __main__ - Epoch   2, Step:    3400, Batch Loss:    65.129486, Lr: 0.000099, Tokens per sec:   1466
2023-03-14 10:52:57,044 - INFO - __main__ - Epoch   2, Step:    3500, Batch Loss:    61.944492, Lr: 0.000099, Tokens per sec:   1494
2023-03-14 10:53:33,622 - INFO - __main__ - Epoch   2, Step:    3600, Batch Loss:    49.618206, Lr: 0.000099, Tokens per sec:   1471
2023-03-14 10:54:10,069 - INFO - __main__ - Epoch   2, Step:    3700, Batch Loss:    58.095211, Lr: 0.000099, Tokens per sec:   1512
2023-03-14 10:54:46,127 - INFO - __main__ - Epoch   2, Step:    3800, Batch Loss:    78.504585, Lr: 0.000099, Tokens per sec:   1487
2023-03-14 10:55:23,083 - INFO - __main__ - Epoch   2, Step:    3900, Batch Loss:    56.859646, Lr: 0.000099, Tokens per sec:   1471
2023-03-14 10:55:59,640 - INFO - __main__ - Epoch   2, Step:    4000, Batch Loss:    53.508682, Lr: 0.000099, Tokens per sec:   1472
2023-03-14 10:56:36,583 - INFO - __main__ - Epoch   2, Step:    4100, Batch Loss:    51.371830, Lr: 0.000099, Tokens per sec:   1418
2023-03-14 10:57:13,193 - INFO - __main__ - Epoch   2, Step:    4200, Batch Loss:    67.791824, Lr: 0.000099, Tokens per sec:   1455
2023-03-14 10:57:49,880 - INFO - __main__ - Epoch   2, Step:    4300, Batch Loss:    79.073082, Lr: 0.000099, Tokens per sec:   1484
2023-03-14 10:58:12,235 - INFO - __main__ - Epoch   2: total training loss 139190.40
2023-03-14 10:58:12,236 - INFO - __main__ - Epoch 3
2023-03-14 10:58:27,856 - INFO - __main__ - Epoch   3, Step:    4400, Batch Loss:    70.158569, Lr: 0.000098, Tokens per sec:   1461
2023-03-14 10:59:04,874 - INFO - __main__ - Epoch   3, Step:    4500, Batch Loss:    61.868736, Lr: 0.000098, Tokens per sec:   1453
2023-03-14 10:59:41,738 - INFO - __main__ - Epoch   3, Step:    4600, Batch Loss:    46.825935, Lr: 0.000098, Tokens per sec:   1451
2023-03-14 11:00:18,123 - INFO - __main__ - Epoch   3, Step:    4700, Batch Loss:    53.883434, Lr: 0.000098, Tokens per sec:   1500
2023-03-14 11:00:54,811 - INFO - __main__ - Epoch   3, Step:    4800, Batch Loss:    69.113876, Lr: 0.000098, Tokens per sec:   1487
2023-03-14 11:01:31,905 - INFO - __main__ - Epoch   3, Step:    4900, Batch Loss:    54.505188, Lr: 0.000098, Tokens per sec:   1442
2023-03-14 11:02:08,545 - INFO - __main__ - Epoch   3, Step:    5000, Batch Loss:    43.805767, Lr: 0.000098, Tokens per sec:   1473
2023-03-14 11:02:45,039 - INFO - __main__ - Epoch   3, Step:    5100, Batch Loss:    60.791107, Lr: 0.000098, Tokens per sec:   1476
2023-03-14 11:03:21,670 - INFO - __main__ - Epoch   3, Step:    5200, Batch Loss:    57.666687, Lr: 0.000098, Tokens per sec:   1482
2023-03-14 11:03:58,490 - INFO - __main__ - Epoch   3, Step:    5300, Batch Loss:    53.489296, Lr: 0.000098, Tokens per sec:   1442
2023-03-14 11:04:35,500 - INFO - __main__ - Epoch   3, Step:    5400, Batch Loss:    54.719536, Lr: 0.000098, Tokens per sec:   1454
2023-03-14 11:05:12,000 - INFO - __main__ - Epoch   3, Step:    5500, Batch Loss:    68.672073, Lr: 0.000098, Tokens per sec:   1450
2023-03-14 11:05:48,834 - INFO - __main__ - Epoch   3, Step:    5600, Batch Loss:    52.159878, Lr: 0.000098, Tokens per sec:   1463
2023-03-14 11:06:25,778 - INFO - __main__ - Epoch   3, Step:    5700, Batch Loss:    42.823307, Lr: 0.000098, Tokens per sec:   1466
2023-03-14 11:07:02,812 - INFO - __main__ - Epoch   3, Step:    5800, Batch Loss:    73.955055, Lr: 0.000098, Tokens per sec:   1441
2023-03-14 11:07:40,691 - INFO - __main__ - Epoch   3, Step:    5900, Batch Loss:    59.065948, Lr: 0.000098, Tokens per sec:   1419
2023-03-14 11:08:17,257 - INFO - __main__ - Epoch   3, Step:    6000, Batch Loss:    60.702328, Lr: 0.000098, Tokens per sec:   1487
2023-03-14 11:08:53,378 - INFO - __main__ - Epoch   3, Step:    6100, Batch Loss:    43.231003, Lr: 0.000098, Tokens per sec:   1479
2023-03-14 11:09:30,100 - INFO - __main__ - Epoch   3, Step:    6200, Batch Loss:    53.952080, Lr: 0.000098, Tokens per sec:   1476
2023-03-14 11:10:07,059 - INFO - __main__ - Epoch   3, Step:    6300, Batch Loss:    62.366058, Lr: 0.000098, Tokens per sec:   1454
2023-03-14 11:10:44,096 - INFO - __main__ - Epoch   3, Step:    6400, Batch Loss:    48.538498, Lr: 0.000098, Tokens per sec:   1469
2023-03-14 11:11:20,859 - INFO - __main__ - Epoch   3, Step:    6500, Batch Loss:    59.688660, Lr: 0.000098, Tokens per sec:   1457
2023-03-14 11:11:34,523 - INFO - __main__ - Epoch   3: total training loss 124389.39
2023-03-14 11:11:34,524 - INFO - __main__ - Epoch 4
2023-03-14 11:11:57,927 - INFO - __main__ - Epoch   4, Step:    6600, Batch Loss:    43.898552, Lr: 0.000097, Tokens per sec:   1471
2023-03-14 11:12:34,992 - INFO - __main__ - Epoch   4, Step:    6700, Batch Loss:    57.320732, Lr: 0.000097, Tokens per sec:   1446
2023-03-14 11:13:11,489 - INFO - __main__ - Epoch   4, Step:    6800, Batch Loss:    54.480663, Lr: 0.000097, Tokens per sec:   1487
2023-03-14 11:13:47,981 - INFO - __main__ - Epoch   4, Step:    6900, Batch Loss:    53.876762, Lr: 0.000097, Tokens per sec:   1482
2023-03-14 11:14:25,132 - INFO - __main__ - Epoch   4, Step:    7000, Batch Loss:    53.586140, Lr: 0.000097, Tokens per sec:   1430
2023-03-14 11:15:01,189 - INFO - __main__ - Epoch   4, Step:    7100, Batch Loss:    63.709694, Lr: 0.000097, Tokens per sec:   1522
2023-03-14 11:15:37,149 - INFO - __main__ - Epoch   4, Step:    7200, Batch Loss:    46.437973, Lr: 0.000097, Tokens per sec:   1473
2023-03-14 11:16:13,125 - INFO - __main__ - Epoch   4, Step:    7300, Batch Loss:    57.833626, Lr: 0.000097, Tokens per sec:   1515
2023-03-14 11:16:49,121 - INFO - __main__ - Epoch   4, Step:    7400, Batch Loss:    52.980350, Lr: 0.000097, Tokens per sec:   1488
2023-03-14 11:17:25,290 - INFO - __main__ - Epoch   4, Step:    7500, Batch Loss:    47.026646, Lr: 0.000097, Tokens per sec:   1517
2023-03-14 11:18:01,812 - INFO - __main__ - Epoch   4, Step:    7600, Batch Loss:    57.176373, Lr: 0.000097, Tokens per sec:   1487
2023-03-14 11:18:38,245 - INFO - __main__ - Epoch   4, Step:    7700, Batch Loss:    50.153023, Lr: 0.000097, Tokens per sec:   1450
2023-03-14 11:19:14,096 - INFO - __main__ - Epoch   4, Step:    7800, Batch Loss:    51.572708, Lr: 0.000097, Tokens per sec:   1491
2023-03-14 11:19:36,693 - INFO - __main__ - Epoch   4, Step:    7900, Batch Loss:    63.711288, Lr: 0.000097, Tokens per sec:   2361
2023-03-14 11:19:56,518 - INFO - __main__ - Epoch   4, Step:    8000, Batch Loss:    57.631596, Lr: 0.000097, Tokens per sec:   2714
2023-03-14 11:20:16,232 - INFO - __main__ - Epoch   4, Step:    8100, Batch Loss:    55.720760, Lr: 0.000097, Tokens per sec:   2746
2023-03-14 11:20:36,179 - INFO - __main__ - Epoch   4, Step:    8200, Batch Loss:    43.343670, Lr: 0.000097, Tokens per sec:   2675
2023-03-14 11:20:55,856 - INFO - __main__ - Epoch   4, Step:    8300, Batch Loss:    41.685818, Lr: 0.000097, Tokens per sec:   2723
2023-03-14 11:21:16,064 - INFO - __main__ - Epoch   4, Step:    8400, Batch Loss:    46.898495, Lr: 0.000097, Tokens per sec:   2654
2023-03-14 11:21:35,843 - INFO - __main__ - Epoch   4, Step:    8500, Batch Loss:    52.935040, Lr: 0.000097, Tokens per sec:   2765
2023-03-14 11:21:55,672 - INFO - __main__ - Epoch   4, Step:    8600, Batch Loss:    38.984879, Lr: 0.000097, Tokens per sec:   2675
2023-03-14 11:22:15,516 - INFO - __main__ - Epoch   4, Step:    8700, Batch Loss:    42.379883, Lr: 0.000097, Tokens per sec:   2721
2023-03-14 11:22:18,697 - INFO - __main__ - Epoch   4: total training loss 113549.45
2023-03-14 11:22:18,697 - INFO - __main__ - Epoch 5
2023-03-14 11:22:35,641 - INFO - __main__ - Epoch   5, Step:    8800, Batch Loss:    50.147984, Lr: 0.000096, Tokens per sec:   2647
2023-03-14 11:22:55,456 - INFO - __main__ - Epoch   5, Step:    8900, Batch Loss:    55.129715, Lr: 0.000096, Tokens per sec:   2740
2023-03-14 11:23:15,152 - INFO - __main__ - Epoch   5, Step:    9000, Batch Loss:    49.137051, Lr: 0.000096, Tokens per sec:   2741
2023-03-14 11:23:34,900 - INFO - __main__ - Epoch   5, Step:    9100, Batch Loss:    42.569042, Lr: 0.000096, Tokens per sec:   2677
2023-03-14 11:23:54,642 - INFO - __main__ - Epoch   5, Step:    9200, Batch Loss:    56.550892, Lr: 0.000096, Tokens per sec:   2759
2023-03-14 11:24:14,363 - INFO - __main__ - Epoch   5, Step:    9300, Batch Loss:    44.540684, Lr: 0.000096, Tokens per sec:   2694
2023-03-14 11:24:34,128 - INFO - __main__ - Epoch   5, Step:    9400, Batch Loss:    40.390549, Lr: 0.000096, Tokens per sec:   2718
2023-03-14 11:24:53,770 - INFO - __main__ - Epoch   5, Step:    9500, Batch Loss:    42.328781, Lr: 0.000096, Tokens per sec:   2712
2023-03-14 11:25:13,851 - INFO - __main__ - Epoch   5, Step:    9600, Batch Loss:    63.594696, Lr: 0.000096, Tokens per sec:   2655
2023-03-14 11:25:33,500 - INFO - __main__ - Epoch   5, Step:    9700, Batch Loss:    51.129848, Lr: 0.000096, Tokens per sec:   2790
2023-03-14 11:25:53,263 - INFO - __main__ - Epoch   5, Step:    9800, Batch Loss:    36.280685, Lr: 0.000096, Tokens per sec:   2722
2023-03-14 11:26:13,052 - INFO - __main__ - Epoch   5, Step:    9900, Batch Loss:    49.833611, Lr: 0.000096, Tokens per sec:   2674
2023-03-14 11:26:33,035 - INFO - __main__ - Epoch   5, Step:   10000, Batch Loss:    49.101822, Lr: 0.000096, Tokens per sec:   2708
2023-03-14 11:26:53,200 - INFO - __main__ - Epoch   5, Step:   10100, Batch Loss:    54.107960, Lr: 0.000096, Tokens per sec:   2706
2023-03-14 11:27:14,382 - INFO - __main__ - Epoch   5, Step:   10200, Batch Loss:    50.767654, Lr: 0.000096, Tokens per sec:   2559
2023-03-14 11:27:35,272 - INFO - __main__ - Epoch   5, Step:   10300, Batch Loss:    49.000595, Lr: 0.000096, Tokens per sec:   2596
2023-03-14 11:27:54,997 - INFO - __main__ - Epoch   5, Step:   10400, Batch Loss:    54.565662, Lr: 0.000096, Tokens per sec:   2734
2023-03-14 11:28:15,137 - INFO - __main__ - Epoch   5, Step:   10500, Batch Loss:    49.590660, Lr: 0.000096, Tokens per sec:   2700
2023-03-14 11:28:34,843 - INFO - __main__ - Epoch   5, Step:   10600, Batch Loss:    37.501823, Lr: 0.000096, Tokens per sec:   2748
2023-03-14 11:28:55,078 - INFO - __main__ - Epoch   5, Step:   10700, Batch Loss:    46.824322, Lr: 0.000096, Tokens per sec:   2642
2023-03-14 11:29:15,117 - INFO - __main__ - Epoch   5, Step:   10800, Batch Loss:    39.083424, Lr: 0.000096, Tokens per sec:   2695
2023-03-14 11:29:34,020 - INFO - __main__ - Epoch   5: total training loss 104710.94
2023-03-14 11:29:34,020 - INFO - __main__ - Epoch 6
2023-03-14 11:29:35,314 - INFO - __main__ - Epoch   6, Step:   10900, Batch Loss:    51.656742, Lr: 0.000095, Tokens per sec:   2323
2023-03-14 11:29:54,711 - INFO - __main__ - Epoch   6, Step:   11000, Batch Loss:    31.761114, Lr: 0.000095, Tokens per sec:   2824
2023-03-14 11:30:14,269 - INFO - __main__ - Epoch   6, Step:   11100, Batch Loss:    50.149567, Lr: 0.000095, Tokens per sec:   2779
2023-03-14 11:30:33,729 - INFO - __main__ - Epoch   6, Step:   11200, Batch Loss:    41.555798, Lr: 0.000095, Tokens per sec:   2769
2023-03-14 11:30:53,277 - INFO - __main__ - Epoch   6, Step:   11300, Batch Loss:    63.204514, Lr: 0.000095, Tokens per sec:   2749
2023-03-14 11:31:12,847 - INFO - __main__ - Epoch   6, Step:   11400, Batch Loss:    35.393871, Lr: 0.000095, Tokens per sec:   2760
2023-03-14 11:31:36,743 - INFO - __main__ - Epoch   6, Step:   11500, Batch Loss:    32.309933, Lr: 0.000095, Tokens per sec:   2244
2023-03-14 11:32:12,742 - INFO - __main__ - Epoch   6, Step:   11600, Batch Loss:    36.398144, Lr: 0.000095, Tokens per sec:   1476
2023-03-14 11:32:48,648 - INFO - __main__ - Epoch   6, Step:   11700, Batch Loss:    48.012577, Lr: 0.000095, Tokens per sec:   1494
2023-03-14 11:33:24,476 - INFO - __main__ - Epoch   6, Step:   11800, Batch Loss:    36.020226, Lr: 0.000095, Tokens per sec:   1492
2023-03-14 11:34:00,910 - INFO - __main__ - Epoch   6, Step:   11900, Batch Loss:    48.118793, Lr: 0.000095, Tokens per sec:   1468
2023-03-14 11:34:37,279 - INFO - __main__ - Epoch   6, Step:   12000, Batch Loss:    48.437244, Lr: 0.000095, Tokens per sec:   1478
2023-03-14 11:35:14,008 - INFO - __main__ - Epoch   6, Step:   12100, Batch Loss:    40.848446, Lr: 0.000095, Tokens per sec:   1459
2023-03-14 11:35:50,304 - INFO - __main__ - Epoch   6, Step:   12200, Batch Loss:    35.649502, Lr: 0.000095, Tokens per sec:   1492
2023-03-14 11:36:26,933 - INFO - __main__ - Epoch   6, Step:   12300, Batch Loss:    56.148205, Lr: 0.000095, Tokens per sec:   1497
2023-03-14 11:37:03,075 - INFO - __main__ - Epoch   6, Step:   12400, Batch Loss:    36.999435, Lr: 0.000095, Tokens per sec:   1488
2023-03-14 11:37:38,668 - INFO - __main__ - Epoch   6, Step:   12500, Batch Loss:    41.499767, Lr: 0.000095, Tokens per sec:   1496
2023-03-14 11:38:14,370 - INFO - __main__ - Epoch   6, Step:   12600, Batch Loss:    36.755596, Lr: 0.000095, Tokens per sec:   1516
2023-03-14 11:38:50,951 - INFO - __main__ - Epoch   6, Step:   12700, Batch Loss:    54.448868, Lr: 0.000095, Tokens per sec:   1470
2023-03-14 11:39:26,731 - INFO - __main__ - Epoch   6, Step:   12800, Batch Loss:    47.266869, Lr: 0.000095, Tokens per sec:   1494
2023-03-14 11:40:02,850 - INFO - __main__ - Epoch   6, Step:   12900, Batch Loss:    39.144554, Lr: 0.000095, Tokens per sec:   1478
2023-03-14 11:40:38,762 - INFO - __main__ - Epoch   6, Step:   13000, Batch Loss:    45.880863, Lr: 0.000095, Tokens per sec:   1508
2023-03-14 11:41:05,395 - INFO - __main__ - Epoch   6: total training loss 97075.30
2023-03-14 11:41:05,395 - INFO - __main__ - Epoch 7
2023-03-14 11:41:14,627 - INFO - __main__ - Epoch   7, Step:   13100, Batch Loss:    34.936478, Lr: 0.000094, Tokens per sec:   1563
2023-03-14 11:41:50,653 - INFO - __main__ - Epoch   7, Step:   13200, Batch Loss:    42.256794, Lr: 0.000094, Tokens per sec:   1490
2023-03-14 11:42:27,232 - INFO - __main__ - Epoch   7, Step:   13300, Batch Loss:    48.350708, Lr: 0.000094, Tokens per sec:   1489
2023-03-14 11:43:03,386 - INFO - __main__ - Epoch   7, Step:   13400, Batch Loss:    47.000584, Lr: 0.000094, Tokens per sec:   1478
2023-03-14 11:43:39,376 - INFO - __main__ - Epoch   7, Step:   13500, Batch Loss:    38.885391, Lr: 0.000094, Tokens per sec:   1499
2023-03-14 11:44:15,418 - INFO - __main__ - Epoch   7, Step:   13600, Batch Loss:    32.487068, Lr: 0.000094, Tokens per sec:   1482
2023-03-14 11:44:52,009 - INFO - __main__ - Epoch   7, Step:   13700, Batch Loss:    38.880852, Lr: 0.000094, Tokens per sec:   1454
2023-03-14 11:45:28,000 - INFO - __main__ - Epoch   7, Step:   13800, Batch Loss:    54.068691, Lr: 0.000094, Tokens per sec:   1472
2023-03-14 11:46:03,919 - INFO - __main__ - Epoch   7, Step:   13900, Batch Loss:    37.152737, Lr: 0.000094, Tokens per sec:   1516
2023-03-14 11:46:39,873 - INFO - __main__ - Epoch   7, Step:   14000, Batch Loss:    47.619865, Lr: 0.000094, Tokens per sec:   1511
2023-03-14 11:47:16,357 - INFO - __main__ - Epoch   7, Step:   14100, Batch Loss:    40.699474, Lr: 0.000094, Tokens per sec:   1473
2023-03-14 11:47:54,809 - INFO - __main__ - Epoch   7, Step:   14200, Batch Loss:    30.508049, Lr: 0.000094, Tokens per sec:   1415
2023-03-14 11:48:32,494 - INFO - __main__ - Epoch   7, Step:   14300, Batch Loss:    38.851986, Lr: 0.000094, Tokens per sec:   1427
2023-03-14 11:49:10,558 - INFO - __main__ - Epoch   7, Step:   14400, Batch Loss:    46.730347, Lr: 0.000094, Tokens per sec:   1428
2023-03-14 11:49:49,173 - INFO - __main__ - Epoch   7, Step:   14500, Batch Loss:    48.087048, Lr: 0.000094, Tokens per sec:   1416
2023-03-14 11:50:27,599 - INFO - __main__ - Epoch   7, Step:   14600, Batch Loss:    46.697128, Lr: 0.000094, Tokens per sec:   1402
2023-03-14 11:51:05,924 - INFO - __main__ - Epoch   7, Step:   14700, Batch Loss:    41.332085, Lr: 0.000094, Tokens per sec:   1397
2023-03-14 11:51:43,900 - INFO - __main__ - Epoch   7, Step:   14800, Batch Loss:    44.461555, Lr: 0.000094, Tokens per sec:   1412
2023-03-14 11:52:22,775 - INFO - __main__ - Epoch   7, Step:   14900, Batch Loss:    41.287216, Lr: 0.000094, Tokens per sec:   1379
2023-03-14 11:53:01,484 - INFO - __main__ - Epoch   7, Step:   15000, Batch Loss:    36.619770, Lr: 0.000094, Tokens per sec:   1379
2023-03-14 11:53:39,840 - INFO - __main__ - Epoch   7, Step:   15100, Batch Loss:    28.908680, Lr: 0.000094, Tokens per sec:   1391
2023-03-14 11:54:17,715 - INFO - __main__ - Epoch   7, Step:   15200, Batch Loss:    41.586906, Lr: 0.000094, Tokens per sec:   1407
2023-03-14 11:54:38,159 - INFO - __main__ - Epoch   7: total training loss 90307.46
2023-03-14 11:54:38,161 - INFO - __main__ - Epoch 8
2023-03-14 11:54:56,602 - INFO - __main__ - Epoch   8, Step:   15300, Batch Loss:    42.793625, Lr: 0.000093, Tokens per sec:   1344
2023-03-14 11:55:34,548 - INFO - __main__ - Epoch   8, Step:   15400, Batch Loss:    41.123093, Lr: 0.000093, Tokens per sec:   1441
2023-03-14 11:56:13,275 - INFO - __main__ - Epoch   8, Step:   15500, Batch Loss:    53.789383, Lr: 0.000093, Tokens per sec:   1390
2023-03-14 11:56:51,463 - INFO - __main__ - Epoch   8, Step:   15600, Batch Loss:    43.685463, Lr: 0.000093, Tokens per sec:   1425
2023-03-14 11:57:29,727 - INFO - __main__ - Epoch   8, Step:   15700, Batch Loss:    40.460381, Lr: 0.000093, Tokens per sec:   1422
2023-03-14 11:58:07,913 - INFO - __main__ - Epoch   8, Step:   15800, Batch Loss:    30.867477, Lr: 0.000093, Tokens per sec:   1405
2023-03-14 11:58:46,545 - INFO - __main__ - Epoch   8, Step:   15900, Batch Loss:    54.335339, Lr: 0.000093, Tokens per sec:   1392
2023-03-14 11:59:24,629 - INFO - __main__ - Epoch   8, Step:   16000, Batch Loss:    37.255096, Lr: 0.000093, Tokens per sec:   1397
2023-03-14 12:00:02,731 - INFO - __main__ - Epoch   8, Step:   16100, Batch Loss:    40.853870, Lr: 0.000093, Tokens per sec:   1420
2023-03-14 12:00:40,340 - INFO - __main__ - Epoch   8, Step:   16200, Batch Loss:    41.318504, Lr: 0.000093, Tokens per sec:   1443
2023-03-14 12:01:18,742 - INFO - __main__ - Epoch   8, Step:   16300, Batch Loss:    36.068218, Lr: 0.000093, Tokens per sec:   1376
2023-03-14 12:01:57,016 - INFO - __main__ - Epoch   8, Step:   16400, Batch Loss:    31.468128, Lr: 0.000093, Tokens per sec:   1419
2023-03-14 12:02:35,531 - INFO - __main__ - Epoch   8, Step:   16500, Batch Loss:    43.256050, Lr: 0.000093, Tokens per sec:   1403
2023-03-14 12:03:13,944 - INFO - __main__ - Epoch   8, Step:   16600, Batch Loss:    44.133644, Lr: 0.000093, Tokens per sec:   1395
2023-03-14 12:03:52,441 - INFO - __main__ - Epoch   8, Step:   16700, Batch Loss:    46.564064, Lr: 0.000093, Tokens per sec:   1377
2023-03-14 12:04:31,027 - INFO - __main__ - Epoch   8, Step:   16800, Batch Loss:    41.181740, Lr: 0.000093, Tokens per sec:   1399
2023-03-14 12:05:09,242 - INFO - __main__ - Epoch   8, Step:   16900, Batch Loss:    31.841557, Lr: 0.000093, Tokens per sec:   1411
2023-03-14 12:05:46,152 - INFO - __main__ - Epoch   8, Step:   17000, Batch Loss:    45.746246, Lr: 0.000093, Tokens per sec:   1471
2023-03-14 12:06:23,981 - INFO - __main__ - Epoch   8, Step:   17100, Batch Loss:    50.855648, Lr: 0.000093, Tokens per sec:   1412
2023-03-14 12:07:02,275 - INFO - __main__ - Epoch   8, Step:   17200, Batch Loss:    35.136597, Lr: 0.000093, Tokens per sec:   1410
2023-03-14 12:07:40,172 - INFO - __main__ - Epoch   8, Step:   17300, Batch Loss:    43.405151, Lr: 0.000093, Tokens per sec:   1407
2023-03-14 12:08:18,814 - INFO - __main__ - Epoch   8, Step:   17400, Batch Loss:    46.759277, Lr: 0.000093, Tokens per sec:   1404
2023-03-14 12:08:30,922 - INFO - __main__ - Epoch   8: total training loss 84228.20
2023-03-14 12:08:30,924 - INFO - __main__ - Epoch 9
2023-03-14 12:08:57,168 - INFO - __main__ - Epoch   9, Step:   17500, Batch Loss:    36.037762, Lr: 0.000092, Tokens per sec:   1386
2023-03-14 12:09:35,246 - INFO - __main__ - Epoch   9, Step:   17600, Batch Loss:    30.417616, Lr: 0.000092, Tokens per sec:   1412
2023-03-14 12:10:12,522 - INFO - __main__ - Epoch   9, Step:   17700, Batch Loss:    31.869059, Lr: 0.000092, Tokens per sec:   1464
2023-03-14 12:10:50,655 - INFO - __main__ - Epoch   9, Step:   17800, Batch Loss:    25.052103, Lr: 0.000092, Tokens per sec:   1423
2023-03-14 12:11:20,716 - INFO - __main__ - Epoch   9, Step:   17900, Batch Loss:    32.172531, Lr: 0.000092, Tokens per sec:   1776
2023-03-14 12:11:45,471 - INFO - __main__ - Epoch   9, Step:   18000, Batch Loss:    34.777958, Lr: 0.000092, Tokens per sec:   2170
2023-03-14 12:12:10,415 - INFO - __main__ - Epoch   9, Step:   18100, Batch Loss:    43.721565, Lr: 0.000092, Tokens per sec:   2169
2023-03-14 12:12:35,923 - INFO - __main__ - Epoch   9, Step:   18200, Batch Loss:    25.770691, Lr: 0.000092, Tokens per sec:   2165
2023-03-14 12:13:00,909 - INFO - __main__ - Epoch   9, Step:   18300, Batch Loss:    21.325298, Lr: 0.000092, Tokens per sec:   2154
2023-03-14 12:13:26,055 - INFO - __main__ - Epoch   9, Step:   18400, Batch Loss:    42.406151, Lr: 0.000092, Tokens per sec:   2124
2023-03-14 12:13:50,366 - INFO - __main__ - Epoch   9, Step:   18500, Batch Loss:    41.843563, Lr: 0.000092, Tokens per sec:   2228
2023-03-14 12:14:14,838 - INFO - __main__ - Epoch   9, Step:   18600, Batch Loss:    53.961742, Lr: 0.000092, Tokens per sec:   2196
2023-03-14 12:14:40,400 - INFO - __main__ - Epoch   9, Step:   18700, Batch Loss:    21.239128, Lr: 0.000092, Tokens per sec:   2118
2023-03-14 12:15:00,248 - INFO - __main__ - Epoch   9, Step:   18800, Batch Loss:    42.661854, Lr: 0.000092, Tokens per sec:   2699
2023-03-14 12:15:18,637 - INFO - __main__ - Epoch   9, Step:   18900, Batch Loss:    35.652283, Lr: 0.000092, Tokens per sec:   2915
2023-03-14 12:15:36,984 - INFO - __main__ - Epoch   9, Step:   19000, Batch Loss:    41.661839, Lr: 0.000092, Tokens per sec:   2927
2023-03-14 12:15:54,945 - INFO - __main__ - Epoch   9, Step:   19100, Batch Loss:    39.003551, Lr: 0.000092, Tokens per sec:   3025
2023-03-14 12:16:14,057 - INFO - __main__ - Epoch   9, Step:   19200, Batch Loss:    41.005966, Lr: 0.000092, Tokens per sec:   2797
2023-03-14 12:16:33,343 - INFO - __main__ - Epoch   9, Step:   19300, Batch Loss:    27.842484, Lr: 0.000092, Tokens per sec:   2754
2023-03-14 12:16:51,400 - INFO - __main__ - Epoch   9, Step:   19400, Batch Loss:    34.427105, Lr: 0.000092, Tokens per sec:   2998
2023-03-14 12:17:10,627 - INFO - __main__ - Epoch   9, Step:   19500, Batch Loss:    33.059437, Lr: 0.000092, Tokens per sec:   2767
2023-03-14 12:17:29,430 - INFO - __main__ - Epoch   9, Step:   19600, Batch Loss:    32.808811, Lr: 0.000092, Tokens per sec:   2844
2023-03-14 12:17:31,492 - INFO - __main__ - Epoch   9: total training loss 78713.11
2023-03-14 12:17:31,493 - INFO - __main__ - Epoch 10
2023-03-14 12:17:47,801 - INFO - __main__ - Epoch  10, Step:   19700, Batch Loss:    32.242012, Lr: 0.000091, Tokens per sec:   2933
2023-03-14 12:18:06,212 - INFO - __main__ - Epoch  10, Step:   19800, Batch Loss:    37.912853, Lr: 0.000091, Tokens per sec:   2947
2023-03-14 12:18:25,269 - INFO - __main__ - Epoch  10, Step:   19900, Batch Loss:    29.981163, Lr: 0.000091, Tokens per sec:   2877
2023-03-14 12:18:43,495 - INFO - __main__ - Epoch  10, Step:   20000, Batch Loss:    30.841921, Lr: 0.000091, Tokens per sec:   2918
2023-03-14 12:19:01,873 - INFO - __main__ - Epoch  10, Step:   20100, Batch Loss:    34.819180, Lr: 0.000091, Tokens per sec:   2966
2023-03-14 12:19:20,701 - INFO - __main__ - Epoch  10, Step:   20200, Batch Loss:    36.381268, Lr: 0.000091, Tokens per sec:   2879
2023-03-14 12:19:39,319 - INFO - __main__ - Epoch  10, Step:   20300, Batch Loss:    37.548801, Lr: 0.000091, Tokens per sec:   2915
2023-03-14 12:19:57,800 - INFO - __main__ - Epoch  10, Step:   20400, Batch Loss:    34.691296, Lr: 0.000091, Tokens per sec:   2856
2023-03-14 12:20:15,954 - INFO - __main__ - Epoch  10, Step:   20500, Batch Loss:    23.944956, Lr: 0.000091, Tokens per sec:   2991
2023-03-14 12:20:34,260 - INFO - __main__ - Epoch  10, Step:   20600, Batch Loss:    41.702503, Lr: 0.000091, Tokens per sec:   2972
2023-03-14 12:20:53,831 - INFO - __main__ - Epoch  10, Step:   20700, Batch Loss:    42.064640, Lr: 0.000091, Tokens per sec:   2731
2023-03-14 12:21:13,078 - INFO - __main__ - Epoch  10, Step:   20800, Batch Loss:    42.848011, Lr: 0.000091, Tokens per sec:   2774
2023-03-14 12:21:31,360 - INFO - __main__ - Epoch  10, Step:   20900, Batch Loss:    29.556225, Lr: 0.000091, Tokens per sec:   2929
2023-03-14 12:21:50,652 - INFO - __main__ - Epoch  10, Step:   21000, Batch Loss:    30.024977, Lr: 0.000091, Tokens per sec:   2814
2023-03-14 12:22:10,547 - INFO - __main__ - Epoch  10, Step:   21100, Batch Loss:    37.792881, Lr: 0.000091, Tokens per sec:   2666
2023-03-14 12:22:30,475 - INFO - __main__ - Epoch  10, Step:   21200, Batch Loss:    23.134708, Lr: 0.000091, Tokens per sec:   2682
2023-03-14 12:22:50,394 - INFO - __main__ - Epoch  10, Step:   21300, Batch Loss:    34.922546, Lr: 0.000091, Tokens per sec:   2683
2023-03-14 12:23:10,325 - INFO - __main__ - Epoch  10, Step:   21400, Batch Loss:    41.059128, Lr: 0.000091, Tokens per sec:   2715
2023-03-14 12:23:30,284 - INFO - __main__ - Epoch  10, Step:   21500, Batch Loss:    35.788368, Lr: 0.000091, Tokens per sec:   2691
2023-03-14 12:23:49,729 - INFO - __main__ - Epoch  10, Step:   21600, Batch Loss:    15.484675, Lr: 0.000091, Tokens per sec:   2738
2023-03-14 12:24:08,257 - INFO - __main__ - Epoch  10, Step:   21700, Batch Loss:    37.567280, Lr: 0.000091, Tokens per sec:   2966
2023-03-14 12:24:25,284 - INFO - __main__ - Epoch  10: total training loss 73692.44
2023-03-14 12:24:25,285 - INFO - __main__ - Epoch 11
2023-03-14 12:24:27,495 - INFO - __main__ - Epoch  11, Step:   21800, Batch Loss:    26.420492, Lr: 0.000090, Tokens per sec:   2344
2023-03-14 12:24:46,410 - INFO - __main__ - Epoch  11, Step:   21900, Batch Loss:    28.514637, Lr: 0.000090, Tokens per sec:   2873
2023-03-14 12:25:05,149 - INFO - __main__ - Epoch  11, Step:   22000, Batch Loss:    34.672810, Lr: 0.000090, Tokens per sec:   2806
2023-03-14 12:25:23,783 - INFO - __main__ - Epoch  11, Step:   22100, Batch Loss:    35.496265, Lr: 0.000090, Tokens per sec:   2892
2023-03-14 12:25:42,594 - INFO - __main__ - Epoch  11, Step:   22200, Batch Loss:    24.748001, Lr: 0.000090, Tokens per sec:   2853
2023-03-14 12:26:01,687 - INFO - __main__ - Epoch  11, Step:   22300, Batch Loss:    25.760916, Lr: 0.000090, Tokens per sec:   2824
2023-03-14 12:26:20,281 - INFO - __main__ - Epoch  11, Step:   22400, Batch Loss:    27.983124, Lr: 0.000090, Tokens per sec:   2893
2023-03-14 12:26:39,069 - INFO - __main__ - Epoch  11, Step:   22500, Batch Loss:    27.784288, Lr: 0.000090, Tokens per sec:   2894
2023-03-14 12:26:57,828 - INFO - __main__ - Epoch  11, Step:   22600, Batch Loss:    26.923454, Lr: 0.000090, Tokens per sec:   2851
2023-03-14 12:27:16,701 - INFO - __main__ - Epoch  11, Step:   22700, Batch Loss:    35.836124, Lr: 0.000090, Tokens per sec:   2895
2023-03-14 12:27:35,014 - INFO - __main__ - Epoch  11, Step:   22800, Batch Loss:    28.375017, Lr: 0.000090, Tokens per sec:   2995
2023-03-14 12:27:53,406 - INFO - __main__ - Epoch  11, Step:   22900, Batch Loss:    37.305786, Lr: 0.000090, Tokens per sec:   2884
2023-03-14 12:28:12,206 - INFO - __main__ - Epoch  11, Step:   23000, Batch Loss:    23.674894, Lr: 0.000090, Tokens per sec:   2886
2023-03-14 12:28:30,706 - INFO - __main__ - Epoch  11, Step:   23100, Batch Loss:    24.336754, Lr: 0.000090, Tokens per sec:   2903
2023-03-14 12:28:49,031 - INFO - __main__ - Epoch  11, Step:   23200, Batch Loss:    34.319820, Lr: 0.000090, Tokens per sec:   2955
2023-03-14 12:29:07,384 - INFO - __main__ - Epoch  11, Step:   23300, Batch Loss:    29.513083, Lr: 0.000090, Tokens per sec:   2900
2023-03-14 12:29:26,171 - INFO - __main__ - Epoch  11, Step:   23400, Batch Loss:    31.099163, Lr: 0.000090, Tokens per sec:   2846
2023-03-14 12:29:46,096 - INFO - __main__ - Epoch  11, Step:   23500, Batch Loss:    35.523052, Lr: 0.000090, Tokens per sec:   2713
2023-03-14 12:30:05,469 - INFO - __main__ - Epoch  11, Step:   23600, Batch Loss:    29.687014, Lr: 0.000090, Tokens per sec:   2806
2023-03-14 12:30:23,881 - INFO - __main__ - Epoch  11, Step:   23700, Batch Loss:    26.202091, Lr: 0.000090, Tokens per sec:   2860
2023-03-14 12:30:42,030 - INFO - __main__ - Epoch  11, Step:   23800, Batch Loss:    34.716850, Lr: 0.000090, Tokens per sec:   2952
2023-03-14 12:31:00,086 - INFO - __main__ - Epoch  11, Step:   23900, Batch Loss:    33.110199, Lr: 0.000090, Tokens per sec:   3024
2023-03-14 12:31:13,166 - INFO - __main__ - Epoch  11: total training loss 69104.27
2023-03-14 12:31:13,167 - INFO - __main__ - Epoch 12
2023-03-14 12:31:19,303 - INFO - __main__ - Epoch  12, Step:   24000, Batch Loss:    23.796244, Lr: 0.000090, Tokens per sec:   2705
2023-03-14 12:31:37,515 - INFO - __main__ - Epoch  12, Step:   24100, Batch Loss:    31.995485, Lr: 0.000090, Tokens per sec:   2938
2023-03-14 12:31:55,464 - INFO - __main__ - Epoch  12, Step:   24200, Batch Loss:    29.890520, Lr: 0.000090, Tokens per sec:   2956
2023-03-14 12:32:13,523 - INFO - __main__ - Epoch  12, Step:   24300, Batch Loss:    32.152073, Lr: 0.000090, Tokens per sec:   2975
2023-03-14 12:32:32,071 - INFO - __main__ - Epoch  12, Step:   24400, Batch Loss:    30.589329, Lr: 0.000090, Tokens per sec:   2897
2023-03-14 12:32:50,564 - INFO - __main__ - Epoch  12, Step:   24500, Batch Loss:    28.191484, Lr: 0.000090, Tokens per sec:   2930
2023-03-14 12:33:09,293 - INFO - __main__ - Epoch  12, Step:   24600, Batch Loss:    21.506115, Lr: 0.000090, Tokens per sec:   2844
2023-03-14 12:33:28,534 - INFO - __main__ - Epoch  12, Step:   24700, Batch Loss:    24.247869, Lr: 0.000090, Tokens per sec:   2805
2023-03-14 12:33:47,884 - INFO - __main__ - Epoch  12, Step:   24800, Batch Loss:    30.441923, Lr: 0.000090, Tokens per sec:   2792
2023-03-14 12:34:06,874 - INFO - __main__ - Epoch  12, Step:   24900, Batch Loss:    32.421772, Lr: 0.000090, Tokens per sec:   2860
2023-03-14 12:34:26,099 - INFO - __main__ - Epoch  12, Step:   25000, Batch Loss:    15.255115, Lr: 0.000090, Tokens per sec:   2778
2023-03-14 12:34:44,321 - INFO - __main__ - Epoch  12, Step:   25100, Batch Loss:    35.521954, Lr: 0.000090, Tokens per sec:   2948
2023-03-14 12:35:03,782 - INFO - __main__ - Epoch  12, Step:   25200, Batch Loss:    31.277733, Lr: 0.000090, Tokens per sec:   2801
2023-03-14 12:35:33,871 - INFO - __main__ - Epoch  12, Step:   25300, Batch Loss:    23.734570, Lr: 0.000090, Tokens per sec:   1765
2023-03-14 12:36:11,519 - INFO - __main__ - Epoch  12, Step:   25400, Batch Loss:    24.351103, Lr: 0.000090, Tokens per sec:   1450
2023-03-14 12:36:50,218 - INFO - __main__ - Epoch  12, Step:   25500, Batch Loss:    23.952501, Lr: 0.000090, Tokens per sec:   1379
2023-03-14 12:37:29,251 - INFO - __main__ - Epoch  12, Step:   25600, Batch Loss:    28.968264, Lr: 0.000090, Tokens per sec:   1366
2023-03-14 12:38:07,916 - INFO - __main__ - Epoch  12, Step:   25700, Batch Loss:    42.250896, Lr: 0.000090, Tokens per sec:   1401
2023-03-14 12:38:46,906 - INFO - __main__ - Epoch  12, Step:   25800, Batch Loss:    31.131327, Lr: 0.000090, Tokens per sec:   1390
2023-03-14 12:39:25,589 - INFO - __main__ - Epoch  12, Step:   25900, Batch Loss:    37.514416, Lr: 0.000090, Tokens per sec:   1395
2023-03-14 12:40:04,378 - INFO - __main__ - Epoch  12, Step:   26000, Batch Loss:    25.909781, Lr: 0.000090, Tokens per sec:   1381
2023-03-14 12:40:43,170 - INFO - __main__ - Epoch  12, Step:   26100, Batch Loss:    23.765654, Lr: 0.000090, Tokens per sec:   1415
2023-03-14 12:41:01,387 - INFO - __main__ - Epoch  12: total training loss 64850.10
2023-03-14 12:41:01,388 - INFO - __main__ - Epoch 13
2023-03-14 12:41:21,818 - INFO - __main__ - Epoch  13, Step:   26200, Batch Loss:    19.694891, Lr: 0.000089, Tokens per sec:   1374
2023-03-14 12:42:00,729 - INFO - __main__ - Epoch  13, Step:   26300, Batch Loss:    21.872438, Lr: 0.000089, Tokens per sec:   1378
2023-03-14 12:42:39,223 - INFO - __main__ - Epoch  13, Step:   26400, Batch Loss:    28.838928, Lr: 0.000089, Tokens per sec:   1398
2023-03-14 12:43:17,230 - INFO - __main__ - Epoch  13, Step:   26500, Batch Loss:    20.796780, Lr: 0.000089, Tokens per sec:   1409
2023-03-14 12:43:55,735 - INFO - __main__ - Epoch  13, Step:   26600, Batch Loss:    30.190941, Lr: 0.000089, Tokens per sec:   1400
2023-03-14 12:44:34,461 - INFO - __main__ - Epoch  13, Step:   26700, Batch Loss:    21.321438, Lr: 0.000089, Tokens per sec:   1402
2023-03-14 12:45:12,839 - INFO - __main__ - Epoch  13, Step:   26800, Batch Loss:    30.939590, Lr: 0.000089, Tokens per sec:   1417
2023-03-14 12:45:51,150 - INFO - __main__ - Epoch  13, Step:   26900, Batch Loss:    28.363531, Lr: 0.000089, Tokens per sec:   1401
2023-03-14 12:46:29,187 - INFO - __main__ - Epoch  13, Step:   27000, Batch Loss:    26.709850, Lr: 0.000089, Tokens per sec:   1425
2023-03-14 12:47:06,673 - INFO - __main__ - Epoch  13, Step:   27100, Batch Loss:    22.751024, Lr: 0.000089, Tokens per sec:   1449
2023-03-14 12:47:44,399 - INFO - __main__ - Epoch  13, Step:   27200, Batch Loss:    27.629702, Lr: 0.000089, Tokens per sec:   1438
2023-03-14 12:48:22,934 - INFO - __main__ - Epoch  13, Step:   27300, Batch Loss:    35.974773, Lr: 0.000089, Tokens per sec:   1394
2023-03-14 12:49:00,746 - INFO - __main__ - Epoch  13, Step:   27400, Batch Loss:    32.906239, Lr: 0.000089, Tokens per sec:   1409
2023-03-14 12:49:38,452 - INFO - __main__ - Epoch  13, Step:   27500, Batch Loss:    24.015083, Lr: 0.000089, Tokens per sec:   1432
2023-03-14 12:50:16,432 - INFO - __main__ - Epoch  13, Step:   27600, Batch Loss:    31.870882, Lr: 0.000089, Tokens per sec:   1418
2023-03-14 12:50:53,946 - INFO - __main__ - Epoch  13, Step:   27700, Batch Loss:    25.596478, Lr: 0.000089, Tokens per sec:   1444
2023-03-14 12:51:32,217 - INFO - __main__ - Epoch  13, Step:   27800, Batch Loss:    30.676048, Lr: 0.000089, Tokens per sec:   1419
2023-03-14 12:52:10,392 - INFO - __main__ - Epoch  13, Step:   27900, Batch Loss:    21.703165, Lr: 0.000089, Tokens per sec:   1412
2023-03-14 12:52:47,742 - INFO - __main__ - Epoch  13, Step:   28000, Batch Loss:    21.814014, Lr: 0.000089, Tokens per sec:   1414
2023-03-14 12:53:25,651 - INFO - __main__ - Epoch  13, Step:   28100, Batch Loss:    24.711533, Lr: 0.000089, Tokens per sec:   1386
2023-03-14 12:54:04,058 - INFO - __main__ - Epoch  13, Step:   28200, Batch Loss:    32.080933, Lr: 0.000089, Tokens per sec:   1396
2023-03-14 12:54:42,910 - INFO - __main__ - Epoch  13, Step:   28300, Batch Loss:    25.014523, Lr: 0.000089, Tokens per sec:   1387
2023-03-14 12:54:53,198 - INFO - __main__ - Epoch  13: total training loss 60897.29
2023-03-14 12:54:53,199 - INFO - __main__ - Epoch 14
2023-03-14 12:55:21,545 - INFO - __main__ - Epoch  14, Step:   28400, Batch Loss:    22.880896, Lr: 0.000088, Tokens per sec:   1397
2023-03-14 12:55:59,239 - INFO - __main__ - Epoch  14, Step:   28500, Batch Loss:    31.860733, Lr: 0.000088, Tokens per sec:   1437
2023-03-14 12:56:37,523 - INFO - __main__ - Epoch  14, Step:   28600, Batch Loss:    26.853703, Lr: 0.000088, Tokens per sec:   1404
2023-03-14 12:57:15,845 - INFO - __main__ - Epoch  14, Step:   28700, Batch Loss:    24.277594, Lr: 0.000088, Tokens per sec:   1399
2023-03-14 12:57:53,080 - INFO - __main__ - Epoch  14, Step:   28800, Batch Loss:    27.390980, Lr: 0.000088, Tokens per sec:   1442
2023-03-14 12:58:31,624 - INFO - __main__ - Epoch  14, Step:   28900, Batch Loss:    31.755819, Lr: 0.000088, Tokens per sec:   1383
2023-03-14 12:59:09,933 - INFO - __main__ - Epoch  14, Step:   29000, Batch Loss:    29.331585, Lr: 0.000088, Tokens per sec:   1419
2023-03-14 12:59:47,537 - INFO - __main__ - Epoch  14, Step:   29100, Batch Loss:    20.359129, Lr: 0.000088, Tokens per sec:   1428
2023-03-14 13:00:26,197 - INFO - __main__ - Epoch  14, Step:   29200, Batch Loss:    24.332056, Lr: 0.000088, Tokens per sec:   1399
2023-03-14 13:01:04,388 - INFO - __main__ - Epoch  14, Step:   29300, Batch Loss:    19.213644, Lr: 0.000088, Tokens per sec:   1396
2023-03-14 13:01:42,371 - INFO - __main__ - Epoch  14, Step:   29400, Batch Loss:    31.867056, Lr: 0.000088, Tokens per sec:   1415
2023-03-14 13:02:20,403 - INFO - __main__ - Epoch  14, Step:   29500, Batch Loss:    27.644444, Lr: 0.000088, Tokens per sec:   1421
2023-03-14 13:02:58,802 - INFO - __main__ - Epoch  14, Step:   29600, Batch Loss:    25.500366, Lr: 0.000088, Tokens per sec:   1407
2023-03-14 13:03:36,695 - INFO - __main__ - Epoch  14, Step:   29700, Batch Loss:    28.337603, Lr: 0.000088, Tokens per sec:   1408
2023-03-14 13:04:14,839 - INFO - __main__ - Epoch  14, Step:   29800, Batch Loss:    27.067848, Lr: 0.000088, Tokens per sec:   1400
2023-03-14 13:04:52,796 - INFO - __main__ - Epoch  14, Step:   29900, Batch Loss:    32.487995, Lr: 0.000088, Tokens per sec:   1441
2023-03-14 13:05:30,937 - INFO - __main__ - Epoch  14, Step:   30000, Batch Loss:    23.519691, Lr: 0.000088, Tokens per sec:   1391
2023-03-14 13:06:09,593 - INFO - __main__ - Epoch  14, Step:   30100, Batch Loss:    27.993744, Lr: 0.000088, Tokens per sec:   1411
2023-03-14 13:06:47,905 - INFO - __main__ - Epoch  14, Step:   30200, Batch Loss:    25.825638, Lr: 0.000088, Tokens per sec:   1422
2023-03-14 13:07:26,268 - INFO - __main__ - Epoch  14, Step:   30300, Batch Loss:    23.717104, Lr: 0.000088, Tokens per sec:   1403
2023-03-14 13:08:04,137 - INFO - __main__ - Epoch  14, Step:   30400, Batch Loss:    27.857473, Lr: 0.000088, Tokens per sec:   1413
2023-03-14 13:08:42,906 - INFO - __main__ - Epoch  14, Step:   30500, Batch Loss:    33.375023, Lr: 0.000088, Tokens per sec:   1380
2023-03-14 13:08:45,175 - INFO - __main__ - Epoch  14: total training loss 57328.26
2023-03-14 13:08:45,176 - INFO - __main__ - Epoch 15
2023-03-14 13:09:21,799 - INFO - __main__ - Epoch  15, Step:   30600, Batch Loss:    20.365509, Lr: 0.000087, Tokens per sec:   1373
2023-03-14 13:10:00,198 - INFO - __main__ - Epoch  15, Step:   30700, Batch Loss:    23.124691, Lr: 0.000087, Tokens per sec:   1403
2023-03-14 13:10:38,148 - INFO - __main__ - Epoch  15, Step:   30800, Batch Loss:    14.139315, Lr: 0.000087, Tokens per sec:   1390
2023-03-14 13:11:16,680 - INFO - __main__ - Epoch  15, Step:   30900, Batch Loss:    23.739494, Lr: 0.000087, Tokens per sec:   1371
2023-03-14 13:11:55,122 - INFO - __main__ - Epoch  15, Step:   31000, Batch Loss:    26.837454, Lr: 0.000087, Tokens per sec:   1414
2023-03-14 13:12:33,096 - INFO - __main__ - Epoch  15, Step:   31100, Batch Loss:    25.590673, Lr: 0.000087, Tokens per sec:   1429
2023-03-14 13:13:11,144 - INFO - __main__ - Epoch  15, Step:   31200, Batch Loss:    23.923359, Lr: 0.000087, Tokens per sec:   1427
2023-03-14 13:13:49,087 - INFO - __main__ - Epoch  15, Step:   31300, Batch Loss:    35.012981, Lr: 0.000087, Tokens per sec:   1418
2023-03-14 13:14:12,262 - INFO - __main__ - Epoch  15, Step:   31400, Batch Loss:    27.139158, Lr: 0.000087, Tokens per sec:   2332
2023-03-14 13:14:36,716 - INFO - __main__ - Epoch  15, Step:   31500, Batch Loss:    25.823652, Lr: 0.000087, Tokens per sec:   2209
2023-03-14 13:15:01,559 - INFO - __main__ - Epoch  15, Step:   31600, Batch Loss:    32.744404, Lr: 0.000087, Tokens per sec:   2146
2023-03-14 13:15:26,386 - INFO - __main__ - Epoch  15, Step:   31700, Batch Loss:    35.944698, Lr: 0.000087, Tokens per sec:   2173
2023-03-14 13:15:50,627 - INFO - __main__ - Epoch  15, Step:   31800, Batch Loss:    27.455990, Lr: 0.000087, Tokens per sec:   2211
2023-03-14 13:16:15,748 - INFO - __main__ - Epoch  15, Step:   31900, Batch Loss:    26.913671, Lr: 0.000087, Tokens per sec:   2148
2023-03-14 13:16:40,823 - INFO - __main__ - Epoch  15, Step:   32000, Batch Loss:    18.253616, Lr: 0.000087, Tokens per sec:   2164
2023-03-14 13:17:06,361 - INFO - __main__ - Epoch  15, Step:   32100, Batch Loss:    20.719200, Lr: 0.000087, Tokens per sec:   2101
2023-03-14 13:17:30,546 - INFO - __main__ - Epoch  15, Step:   32200, Batch Loss:    28.329634, Lr: 0.000087, Tokens per sec:   2244
2023-03-14 13:17:49,865 - INFO - __main__ - Epoch  15, Step:   32300, Batch Loss:    20.232082, Lr: 0.000087, Tokens per sec:   2781
2023-03-14 13:18:09,517 - INFO - __main__ - Epoch  15, Step:   32400, Batch Loss:    25.018284, Lr: 0.000087, Tokens per sec:   2763
2023-03-14 13:18:28,981 - INFO - __main__ - Epoch  15, Step:   32500, Batch Loss:    24.662342, Lr: 0.000087, Tokens per sec:   2787
2023-03-14 13:18:48,802 - INFO - __main__ - Epoch  15, Step:   32600, Batch Loss:    29.349966, Lr: 0.000087, Tokens per sec:   2728
2023-03-14 13:19:05,955 - INFO - __main__ - Epoch  15: total training loss 53987.49
2023-03-14 13:19:05,956 - INFO - __main__ - Epoch 16
2023-03-14 13:19:09,239 - INFO - __main__ - Epoch  16, Step:   32700, Batch Loss:    23.482019, Lr: 0.000086, Tokens per sec:   2524
2023-03-14 13:19:29,329 - INFO - __main__ - Epoch  16, Step:   32800, Batch Loss:    23.742706, Lr: 0.000086, Tokens per sec:   2673
2023-03-14 13:19:49,408 - INFO - __main__ - Epoch  16, Step:   32900, Batch Loss:    22.140396, Lr: 0.000086, Tokens per sec:   2685
2023-03-14 13:20:09,157 - INFO - __main__ - Epoch  16, Step:   33000, Batch Loss:    32.957737, Lr: 0.000086, Tokens per sec:   2741
2023-03-14 13:20:28,896 - INFO - __main__ - Epoch  16, Step:   33100, Batch Loss:    24.574871, Lr: 0.000086, Tokens per sec:   2713
2023-03-14 13:20:48,790 - INFO - __main__ - Epoch  16, Step:   33200, Batch Loss:    20.696053, Lr: 0.000086, Tokens per sec:   2732
2023-03-14 13:21:08,729 - INFO - __main__ - Epoch  16, Step:   33300, Batch Loss:    23.220627, Lr: 0.000086, Tokens per sec:   2655
2023-03-14 13:21:28,046 - INFO - __main__ - Epoch  16, Step:   33400, Batch Loss:    22.163389, Lr: 0.000086, Tokens per sec:   2786
2023-03-14 13:21:47,812 - INFO - __main__ - Epoch  16, Step:   33500, Batch Loss:    20.199528, Lr: 0.000086, Tokens per sec:   2722
2023-03-14 13:22:07,650 - INFO - __main__ - Epoch  16, Step:   33600, Batch Loss:    25.724966, Lr: 0.000086, Tokens per sec:   2705
2023-03-14 13:22:27,788 - INFO - __main__ - Epoch  16, Step:   33700, Batch Loss:    21.042490, Lr: 0.000086, Tokens per sec:   2660
2023-03-14 13:22:58,822 - INFO - __main__ - Epoch  16, Step:   33800, Batch Loss:    22.151527, Lr: 0.000086, Tokens per sec:   1738
2023-03-14 13:23:37,153 - INFO - __main__ - Epoch  16, Step:   33900, Batch Loss:    30.482821, Lr: 0.000086, Tokens per sec:   1423
2023-03-14 13:24:16,184 - INFO - __main__ - Epoch  16, Step:   34000, Batch Loss:    16.293299, Lr: 0.000086, Tokens per sec:   1343
2023-03-14 13:24:55,169 - INFO - __main__ - Epoch  16, Step:   34100, Batch Loss:    25.785757, Lr: 0.000086, Tokens per sec:   1391
2023-03-14 13:25:33,745 - INFO - __main__ - Epoch  16, Step:   34200, Batch Loss:    25.743008, Lr: 0.000086, Tokens per sec:   1407
2023-03-14 13:26:12,404 - INFO - __main__ - Epoch  16, Step:   34300, Batch Loss:    30.834055, Lr: 0.000086, Tokens per sec:   1374
2023-03-14 13:26:51,057 - INFO - __main__ - Epoch  16, Step:   34400, Batch Loss:    21.208021, Lr: 0.000086, Tokens per sec:   1391
2023-03-14 13:27:29,747 - INFO - __main__ - Epoch  16, Step:   34500, Batch Loss:    19.400326, Lr: 0.000086, Tokens per sec:   1421
2023-03-14 13:28:08,629 - INFO - __main__ - Epoch  16, Step:   34600, Batch Loss:    25.785551, Lr: 0.000086, Tokens per sec:   1392
2023-03-14 13:28:46,817 - INFO - __main__ - Epoch  16, Step:   34700, Batch Loss:    19.607471, Lr: 0.000086, Tokens per sec:   1416
2023-03-14 13:29:25,693 - INFO - __main__ - Epoch  16, Step:   34800, Batch Loss:    17.047276, Lr: 0.000086, Tokens per sec:   1383
2023-03-14 13:29:50,219 - INFO - __main__ - Epoch  16: total training loss 50873.03
2023-03-14 13:29:50,220 - INFO - __main__ - Epoch 17
2023-03-14 13:30:04,356 - INFO - __main__ - Epoch  17, Step:   34900, Batch Loss:    16.517862, Lr: 0.000085, Tokens per sec:   1369
2023-03-14 13:30:43,021 - INFO - __main__ - Epoch  17, Step:   35000, Batch Loss:    15.951963, Lr: 0.000085, Tokens per sec:   1378
2023-03-14 13:31:21,638 - INFO - __main__ - Epoch  17, Step:   35100, Batch Loss:    13.959490, Lr: 0.000085, Tokens per sec:   1372
2023-03-14 13:32:00,364 - INFO - __main__ - Epoch  17, Step:   35200, Batch Loss:    16.920105, Lr: 0.000085, Tokens per sec:   1396
2023-03-14 13:32:39,217 - INFO - __main__ - Epoch  17, Step:   35300, Batch Loss:    24.665775, Lr: 0.000085, Tokens per sec:   1385
2023-03-14 13:33:17,708 - INFO - __main__ - Epoch  17, Step:   35400, Batch Loss:    22.831007, Lr: 0.000085, Tokens per sec:   1385
2023-03-14 13:33:56,495 - INFO - __main__ - Epoch  17, Step:   35500, Batch Loss:    16.024702, Lr: 0.000085, Tokens per sec:   1385
2023-03-14 13:34:34,886 - INFO - __main__ - Epoch  17, Step:   35600, Batch Loss:    22.690825, Lr: 0.000085, Tokens per sec:   1426
2023-03-14 13:35:13,903 - INFO - __main__ - Epoch  17, Step:   35700, Batch Loss:    20.928423, Lr: 0.000085, Tokens per sec:   1421
2023-03-14 13:35:52,949 - INFO - __main__ - Epoch  17, Step:   35800, Batch Loss:    32.559952, Lr: 0.000085, Tokens per sec:   1397
2023-03-14 13:36:31,292 - INFO - __main__ - Epoch  17, Step:   35900, Batch Loss:    20.082952, Lr: 0.000085, Tokens per sec:   1408
2023-03-14 13:37:10,198 - INFO - __main__ - Epoch  17, Step:   36000, Batch Loss:    14.299208, Lr: 0.000085, Tokens per sec:   1370
2023-03-14 13:37:48,860 - INFO - __main__ - Epoch  17, Step:   36100, Batch Loss:    23.764591, Lr: 0.000085, Tokens per sec:   1393
2023-03-14 13:38:27,721 - INFO - __main__ - Epoch  17, Step:   36200, Batch Loss:    27.499660, Lr: 0.000085, Tokens per sec:   1398
2023-03-14 13:39:06,629 - INFO - __main__ - Epoch  17, Step:   36300, Batch Loss:    20.811476, Lr: 0.000085, Tokens per sec:   1398
2023-03-14 13:39:45,370 - INFO - __main__ - Epoch  17, Step:   36400, Batch Loss:    22.927958, Lr: 0.000085, Tokens per sec:   1371
2023-03-14 13:40:23,391 - INFO - __main__ - Epoch  17, Step:   36500, Batch Loss:    31.766804, Lr: 0.000085, Tokens per sec:   1404
2023-03-14 13:41:01,710 - INFO - __main__ - Epoch  17, Step:   36600, Batch Loss:    26.083479, Lr: 0.000085, Tokens per sec:   1413
2023-03-14 13:41:40,363 - INFO - __main__ - Epoch  17, Step:   36700, Batch Loss:    21.837265, Lr: 0.000085, Tokens per sec:   1379
2023-03-14 13:42:19,057 - INFO - __main__ - Epoch  17, Step:   36800, Batch Loss:    16.443066, Lr: 0.000085, Tokens per sec:   1372
2023-03-14 13:42:56,927 - INFO - __main__ - Epoch  17, Step:   36900, Batch Loss:    23.365952, Lr: 0.000085, Tokens per sec:   1440
2023-03-14 13:43:35,728 - INFO - __main__ - Epoch  17, Step:   37000, Batch Loss:    23.256821, Lr: 0.000085, Tokens per sec:   1381
2023-03-14 13:43:52,146 - INFO - __main__ - Epoch  17: total training loss 47947.76
2023-03-14 13:43:52,147 - INFO - __main__ - Epoch 18
2023-03-14 13:44:14,411 - INFO - __main__ - Epoch  18, Step:   37100, Batch Loss:    15.127249, Lr: 0.000084, Tokens per sec:   1357
2023-03-14 13:44:52,659 - INFO - __main__ - Epoch  18, Step:   37200, Batch Loss:    16.368061, Lr: 0.000084, Tokens per sec:   1415
2023-03-14 13:45:31,099 - INFO - __main__ - Epoch  18, Step:   37300, Batch Loss:    27.657118, Lr: 0.000084, Tokens per sec:   1396
2023-03-14 13:46:09,906 - INFO - __main__ - Epoch  18, Step:   37400, Batch Loss:    24.775803, Lr: 0.000084, Tokens per sec:   1400
2023-03-14 13:46:48,419 - INFO - __main__ - Epoch  18, Step:   37500, Batch Loss:    24.462795, Lr: 0.000084, Tokens per sec:   1398
2023-03-14 13:47:27,250 - INFO - __main__ - Epoch  18, Step:   37600, Batch Loss:    17.418678, Lr: 0.000084, Tokens per sec:   1371
2023-03-14 13:48:05,968 - INFO - __main__ - Epoch  18, Step:   37700, Batch Loss:    28.846684, Lr: 0.000084, Tokens per sec:   1400
2023-03-14 13:48:44,394 - INFO - __main__ - Epoch  18, Step:   37800, Batch Loss:    17.234634, Lr: 0.000084, Tokens per sec:   1389
2023-03-14 13:49:23,142 - INFO - __main__ - Epoch  18, Step:   37900, Batch Loss:    27.859398, Lr: 0.000084, Tokens per sec:   1400
2023-03-14 13:50:01,795 - INFO - __main__ - Epoch  18, Step:   38000, Batch Loss:    15.135715, Lr: 0.000084, Tokens per sec:   1389
2023-03-14 13:50:40,307 - INFO - __main__ - Epoch  18, Step:   38100, Batch Loss:    13.101748, Lr: 0.000084, Tokens per sec:   1390
2023-03-14 13:51:19,310 - INFO - __main__ - Epoch  18, Step:   38200, Batch Loss:    20.318335, Lr: 0.000084, Tokens per sec:   1379
2023-03-14 13:51:58,314 - INFO - __main__ - Epoch  18, Step:   38300, Batch Loss:    22.126011, Lr: 0.000084, Tokens per sec:   1371
2023-03-14 13:52:36,945 - INFO - __main__ - Epoch  18, Step:   38400, Batch Loss:    19.624159, Lr: 0.000084, Tokens per sec:   1393
2023-03-14 13:53:15,771 - INFO - __main__ - Epoch  18, Step:   38500, Batch Loss:    19.409286, Lr: 0.000084, Tokens per sec:   1389
2023-03-14 13:53:54,794 - INFO - __main__ - Epoch  18, Step:   38600, Batch Loss:    27.291157, Lr: 0.000084, Tokens per sec:   1386
2023-03-14 13:54:33,760 - INFO - __main__ - Epoch  18, Step:   38700, Batch Loss:    23.878380, Lr: 0.000084, Tokens per sec:   1377
2023-03-14 13:55:12,714 - INFO - __main__ - Epoch  18, Step:   38800, Batch Loss:    19.123070, Lr: 0.000084, Tokens per sec:   1380
2023-03-14 13:55:51,833 - INFO - __main__ - Epoch  18, Step:   38900, Batch Loss:    17.347355, Lr: 0.000084, Tokens per sec:   1360
2023-03-14 13:56:31,038 - INFO - __main__ - Epoch  18, Step:   39000, Batch Loss:    16.578947, Lr: 0.000084, Tokens per sec:   1383
2023-03-14 13:57:09,860 - INFO - __main__ - Epoch  18, Step:   39100, Batch Loss:    18.432898, Lr: 0.000084, Tokens per sec:   1420
2023-03-14 13:57:48,481 - INFO - __main__ - Epoch  18, Step:   39200, Batch Loss:    23.176826, Lr: 0.000084, Tokens per sec:   1384
2023-03-14 13:57:57,139 - INFO - __main__ - Epoch  18: total training loss 45325.58
2023-03-14 13:57:57,140 - INFO - __main__ - Epoch 19
2023-03-14 13:58:27,807 - INFO - __main__ - Epoch  19, Step:   39300, Batch Loss:    20.142241, Lr: 0.000083, Tokens per sec:   1369
2023-03-14 13:59:06,559 - INFO - __main__ - Epoch  19, Step:   39400, Batch Loss:    21.813005, Lr: 0.000083, Tokens per sec:   1379
2023-03-14 13:59:45,483 - INFO - __main__ - Epoch  19, Step:   39500, Batch Loss:    12.348600, Lr: 0.000083, Tokens per sec:   1383
2023-03-14 14:00:24,601 - INFO - __main__ - Epoch  19, Step:   39600, Batch Loss:    23.695389, Lr: 0.000083, Tokens per sec:   1394
2023-03-14 14:01:03,640 - INFO - __main__ - Epoch  19, Step:   39700, Batch Loss:    19.532248, Lr: 0.000083, Tokens per sec:   1369
2023-03-14 14:01:29,695 - INFO - __main__ - Epoch  19, Step:   39800, Batch Loss:    20.109341, Lr: 0.000083, Tokens per sec:   2078
2023-03-14 14:01:55,935 - INFO - __main__ - Epoch  19, Step:   39900, Batch Loss:    18.118372, Lr: 0.000083, Tokens per sec:   2039
2023-03-14 14:02:21,936 - INFO - __main__ - Epoch  19, Step:   40000, Batch Loss:    19.696505, Lr: 0.000083, Tokens per sec:   2064
2023-03-14 14:02:48,214 - INFO - __main__ - Epoch  19, Step:   40100, Batch Loss:    20.547438, Lr: 0.000083, Tokens per sec:   2088
2023-03-14 14:03:14,177 - INFO - __main__ - Epoch  19, Step:   40200, Batch Loss:    17.582302, Lr: 0.000083, Tokens per sec:   2095
2023-03-14 14:03:40,549 - INFO - __main__ - Epoch  19, Step:   40300, Batch Loss:    22.083099, Lr: 0.000083, Tokens per sec:   2019
2023-03-14 14:04:06,955 - INFO - __main__ - Epoch  19, Step:   40400, Batch Loss:    23.980480, Lr: 0.000083, Tokens per sec:   2021
2023-03-14 14:04:33,252 - INFO - __main__ - Epoch  19, Step:   40500, Batch Loss:    19.978119, Lr: 0.000083, Tokens per sec:   2046
2023-03-14 14:04:56,117 - INFO - __main__ - Epoch  19, Step:   40600, Batch Loss:    22.020260, Lr: 0.000083, Tokens per sec:   2346
2023-03-14 14:05:15,916 - INFO - __main__ - Epoch  19, Step:   40700, Batch Loss:    26.802027, Lr: 0.000083, Tokens per sec:   2732
2023-03-14 14:05:36,067 - INFO - __main__ - Epoch  19, Step:   40800, Batch Loss:    22.860394, Lr: 0.000083, Tokens per sec:   2660
2023-03-14 14:05:56,146 - INFO - __main__ - Epoch  19, Step:   40900, Batch Loss:    20.154051, Lr: 0.000083, Tokens per sec:   2671
2023-03-14 14:06:16,521 - INFO - __main__ - Epoch  19, Step:   41000, Batch Loss:    21.311474, Lr: 0.000083, Tokens per sec:   2596
2023-03-14 14:06:36,485 - INFO - __main__ - Epoch  19, Step:   41100, Batch Loss:    20.149290, Lr: 0.000083, Tokens per sec:   2665
2023-03-14 14:06:56,603 - INFO - __main__ - Epoch  19, Step:   41200, Batch Loss:    15.209071, Lr: 0.000083, Tokens per sec:   2730
2023-03-14 14:07:16,737 - INFO - __main__ - Epoch  19, Step:   41300, Batch Loss:    18.753803, Lr: 0.000083, Tokens per sec:   2707
2023-03-14 14:07:36,792 - INFO - __main__ - Epoch  19, Step:   41400, Batch Loss:    21.583239, Lr: 0.000083, Tokens per sec:   2684
2023-03-14 14:07:37,023 - INFO - __main__ - Epoch  19: total training loss 42831.67
2023-03-14 14:07:37,024 - INFO - __main__ - Epoch 20
2023-03-14 14:07:57,161 - INFO - __main__ - Epoch  20, Step:   41500, Batch Loss:    13.974382, Lr: 0.000083, Tokens per sec:   2630
2023-03-14 14:08:17,238 - INFO - __main__ - Epoch  20, Step:   41600, Batch Loss:    13.738715, Lr: 0.000083, Tokens per sec:   2714
2023-03-14 14:08:37,045 - INFO - __main__ - Epoch  20, Step:   41700, Batch Loss:    19.652061, Lr: 0.000083, Tokens per sec:   2709
2023-03-14 14:08:57,096 - INFO - __main__ - Epoch  20, Step:   41800, Batch Loss:    21.786274, Lr: 0.000083, Tokens per sec:   2708
2023-03-14 14:09:17,296 - INFO - __main__ - Epoch  20, Step:   41900, Batch Loss:    15.047972, Lr: 0.000083, Tokens per sec:   2655
2023-03-14 14:09:37,216 - INFO - __main__ - Epoch  20, Step:   42000, Batch Loss:    20.293514, Lr: 0.000083, Tokens per sec:   2719
2023-03-14 14:09:57,204 - INFO - __main__ - Epoch  20, Step:   42100, Batch Loss:    22.591257, Lr: 0.000083, Tokens per sec:   2732
2023-03-14 14:10:17,217 - INFO - __main__ - Epoch  20, Step:   42200, Batch Loss:    15.324705, Lr: 0.000083, Tokens per sec:   2674
2023-03-14 14:10:36,749 - INFO - __main__ - Epoch  20, Step:   42300, Batch Loss:    21.886194, Lr: 0.000083, Tokens per sec:   2731
2023-03-14 14:10:56,982 - INFO - __main__ - Epoch  20, Step:   42400, Batch Loss:    15.659925, Lr: 0.000083, Tokens per sec:   2665
2023-03-14 14:11:16,892 - INFO - __main__ - Epoch  20, Step:   42500, Batch Loss:    22.162741, Lr: 0.000083, Tokens per sec:   2680
2023-03-14 14:11:36,966 - INFO - __main__ - Epoch  20, Step:   42600, Batch Loss:    17.207041, Lr: 0.000083, Tokens per sec:   2666
2023-03-14 14:11:57,047 - INFO - __main__ - Epoch  20, Step:   42700, Batch Loss:    17.348909, Lr: 0.000083, Tokens per sec:   2680
2023-03-14 14:12:17,164 - INFO - __main__ - Epoch  20, Step:   42800, Batch Loss:    17.955894, Lr: 0.000083, Tokens per sec:   2728
2023-03-14 14:12:37,249 - INFO - __main__ - Epoch  20, Step:   42900, Batch Loss:    19.186285, Lr: 0.000083, Tokens per sec:   2712
2023-03-14 14:12:57,425 - INFO - __main__ - Epoch  20, Step:   43000, Batch Loss:    15.673807, Lr: 0.000083, Tokens per sec:   2634
2023-03-14 14:13:17,537 - INFO - __main__ - Epoch  20, Step:   43100, Batch Loss:    20.191286, Lr: 0.000083, Tokens per sec:   2696
2023-03-14 14:13:37,579 - INFO - __main__ - Epoch  20, Step:   43200, Batch Loss:    20.352968, Lr: 0.000083, Tokens per sec:   2667
2023-03-14 14:13:57,715 - INFO - __main__ - Epoch  20, Step:   43300, Batch Loss:    19.694984, Lr: 0.000083, Tokens per sec:   2662
2023-03-14 14:14:18,109 - INFO - __main__ - Epoch  20, Step:   43400, Batch Loss:    18.359287, Lr: 0.000083, Tokens per sec:   2609
2023-03-14 14:14:38,577 - INFO - __main__ - Epoch  20, Step:   43500, Batch Loss:    25.485102, Lr: 0.000083, Tokens per sec:   2613
2023-03-14 14:14:54,969 - INFO - __main__ - Epoch  20: total training loss 40489.75
2023-03-14 14:14:54,970 - INFO - __main__ - Epoch 21
2023-03-14 14:14:59,328 - INFO - __main__ - Epoch  21, Step:   43600, Batch Loss:    14.669634, Lr: 0.000082, Tokens per sec:   2417
2023-03-14 14:15:19,485 - INFO - __main__ - Epoch  21, Step:   43700, Batch Loss:    16.470358, Lr: 0.000082, Tokens per sec:   2712
2023-03-14 14:15:39,625 - INFO - __main__ - Epoch  21, Step:   43800, Batch Loss:    14.572998, Lr: 0.000082, Tokens per sec:   2673
2023-03-14 14:15:59,805 - INFO - __main__ - Epoch  21, Step:   43900, Batch Loss:    13.650316, Lr: 0.000082, Tokens per sec:   2644
2023-03-14 14:16:19,741 - INFO - __main__ - Epoch  21, Step:   44000, Batch Loss:    15.733141, Lr: 0.000082, Tokens per sec:   2707
2023-03-14 14:16:39,742 - INFO - __main__ - Epoch  21, Step:   44100, Batch Loss:    13.427061, Lr: 0.000082, Tokens per sec:   2695
2023-03-14 14:16:59,799 - INFO - __main__ - Epoch  21, Step:   44200, Batch Loss:    14.182679, Lr: 0.000082, Tokens per sec:   2638
2023-03-14 14:17:19,647 - INFO - __main__ - Epoch  21, Step:   44300, Batch Loss:    15.308618, Lr: 0.000082, Tokens per sec:   2671
2023-03-14 14:17:39,742 - INFO - __main__ - Epoch  21, Step:   44400, Batch Loss:    14.680563, Lr: 0.000082, Tokens per sec:   2695
2023-03-14 14:18:00,014 - INFO - __main__ - Epoch  21, Step:   44500, Batch Loss:    16.455698, Lr: 0.000082, Tokens per sec:   2663
2023-03-14 14:18:20,048 - INFO - __main__ - Epoch  21, Step:   44600, Batch Loss:    21.345850, Lr: 0.000082, Tokens per sec:   2694
2023-03-14 14:18:40,160 - INFO - __main__ - Epoch  21, Step:   44700, Batch Loss:    15.880915, Lr: 0.000082, Tokens per sec:   2704
2023-03-14 14:19:00,085 - INFO - __main__ - Epoch  21, Step:   44800, Batch Loss:    23.269884, Lr: 0.000082, Tokens per sec:   2666
2023-03-14 14:19:20,041 - INFO - __main__ - Epoch  21, Step:   44900, Batch Loss:    23.492115, Lr: 0.000082, Tokens per sec:   2732
2023-03-14 14:19:40,009 - INFO - __main__ - Epoch  21, Step:   45000, Batch Loss:    21.641035, Lr: 0.000082, Tokens per sec:   2713
2023-03-14 14:20:00,067 - INFO - __main__ - Epoch  21, Step:   45100, Batch Loss:    15.742084, Lr: 0.000082, Tokens per sec:   2687
2023-03-14 14:20:19,813 - INFO - __main__ - Epoch  21, Step:   45200, Batch Loss:    19.374893, Lr: 0.000082, Tokens per sec:   2765
2023-03-14 14:20:39,684 - INFO - __main__ - Epoch  21, Step:   45300, Batch Loss:    16.842302, Lr: 0.000082, Tokens per sec:   2716
2023-03-14 14:20:59,972 - INFO - __main__ - Epoch  21, Step:   45400, Batch Loss:    21.394600, Lr: 0.000082, Tokens per sec:   2611
2023-03-14 14:21:19,869 - INFO - __main__ - Epoch  21, Step:   45500, Batch Loss:    20.424747, Lr: 0.000082, Tokens per sec:   2703
2023-03-14 14:21:39,751 - INFO - __main__ - Epoch  21, Step:   45600, Batch Loss:    15.621042, Lr: 0.000082, Tokens per sec:   2695
2023-03-14 14:21:59,247 - INFO - __main__ - Epoch  21, Step:   45700, Batch Loss:    19.829994, Lr: 0.000082, Tokens per sec:   2766
2023-03-14 14:22:10,790 - INFO - __main__ - Epoch  21: total training loss 38326.55
2023-03-14 14:22:10,791 - INFO - __main__ - Epoch 22
2023-03-14 14:22:18,626 - INFO - __main__ - Epoch  22, Step:   45800, Batch Loss:    16.054165, Lr: 0.000081, Tokens per sec:   2848
2023-03-14 14:22:38,061 - INFO - __main__ - Epoch  22, Step:   45900, Batch Loss:    13.848684, Lr: 0.000081, Tokens per sec:   2752
2023-03-14 14:22:58,131 - INFO - __main__ - Epoch  22, Step:   46000, Batch Loss:    16.955444, Lr: 0.000081, Tokens per sec:   2686
2023-03-14 14:23:18,192 - INFO - __main__ - Epoch  22, Step:   46100, Batch Loss:    15.616236, Lr: 0.000081, Tokens per sec:   2684
2023-03-14 14:23:37,731 - INFO - __main__ - Epoch  22, Step:   46200, Batch Loss:    15.028104, Lr: 0.000081, Tokens per sec:   2810
2023-03-14 14:23:57,178 - INFO - __main__ - Epoch  22, Step:   46300, Batch Loss:    19.821945, Lr: 0.000081, Tokens per sec:   2809
2023-03-14 14:24:17,177 - INFO - __main__ - Epoch  22, Step:   46400, Batch Loss:    17.866398, Lr: 0.000081, Tokens per sec:   2695
2023-03-14 14:24:37,118 - INFO - __main__ - Epoch  22, Step:   46500, Batch Loss:    20.374182, Lr: 0.000081, Tokens per sec:   2690
2023-03-14 14:24:56,982 - INFO - __main__ - Epoch  22, Step:   46600, Batch Loss:    21.081192, Lr: 0.000081, Tokens per sec:   2667
2023-03-14 14:25:16,096 - INFO - __main__ - Epoch  22, Step:   46700, Batch Loss:    18.963533, Lr: 0.000081, Tokens per sec:   2812
2023-03-14 14:25:35,412 - INFO - __main__ - Epoch  22, Step:   46800, Batch Loss:    15.304811, Lr: 0.000081, Tokens per sec:   2742
2023-03-14 14:25:55,388 - INFO - __main__ - Epoch  22, Step:   46900, Batch Loss:    16.142340, Lr: 0.000081, Tokens per sec:   2691
2023-03-14 14:26:15,326 - INFO - __main__ - Epoch  22, Step:   47000, Batch Loss:    22.387733, Lr: 0.000081, Tokens per sec:   2716
2023-03-14 14:26:35,251 - INFO - __main__ - Epoch  22, Step:   47100, Batch Loss:    16.033833, Lr: 0.000081, Tokens per sec:   2680
2023-03-14 14:26:55,226 - INFO - __main__ - Epoch  22, Step:   47200, Batch Loss:    12.364641, Lr: 0.000081, Tokens per sec:   2694
2023-03-14 14:27:14,692 - INFO - __main__ - Epoch  22, Step:   47300, Batch Loss:    13.195908, Lr: 0.000081, Tokens per sec:   2745
2023-03-14 14:27:34,375 - INFO - __main__ - Epoch  22, Step:   47400, Batch Loss:    15.495882, Lr: 0.000081, Tokens per sec:   2708
2023-03-14 14:27:54,339 - INFO - __main__ - Epoch  22, Step:   47500, Batch Loss:    23.226215, Lr: 0.000081, Tokens per sec:   2721
2023-03-14 14:28:14,322 - INFO - __main__ - Epoch  22, Step:   47600, Batch Loss:    18.232788, Lr: 0.000081, Tokens per sec:   2708
2023-03-14 14:28:33,913 - INFO - __main__ - Epoch  22, Step:   47700, Batch Loss:    20.343639, Lr: 0.000081, Tokens per sec:   2773
2023-03-14 14:28:53,612 - INFO - __main__ - Epoch  22, Step:   47800, Batch Loss:    16.924063, Lr: 0.000081, Tokens per sec:   2732
2023-03-14 14:29:13,541 - INFO - __main__ - Epoch  22, Step:   47900, Batch Loss:    14.243852, Lr: 0.000081, Tokens per sec:   2693
2023-03-14 14:29:21,148 - INFO - __main__ - Epoch  22: total training loss 36363.56
2023-03-14 14:29:21,149 - INFO - __main__ - Epoch 23
2023-03-14 14:29:33,883 - INFO - __main__ - Epoch  23, Step:   48000, Batch Loss:    14.328992, Lr: 0.000080, Tokens per sec:   2621
2023-03-14 14:29:53,934 - INFO - __main__ - Epoch  23, Step:   48100, Batch Loss:    14.332710, Lr: 0.000080, Tokens per sec:   2689
2023-03-14 14:30:13,857 - INFO - __main__ - Epoch  23, Step:   48200, Batch Loss:    14.572061, Lr: 0.000080, Tokens per sec:   2743
2023-03-14 14:30:33,821 - INFO - __main__ - Epoch  23, Step:   48300, Batch Loss:    12.344054, Lr: 0.000080, Tokens per sec:   2676
2023-03-14 14:30:53,143 - INFO - __main__ - Epoch  23, Step:   48400, Batch Loss:    12.151853, Lr: 0.000080, Tokens per sec:   2826
2023-03-14 14:31:12,635 - INFO - __main__ - Epoch  23, Step:   48500, Batch Loss:    18.789417, Lr: 0.000080, Tokens per sec:   2776
2023-03-14 14:31:31,914 - INFO - __main__ - Epoch  23, Step:   48600, Batch Loss:    17.046568, Lr: 0.000080, Tokens per sec:   2806
2023-03-14 14:31:51,194 - INFO - __main__ - Epoch  23, Step:   48700, Batch Loss:    16.320364, Lr: 0.000080, Tokens per sec:   2799
2023-03-14 14:32:09,926 - INFO - __main__ - Epoch  23, Step:   48800, Batch Loss:    12.556456, Lr: 0.000080, Tokens per sec:   2858
2023-03-14 14:32:29,786 - INFO - __main__ - Epoch  23, Step:   48900, Batch Loss:    14.229726, Lr: 0.000080, Tokens per sec:   2717
2023-03-14 14:32:49,768 - INFO - __main__ - Epoch  23, Step:   49000, Batch Loss:    15.826534, Lr: 0.000080, Tokens per sec:   2699
2023-03-14 14:33:09,373 - INFO - __main__ - Epoch  23, Step:   49100, Batch Loss:    17.379059, Lr: 0.000080, Tokens per sec:   2732
2023-03-14 14:33:28,444 - INFO - __main__ - Epoch  23, Step:   49200, Batch Loss:     9.365415, Lr: 0.000080, Tokens per sec:   2828
2023-03-14 14:33:47,274 - INFO - __main__ - Epoch  23, Step:   49300, Batch Loss:    19.689003, Lr: 0.000080, Tokens per sec:   2861
2023-03-14 14:34:07,059 - INFO - __main__ - Epoch  23, Step:   49400, Batch Loss:    17.755575, Lr: 0.000080, Tokens per sec:   2760
2023-03-14 14:34:27,022 - INFO - __main__ - Epoch  23, Step:   49500, Batch Loss:    17.289993, Lr: 0.000080, Tokens per sec:   2695
2023-03-14 14:34:46,662 - INFO - __main__ - Epoch  23, Step:   49600, Batch Loss:    11.689660, Lr: 0.000080, Tokens per sec:   2761
2023-03-14 14:35:06,297 - INFO - __main__ - Epoch  23, Step:   49700, Batch Loss:    17.398499, Lr: 0.000080, Tokens per sec:   2714
2023-03-14 14:35:25,992 - INFO - __main__ - Epoch  23, Step:   49800, Batch Loss:    17.334415, Lr: 0.000080, Tokens per sec:   2749
2023-03-14 14:35:45,923 - INFO - __main__ - Epoch  23, Step:   49900, Batch Loss:    16.265892, Lr: 0.000080, Tokens per sec:   2668
2023-03-14 14:36:06,021 - INFO - __main__ - Epoch  23, Step:   50000, Batch Loss:    14.860384, Lr: 0.000080, Tokens per sec:   2631
2023-03-14 14:36:25,465 - INFO - __main__ - Epoch  23, Step:   50100, Batch Loss:    14.879910, Lr: 0.000080, Tokens per sec:   2721
2023-03-14 14:36:28,894 - INFO - __main__ - Epoch  23: total training loss 34435.71
2023-03-14 14:36:28,895 - INFO - __main__ - Epoch 24
2023-03-14 14:36:44,790 - INFO - __main__ - Epoch  24, Step:   50200, Batch Loss:    11.521066, Lr: 0.000079, Tokens per sec:   2838
2023-03-14 14:37:03,741 - INFO - __main__ - Epoch  24, Step:   50300, Batch Loss:    18.798164, Lr: 0.000079, Tokens per sec:   2830
2023-03-14 14:37:22,911 - INFO - __main__ - Epoch  24, Step:   50400, Batch Loss:    18.243580, Lr: 0.000079, Tokens per sec:   2842
2023-03-14 14:37:42,770 - INFO - __main__ - Epoch  24, Step:   50500, Batch Loss:    11.259751, Lr: 0.000079, Tokens per sec:   2688
2023-03-14 14:38:02,338 - INFO - __main__ - Epoch  24, Step:   50600, Batch Loss:    14.817482, Lr: 0.000079, Tokens per sec:   2766
2023-03-14 14:38:22,315 - INFO - __main__ - Epoch  24, Step:   50700, Batch Loss:    13.334650, Lr: 0.000079, Tokens per sec:   2684
2023-03-14 14:38:42,235 - INFO - __main__ - Epoch  24, Step:   50800, Batch Loss:    14.466012, Lr: 0.000079, Tokens per sec:   2726
2023-03-14 14:39:01,636 - INFO - __main__ - Epoch  24, Step:   50900, Batch Loss:    12.848701, Lr: 0.000079, Tokens per sec:   2722
2023-03-14 14:39:20,099 - INFO - __main__ - Epoch  24, Step:   51000, Batch Loss:    13.297462, Lr: 0.000079, Tokens per sec:   2880
2023-03-14 14:39:38,754 - INFO - __main__ - Epoch  24, Step:   51100, Batch Loss:    14.086955, Lr: 0.000079, Tokens per sec:   2893
2023-03-14 14:39:57,382 - INFO - __main__ - Epoch  24, Step:   51200, Batch Loss:    14.598204, Lr: 0.000079, Tokens per sec:   2931
2023-03-14 14:40:15,965 - INFO - __main__ - Epoch  24, Step:   51300, Batch Loss:    11.090586, Lr: 0.000079, Tokens per sec:   2914
2023-03-14 14:40:35,593 - INFO - __main__ - Epoch  24, Step:   51400, Batch Loss:     9.668215, Lr: 0.000079, Tokens per sec:   2758
2023-03-14 14:40:54,680 - INFO - __main__ - Epoch  24, Step:   51500, Batch Loss:    12.827929, Lr: 0.000079, Tokens per sec:   2833
2023-03-14 14:41:13,647 - INFO - __main__ - Epoch  24, Step:   51600, Batch Loss:    19.982468, Lr: 0.000079, Tokens per sec:   2838
2023-03-14 14:41:32,454 - INFO - __main__ - Epoch  24, Step:   51700, Batch Loss:    16.572214, Lr: 0.000079, Tokens per sec:   2825
2023-03-14 14:41:51,861 - INFO - __main__ - Epoch  24, Step:   51800, Batch Loss:    14.746621, Lr: 0.000079, Tokens per sec:   2738
2023-03-14 14:42:11,844 - INFO - __main__ - Epoch  24, Step:   51900, Batch Loss:    15.609495, Lr: 0.000079, Tokens per sec:   2739
2023-03-14 14:42:31,505 - INFO - __main__ - Epoch  24, Step:   52000, Batch Loss:    16.637390, Lr: 0.000079, Tokens per sec:   2726
2023-03-14 14:42:51,116 - INFO - __main__ - Epoch  24, Step:   52100, Batch Loss:    10.150215, Lr: 0.000079, Tokens per sec:   2747
2023-03-14 14:43:11,174 - INFO - __main__ - Epoch  24, Step:   52200, Batch Loss:    12.125259, Lr: 0.000079, Tokens per sec:   2680
2023-03-14 14:43:30,356 - INFO - __main__ - Epoch  24: total training loss 32734.75
2023-03-14 14:43:30,357 - INFO - __main__ - Epoch 25
2023-03-14 14:43:31,444 - INFO - __main__ - Epoch  25, Step:   52300, Batch Loss:    10.745622, Lr: 0.000079, Tokens per sec:   2007
2023-03-14 14:43:50,330 - INFO - __main__ - Epoch  25, Step:   52400, Batch Loss:    14.091741, Lr: 0.000079, Tokens per sec:   2852
2023-03-14 14:44:09,511 - INFO - __main__ - Epoch  25, Step:   52500, Batch Loss:    11.498549, Lr: 0.000079, Tokens per sec:   2807
2023-03-14 14:44:28,683 - INFO - __main__ - Epoch  25, Step:   52600, Batch Loss:    14.223047, Lr: 0.000079, Tokens per sec:   2757
2023-03-14 14:44:48,594 - INFO - __main__ - Epoch  25, Step:   52700, Batch Loss:    18.412455, Lr: 0.000079, Tokens per sec:   2701
2023-03-14 14:45:07,336 - INFO - __main__ - Epoch  25, Step:   52800, Batch Loss:    14.479073, Lr: 0.000079, Tokens per sec:   2864
2023-03-14 14:45:26,122 - INFO - __main__ - Epoch  25, Step:   52900, Batch Loss:    17.239254, Lr: 0.000079, Tokens per sec:   2849
2023-03-14 14:45:44,803 - INFO - __main__ - Epoch  25, Step:   53000, Batch Loss:    13.782597, Lr: 0.000079, Tokens per sec:   2834
2023-03-14 14:46:04,391 - INFO - __main__ - Epoch  25, Step:   53100, Batch Loss:    23.155144, Lr: 0.000079, Tokens per sec:   2805
2023-03-14 14:46:23,751 - INFO - __main__ - Epoch  25, Step:   53200, Batch Loss:    11.710062, Lr: 0.000079, Tokens per sec:   2771
2023-03-14 14:46:43,610 - INFO - __main__ - Epoch  25, Step:   53300, Batch Loss:    12.392328, Lr: 0.000079, Tokens per sec:   2686
2023-03-14 14:47:03,455 - INFO - __main__ - Epoch  25, Step:   53400, Batch Loss:    21.096092, Lr: 0.000079, Tokens per sec:   2756
2023-03-14 14:47:22,319 - INFO - __main__ - Epoch  25, Step:   53500, Batch Loss:    17.636374, Lr: 0.000079, Tokens per sec:   2775
2023-03-14 14:47:42,334 - INFO - __main__ - Epoch  25, Step:   53600, Batch Loss:     9.222238, Lr: 0.000079, Tokens per sec:   2634
2023-03-14 14:48:01,882 - INFO - __main__ - Epoch  25, Step:   53700, Batch Loss:    12.155669, Lr: 0.000079, Tokens per sec:   2786
2023-03-14 14:48:20,369 - INFO - __main__ - Epoch  25, Step:   53800, Batch Loss:    18.261290, Lr: 0.000079, Tokens per sec:   2893
2023-03-14 14:48:39,616 - INFO - __main__ - Epoch  25, Step:   53900, Batch Loss:    15.481772, Lr: 0.000079, Tokens per sec:   2837
2023-03-14 14:48:59,751 - INFO - __main__ - Epoch  25, Step:   54000, Batch Loss:    16.433435, Lr: 0.000079, Tokens per sec:   2739
2023-03-14 14:49:19,600 - INFO - __main__ - Epoch  25, Step:   54100, Batch Loss:    13.839334, Lr: 0.000079, Tokens per sec:   2720
2023-03-14 14:49:39,638 - INFO - __main__ - Epoch  25, Step:   54200, Batch Loss:    14.579048, Lr: 0.000079, Tokens per sec:   2720
2023-03-14 14:49:59,286 - INFO - __main__ - Epoch  25, Step:   54300, Batch Loss:    25.182592, Lr: 0.000079, Tokens per sec:   2746
2023-03-14 14:50:18,954 - INFO - __main__ - Epoch  25, Step:   54400, Batch Loss:    15.966533, Lr: 0.000079, Tokens per sec:   2751
2023-03-14 14:50:33,541 - INFO - __main__ - Epoch  25: total training loss 31025.16
2023-03-14 14:50:33,542 - INFO - __main__ - Epoch 26
2023-03-14 14:50:38,740 - INFO - __main__ - Epoch  26, Step:   54500, Batch Loss:    10.770522, Lr: 0.000078, Tokens per sec:   2645
2023-03-14 14:50:58,635 - INFO - __main__ - Epoch  26, Step:   54600, Batch Loss:     7.904058, Lr: 0.000078, Tokens per sec:   2658
2023-03-14 14:51:18,490 - INFO - __main__ - Epoch  26, Step:   54700, Batch Loss:    10.684008, Lr: 0.000078, Tokens per sec:   2716
2023-03-14 14:51:38,678 - INFO - __main__ - Epoch  26, Step:   54800, Batch Loss:    12.331454, Lr: 0.000078, Tokens per sec:   2707
2023-03-14 14:51:58,811 - INFO - __main__ - Epoch  26, Step:   54900, Batch Loss:    15.191532, Lr: 0.000078, Tokens per sec:   2669
2023-03-14 14:52:18,661 - INFO - __main__ - Epoch  26, Step:   55000, Batch Loss:    12.150397, Lr: 0.000078, Tokens per sec:   2689
2023-03-14 14:52:38,721 - INFO - __main__ - Epoch  26, Step:   55100, Batch Loss:     8.941692, Lr: 0.000078, Tokens per sec:   2659
2023-03-14 14:52:58,794 - INFO - __main__ - Epoch  26, Step:   55200, Batch Loss:    15.396046, Lr: 0.000078, Tokens per sec:   2673
2023-03-14 14:53:19,031 - INFO - __main__ - Epoch  26, Step:   55300, Batch Loss:    13.153116, Lr: 0.000078, Tokens per sec:   2662
2023-03-14 14:53:38,881 - INFO - __main__ - Epoch  26, Step:   55400, Batch Loss:    11.540493, Lr: 0.000078, Tokens per sec:   2690
2023-03-14 14:53:58,867 - INFO - __main__ - Epoch  26, Step:   55500, Batch Loss:    12.087799, Lr: 0.000078, Tokens per sec:   2723
2023-03-14 14:54:18,666 - INFO - __main__ - Epoch  26, Step:   55600, Batch Loss:    11.885052, Lr: 0.000078, Tokens per sec:   2713
2023-03-14 14:54:38,928 - INFO - __main__ - Epoch  26, Step:   55700, Batch Loss:     9.552341, Lr: 0.000078, Tokens per sec:   2665
2023-03-14 14:54:58,543 - INFO - __main__ - Epoch  26, Step:   55800, Batch Loss:    13.920006, Lr: 0.000078, Tokens per sec:   2737
2023-03-14 14:55:17,548 - INFO - __main__ - Epoch  26, Step:   55900, Batch Loss:    17.397348, Lr: 0.000078, Tokens per sec:   2822
2023-03-14 14:55:37,305 - INFO - __main__ - Epoch  26, Step:   56000, Batch Loss:    18.328962, Lr: 0.000078, Tokens per sec:   2700
2023-03-14 14:55:55,900 - INFO - __main__ - Epoch  26, Step:   56100, Batch Loss:    11.583252, Lr: 0.000078, Tokens per sec:   2930
2023-03-14 14:56:15,327 - INFO - __main__ - Epoch  26, Step:   56200, Batch Loss:    14.112920, Lr: 0.000078, Tokens per sec:   2804
2023-03-14 14:56:34,476 - INFO - __main__ - Epoch  26, Step:   56300, Batch Loss:    14.754286, Lr: 0.000078, Tokens per sec:   2825
2023-03-14 14:56:54,276 - INFO - __main__ - Epoch  26, Step:   56400, Batch Loss:    19.467085, Lr: 0.000078, Tokens per sec:   2729
2023-03-14 14:57:13,481 - INFO - __main__ - Epoch  26, Step:   56500, Batch Loss:    12.629616, Lr: 0.000078, Tokens per sec:   2792
2023-03-14 14:57:33,598 - INFO - __main__ - Epoch  26, Step:   56600, Batch Loss:    13.931843, Lr: 0.000078, Tokens per sec:   2684
2023-03-14 14:57:44,131 - INFO - __main__ - Epoch  26: total training loss 29550.86
2023-03-14 14:57:44,133 - INFO - __main__ - Epoch 27
2023-03-14 14:57:53,438 - INFO - __main__ - Epoch  27, Step:   56700, Batch Loss:    11.032396, Lr: 0.000077, Tokens per sec:   2694
2023-03-14 14:58:12,889 - INFO - __main__ - Epoch  27, Step:   56800, Batch Loss:    14.266329, Lr: 0.000077, Tokens per sec:   2778
2023-03-14 14:58:31,027 - INFO - __main__ - Epoch  27, Step:   56900, Batch Loss:    10.470525, Lr: 0.000077, Tokens per sec:   2937
2023-03-14 14:58:50,281 - INFO - __main__ - Epoch  27, Step:   57000, Batch Loss:    14.304183, Lr: 0.000077, Tokens per sec:   2790
2023-03-14 14:59:09,858 - INFO - __main__ - Epoch  27, Step:   57100, Batch Loss:     8.594400, Lr: 0.000077, Tokens per sec:   2771
2023-03-14 14:59:29,303 - INFO - __main__ - Epoch  27, Step:   57200, Batch Loss:    14.935143, Lr: 0.000077, Tokens per sec:   2744
2023-03-14 14:59:48,794 - INFO - __main__ - Epoch  27, Step:   57300, Batch Loss:    18.554487, Lr: 0.000077, Tokens per sec:   2783
2023-03-14 15:00:08,149 - INFO - __main__ - Epoch  27, Step:   57400, Batch Loss:    16.795988, Lr: 0.000077, Tokens per sec:   2855
2023-03-14 15:00:27,178 - INFO - __main__ - Epoch  27, Step:   57500, Batch Loss:    12.683117, Lr: 0.000077, Tokens per sec:   2796
2023-03-14 15:00:46,287 - INFO - __main__ - Epoch  27, Step:   57600, Batch Loss:     7.086284, Lr: 0.000077, Tokens per sec:   2840
2023-03-14 15:01:06,375 - INFO - __main__ - Epoch  27, Step:   57700, Batch Loss:    14.571476, Lr: 0.000077, Tokens per sec:   2747
2023-03-14 15:01:26,209 - INFO - __main__ - Epoch  27, Step:   57800, Batch Loss:     9.821337, Lr: 0.000077, Tokens per sec:   2698
2023-03-14 15:01:44,542 - INFO - __main__ - Epoch  27, Step:   57900, Batch Loss:     9.342316, Lr: 0.000077, Tokens per sec:   2934
2023-03-14 15:02:03,410 - INFO - __main__ - Epoch  27, Step:   58000, Batch Loss:    13.200538, Lr: 0.000077, Tokens per sec:   2852
2023-03-14 15:02:23,450 - INFO - __main__ - Epoch  27, Step:   58100, Batch Loss:    15.781992, Lr: 0.000077, Tokens per sec:   2685
2023-03-14 15:02:42,959 - INFO - __main__ - Epoch  27, Step:   58200, Batch Loss:    13.397370, Lr: 0.000077, Tokens per sec:   2746
2023-03-14 15:03:01,347 - INFO - __main__ - Epoch  27, Step:   58300, Batch Loss:    16.224363, Lr: 0.000077, Tokens per sec:   2960
2023-03-14 15:03:19,635 - INFO - __main__ - Epoch  27, Step:   58400, Batch Loss:    12.492500, Lr: 0.000077, Tokens per sec:   2927
2023-03-14 15:03:39,035 - INFO - __main__ - Epoch  27, Step:   58500, Batch Loss:    11.865803, Lr: 0.000077, Tokens per sec:   2718
2023-03-14 15:03:59,012 - INFO - __main__ - Epoch  27, Step:   58600, Batch Loss:    11.117341, Lr: 0.000077, Tokens per sec:   2665
2023-03-14 15:04:17,579 - INFO - __main__ - Epoch  27, Step:   58700, Batch Loss:    15.176205, Lr: 0.000077, Tokens per sec:   2868
2023-03-14 15:04:35,765 - INFO - __main__ - Epoch  27, Step:   58800, Batch Loss:    13.936551, Lr: 0.000077, Tokens per sec:   2949
2023-03-14 15:04:41,892 - INFO - __main__ - Epoch  27: total training loss 28076.32
2023-03-14 15:04:41,893 - INFO - __main__ - Epoch 28
2023-03-14 15:04:55,756 - INFO - __main__ - Epoch  28, Step:   58900, Batch Loss:    14.080803, Lr: 0.000076, Tokens per sec:   2580
2023-03-14 15:05:14,505 - INFO - __main__ - Epoch  28, Step:   59000, Batch Loss:     9.114977, Lr: 0.000076, Tokens per sec:   2892
2023-03-14 15:05:33,979 - INFO - __main__ - Epoch  28, Step:   59100, Batch Loss:    15.530978, Lr: 0.000076, Tokens per sec:   2765
2023-03-14 15:05:52,320 - INFO - __main__ - Epoch  28, Step:   59200, Batch Loss:    13.711111, Lr: 0.000076, Tokens per sec:   2972
2023-03-14 15:06:11,938 - INFO - __main__ - Epoch  28, Step:   59300, Batch Loss:    10.079028, Lr: 0.000076, Tokens per sec:   2742
2023-03-14 15:06:31,172 - INFO - __main__ - Epoch  28, Step:   59400, Batch Loss:    11.542337, Lr: 0.000076, Tokens per sec:   2796
2023-03-14 15:06:50,088 - INFO - __main__ - Epoch  28, Step:   59500, Batch Loss:     9.263677, Lr: 0.000076, Tokens per sec:   2856
2023-03-14 15:07:10,246 - INFO - __main__ - Epoch  28, Step:   59600, Batch Loss:    12.833236, Lr: 0.000076, Tokens per sec:   2705
2023-03-14 15:07:28,858 - INFO - __main__ - Epoch  28, Step:   59700, Batch Loss:    13.438419, Lr: 0.000076, Tokens per sec:   2882
2023-03-14 15:07:47,060 - INFO - __main__ - Epoch  28, Step:   59800, Batch Loss:    10.307866, Lr: 0.000076, Tokens per sec:   2958
2023-03-14 15:08:05,574 - INFO - __main__ - Epoch  28, Step:   59900, Batch Loss:    15.738251, Lr: 0.000076, Tokens per sec:   2837
2023-03-14 15:08:24,020 - INFO - __main__ - Epoch  28, Step:   60000, Batch Loss:    11.801031, Lr: 0.000076, Tokens per sec:   2933
2023-03-14 15:08:42,360 - INFO - __main__ - Epoch  28, Step:   60100, Batch Loss:    15.644678, Lr: 0.000076, Tokens per sec:   2986
2023-03-14 15:09:00,767 - INFO - __main__ - Epoch  28, Step:   60200, Batch Loss:    13.955218, Lr: 0.000076, Tokens per sec:   2937
2023-03-14 15:09:20,377 - INFO - __main__ - Epoch  28, Step:   60300, Batch Loss:    10.668879, Lr: 0.000076, Tokens per sec:   2667
2023-03-14 15:09:39,646 - INFO - __main__ - Epoch  28, Step:   60400, Batch Loss:    11.575486, Lr: 0.000076, Tokens per sec:   2809
2023-03-14 15:09:58,607 - INFO - __main__ - Epoch  28, Step:   60500, Batch Loss:    14.488445, Lr: 0.000076, Tokens per sec:   2824
2023-03-14 15:10:17,601 - INFO - __main__ - Epoch  28, Step:   60600, Batch Loss:    11.875367, Lr: 0.000076, Tokens per sec:   2850
2023-03-14 15:10:36,533 - INFO - __main__ - Epoch  28, Step:   60700, Batch Loss:    13.360277, Lr: 0.000076, Tokens per sec:   2852
2023-03-14 15:10:55,162 - INFO - __main__ - Epoch  28, Step:   60800, Batch Loss:    15.799147, Lr: 0.000076, Tokens per sec:   2928
2023-03-14 15:11:14,635 - INFO - __main__ - Epoch  28, Step:   60900, Batch Loss:    11.195641, Lr: 0.000076, Tokens per sec:   2744
2023-03-14 15:11:34,533 - INFO - __main__ - Epoch  28, Step:   61000, Batch Loss:    13.392133, Lr: 0.000076, Tokens per sec:   2684
2023-03-14 15:11:36,979 - INFO - __main__ - Epoch  28: total training loss 26695.86
2023-03-14 15:11:36,980 - INFO - __main__ - Epoch 29
2023-03-14 15:11:53,708 - INFO - __main__ - Epoch  29, Step:   61100, Batch Loss:     7.052758, Lr: 0.000075, Tokens per sec:   2828
2023-03-14 15:12:13,392 - INFO - __main__ - Epoch  29, Step:   61200, Batch Loss:    10.173434, Lr: 0.000075, Tokens per sec:   2758
2023-03-14 15:12:33,307 - INFO - __main__ - Epoch  29, Step:   61300, Batch Loss:    11.755061, Lr: 0.000075, Tokens per sec:   2635
2023-03-14 15:12:52,883 - INFO - __main__ - Epoch  29, Step:   61400, Batch Loss:    14.779135, Lr: 0.000075, Tokens per sec:   2742
2023-03-14 15:13:12,803 - INFO - __main__ - Epoch  29, Step:   61500, Batch Loss:     8.762846, Lr: 0.000075, Tokens per sec:   2747
2023-03-14 15:13:31,939 - INFO - __main__ - Epoch  29, Step:   61600, Batch Loss:    15.434098, Lr: 0.000075, Tokens per sec:   2855
2023-03-14 15:13:50,682 - INFO - __main__ - Epoch  29, Step:   61700, Batch Loss:    12.119143, Lr: 0.000075, Tokens per sec:   2899
2023-03-14 15:14:10,045 - INFO - __main__ - Epoch  29, Step:   61800, Batch Loss:    10.574579, Lr: 0.000075, Tokens per sec:   2814
2023-03-14 15:14:28,861 - INFO - __main__ - Epoch  29, Step:   61900, Batch Loss:    14.819241, Lr: 0.000075, Tokens per sec:   2906
2023-03-14 15:14:47,894 - INFO - __main__ - Epoch  29, Step:   62000, Batch Loss:    12.526501, Lr: 0.000075, Tokens per sec:   2771
2023-03-14 15:15:07,814 - INFO - __main__ - Epoch  29, Step:   62100, Batch Loss:    11.197432, Lr: 0.000075, Tokens per sec:   2665
2023-03-14 15:15:27,852 - INFO - __main__ - Epoch  29, Step:   62200, Batch Loss:     9.339279, Lr: 0.000075, Tokens per sec:   2711
2023-03-14 15:15:46,948 - INFO - __main__ - Epoch  29, Step:   62300, Batch Loss:     8.968955, Lr: 0.000075, Tokens per sec:   2804
2023-03-14 15:16:05,950 - INFO - __main__ - Epoch  29, Step:   62400, Batch Loss:    11.684643, Lr: 0.000075, Tokens per sec:   2852
2023-03-14 15:16:25,125 - INFO - __main__ - Epoch  29, Step:   62500, Batch Loss:    13.694880, Lr: 0.000075, Tokens per sec:   2802
2023-03-14 15:16:43,240 - INFO - __main__ - Epoch  29, Step:   62600, Batch Loss:    14.993166, Lr: 0.000075, Tokens per sec:   2954
2023-03-14 15:17:01,828 - INFO - __main__ - Epoch  29, Step:   62700, Batch Loss:    14.508598, Lr: 0.000075, Tokens per sec:   2906
2023-03-14 15:17:19,943 - INFO - __main__ - Epoch  29, Step:   62800, Batch Loss:    14.943682, Lr: 0.000075, Tokens per sec:   2931
2023-03-14 15:17:39,099 - INFO - __main__ - Epoch  29, Step:   62900, Batch Loss:    15.670763, Lr: 0.000075, Tokens per sec:   2795
2023-03-14 15:17:59,140 - INFO - __main__ - Epoch  29, Step:   63000, Batch Loss:    14.257381, Lr: 0.000075, Tokens per sec:   2663
2023-03-14 15:18:19,134 - INFO - __main__ - Epoch  29, Step:   63100, Batch Loss:    13.604484, Lr: 0.000075, Tokens per sec:   2666
2023-03-14 15:18:37,082 - INFO - __main__ - Epoch  29: total training loss 25415.40
2023-03-14 15:18:37,084 - INFO - __main__ - Epoch 30
2023-03-14 15:18:39,139 - INFO - __main__ - Epoch  30, Step:   63200, Batch Loss:     9.113931, Lr: 0.000075, Tokens per sec:   2179
2023-03-14 15:18:57,518 - INFO - __main__ - Epoch  30, Step:   63300, Batch Loss:     9.239431, Lr: 0.000075, Tokens per sec:   2982
2023-03-14 15:19:15,988 - INFO - __main__ - Epoch  30, Step:   63400, Batch Loss:     8.330211, Lr: 0.000075, Tokens per sec:   2899
2023-03-14 15:19:34,978 - INFO - __main__ - Epoch  30, Step:   63500, Batch Loss:    13.185400, Lr: 0.000075, Tokens per sec:   2832
2023-03-14 15:19:54,118 - INFO - __main__ - Epoch  30, Step:   63600, Batch Loss:    11.096407, Lr: 0.000075, Tokens per sec:   2813
2023-03-14 15:20:13,203 - INFO - __main__ - Epoch  30, Step:   63700, Batch Loss:    12.759366, Lr: 0.000075, Tokens per sec:   2854
2023-03-14 15:20:31,747 - INFO - __main__ - Epoch  30, Step:   63800, Batch Loss:    13.008300, Lr: 0.000075, Tokens per sec:   2942
2023-03-14 15:20:51,525 - INFO - __main__ - Epoch  30, Step:   63900, Batch Loss:    10.470693, Lr: 0.000075, Tokens per sec:   2724
2023-03-14 15:21:10,893 - INFO - __main__ - Epoch  30, Step:   64000, Batch Loss:     8.993776, Lr: 0.000075, Tokens per sec:   2779
2023-03-14 15:21:30,865 - INFO - __main__ - Epoch  30, Step:   64100, Batch Loss:     8.326969, Lr: 0.000075, Tokens per sec:   2716
2023-03-14 15:21:49,790 - INFO - __main__ - Epoch  30, Step:   64200, Batch Loss:    12.569935, Lr: 0.000075, Tokens per sec:   2835
2023-03-14 15:22:09,816 - INFO - __main__ - Epoch  30, Step:   64300, Batch Loss:    13.785953, Lr: 0.000075, Tokens per sec:   2646
2023-03-14 15:22:29,786 - INFO - __main__ - Epoch  30, Step:   64400, Batch Loss:    11.354486, Lr: 0.000075, Tokens per sec:   2682
2023-03-14 15:22:48,100 - INFO - __main__ - Epoch  30, Step:   64500, Batch Loss:    15.379704, Lr: 0.000075, Tokens per sec:   2953
2023-03-14 15:23:07,899 - INFO - __main__ - Epoch  30, Step:   64600, Batch Loss:    11.915313, Lr: 0.000075, Tokens per sec:   2714
2023-03-14 15:23:27,829 - INFO - __main__ - Epoch  30, Step:   64700, Batch Loss:    10.289181, Lr: 0.000075, Tokens per sec:   2633
2023-03-14 15:23:46,722 - INFO - __main__ - Epoch  30, Step:   64800, Batch Loss:     6.777339, Lr: 0.000075, Tokens per sec:   2843
2023-03-14 15:24:05,430 - INFO - __main__ - Epoch  30, Step:   64900, Batch Loss:    14.475613, Lr: 0.000075, Tokens per sec:   2940
2023-03-14 15:24:24,357 - INFO - __main__ - Epoch  30, Step:   65000, Batch Loss:    14.659757, Lr: 0.000075, Tokens per sec:   2819
2023-03-14 15:24:43,962 - INFO - __main__ - Epoch  30, Step:   65100, Batch Loss:    10.337428, Lr: 0.000075, Tokens per sec:   2754
2023-03-14 15:25:04,053 - INFO - __main__ - Epoch  30, Step:   65200, Batch Loss:    11.921246, Lr: 0.000075, Tokens per sec:   2642
2023-03-14 15:25:22,473 - INFO - __main__ - Epoch  30, Step:   65300, Batch Loss:    12.044761, Lr: 0.000075, Tokens per sec:   2961
2023-03-14 15:25:35,597 - INFO - __main__ - Epoch  30: total training loss 24273.10
2023-03-14 15:25:35,598 - INFO - __main__ - Epoch 31
2023-03-14 15:25:41,569 - INFO - __main__ - Epoch  31, Step:   65400, Batch Loss:    11.136726, Lr: 0.000074, Tokens per sec:   2692
2023-03-14 15:25:59,690 - INFO - __main__ - Epoch  31, Step:   65500, Batch Loss:     7.560339, Lr: 0.000074, Tokens per sec:   2977
2023-03-14 15:26:18,810 - INFO - __main__ - Epoch  31, Step:   65600, Batch Loss:    11.729198, Lr: 0.000074, Tokens per sec:   2824
2023-03-14 15:26:37,455 - INFO - __main__ - Epoch  31, Step:   65700, Batch Loss:     8.494658, Lr: 0.000074, Tokens per sec:   2893
2023-03-14 15:26:55,942 - INFO - __main__ - Epoch  31, Step:   65800, Batch Loss:     9.182605, Lr: 0.000074, Tokens per sec:   2864
2023-03-14 15:27:16,010 - INFO - __main__ - Epoch  31, Step:   65900, Batch Loss:     8.848279, Lr: 0.000074, Tokens per sec:   2655
2023-03-14 15:27:35,484 - INFO - __main__ - Epoch  31, Step:   66000, Batch Loss:    11.177078, Lr: 0.000074, Tokens per sec:   2742
2023-03-14 15:27:55,493 - INFO - __main__ - Epoch  31, Step:   66100, Batch Loss:    11.118745, Lr: 0.000074, Tokens per sec:   2698
2023-03-14 15:28:15,172 - INFO - __main__ - Epoch  31, Step:   66200, Batch Loss:     7.156141, Lr: 0.000074, Tokens per sec:   2746
2023-03-14 15:28:34,547 - INFO - __main__ - Epoch  31, Step:   66300, Batch Loss:    12.364701, Lr: 0.000074, Tokens per sec:   2727
2023-03-14 15:28:54,120 - INFO - __main__ - Epoch  31, Step:   66400, Batch Loss:    11.406899, Lr: 0.000074, Tokens per sec:   2807
2023-03-14 15:29:13,992 - INFO - __main__ - Epoch  31, Step:   66500, Batch Loss:     8.061291, Lr: 0.000074, Tokens per sec:   2723
2023-03-14 15:29:33,749 - INFO - __main__ - Epoch  31, Step:   66600, Batch Loss:    11.224330, Lr: 0.000074, Tokens per sec:   2702
2023-03-14 15:29:53,861 - INFO - __main__ - Epoch  31, Step:   66700, Batch Loss:     8.689542, Lr: 0.000074, Tokens per sec:   2669
2023-03-14 15:30:12,636 - INFO - __main__ - Epoch  31, Step:   66800, Batch Loss:     8.678018, Lr: 0.000074, Tokens per sec:   2875
2023-03-14 15:30:30,674 - INFO - __main__ - Epoch  31, Step:   66900, Batch Loss:    10.425106, Lr: 0.000074, Tokens per sec:   2977
2023-03-14 15:30:49,931 - INFO - __main__ - Epoch  31, Step:   67000, Batch Loss:    12.164895, Lr: 0.000074, Tokens per sec:   2866
2023-03-14 15:31:09,364 - INFO - __main__ - Epoch  31, Step:   67100, Batch Loss:    11.751643, Lr: 0.000074, Tokens per sec:   2773
2023-03-14 15:31:28,286 - INFO - __main__ - Epoch  31, Step:   67200, Batch Loss:    11.090324, Lr: 0.000074, Tokens per sec:   2831
2023-03-14 15:31:46,371 - INFO - __main__ - Epoch  31, Step:   67300, Batch Loss:    15.306852, Lr: 0.000074, Tokens per sec:   2998
2023-03-14 15:32:04,895 - INFO - __main__ - Epoch  31, Step:   67400, Batch Loss:     9.072284, Lr: 0.000074, Tokens per sec:   2892
2023-03-14 15:32:23,324 - INFO - __main__ - Epoch  31, Step:   67500, Batch Loss:     9.681825, Lr: 0.000074, Tokens per sec:   2896
2023-03-14 15:32:33,156 - INFO - __main__ - Epoch  31: total training loss 23213.52
2023-03-14 15:32:33,157 - INFO - __main__ - Epoch 32
2023-03-14 15:32:43,428 - INFO - __main__ - Epoch  32, Step:   67600, Batch Loss:     7.118982, Lr: 0.000073, Tokens per sec:   2653
2023-03-14 15:33:03,289 - INFO - __main__ - Epoch  32, Step:   67700, Batch Loss:     8.239180, Lr: 0.000073, Tokens per sec:   2735
2023-03-14 15:33:22,541 - INFO - __main__ - Epoch  32, Step:   67800, Batch Loss:     8.105241, Lr: 0.000073, Tokens per sec:   2828
2023-03-14 15:33:40,899 - INFO - __main__ - Epoch  32, Step:   67900, Batch Loss:     8.625096, Lr: 0.000073, Tokens per sec:   2936
2023-03-14 15:33:59,219 - INFO - __main__ - Epoch  32, Step:   68000, Batch Loss:     6.866345, Lr: 0.000073, Tokens per sec:   2852
2023-03-14 15:34:17,719 - INFO - __main__ - Epoch  32, Step:   68100, Batch Loss:     8.746693, Lr: 0.000073, Tokens per sec:   2933
2023-03-14 15:34:36,127 - INFO - __main__ - Epoch  32, Step:   68200, Batch Loss:     8.211460, Lr: 0.000073, Tokens per sec:   2924
2023-03-14 15:34:54,633 - INFO - __main__ - Epoch  32, Step:   68300, Batch Loss:    14.834616, Lr: 0.000073, Tokens per sec:   2881
2023-03-14 15:35:13,427 - INFO - __main__ - Epoch  32, Step:   68400, Batch Loss:     9.867332, Lr: 0.000073, Tokens per sec:   2844
2023-03-14 15:35:31,736 - INFO - __main__ - Epoch  32, Step:   68500, Batch Loss:    11.259814, Lr: 0.000073, Tokens per sec:   2966
2023-03-14 15:35:49,860 - INFO - __main__ - Epoch  32, Step:   68600, Batch Loss:    11.475763, Lr: 0.000073, Tokens per sec:   2986
2023-03-14 15:36:07,984 - INFO - __main__ - Epoch  32, Step:   68700, Batch Loss:    11.737434, Lr: 0.000073, Tokens per sec:   2897
2023-03-14 15:36:26,078 - INFO - __main__ - Epoch  32, Step:   68800, Batch Loss:    12.223368, Lr: 0.000073, Tokens per sec:   3010
2023-03-14 15:36:45,631 - INFO - __main__ - Epoch  32, Step:   68900, Batch Loss:     8.772715, Lr: 0.000073, Tokens per sec:   2767
2023-03-14 15:37:05,606 - INFO - __main__ - Epoch  32, Step:   69000, Batch Loss:    13.645350, Lr: 0.000073, Tokens per sec:   2699
2023-03-14 15:37:25,532 - INFO - __main__ - Epoch  32, Step:   69100, Batch Loss:     9.634536, Lr: 0.000073, Tokens per sec:   2702
2023-03-14 15:37:45,091 - INFO - __main__ - Epoch  32, Step:   69200, Batch Loss:     6.937262, Lr: 0.000073, Tokens per sec:   2818
2023-03-14 15:38:03,897 - INFO - __main__ - Epoch  32, Step:   69300, Batch Loss:    11.768850, Lr: 0.000073, Tokens per sec:   2872
2023-03-14 15:38:22,654 - INFO - __main__ - Epoch  32, Step:   69400, Batch Loss:     9.130033, Lr: 0.000073, Tokens per sec:   2876
2023-03-14 15:38:40,826 - INFO - __main__ - Epoch  32, Step:   69500, Batch Loss:     8.379792, Lr: 0.000073, Tokens per sec:   2963
2023-03-14 15:38:58,957 - INFO - __main__ - Epoch  32, Step:   69600, Batch Loss:     9.884712, Lr: 0.000073, Tokens per sec:   2996
2023-03-14 15:39:16,920 - INFO - __main__ - Epoch  32, Step:   69700, Batch Loss:     9.323044, Lr: 0.000073, Tokens per sec:   2912
2023-03-14 15:39:21,980 - INFO - __main__ - Epoch  32: total training loss 22245.40
2023-03-14 15:39:21,981 - INFO - __main__ - Epoch 33
2023-03-14 15:39:36,608 - INFO - __main__ - Epoch  33, Step:   69800, Batch Loss:     7.227716, Lr: 0.000072, Tokens per sec:   2665
2023-03-14 15:39:55,183 - INFO - __main__ - Epoch  33, Step:   69900, Batch Loss:     9.443720, Lr: 0.000072, Tokens per sec:   2881
2023-03-14 15:40:13,485 - INFO - __main__ - Epoch  33, Step:   70000, Batch Loss:     7.555745, Lr: 0.000072, Tokens per sec:   2958
2023-03-14 15:40:32,389 - INFO - __main__ - Epoch  33, Step:   70100, Batch Loss:     6.984283, Lr: 0.000072, Tokens per sec:   2781
2023-03-14 15:40:51,771 - INFO - __main__ - Epoch  33, Step:   70200, Batch Loss:    11.033695, Lr: 0.000072, Tokens per sec:   2809
2023-03-14 15:41:10,957 - INFO - __main__ - Epoch  33, Step:   70300, Batch Loss:     8.144876, Lr: 0.000072, Tokens per sec:   2799
2023-03-14 15:41:29,856 - INFO - __main__ - Epoch  33, Step:   70400, Batch Loss:     6.722566, Lr: 0.000072, Tokens per sec:   2837
2023-03-14 15:41:50,019 - INFO - __main__ - Epoch  33, Step:   70500, Batch Loss:     7.247252, Lr: 0.000072, Tokens per sec:   2661
2023-03-14 15:42:09,681 - INFO - __main__ - Epoch  33, Step:   70600, Batch Loss:     7.277192, Lr: 0.000072, Tokens per sec:   2781
2023-03-14 15:42:29,772 - INFO - __main__ - Epoch  33, Step:   70700, Batch Loss:    12.193171, Lr: 0.000072, Tokens per sec:   2672
2023-03-14 15:42:49,822 - INFO - __main__ - Epoch  33, Step:   70800, Batch Loss:    12.398920, Lr: 0.000072, Tokens per sec:   2670
2023-03-14 15:43:09,930 - INFO - __main__ - Epoch  33, Step:   70900, Batch Loss:    11.374277, Lr: 0.000072, Tokens per sec:   2657
2023-03-14 15:43:29,964 - INFO - __main__ - Epoch  33, Step:   71000, Batch Loss:     8.764736, Lr: 0.000072, Tokens per sec:   2661
2023-03-14 15:43:50,210 - INFO - __main__ - Epoch  33, Step:   71100, Batch Loss:     8.645306, Lr: 0.000072, Tokens per sec:   2639
2023-03-14 15:44:10,409 - INFO - __main__ - Epoch  33, Step:   71200, Batch Loss:     9.332169, Lr: 0.000072, Tokens per sec:   2654
2023-03-14 15:44:30,634 - INFO - __main__ - Epoch  33, Step:   71300, Batch Loss:     9.682216, Lr: 0.000072, Tokens per sec:   2678
2023-03-14 15:44:50,803 - INFO - __main__ - Epoch  33, Step:   71400, Batch Loss:     8.533124, Lr: 0.000072, Tokens per sec:   2653
2023-03-14 15:45:11,104 - INFO - __main__ - Epoch  33, Step:   71500, Batch Loss:    11.852225, Lr: 0.000072, Tokens per sec:   2662
2023-03-14 15:45:31,465 - INFO - __main__ - Epoch  33, Step:   71600, Batch Loss:     9.148005, Lr: 0.000072, Tokens per sec:   2675
2023-03-14 15:45:51,614 - INFO - __main__ - Epoch  33, Step:   71700, Batch Loss:    11.557795, Lr: 0.000072, Tokens per sec:   2671
2023-03-14 15:46:11,918 - INFO - __main__ - Epoch  33, Step:   71800, Batch Loss:    12.020149, Lr: 0.000072, Tokens per sec:   2745
2023-03-14 15:46:31,939 - INFO - __main__ - Epoch  33, Step:   71900, Batch Loss:     9.509790, Lr: 0.000072, Tokens per sec:   2673
2023-03-14 15:46:33,392 - INFO - __main__ - Epoch  33: total training loss 21221.09
2023-03-14 15:46:33,393 - INFO - __main__ - Epoch 34
2023-03-14 15:46:52,396 - INFO - __main__ - Epoch  34, Step:   72000, Batch Loss:    12.061470, Lr: 0.000072, Tokens per sec:   2625
2023-03-14 15:47:12,224 - INFO - __main__ - Epoch  34, Step:   72100, Batch Loss:     7.951021, Lr: 0.000072, Tokens per sec:   2722
2023-03-14 15:47:32,194 - INFO - __main__ - Epoch  34, Step:   72200, Batch Loss:     7.426429, Lr: 0.000072, Tokens per sec:   2700
2023-03-14 15:47:52,033 - INFO - __main__ - Epoch  34, Step:   72300, Batch Loss:     7.205051, Lr: 0.000072, Tokens per sec:   2720
2023-03-14 15:48:11,258 - INFO - __main__ - Epoch  34, Step:   72400, Batch Loss:    12.799557, Lr: 0.000072, Tokens per sec:   2828
2023-03-14 15:48:30,121 - INFO - __main__ - Epoch  34, Step:   72500, Batch Loss:     8.618695, Lr: 0.000072, Tokens per sec:   2782
2023-03-14 15:48:48,719 - INFO - __main__ - Epoch  34, Step:   72600, Batch Loss:    10.635774, Lr: 0.000072, Tokens per sec:   2979
2023-03-14 15:49:06,859 - INFO - __main__ - Epoch  34, Step:   72700, Batch Loss:     7.297326, Lr: 0.000072, Tokens per sec:   2905
2023-03-14 15:49:25,063 - INFO - __main__ - Epoch  34, Step:   72800, Batch Loss:     9.436154, Lr: 0.000072, Tokens per sec:   2957
2023-03-14 15:49:43,184 - INFO - __main__ - Epoch  34, Step:   72900, Batch Loss:     8.292870, Lr: 0.000072, Tokens per sec:   2942
2023-03-14 15:50:01,397 - INFO - __main__ - Epoch  34, Step:   73000, Batch Loss:     7.797597, Lr: 0.000072, Tokens per sec:   2939
2023-03-14 15:50:20,857 - INFO - __main__ - Epoch  34, Step:   73100, Batch Loss:     9.163499, Lr: 0.000072, Tokens per sec:   2747
2023-03-14 15:50:40,943 - INFO - __main__ - Epoch  34, Step:   73200, Batch Loss:     7.451860, Lr: 0.000072, Tokens per sec:   2705
2023-03-14 15:51:00,630 - INFO - __main__ - Epoch  34, Step:   73300, Batch Loss:     8.129168, Lr: 0.000072, Tokens per sec:   2777
2023-03-14 15:51:20,610 - INFO - __main__ - Epoch  34, Step:   73400, Batch Loss:     8.974027, Lr: 0.000072, Tokens per sec:   2685
2023-03-14 15:51:39,889 - INFO - __main__ - Epoch  34, Step:   73500, Batch Loss:    11.372313, Lr: 0.000072, Tokens per sec:   2846
2023-03-14 15:51:59,293 - INFO - __main__ - Epoch  34, Step:   73600, Batch Loss:     7.427356, Lr: 0.000072, Tokens per sec:   2728
2023-03-14 15:52:17,706 - INFO - __main__ - Epoch  34, Step:   73700, Batch Loss:     7.900928, Lr: 0.000072, Tokens per sec:   2930
2023-03-14 15:52:37,492 - INFO - __main__ - Epoch  34, Step:   73800, Batch Loss:     8.715708, Lr: 0.000072, Tokens per sec:   2720
2023-03-14 15:52:55,757 - INFO - __main__ - Epoch  34, Step:   73900, Batch Loss:    13.055802, Lr: 0.000072, Tokens per sec:   2950
2023-03-14 15:53:13,775 - INFO - __main__ - Epoch  34, Step:   74000, Batch Loss:    11.231038, Lr: 0.000072, Tokens per sec:   3003
2023-03-14 15:53:29,333 - INFO - __main__ - Epoch  34: total training loss 20342.00
2023-03-14 15:53:29,334 - INFO - __main__ - Epoch 35
2023-03-14 15:53:32,445 - INFO - __main__ - Epoch  35, Step:   74100, Batch Loss:    10.199564, Lr: 0.000071, Tokens per sec:   2474
2023-03-14 15:53:52,436 - INFO - __main__ - Epoch  35, Step:   74200, Batch Loss:     8.404443, Lr: 0.000071, Tokens per sec:   2740
2023-03-14 15:54:12,402 - INFO - __main__ - Epoch  35, Step:   74300, Batch Loss:     6.716854, Lr: 0.000071, Tokens per sec:   2711
2023-03-14 15:54:32,365 - INFO - __main__ - Epoch  35, Step:   74400, Batch Loss:     7.665120, Lr: 0.000071, Tokens per sec:   2649
2023-03-14 15:54:51,998 - INFO - __main__ - Epoch  35, Step:   74500, Batch Loss:    10.292974, Lr: 0.000071, Tokens per sec:   2763
2023-03-14 15:55:11,973 - INFO - __main__ - Epoch  35, Step:   74600, Batch Loss:     7.257185, Lr: 0.000071, Tokens per sec:   2709
2023-03-14 15:55:31,948 - INFO - __main__ - Epoch  35, Step:   74700, Batch Loss:     9.221735, Lr: 0.000071, Tokens per sec:   2673
2023-03-14 15:55:51,928 - INFO - __main__ - Epoch  35, Step:   74800, Batch Loss:     8.419636, Lr: 0.000071, Tokens per sec:   2670
2023-03-14 15:56:10,304 - INFO - __main__ - Epoch  35, Step:   74900, Batch Loss:     9.245154, Lr: 0.000071, Tokens per sec:   2975
2023-03-14 15:56:28,404 - INFO - __main__ - Epoch  35, Step:   75000, Batch Loss:     8.489366, Lr: 0.000071, Tokens per sec:   2963
2023-03-14 15:56:47,959 - INFO - __main__ - Epoch  35, Step:   75100, Batch Loss:     9.890455, Lr: 0.000071, Tokens per sec:   2783
2023-03-14 15:57:07,915 - INFO - __main__ - Epoch  35, Step:   75200, Batch Loss:     8.722778, Lr: 0.000071, Tokens per sec:   2684
2023-03-14 15:57:27,891 - INFO - __main__ - Epoch  35, Step:   75300, Batch Loss:     9.609488, Lr: 0.000071, Tokens per sec:   2704
2023-03-14 15:57:47,914 - INFO - __main__ - Epoch  35, Step:   75400, Batch Loss:     9.423983, Lr: 0.000071, Tokens per sec:   2699
2023-03-14 15:58:07,835 - INFO - __main__ - Epoch  35, Step:   75500, Batch Loss:    10.324533, Lr: 0.000071, Tokens per sec:   2696
2023-03-14 15:58:26,230 - INFO - __main__ - Epoch  35, Step:   75600, Batch Loss:    12.665571, Lr: 0.000071, Tokens per sec:   2940
2023-03-14 15:58:44,831 - INFO - __main__ - Epoch  35, Step:   75700, Batch Loss:     8.620000, Lr: 0.000071, Tokens per sec:   2901
2023-03-14 15:59:03,196 - INFO - __main__ - Epoch  35, Step:   75800, Batch Loss:    12.194053, Lr: 0.000071, Tokens per sec:   2910
2023-03-14 15:59:22,943 - INFO - __main__ - Epoch  35, Step:   75900, Batch Loss:    13.027003, Lr: 0.000071, Tokens per sec:   2688
2023-03-14 15:59:42,906 - INFO - __main__ - Epoch  35, Step:   76000, Batch Loss:     9.419991, Lr: 0.000071, Tokens per sec:   2692
2023-03-14 16:00:02,862 - INFO - __main__ - Epoch  35, Step:   76100, Batch Loss:    10.276243, Lr: 0.000071, Tokens per sec:   2679
2023-03-14 16:00:22,712 - INFO - __main__ - Epoch  35, Step:   76200, Batch Loss:    12.583609, Lr: 0.000071, Tokens per sec:   2689
2023-03-14 16:00:35,669 - INFO - __main__ - Epoch  35: total training loss 19455.23
2023-03-14 16:00:35,670 - INFO - __main__ - Epoch 36
2023-03-14 16:00:42,250 - INFO - __main__ - Epoch  36, Step:   76300, Batch Loss:     8.574912, Lr: 0.000070, Tokens per sec:   2981
2023-03-14 16:01:00,203 - INFO - __main__ - Epoch  36, Step:   76400, Batch Loss:     8.755082, Lr: 0.000070, Tokens per sec:   2982
2023-03-14 16:01:19,614 - INFO - __main__ - Epoch  36, Step:   76500, Batch Loss:    10.462924, Lr: 0.000070, Tokens per sec:   2815
2023-03-14 16:01:39,209 - INFO - __main__ - Epoch  36, Step:   76600, Batch Loss:     8.356760, Lr: 0.000070, Tokens per sec:   2775
2023-03-14 16:01:57,528 - INFO - __main__ - Epoch  36, Step:   76700, Batch Loss:     7.581816, Lr: 0.000070, Tokens per sec:   2918
2023-03-14 16:02:17,575 - INFO - __main__ - Epoch  36, Step:   76800, Batch Loss:    11.818633, Lr: 0.000070, Tokens per sec:   2721
2023-03-14 16:02:37,505 - INFO - __main__ - Epoch  36, Step:   76900, Batch Loss:     6.798961, Lr: 0.000070, Tokens per sec:   2673
2023-03-14 16:02:57,373 - INFO - __main__ - Epoch  36, Step:   77000, Batch Loss:     7.463047, Lr: 0.000070, Tokens per sec:   2747
2023-03-14 16:03:17,456 - INFO - __main__ - Epoch  36, Step:   77100, Batch Loss:     4.839792, Lr: 0.000070, Tokens per sec:   2696
2023-03-14 16:03:37,194 - INFO - __main__ - Epoch  36, Step:   77200, Batch Loss:     9.730887, Lr: 0.000070, Tokens per sec:   2690
2023-03-14 16:03:56,848 - INFO - __main__ - Epoch  36, Step:   77300, Batch Loss:     7.635911, Lr: 0.000070, Tokens per sec:   2740
2023-03-14 16:04:16,622 - INFO - __main__ - Epoch  36, Step:   77400, Batch Loss:    11.320808, Lr: 0.000070, Tokens per sec:   2717
2023-03-14 16:04:36,512 - INFO - __main__ - Epoch  36, Step:   77500, Batch Loss:     9.052146, Lr: 0.000070, Tokens per sec:   2656
2023-03-14 16:04:55,774 - INFO - __main__ - Epoch  36, Step:   77600, Batch Loss:     8.800757, Lr: 0.000070, Tokens per sec:   2795
2023-03-14 16:05:15,463 - INFO - __main__ - Epoch  36, Step:   77700, Batch Loss:    11.477130, Lr: 0.000070, Tokens per sec:   2712
2023-03-14 16:05:35,301 - INFO - __main__ - Epoch  36, Step:   77800, Batch Loss:     5.765125, Lr: 0.000070, Tokens per sec:   2708
2023-03-14 16:05:55,127 - INFO - __main__ - Epoch  36, Step:   77900, Batch Loss:     7.727639, Lr: 0.000070, Tokens per sec:   2710
2023-03-14 16:06:15,088 - INFO - __main__ - Epoch  36, Step:   78000, Batch Loss:     7.051738, Lr: 0.000070, Tokens per sec:   2698
2023-03-14 16:06:35,074 - INFO - __main__ - Epoch  36, Step:   78100, Batch Loss:     9.195430, Lr: 0.000070, Tokens per sec:   2705
2023-03-14 16:06:55,055 - INFO - __main__ - Epoch  36, Step:   78200, Batch Loss:     6.953838, Lr: 0.000070, Tokens per sec:   2664
2023-03-14 16:07:15,027 - INFO - __main__ - Epoch  36, Step:   78300, Batch Loss:     9.668607, Lr: 0.000070, Tokens per sec:   2707
2023-03-14 16:07:35,061 - INFO - __main__ - Epoch  36, Step:   78400, Batch Loss:     9.953768, Lr: 0.000070, Tokens per sec:   2690
2023-03-14 16:07:44,103 - INFO - __main__ - Epoch  36: total training loss 18710.18
2023-03-14 16:07:44,104 - INFO - __main__ - Epoch 37
2023-03-14 16:07:55,585 - INFO - __main__ - Epoch  37, Step:   78500, Batch Loss:     9.817473, Lr: 0.000070, Tokens per sec:   2666
2023-03-14 16:08:15,550 - INFO - __main__ - Epoch  37, Step:   78600, Batch Loss:     6.831439, Lr: 0.000070, Tokens per sec:   2652
2023-03-14 16:08:35,141 - INFO - __main__ - Epoch  37, Step:   78700, Batch Loss:     7.478402, Lr: 0.000070, Tokens per sec:   2730
2023-03-14 16:08:54,209 - INFO - __main__ - Epoch  37, Step:   78800, Batch Loss:     8.295169, Lr: 0.000070, Tokens per sec:   2824
2023-03-14 16:09:13,842 - INFO - __main__ - Epoch  37, Step:   78900, Batch Loss:     8.214173, Lr: 0.000070, Tokens per sec:   2724
2023-03-14 16:09:33,889 - INFO - __main__ - Epoch  37, Step:   79000, Batch Loss:     9.336585, Lr: 0.000070, Tokens per sec:   2712
2023-03-14 16:09:52,017 - INFO - __main__ - Epoch  37, Step:   79100, Batch Loss:     6.666894, Lr: 0.000070, Tokens per sec:   2921
2023-03-14 16:10:11,297 - INFO - __main__ - Epoch  37, Step:   79200, Batch Loss:     6.972890, Lr: 0.000070, Tokens per sec:   2791
2023-03-14 16:10:31,080 - INFO - __main__ - Epoch  37, Step:   79300, Batch Loss:     8.174792, Lr: 0.000070, Tokens per sec:   2721
2023-03-14 16:10:51,060 - INFO - __main__ - Epoch  37, Step:   79400, Batch Loss:     6.368223, Lr: 0.000070, Tokens per sec:   2713
2023-03-14 16:11:10,954 - INFO - __main__ - Epoch  37, Step:   79500, Batch Loss:    10.948170, Lr: 0.000070, Tokens per sec:   2668
2023-03-14 16:11:30,908 - INFO - __main__ - Epoch  37, Step:   79600, Batch Loss:    11.467516, Lr: 0.000070, Tokens per sec:   2690
2023-03-14 16:11:50,884 - INFO - __main__ - Epoch  37, Step:   79700, Batch Loss:     7.178519, Lr: 0.000070, Tokens per sec:   2710
2023-03-14 16:12:10,795 - INFO - __main__ - Epoch  37, Step:   79800, Batch Loss:     7.895945, Lr: 0.000070, Tokens per sec:   2703
2023-03-14 16:12:30,768 - INFO - __main__ - Epoch  37, Step:   79900, Batch Loss:     9.231628, Lr: 0.000070, Tokens per sec:   2672
2023-03-14 16:12:50,791 - INFO - __main__ - Epoch  37, Step:   80000, Batch Loss:     6.447976, Lr: 0.000070, Tokens per sec:   2702
2023-03-14 16:13:10,701 - INFO - __main__ - Epoch  37, Step:   80100, Batch Loss:     7.835780, Lr: 0.000070, Tokens per sec:   2760
2023-03-14 16:13:30,649 - INFO - __main__ - Epoch  37, Step:   80200, Batch Loss:     7.254412, Lr: 0.000070, Tokens per sec:   2726
2023-03-14 16:13:50,607 - INFO - __main__ - Epoch  37, Step:   80300, Batch Loss:     7.718185, Lr: 0.000070, Tokens per sec:   2739
2023-03-14 16:14:10,548 - INFO - __main__ - Epoch  37, Step:   80400, Batch Loss:     7.581264, Lr: 0.000070, Tokens per sec:   2673
2023-03-14 16:14:30,373 - INFO - __main__ - Epoch  37, Step:   80500, Batch Loss:     8.582833, Lr: 0.000070, Tokens per sec:   2740
2023-03-14 16:14:50,387 - INFO - __main__ - Epoch  37, Step:   80600, Batch Loss:     8.266025, Lr: 0.000070, Tokens per sec:   2701
2023-03-14 16:14:55,016 - INFO - __main__ - Epoch  37: total training loss 17998.83
2023-03-14 16:14:55,017 - INFO - __main__ - Epoch 38
2023-03-14 16:15:10,697 - INFO - __main__ - Epoch  38, Step:   80700, Batch Loss:     7.564509, Lr: 0.000069, Tokens per sec:   2640
2023-03-14 16:15:30,504 - INFO - __main__ - Epoch  38, Step:   80800, Batch Loss:     5.605730, Lr: 0.000069, Tokens per sec:   2704
2023-03-14 16:15:50,407 - INFO - __main__ - Epoch  38, Step:   80900, Batch Loss:     6.273124, Lr: 0.000069, Tokens per sec:   2709
2023-03-14 16:16:10,345 - INFO - __main__ - Epoch  38, Step:   81000, Batch Loss:     7.387761, Lr: 0.000069, Tokens per sec:   2684
2023-03-14 16:16:30,318 - INFO - __main__ - Epoch  38, Step:   81100, Batch Loss:     7.731820, Lr: 0.000069, Tokens per sec:   2686
2023-03-14 16:16:50,413 - INFO - __main__ - Epoch  38, Step:   81200, Batch Loss:     6.982307, Lr: 0.000069, Tokens per sec:   2702
2023-03-14 16:17:10,309 - INFO - __main__ - Epoch  38, Step:   81300, Batch Loss:    10.008101, Lr: 0.000069, Tokens per sec:   2702
2023-03-14 16:17:28,694 - INFO - __main__ - Epoch  38, Step:   81400, Batch Loss:     8.258660, Lr: 0.000069, Tokens per sec:   2965
2023-03-14 16:17:46,658 - INFO - __main__ - Epoch  38, Step:   81500, Batch Loss:     6.959838, Lr: 0.000069, Tokens per sec:   3010
2023-03-14 16:18:04,633 - INFO - __main__ - Epoch  38, Step:   81600, Batch Loss:     7.946424, Lr: 0.000069, Tokens per sec:   3007
2023-03-14 16:18:22,581 - INFO - __main__ - Epoch  38, Step:   81700, Batch Loss:     7.471869, Lr: 0.000069, Tokens per sec:   2949
2023-03-14 16:18:41,773 - INFO - __main__ - Epoch  38, Step:   81800, Batch Loss:     8.898836, Lr: 0.000069, Tokens per sec:   2811
2023-03-14 16:19:01,663 - INFO - __main__ - Epoch  38, Step:   81900, Batch Loss:     7.081811, Lr: 0.000069, Tokens per sec:   2673
2023-03-14 16:19:20,591 - INFO - __main__ - Epoch  38, Step:   82000, Batch Loss:     6.058037, Lr: 0.000069, Tokens per sec:   2804
2023-03-14 16:19:40,618 - INFO - __main__ - Epoch  38, Step:   82100, Batch Loss:     8.567902, Lr: 0.000069, Tokens per sec:   2694
2023-03-14 16:20:00,803 - INFO - __main__ - Epoch  38, Step:   82200, Batch Loss:     9.852290, Lr: 0.000069, Tokens per sec:   2676
2023-03-14 16:20:20,643 - INFO - __main__ - Epoch  38, Step:   82300, Batch Loss:     6.863595, Lr: 0.000069, Tokens per sec:   2748
2023-03-14 16:20:40,518 - INFO - __main__ - Epoch  38, Step:   82400, Batch Loss:     9.560250, Lr: 0.000069, Tokens per sec:   2721
2023-03-14 16:21:00,449 - INFO - __main__ - Epoch  38, Step:   82500, Batch Loss:     5.997841, Lr: 0.000069, Tokens per sec:   2719
2023-03-14 16:21:20,294 - INFO - __main__ - Epoch  38, Step:   82600, Batch Loss:    11.198703, Lr: 0.000069, Tokens per sec:   2673
2023-03-14 16:21:40,375 - INFO - __main__ - Epoch  38, Step:   82700, Batch Loss:     8.967834, Lr: 0.000069, Tokens per sec:   2683
2023-03-14 16:22:00,385 - INFO - __main__ - Epoch  38, Step:   82800, Batch Loss:     8.433909, Lr: 0.000069, Tokens per sec:   2719
2023-03-14 16:22:00,845 - INFO - __main__ - Epoch  38: total training loss 17335.46
2023-03-14 16:22:00,846 - INFO - __main__ - Epoch 39
2023-03-14 16:22:20,449 - INFO - __main__ - Epoch  39, Step:   82900, Batch Loss:     7.190710, Lr: 0.000068, Tokens per sec:   2656
2023-03-14 16:22:40,102 - INFO - __main__ - Epoch  39, Step:   83000, Batch Loss:     6.460227, Lr: 0.000068, Tokens per sec:   2729
2023-03-14 16:23:00,199 - INFO - __main__ - Epoch  39, Step:   83100, Batch Loss:     5.862234, Lr: 0.000068, Tokens per sec:   2717
2023-03-14 16:23:20,095 - INFO - __main__ - Epoch  39, Step:   83200, Batch Loss:     5.759600, Lr: 0.000068, Tokens per sec:   2701
2023-03-14 16:23:39,998 - INFO - __main__ - Epoch  39, Step:   83300, Batch Loss:     7.399722, Lr: 0.000068, Tokens per sec:   2741
2023-03-14 16:23:59,524 - INFO - __main__ - Epoch  39, Step:   83400, Batch Loss:     8.261779, Lr: 0.000068, Tokens per sec:   2794
2023-03-14 16:24:19,501 - INFO - __main__ - Epoch  39, Step:   83500, Batch Loss:     7.136516, Lr: 0.000068, Tokens per sec:   2662
2023-03-14 16:24:39,365 - INFO - __main__ - Epoch  39, Step:   83600, Batch Loss:     7.106809, Lr: 0.000068, Tokens per sec:   2718
2023-03-14 16:24:59,438 - INFO - __main__ - Epoch  39, Step:   83700, Batch Loss:     9.696608, Lr: 0.000068, Tokens per sec:   2681
2023-03-14 16:25:19,373 - INFO - __main__ - Epoch  39, Step:   83800, Batch Loss:     8.432521, Lr: 0.000068, Tokens per sec:   2702
2023-03-14 16:25:39,665 - INFO - __main__ - Epoch  39, Step:   83900, Batch Loss:     9.807935, Lr: 0.000068, Tokens per sec:   2681
2023-03-14 16:25:58,331 - INFO - __main__ - Epoch  39, Step:   84000, Batch Loss:     8.264609, Lr: 0.000068, Tokens per sec:   2910
2023-03-14 16:26:17,435 - INFO - __main__ - Epoch  39, Step:   84100, Batch Loss:     8.721292, Lr: 0.000068, Tokens per sec:   2818
2023-03-14 16:26:36,766 - INFO - __main__ - Epoch  39, Step:   84200, Batch Loss:     6.097676, Lr: 0.000068, Tokens per sec:   2772
2023-03-14 16:26:56,061 - INFO - __main__ - Epoch  39, Step:   84300, Batch Loss:     8.694947, Lr: 0.000068, Tokens per sec:   2742
2023-03-14 16:27:15,366 - INFO - __main__ - Epoch  39, Step:   84400, Batch Loss:    10.860610, Lr: 0.000068, Tokens per sec:   2801
2023-03-14 16:27:35,367 - INFO - __main__ - Epoch  39, Step:   84500, Batch Loss:     6.896593, Lr: 0.000068, Tokens per sec:   2677
2023-03-14 16:27:55,365 - INFO - __main__ - Epoch  39, Step:   84600, Batch Loss:     7.324100, Lr: 0.000068, Tokens per sec:   2664
2023-03-14 16:28:18,170 - INFO - __main__ - Epoch  39, Step:   84700, Batch Loss:     8.880054, Lr: 0.000068, Tokens per sec:   2377
2023-03-14 16:28:56,103 - INFO - __main__ - Epoch  39, Step:   84800, Batch Loss:     8.763266, Lr: 0.000068, Tokens per sec:   1436
2023-03-14 16:29:34,270 - INFO - __main__ - Epoch  39, Step:   84900, Batch Loss:     8.809496, Lr: 0.000068, Tokens per sec:   1390
2023-03-14 16:30:04,871 - INFO - __main__ - Epoch  39: total training loss 16641.19
2023-03-14 16:30:04,873 - INFO - __main__ - Epoch 40
2023-03-14 16:30:12,445 - INFO - __main__ - Epoch  40, Step:   85000, Batch Loss:     5.877746, Lr: 0.000068, Tokens per sec:   1327
2023-03-14 16:30:50,131 - INFO - __main__ - Epoch  40, Step:   85100, Batch Loss:     5.653106, Lr: 0.000068, Tokens per sec:   1412
2023-03-14 16:31:28,278 - INFO - __main__ - Epoch  40, Step:   85200, Batch Loss:     7.538390, Lr: 0.000068, Tokens per sec:   1403
2023-03-14 16:32:06,448 - INFO - __main__ - Epoch  40, Step:   85300, Batch Loss:     6.073606, Lr: 0.000068, Tokens per sec:   1401
2023-03-14 16:32:45,434 - INFO - __main__ - Epoch  40, Step:   85400, Batch Loss:     5.822048, Lr: 0.000068, Tokens per sec:   1379
2023-03-14 16:33:23,577 - INFO - __main__ - Epoch  40, Step:   85500, Batch Loss:     5.029067, Lr: 0.000068, Tokens per sec:   1405
2023-03-14 16:34:00,969 - INFO - __main__ - Epoch  40, Step:   85600, Batch Loss:     5.782157, Lr: 0.000068, Tokens per sec:   1431
2023-03-14 16:34:39,496 - INFO - __main__ - Epoch  40, Step:   85700, Batch Loss:     7.355165, Lr: 0.000068, Tokens per sec:   1406
2023-03-14 16:35:18,433 - INFO - __main__ - Epoch  40, Step:   85800, Batch Loss:    11.574928, Lr: 0.000068, Tokens per sec:   1401
2023-03-14 16:35:57,247 - INFO - __main__ - Epoch  40, Step:   85900, Batch Loss:     6.161040, Lr: 0.000068, Tokens per sec:   1378
2023-03-14 16:36:35,939 - INFO - __main__ - Epoch  40, Step:   86000, Batch Loss:     8.004729, Lr: 0.000068, Tokens per sec:   1395
2023-03-14 16:37:14,305 - INFO - __main__ - Epoch  40, Step:   86100, Batch Loss:     8.659120, Lr: 0.000068, Tokens per sec:   1427
2023-03-14 16:37:53,226 - INFO - __main__ - Epoch  40, Step:   86200, Batch Loss:     8.839289, Lr: 0.000068, Tokens per sec:   1384
2023-03-14 16:38:31,599 - INFO - __main__ - Epoch  40, Step:   86300, Batch Loss:     5.394560, Lr: 0.000068, Tokens per sec:   1411
2023-03-14 16:39:09,494 - INFO - __main__ - Epoch  40, Step:   86400, Batch Loss:     8.582998, Lr: 0.000068, Tokens per sec:   1406
2023-03-14 16:39:47,885 - INFO - __main__ - Epoch  40, Step:   86500, Batch Loss:     6.979764, Lr: 0.000068, Tokens per sec:   1400
2023-03-14 16:40:26,482 - INFO - __main__ - Epoch  40, Step:   86600, Batch Loss:     6.538890, Lr: 0.000068, Tokens per sec:   1384
2023-03-14 16:41:04,755 - INFO - __main__ - Epoch  40, Step:   86700, Batch Loss:     7.759203, Lr: 0.000068, Tokens per sec:   1401
2023-03-14 16:41:42,027 - INFO - __main__ - Epoch  40, Step:   86800, Batch Loss:     9.562844, Lr: 0.000068, Tokens per sec:   1480
2023-03-14 16:42:19,641 - INFO - __main__ - Epoch  40, Step:   86900, Batch Loss:     8.014635, Lr: 0.000068, Tokens per sec:   1440
2023-03-14 16:42:57,469 - INFO - __main__ - Epoch  40, Step:   87000, Batch Loss:     7.741146, Lr: 0.000068, Tokens per sec:   1411
2023-03-14 16:43:36,164 - INFO - __main__ - Epoch  40, Step:   87100, Batch Loss:     5.440026, Lr: 0.000068, Tokens per sec:   1376
2023-03-14 16:43:59,193 - INFO - __main__ - Epoch  40: total training loss 16042.36
2023-03-14 16:43:59,194 - INFO - __main__ - Epoch 41
2023-03-14 16:44:15,024 - INFO - __main__ - Epoch  41, Step:   87200, Batch Loss:     7.330123, Lr: 0.000067, Tokens per sec:   1348
2023-03-14 16:44:53,637 - INFO - __main__ - Epoch  41, Step:   87300, Batch Loss:     9.666029, Lr: 0.000067, Tokens per sec:   1400
2023-03-14 16:45:32,230 - INFO - __main__ - Epoch  41, Step:   87400, Batch Loss:     6.409248, Lr: 0.000067, Tokens per sec:   1407
2023-03-14 16:46:11,119 - INFO - __main__ - Epoch  41, Step:   87500, Batch Loss:     5.369496, Lr: 0.000067, Tokens per sec:   1373
2023-03-14 16:46:49,896 - INFO - __main__ - Epoch  41, Step:   87600, Batch Loss:     7.514711, Lr: 0.000067, Tokens per sec:   1412
2023-03-14 16:47:28,440 - INFO - __main__ - Epoch  41, Step:   87700, Batch Loss:     5.516596, Lr: 0.000067, Tokens per sec:   1373
2023-03-14 16:48:07,308 - INFO - __main__ - Epoch  41, Step:   87800, Batch Loss:     4.542404, Lr: 0.000067, Tokens per sec:   1411
2023-03-14 16:48:46,259 - INFO - __main__ - Epoch  41, Step:   87900, Batch Loss:     6.982227, Lr: 0.000067, Tokens per sec:   1382
2023-03-14 16:49:25,106 - INFO - __main__ - Epoch  41, Step:   88000, Batch Loss:     6.247238, Lr: 0.000067, Tokens per sec:   1382
2023-03-14 16:50:03,983 - INFO - __main__ - Epoch  41, Step:   88100, Batch Loss:     4.825018, Lr: 0.000067, Tokens per sec:   1364
2023-03-14 16:50:42,896 - INFO - __main__ - Epoch  41, Step:   88200, Batch Loss:     7.654668, Lr: 0.000067, Tokens per sec:   1381
2023-03-14 16:51:21,415 - INFO - __main__ - Epoch  41, Step:   88300, Batch Loss:     5.150945, Lr: 0.000067, Tokens per sec:   1369
2023-03-14 16:52:00,236 - INFO - __main__ - Epoch  41, Step:   88400, Batch Loss:     8.793905, Lr: 0.000067, Tokens per sec:   1395
2023-03-14 16:52:39,145 - INFO - __main__ - Epoch  41, Step:   88500, Batch Loss:     8.107450, Lr: 0.000067, Tokens per sec:   1372
2023-03-14 16:53:17,653 - INFO - __main__ - Epoch  41, Step:   88600, Batch Loss:     6.824697, Lr: 0.000067, Tokens per sec:   1396
2023-03-14 16:53:56,481 - INFO - __main__ - Epoch  41, Step:   88700, Batch Loss:     7.189545, Lr: 0.000067, Tokens per sec:   1398
2023-03-14 16:54:35,389 - INFO - __main__ - Epoch  41, Step:   88800, Batch Loss:     8.148650, Lr: 0.000067, Tokens per sec:   1371
2023-03-14 16:55:14,214 - INFO - __main__ - Epoch  41, Step:   88900, Batch Loss:     6.055656, Lr: 0.000067, Tokens per sec:   1414
2023-03-14 16:55:52,503 - INFO - __main__ - Epoch  41, Step:   89000, Batch Loss:     4.154623, Lr: 0.000067, Tokens per sec:   1405
2023-03-14 16:56:30,986 - INFO - __main__ - Epoch  41, Step:   89100, Batch Loss:     4.051504, Lr: 0.000067, Tokens per sec:   1417
2023-03-14 16:57:09,496 - INFO - __main__ - Epoch  41, Step:   89200, Batch Loss:     7.762928, Lr: 0.000067, Tokens per sec:   1394
2023-03-14 16:57:48,104 - INFO - __main__ - Epoch  41, Step:   89300, Batch Loss:     7.774679, Lr: 0.000067, Tokens per sec:   1391
2023-03-14 16:58:03,215 - INFO - __main__ - Epoch  41: total training loss 15527.67
2023-03-14 16:58:03,215 - INFO - __main__ - Epoch 42
2023-03-14 16:58:26,745 - INFO - __main__ - Epoch  42, Step:   89400, Batch Loss:     4.773730, Lr: 0.000066, Tokens per sec:   1371
2023-03-14 16:59:04,605 - INFO - __main__ - Epoch  42, Step:   89500, Batch Loss:     3.523341, Lr: 0.000066, Tokens per sec:   1408
2023-03-14 16:59:43,326 - INFO - __main__ - Epoch  42, Step:   89600, Batch Loss:     7.354678, Lr: 0.000066, Tokens per sec:   1374
2023-03-14 17:00:22,126 - INFO - __main__ - Epoch  42, Step:   89700, Batch Loss:     5.636277, Lr: 0.000066, Tokens per sec:   1389
2023-03-14 17:01:00,386 - INFO - __main__ - Epoch  42, Step:   89800, Batch Loss:     8.774744, Lr: 0.000066, Tokens per sec:   1402
2023-03-14 17:01:38,888 - INFO - __main__ - Epoch  42, Step:   89900, Batch Loss:     6.562383, Lr: 0.000066, Tokens per sec:   1422
2023-03-14 17:02:17,907 - INFO - __main__ - Epoch  42, Step:   90000, Batch Loss:     5.061244, Lr: 0.000066, Tokens per sec:   1394
2023-03-14 17:02:56,491 - INFO - __main__ - Epoch  42, Step:   90100, Batch Loss:     6.935502, Lr: 0.000066, Tokens per sec:   1392
2023-03-14 17:03:35,521 - INFO - __main__ - Epoch  42, Step:   90200, Batch Loss:     5.698292, Lr: 0.000066, Tokens per sec:   1357
2023-03-14 17:04:14,506 - INFO - __main__ - Epoch  42, Step:   90300, Batch Loss:     4.303995, Lr: 0.000066, Tokens per sec:   1412
2023-03-14 17:04:53,180 - INFO - __main__ - Epoch  42, Step:   90400, Batch Loss:     5.067544, Lr: 0.000066, Tokens per sec:   1373
2023-03-14 17:05:31,958 - INFO - __main__ - Epoch  42, Step:   90500, Batch Loss:     6.736373, Lr: 0.000066, Tokens per sec:   1386
2023-03-14 17:06:10,825 - INFO - __main__ - Epoch  42, Step:   90600, Batch Loss:     8.423961, Lr: 0.000066, Tokens per sec:   1372
2023-03-14 17:06:49,685 - INFO - __main__ - Epoch  42, Step:   90700, Batch Loss:     4.638312, Lr: 0.000066, Tokens per sec:   1386
2023-03-14 17:07:14,608 - INFO - __main__ - Epoch  42, Step:   90800, Batch Loss:     7.408062, Lr: 0.000066, Tokens per sec:   2166
2023-03-14 17:07:40,134 - INFO - __main__ - Epoch  42, Step:   90900, Batch Loss:     7.350770, Lr: 0.000066, Tokens per sec:   2100
2023-03-14 17:08:05,441 - INFO - __main__ - Epoch  42, Step:   91000, Batch Loss:     5.820355, Lr: 0.000066, Tokens per sec:   2120
2023-03-14 17:08:30,486 - INFO - __main__ - Epoch  42, Step:   91100, Batch Loss:     7.435333, Lr: 0.000066, Tokens per sec:   2184
2023-03-14 17:08:55,814 - INFO - __main__ - Epoch  42, Step:   91200, Batch Loss:     8.024485, Lr: 0.000066, Tokens per sec:   2115
2023-03-14 17:09:21,070 - INFO - __main__ - Epoch  42, Step:   91300, Batch Loss:     6.829958, Lr: 0.000066, Tokens per sec:   2113
2023-03-14 17:09:46,572 - INFO - __main__ - Epoch  42, Step:   91400, Batch Loss:     9.914330, Lr: 0.000066, Tokens per sec:   2118
2023-03-14 17:10:12,630 - INFO - __main__ - Epoch  42, Step:   91500, Batch Loss:     7.947228, Lr: 0.000066, Tokens per sec:   2107
2023-03-14 17:10:17,306 - INFO - __main__ - Epoch  42: total training loss 14951.92
2023-03-14 17:10:17,307 - INFO - __main__ - Epoch 43
2023-03-14 17:10:37,813 - INFO - __main__ - Epoch  43, Step:   91600, Batch Loss:     6.484328, Lr: 0.000066, Tokens per sec:   2160
2023-03-14 17:10:57,343 - INFO - __main__ - Epoch  43, Step:   91700, Batch Loss:     4.696743, Lr: 0.000066, Tokens per sec:   2749
2023-03-14 17:11:17,368 - INFO - __main__ - Epoch  43, Step:   91800, Batch Loss:     3.724202, Lr: 0.000066, Tokens per sec:   2658
2023-03-14 17:11:36,592 - INFO - __main__ - Epoch  43, Step:   91900, Batch Loss:     4.661572, Lr: 0.000066, Tokens per sec:   2813
2023-03-14 17:11:56,420 - INFO - __main__ - Epoch  43, Step:   92000, Batch Loss:     6.800566, Lr: 0.000066, Tokens per sec:   2766
2023-03-14 17:12:16,470 - INFO - __main__ - Epoch  43, Step:   92100, Batch Loss:     3.006049, Lr: 0.000066, Tokens per sec:   2704
2023-03-14 17:12:36,213 - INFO - __main__ - Epoch  43, Step:   92200, Batch Loss:     6.457980, Lr: 0.000066, Tokens per sec:   2737
2023-03-14 17:12:54,316 - INFO - __main__ - Epoch  43, Step:   92300, Batch Loss:     6.395579, Lr: 0.000066, Tokens per sec:   2957
2023-03-14 17:13:12,370 - INFO - __main__ - Epoch  43, Step:   92400, Batch Loss:     6.391222, Lr: 0.000066, Tokens per sec:   2963
2023-03-14 17:13:31,170 - INFO - __main__ - Epoch  43, Step:   92500, Batch Loss:     8.458271, Lr: 0.000066, Tokens per sec:   2897
2023-03-14 17:13:51,196 - INFO - __main__ - Epoch  43, Step:   92600, Batch Loss:     7.062677, Lr: 0.000066, Tokens per sec:   2673
2023-03-14 17:14:10,007 - INFO - __main__ - Epoch  43, Step:   92700, Batch Loss:     5.703121, Lr: 0.000066, Tokens per sec:   2853
2023-03-14 17:14:28,486 - INFO - __main__ - Epoch  43, Step:   92800, Batch Loss:     6.159975, Lr: 0.000066, Tokens per sec:   2914
2023-03-14 17:14:46,653 - INFO - __main__ - Epoch  43, Step:   92900, Batch Loss:     5.310225, Lr: 0.000066, Tokens per sec:   3017
2023-03-14 17:15:06,628 - INFO - __main__ - Epoch  43, Step:   93000, Batch Loss:     5.502108, Lr: 0.000066, Tokens per sec:   2689
2023-03-14 17:15:26,572 - INFO - __main__ - Epoch  43, Step:   93100, Batch Loss:     8.457945, Lr: 0.000066, Tokens per sec:   2701
2023-03-14 17:15:46,566 - INFO - __main__ - Epoch  43, Step:   93200, Batch Loss:     7.973628, Lr: 0.000066, Tokens per sec:   2669
2023-03-14 17:16:05,719 - INFO - __main__ - Epoch  43, Step:   93300, Batch Loss:     5.943552, Lr: 0.000066, Tokens per sec:   2782
2023-03-14 17:16:24,253 - INFO - __main__ - Epoch  43, Step:   93400, Batch Loss:     6.367379, Lr: 0.000066, Tokens per sec:   2864
2023-03-14 17:16:44,321 - INFO - __main__ - Epoch  43, Step:   93500, Batch Loss:     8.964986, Lr: 0.000066, Tokens per sec:   2701
2023-03-14 17:17:03,036 - INFO - __main__ - Epoch  43, Step:   93600, Batch Loss:     5.542556, Lr: 0.000066, Tokens per sec:   2802
2023-03-14 17:17:22,477 - INFO - __main__ - Epoch  43: total training loss 14442.59
2023-03-14 17:17:22,478 - INFO - __main__ - Epoch 44
2023-03-14 17:17:23,370 - INFO - __main__ - Epoch  44, Step:   93700, Batch Loss:     5.779143, Lr: 0.000065, Tokens per sec:   1652
2023-03-14 17:17:43,288 - INFO - __main__ - Epoch  44, Step:   93800, Batch Loss:     5.763282, Lr: 0.000065, Tokens per sec:   2726
2023-03-14 17:18:03,319 - INFO - __main__ - Epoch  44, Step:   93900, Batch Loss:     4.704289, Lr: 0.000065, Tokens per sec:   2710
2023-03-14 17:18:23,397 - INFO - __main__ - Epoch  44, Step:   94000, Batch Loss:     4.278822, Lr: 0.000065, Tokens per sec:   2627
2023-03-14 17:18:43,292 - INFO - __main__ - Epoch  44, Step:   94100, Batch Loss:     5.409585, Lr: 0.000065, Tokens per sec:   2674
2023-03-14 17:19:02,838 - INFO - __main__ - Epoch  44, Step:   94200, Batch Loss:     4.532162, Lr: 0.000065, Tokens per sec:   2709
2023-03-14 17:19:22,350 - INFO - __main__ - Epoch  44, Step:   94300, Batch Loss:     8.010386, Lr: 0.000065, Tokens per sec:   2772
2023-03-14 17:19:42,151 - INFO - __main__ - Epoch  44, Step:   94400, Batch Loss:     6.563019, Lr: 0.000065, Tokens per sec:   2696
2023-03-14 17:20:02,334 - INFO - __main__ - Epoch  44, Step:   94500, Batch Loss:    10.421580, Lr: 0.000065, Tokens per sec:   2665
2023-03-14 17:20:22,139 - INFO - __main__ - Epoch  44, Step:   94600, Batch Loss:     6.713377, Lr: 0.000065, Tokens per sec:   2714
2023-03-14 17:20:42,185 - INFO - __main__ - Epoch  44, Step:   94700, Batch Loss:     3.788161, Lr: 0.000065, Tokens per sec:   2692
2023-03-14 17:21:02,406 - INFO - __main__ - Epoch  44, Step:   94800, Batch Loss:     8.546882, Lr: 0.000065, Tokens per sec:   2685
2023-03-14 17:21:22,010 - INFO - __main__ - Epoch  44, Step:   94900, Batch Loss:     9.641721, Lr: 0.000065, Tokens per sec:   2802
2023-03-14 17:21:41,251 - INFO - __main__ - Epoch  44, Step:   95000, Batch Loss:     7.939177, Lr: 0.000065, Tokens per sec:   2827
2023-03-14 17:22:01,246 - INFO - __main__ - Epoch  44, Step:   95100, Batch Loss:     6.559685, Lr: 0.000065, Tokens per sec:   2732
2023-03-14 17:22:21,075 - INFO - __main__ - Epoch  44, Step:   95200, Batch Loss:     6.707579, Lr: 0.000065, Tokens per sec:   2716
2023-03-14 17:22:40,185 - INFO - __main__ - Epoch  44, Step:   95300, Batch Loss:     7.752335, Lr: 0.000065, Tokens per sec:   2830
2023-03-14 17:22:59,569 - INFO - __main__ - Epoch  44, Step:   95400, Batch Loss:     6.508237, Lr: 0.000065, Tokens per sec:   2781
2023-03-14 17:23:19,076 - INFO - __main__ - Epoch  44, Step:   95500, Batch Loss:     8.588468, Lr: 0.000065, Tokens per sec:   2723
2023-03-14 17:23:38,184 - INFO - __main__ - Epoch  44, Step:   95600, Batch Loss:     8.435364, Lr: 0.000065, Tokens per sec:   2785
2023-03-14 17:23:56,351 - INFO - __main__ - Epoch  44, Step:   95700, Batch Loss:     6.836567, Lr: 0.000065, Tokens per sec:   2973
2023-03-14 17:24:15,036 - INFO - __main__ - Epoch  44, Step:   95800, Batch Loss:     6.390410, Lr: 0.000065, Tokens per sec:   2837
2023-03-14 17:24:29,543 - INFO - __main__ - Epoch  44: total training loss 13991.04
2023-03-14 17:24:29,544 - INFO - __main__ - Epoch 45
2023-03-14 17:24:34,692 - INFO - __main__ - Epoch  45, Step:   95900, Batch Loss:     4.833192, Lr: 0.000064, Tokens per sec:   2435
2023-03-14 17:24:54,614 - INFO - __main__ - Epoch  45, Step:   96000, Batch Loss:     5.542012, Lr: 0.000064, Tokens per sec:   2686
2023-03-14 17:25:14,295 - INFO - __main__ - Epoch  45, Step:   96100, Batch Loss:     5.663125, Lr: 0.000064, Tokens per sec:   2719
2023-03-14 17:25:33,299 - INFO - __main__ - Epoch  45, Step:   96200, Batch Loss:     7.327596, Lr: 0.000064, Tokens per sec:   2850
2023-03-14 17:25:52,392 - INFO - __main__ - Epoch  45, Step:   96300, Batch Loss:     4.542261, Lr: 0.000064, Tokens per sec:   2804
2023-03-14 17:26:12,385 - INFO - __main__ - Epoch  45, Step:   96400, Batch Loss:     6.709734, Lr: 0.000064, Tokens per sec:   2702
2023-03-14 17:26:30,805 - INFO - __main__ - Epoch  45, Step:   96500, Batch Loss:     5.867654, Lr: 0.000064, Tokens per sec:   2881
2023-03-14 17:26:49,839 - INFO - __main__ - Epoch  45, Step:   96600, Batch Loss:     5.413083, Lr: 0.000064, Tokens per sec:   2820
2023-03-14 17:27:09,483 - INFO - __main__ - Epoch  45, Step:   96700, Batch Loss:     6.214035, Lr: 0.000064, Tokens per sec:   2742
2023-03-14 17:27:29,397 - INFO - __main__ - Epoch  45, Step:   96800, Batch Loss:     6.209866, Lr: 0.000064, Tokens per sec:   2736
2023-03-14 17:27:49,352 - INFO - __main__ - Epoch  45, Step:   96900, Batch Loss:     5.254293, Lr: 0.000064, Tokens per sec:   2727
2023-03-14 17:28:09,426 - INFO - __main__ - Epoch  45, Step:   97000, Batch Loss:     5.976786, Lr: 0.000064, Tokens per sec:   2638
2023-03-14 17:28:29,035 - INFO - __main__ - Epoch  45, Step:   97100, Batch Loss:     6.723022, Lr: 0.000064, Tokens per sec:   2738
2023-03-14 17:28:49,050 - INFO - __main__ - Epoch  45, Step:   97200, Batch Loss:     6.087369, Lr: 0.000064, Tokens per sec:   2756
2023-03-14 17:29:08,926 - INFO - __main__ - Epoch  45, Step:   97300, Batch Loss:     5.773632, Lr: 0.000064, Tokens per sec:   2705
2023-03-14 17:29:29,071 - INFO - __main__ - Epoch  45, Step:   97400, Batch Loss:     6.638781, Lr: 0.000064, Tokens per sec:   2670
2023-03-14 17:29:49,194 - INFO - __main__ - Epoch  45, Step:   97500, Batch Loss:     4.670276, Lr: 0.000064, Tokens per sec:   2654
2023-03-14 17:30:09,170 - INFO - __main__ - Epoch  45, Step:   97600, Batch Loss:     5.777172, Lr: 0.000064, Tokens per sec:   2658
2023-03-14 17:30:29,406 - INFO - __main__ - Epoch  45, Step:   97700, Batch Loss:     6.115260, Lr: 0.000064, Tokens per sec:   2657
2023-03-14 17:30:49,592 - INFO - __main__ - Epoch  45, Step:   97800, Batch Loss:     9.321646, Lr: 0.000064, Tokens per sec:   2653
2023-03-14 17:31:09,522 - INFO - __main__ - Epoch  45, Step:   97900, Batch Loss:    10.454755, Lr: 0.000064, Tokens per sec:   2768
2023-03-14 17:31:29,493 - INFO - __main__ - Epoch  45, Step:   98000, Batch Loss:     6.306913, Lr: 0.000064, Tokens per sec:   2729
2023-03-14 17:31:40,389 - INFO - __main__ - Epoch  45: total training loss 13563.69
2023-03-14 17:31:40,390 - INFO - __main__ - Epoch 46
2023-03-14 17:31:49,704 - INFO - __main__ - Epoch  46, Step:   98100, Batch Loss:     7.417291, Lr: 0.000064, Tokens per sec:   2591
2023-03-14 17:32:09,952 - INFO - __main__ - Epoch  46, Step:   98200, Batch Loss:     5.588838, Lr: 0.000064, Tokens per sec:   2666
2023-03-14 17:32:30,293 - INFO - __main__ - Epoch  46, Step:   98300, Batch Loss:     4.263325, Lr: 0.000064, Tokens per sec:   2664
2023-03-14 17:32:50,017 - INFO - __main__ - Epoch  46, Step:   98400, Batch Loss:     6.873406, Lr: 0.000064, Tokens per sec:   2728
2023-03-14 17:33:10,297 - INFO - __main__ - Epoch  46, Step:   98500, Batch Loss:     5.588778, Lr: 0.000064, Tokens per sec:   2681
2023-03-14 17:33:29,707 - INFO - __main__ - Epoch  46, Step:   98600, Batch Loss:     6.119633, Lr: 0.000064, Tokens per sec:   2765
2023-03-14 17:33:49,641 - INFO - __main__ - Epoch  46, Step:   98700, Batch Loss:     6.281933, Lr: 0.000064, Tokens per sec:   2705
2023-03-14 17:34:10,125 - INFO - __main__ - Epoch  46, Step:   98800, Batch Loss:     5.147784, Lr: 0.000064, Tokens per sec:   2672
2023-03-14 17:34:30,175 - INFO - __main__ - Epoch  46, Step:   98900, Batch Loss:     6.821708, Lr: 0.000064, Tokens per sec:   2632
2023-03-14 17:34:50,088 - INFO - __main__ - Epoch  46, Step:   99000, Batch Loss:     7.637138, Lr: 0.000064, Tokens per sec:   2672
2023-03-14 17:35:10,417 - INFO - __main__ - Epoch  46, Step:   99100, Batch Loss:     5.327681, Lr: 0.000064, Tokens per sec:   2647
2023-03-14 17:35:30,753 - INFO - __main__ - Epoch  46, Step:   99200, Batch Loss:     6.176116, Lr: 0.000064, Tokens per sec:   2708
2023-03-14 17:35:50,410 - INFO - __main__ - Epoch  46, Step:   99300, Batch Loss:     4.130744, Lr: 0.000064, Tokens per sec:   2707
2023-03-14 17:36:10,538 - INFO - __main__ - Epoch  46, Step:   99400, Batch Loss:     9.477509, Lr: 0.000064, Tokens per sec:   2704
2023-03-14 17:36:30,444 - INFO - __main__ - Epoch  46, Step:   99500, Batch Loss:     6.424160, Lr: 0.000064, Tokens per sec:   2687
2023-03-14 17:36:50,475 - INFO - __main__ - Epoch  46, Step:   99600, Batch Loss:     6.037623, Lr: 0.000064, Tokens per sec:   2693
2023-03-14 17:37:10,569 - INFO - __main__ - Epoch  46, Step:   99700, Batch Loss:     6.258523, Lr: 0.000064, Tokens per sec:   2699
2023-03-14 17:37:29,075 - INFO - __main__ - Epoch  46, Step:   99800, Batch Loss:     6.888685, Lr: 0.000064, Tokens per sec:   2897
2023-03-14 17:37:48,205 - INFO - __main__ - Epoch  46, Step:   99900, Batch Loss:     6.705280, Lr: 0.000064, Tokens per sec:   2807
2023-03-14 17:38:07,765 - INFO - __main__ - Epoch  46, Step:  100000, Batch Loss:     7.545896, Lr: 0.000064, Tokens per sec:   2742
2023-03-14 17:38:27,858 - INFO - __main__ - Epoch  46, Step:  100100, Batch Loss:     5.522690, Lr: 0.000064, Tokens per sec:   2627
2023-03-14 17:38:46,809 - INFO - __main__ - Epoch  46, Step:  100200, Batch Loss:     7.319420, Lr: 0.000064, Tokens per sec:   2870
2023-03-14 17:38:53,426 - INFO - __main__ - Epoch  46: total training loss 13111.65
2023-03-14 17:38:53,427 - INFO - __main__ - Epoch 47
2023-03-14 17:39:07,063 - INFO - __main__ - Epoch  47, Step:  100300, Batch Loss:     8.001116, Lr: 0.000063, Tokens per sec:   2583
2023-03-14 17:39:26,628 - INFO - __main__ - Epoch  47, Step:  100400, Batch Loss:     6.506646, Lr: 0.000063, Tokens per sec:   2741
2023-03-14 17:39:45,341 - INFO - __main__ - Epoch  47, Step:  100500, Batch Loss:     4.374651, Lr: 0.000063, Tokens per sec:   2909
2023-03-14 17:40:05,366 - INFO - __main__ - Epoch  47, Step:  100600, Batch Loss:     4.177195, Lr: 0.000063, Tokens per sec:   2697
2023-03-14 17:40:25,677 - INFO - __main__ - Epoch  47, Step:  100700, Batch Loss:     8.507102, Lr: 0.000063, Tokens per sec:   2657
2023-03-14 17:40:46,132 - INFO - __main__ - Epoch  47, Step:  100800, Batch Loss:     4.378224, Lr: 0.000063, Tokens per sec:   2644
2023-03-14 17:41:04,665 - INFO - __main__ - Epoch  47, Step:  100900, Batch Loss:     6.637976, Lr: 0.000063, Tokens per sec:   2912
2023-03-14 17:41:23,155 - INFO - __main__ - Epoch  47, Step:  101000, Batch Loss:     4.984400, Lr: 0.000063, Tokens per sec:   2867
2023-03-14 17:41:41,894 - INFO - __main__ - Epoch  47, Step:  101100, Batch Loss:     4.750035, Lr: 0.000063, Tokens per sec:   2886
2023-03-14 17:42:00,210 - INFO - __main__ - Epoch  47, Step:  101200, Batch Loss:     5.678417, Lr: 0.000063, Tokens per sec:   2983
2023-03-14 17:42:18,307 - INFO - __main__ - Epoch  47, Step:  101300, Batch Loss:     7.047367, Lr: 0.000063, Tokens per sec:   2934
2023-03-14 17:42:37,327 - INFO - __main__ - Epoch  47, Step:  101400, Batch Loss:     7.191595, Lr: 0.000063, Tokens per sec:   2839
2023-03-14 17:42:56,181 - INFO - __main__ - Epoch  47, Step:  101500, Batch Loss:     4.801420, Lr: 0.000063, Tokens per sec:   2870
2023-03-14 17:43:15,342 - INFO - __main__ - Epoch  47, Step:  101600, Batch Loss:     6.007596, Lr: 0.000063, Tokens per sec:   2798
2023-03-14 17:43:34,575 - INFO - __main__ - Epoch  47, Step:  101700, Batch Loss:     8.508241, Lr: 0.000063, Tokens per sec:   2755
2023-03-14 17:43:54,486 - INFO - __main__ - Epoch  47, Step:  101800, Batch Loss:     4.892429, Lr: 0.000063, Tokens per sec:   2727
2023-03-14 17:44:14,549 - INFO - __main__ - Epoch  47, Step:  101900, Batch Loss:     4.897953, Lr: 0.000063, Tokens per sec:   2645
2023-03-14 17:44:33,872 - INFO - __main__ - Epoch  47, Step:  102000, Batch Loss:     6.769386, Lr: 0.000063, Tokens per sec:   2765
2023-03-14 17:44:53,443 - INFO - __main__ - Epoch  47, Step:  102100, Batch Loss:     7.359344, Lr: 0.000063, Tokens per sec:   2785
2023-03-14 17:45:13,768 - INFO - __main__ - Epoch  47, Step:  102200, Batch Loss:     7.650125, Lr: 0.000063, Tokens per sec:   2613
2023-03-14 17:45:33,194 - INFO - __main__ - Epoch  47, Step:  102300, Batch Loss:     3.003575, Lr: 0.000063, Tokens per sec:   2781
2023-03-14 17:45:53,107 - INFO - __main__ - Epoch  47, Step:  102400, Batch Loss:     6.396728, Lr: 0.000063, Tokens per sec:   2741
2023-03-14 17:45:55,722 - INFO - __main__ - Epoch  47: total training loss 12734.39
2023-03-14 17:45:55,723 - INFO - __main__ - Epoch 48
2023-03-14 17:46:13,721 - INFO - __main__ - Epoch  48, Step:  102500, Batch Loss:     5.556934, Lr: 0.000062, Tokens per sec:   2589
2023-03-14 17:46:32,493 - INFO - __main__ - Epoch  48, Step:  102600, Batch Loss:     5.602223, Lr: 0.000062, Tokens per sec:   2872
2023-03-14 17:46:52,093 - INFO - __main__ - Epoch  48, Step:  102700, Batch Loss:     4.247425, Lr: 0.000062, Tokens per sec:   2746
2023-03-14 17:47:11,994 - INFO - __main__ - Epoch  48, Step:  102800, Batch Loss:     7.168518, Lr: 0.000062, Tokens per sec:   2700
2023-03-14 17:47:31,516 - INFO - __main__ - Epoch  48, Step:  102900, Batch Loss:     6.672457, Lr: 0.000062, Tokens per sec:   2764
2023-03-14 17:47:51,029 - INFO - __main__ - Epoch  48, Step:  103000, Batch Loss:     3.739610, Lr: 0.000062, Tokens per sec:   2776
2023-03-14 17:48:09,702 - INFO - __main__ - Epoch  48, Step:  103100, Batch Loss:     4.184055, Lr: 0.000062, Tokens per sec:   2854
2023-03-14 17:48:28,320 - INFO - __main__ - Epoch  48, Step:  103200, Batch Loss:     3.861139, Lr: 0.000062, Tokens per sec:   2890
2023-03-14 17:48:46,412 - INFO - __main__ - Epoch  48, Step:  103300, Batch Loss:     6.128127, Lr: 0.000062, Tokens per sec:   2962
2023-03-14 17:49:05,284 - INFO - __main__ - Epoch  48, Step:  103400, Batch Loss:     4.835348, Lr: 0.000062, Tokens per sec:   2855
2023-03-14 17:49:25,243 - INFO - __main__ - Epoch  48, Step:  103500, Batch Loss:     5.811333, Lr: 0.000062, Tokens per sec:   2687
2023-03-14 17:49:43,986 - INFO - __main__ - Epoch  48, Step:  103600, Batch Loss:     7.248297, Lr: 0.000062, Tokens per sec:   2844
2023-03-14 17:50:03,962 - INFO - __main__ - Epoch  48, Step:  103700, Batch Loss:     5.467486, Lr: 0.000062, Tokens per sec:   2743
2023-03-14 17:50:22,465 - INFO - __main__ - Epoch  48, Step:  103800, Batch Loss:     6.611370, Lr: 0.000062, Tokens per sec:   2917
2023-03-14 17:50:41,558 - INFO - __main__ - Epoch  48, Step:  103900, Batch Loss:     6.567973, Lr: 0.000062, Tokens per sec:   2787
2023-03-14 17:51:01,190 - INFO - __main__ - Epoch  48, Step:  104000, Batch Loss:     5.293867, Lr: 0.000062, Tokens per sec:   2723
2023-03-14 17:51:21,142 - INFO - __main__ - Epoch  48, Step:  104100, Batch Loss:     6.337869, Lr: 0.000062, Tokens per sec:   2703
2023-03-14 17:51:39,930 - INFO - __main__ - Epoch  48, Step:  104200, Batch Loss:     4.909438, Lr: 0.000062, Tokens per sec:   2892
2023-03-14 17:51:59,570 - INFO - __main__ - Epoch  48, Step:  104300, Batch Loss:     5.321956, Lr: 0.000062, Tokens per sec:   2709
2023-03-14 17:52:17,698 - INFO - __main__ - Epoch  48, Step:  104400, Batch Loss:     6.871757, Lr: 0.000062, Tokens per sec:   2992
2023-03-14 17:52:36,831 - INFO - __main__ - Epoch  48, Step:  104500, Batch Loss:     8.743396, Lr: 0.000062, Tokens per sec:   2863
2023-03-14 17:52:54,554 - INFO - __main__ - Epoch  48: total training loss 12307.39
2023-03-14 17:52:54,555 - INFO - __main__ - Epoch 49
2023-03-14 17:52:56,487 - INFO - __main__ - Epoch  49, Step:  104600, Batch Loss:     5.297787, Lr: 0.000062, Tokens per sec:   2201
2023-03-14 17:53:14,843 - INFO - __main__ - Epoch  49, Step:  104700, Batch Loss:     4.616414, Lr: 0.000062, Tokens per sec:   2914
2023-03-14 17:53:33,033 - INFO - __main__ - Epoch  49, Step:  104800, Batch Loss:     4.610992, Lr: 0.000062, Tokens per sec:   2971
2023-03-14 17:53:51,963 - INFO - __main__ - Epoch  49, Step:  104900, Batch Loss:     6.741687, Lr: 0.000062, Tokens per sec:   2862
2023-03-14 17:54:10,770 - INFO - __main__ - Epoch  49, Step:  105000, Batch Loss:     6.933004, Lr: 0.000062, Tokens per sec:   2886
2023-03-14 17:54:28,884 - INFO - __main__ - Epoch  49, Step:  105100, Batch Loss:     4.997850, Lr: 0.000062, Tokens per sec:   2953
2023-03-14 17:54:47,292 - INFO - __main__ - Epoch  49, Step:  105200, Batch Loss:     6.078650, Lr: 0.000062, Tokens per sec:   2889
2023-03-14 17:55:05,586 - INFO - __main__ - Epoch  49, Step:  105300, Batch Loss:     3.366554, Lr: 0.000062, Tokens per sec:   2958
2023-03-14 17:55:23,664 - INFO - __main__ - Epoch  49, Step:  105400, Batch Loss:     5.082084, Lr: 0.000062, Tokens per sec:   3011
2023-03-14 17:55:42,698 - INFO - __main__ - Epoch  49, Step:  105500, Batch Loss:     4.686675, Lr: 0.000062, Tokens per sec:   2819
2023-03-14 17:56:01,957 - INFO - __main__ - Epoch  49, Step:  105600, Batch Loss:     4.312148, Lr: 0.000062, Tokens per sec:   2797
2023-03-14 17:56:20,181 - INFO - __main__ - Epoch  49, Step:  105700, Batch Loss:     6.633202, Lr: 0.000062, Tokens per sec:   2976
2023-03-14 17:56:39,259 - INFO - __main__ - Epoch  49, Step:  105800, Batch Loss:     6.591824, Lr: 0.000062, Tokens per sec:   2842
2023-03-14 17:56:57,311 - INFO - __main__ - Epoch  49, Step:  105900, Batch Loss:     4.734901, Lr: 0.000062, Tokens per sec:   3017
2023-03-14 17:57:17,233 - INFO - __main__ - Epoch  49, Step:  106000, Batch Loss:     6.803332, Lr: 0.000062, Tokens per sec:   2669
2023-03-14 17:57:37,201 - INFO - __main__ - Epoch  49, Step:  106100, Batch Loss:     4.344467, Lr: 0.000062, Tokens per sec:   2699
2023-03-14 17:57:55,613 - INFO - __main__ - Epoch  49, Step:  106200, Batch Loss:     5.602364, Lr: 0.000062, Tokens per sec:   2933
2023-03-14 17:58:13,798 - INFO - __main__ - Epoch  49, Step:  106300, Batch Loss:     4.619535, Lr: 0.000062, Tokens per sec:   2964
2023-03-14 17:58:33,546 - INFO - __main__ - Epoch  49, Step:  106400, Batch Loss:     4.178463, Lr: 0.000062, Tokens per sec:   2720
2023-03-14 17:58:53,518 - INFO - __main__ - Epoch  49, Step:  106500, Batch Loss:     6.693073, Lr: 0.000062, Tokens per sec:   2673
2023-03-14 17:59:13,619 - INFO - __main__ - Epoch  49, Step:  106600, Batch Loss:     7.901751, Lr: 0.000062, Tokens per sec:   2719
2023-03-14 17:59:32,453 - INFO - __main__ - Epoch  49, Step:  106700, Batch Loss:     5.941434, Lr: 0.000062, Tokens per sec:   2770
2023-03-14 17:59:45,326 - INFO - __main__ - Epoch  49: total training loss 11973.35
2023-03-14 17:59:45,327 - INFO - __main__ - Epoch 50
2023-03-14 17:59:51,396 - INFO - __main__ - Epoch  50, Step:  106800, Batch Loss:     4.755976, Lr: 0.000061, Tokens per sec:   2610
2023-03-14 18:00:11,373 - INFO - __main__ - Epoch  50, Step:  106900, Batch Loss:     5.358062, Lr: 0.000061, Tokens per sec:   2699
2023-03-14 18:00:29,958 - INFO - __main__ - Epoch  50, Step:  107000, Batch Loss:     5.812368, Lr: 0.000061, Tokens per sec:   2971
2023-03-14 18:00:49,103 - INFO - __main__ - Epoch  50, Step:  107100, Batch Loss:     4.417374, Lr: 0.000061, Tokens per sec:   2818
2023-03-14 18:01:08,726 - INFO - __main__ - Epoch  50, Step:  107200, Batch Loss:     5.478970, Lr: 0.000061, Tokens per sec:   2723
2023-03-14 18:01:28,449 - INFO - __main__ - Epoch  50, Step:  107300, Batch Loss:     4.360626, Lr: 0.000061, Tokens per sec:   2713
2023-03-14 18:01:47,563 - INFO - __main__ - Epoch  50, Step:  107400, Batch Loss:     3.668826, Lr: 0.000061, Tokens per sec:   2795
2023-03-14 18:02:07,035 - INFO - __main__ - Epoch  50, Step:  107500, Batch Loss:     4.274524, Lr: 0.000061, Tokens per sec:   2792
2023-03-14 18:02:26,860 - INFO - __main__ - Epoch  50, Step:  107600, Batch Loss:     5.030182, Lr: 0.000061, Tokens per sec:   2724
2023-03-14 18:02:46,765 - INFO - __main__ - Epoch  50, Step:  107700, Batch Loss:     5.839623, Lr: 0.000061, Tokens per sec:   2722
2023-03-14 18:03:06,706 - INFO - __main__ - Epoch  50, Step:  107800, Batch Loss:     4.169095, Lr: 0.000061, Tokens per sec:   2652
2023-03-14 18:03:26,135 - INFO - __main__ - Epoch  50, Step:  107900, Batch Loss:     5.460200, Lr: 0.000061, Tokens per sec:   2755
2023-03-14 18:03:46,070 - INFO - __main__ - Epoch  50, Step:  108000, Batch Loss:     6.121723, Lr: 0.000061, Tokens per sec:   2735
2023-03-14 18:04:05,999 - INFO - __main__ - Epoch  50, Step:  108100, Batch Loss:     5.658369, Lr: 0.000061, Tokens per sec:   2716
2023-03-14 18:04:26,048 - INFO - __main__ - Epoch  50, Step:  108200, Batch Loss:     3.044644, Lr: 0.000061, Tokens per sec:   2658
2023-03-14 18:04:45,563 - INFO - __main__ - Epoch  50, Step:  108300, Batch Loss:     3.674125, Lr: 0.000061, Tokens per sec:   2782
2023-03-14 18:05:05,161 - INFO - __main__ - Epoch  50, Step:  108400, Batch Loss:     5.922089, Lr: 0.000061, Tokens per sec:   2728
2023-03-14 18:05:24,995 - INFO - __main__ - Epoch  50, Step:  108500, Batch Loss:     5.390961, Lr: 0.000061, Tokens per sec:   2725
2023-03-14 18:05:45,011 - INFO - __main__ - Epoch  50, Step:  108600, Batch Loss:     4.946778, Lr: 0.000061, Tokens per sec:   2673
2023-03-14 18:06:04,943 - INFO - __main__ - Epoch  50, Step:  108700, Batch Loss:     4.676928, Lr: 0.000061, Tokens per sec:   2690
2023-03-14 18:06:24,956 - INFO - __main__ - Epoch  50, Step:  108800, Batch Loss:     7.793962, Lr: 0.000061, Tokens per sec:   2686
2023-03-14 18:06:44,591 - INFO - __main__ - Epoch  50, Step:  108900, Batch Loss:     3.833387, Lr: 0.000061, Tokens per sec:   2749
2023-03-14 18:06:54,559 - INFO - __main__ - Epoch  50: total training loss 11653.93
2023-03-14 18:06:54,560 - INFO - __main__ - Epoch 51
2023-03-14 18:07:03,990 - INFO - __main__ - Epoch  51, Step:  109000, Batch Loss:     6.550451, Lr: 0.000061, Tokens per sec:   2843
2023-03-14 18:07:23,014 - INFO - __main__ - Epoch  51, Step:  109100, Batch Loss:     4.050442, Lr: 0.000061, Tokens per sec:   2786
2023-03-14 18:07:41,301 - INFO - __main__ - Epoch  51, Step:  109200, Batch Loss:     5.269179, Lr: 0.000061, Tokens per sec:   2952
2023-03-14 18:07:59,484 - INFO - __main__ - Epoch  51, Step:  109300, Batch Loss:     5.124302, Lr: 0.000061, Tokens per sec:   2995
2023-03-14 18:08:17,677 - INFO - __main__ - Epoch  51, Step:  109400, Batch Loss:     5.243911, Lr: 0.000061, Tokens per sec:   2973
2023-03-14 18:08:37,545 - INFO - __main__ - Epoch  51, Step:  109500, Batch Loss:     6.572953, Lr: 0.000061, Tokens per sec:   2721
2023-03-14 18:08:57,646 - INFO - __main__ - Epoch  51, Step:  109600, Batch Loss:     4.421109, Lr: 0.000061, Tokens per sec:   2654
2023-03-14 18:09:16,543 - INFO - __main__ - Epoch  51, Step:  109700, Batch Loss:     4.569384, Lr: 0.000061, Tokens per sec:   2825
2023-03-14 18:09:34,686 - INFO - __main__ - Epoch  51, Step:  109800, Batch Loss:     3.779575, Lr: 0.000061, Tokens per sec:   2976
2023-03-14 18:09:53,307 - INFO - __main__ - Epoch  51, Step:  109900, Batch Loss:     3.876408, Lr: 0.000061, Tokens per sec:   2904
2023-03-14 18:10:13,098 - INFO - __main__ - Epoch  51, Step:  110000, Batch Loss:     4.693847, Lr: 0.000061, Tokens per sec:   2731
2023-03-14 18:10:33,093 - INFO - __main__ - Epoch  51, Step:  110100, Batch Loss:     6.111444, Lr: 0.000061, Tokens per sec:   2710
2023-03-14 18:10:51,724 - INFO - __main__ - Epoch  51, Step:  110200, Batch Loss:     3.979808, Lr: 0.000061, Tokens per sec:   2890
2023-03-14 18:11:09,864 - INFO - __main__ - Epoch  51, Step:  110300, Batch Loss:     4.331852, Lr: 0.000061, Tokens per sec:   3045
2023-03-14 18:11:29,076 - INFO - __main__ - Epoch  51, Step:  110400, Batch Loss:     6.300421, Lr: 0.000061, Tokens per sec:   2747
2023-03-14 18:11:49,062 - INFO - __main__ - Epoch  51, Step:  110500, Batch Loss:     6.366072, Lr: 0.000061, Tokens per sec:   2639
2023-03-14 18:12:08,905 - INFO - __main__ - Epoch  51, Step:  110600, Batch Loss:     6.204807, Lr: 0.000061, Tokens per sec:   2759
2023-03-14 18:12:27,061 - INFO - __main__ - Epoch  51, Step:  110700, Batch Loss:     4.704985, Lr: 0.000061, Tokens per sec:   2968
2023-03-14 18:12:45,211 - INFO - __main__ - Epoch  51, Step:  110800, Batch Loss:     7.127273, Lr: 0.000061, Tokens per sec:   2965
2023-03-14 18:13:04,398 - INFO - __main__ - Epoch  51, Step:  110900, Batch Loss:     4.542806, Lr: 0.000061, Tokens per sec:   2783
2023-03-14 18:13:23,887 - INFO - __main__ - Epoch  51, Step:  111000, Batch Loss:     7.088331, Lr: 0.000061, Tokens per sec:   2757
2023-03-14 18:13:43,504 - INFO - __main__ - Epoch  51, Step:  111100, Batch Loss:     6.713254, Lr: 0.000061, Tokens per sec:   2750
2023-03-14 18:13:49,326 - INFO - __main__ - Epoch  51: total training loss 11316.20
2023-03-14 18:13:49,327 - INFO - __main__ - Epoch 52
2023-03-14 18:14:03,840 - INFO - __main__ - Epoch  52, Step:  111200, Batch Loss:     3.831131, Lr: 0.000060, Tokens per sec:   2687
2023-03-14 18:14:23,133 - INFO - __main__ - Epoch  52, Step:  111300, Batch Loss:     4.057503, Lr: 0.000060, Tokens per sec:   2743
2023-03-14 18:14:43,118 - INFO - __main__ - Epoch  52, Step:  111400, Batch Loss:     4.339633, Lr: 0.000060, Tokens per sec:   2693
2023-03-14 18:15:02,962 - INFO - __main__ - Epoch  52, Step:  111500, Batch Loss:     5.426902, Lr: 0.000060, Tokens per sec:   2733
2023-03-14 18:15:22,956 - INFO - __main__ - Epoch  52, Step:  111600, Batch Loss:     3.748827, Lr: 0.000060, Tokens per sec:   2651
2023-03-14 18:15:42,854 - INFO - __main__ - Epoch  52, Step:  111700, Batch Loss:     6.694593, Lr: 0.000060, Tokens per sec:   2726
2023-03-14 18:16:01,583 - INFO - __main__ - Epoch  52, Step:  111800, Batch Loss:     3.109167, Lr: 0.000060, Tokens per sec:   2948
2023-03-14 18:16:20,029 - INFO - __main__ - Epoch  52, Step:  111900, Batch Loss:     3.368278, Lr: 0.000060, Tokens per sec:   2942
2023-03-14 18:16:39,734 - INFO - __main__ - Epoch  52, Step:  112000, Batch Loss:     5.903695, Lr: 0.000060, Tokens per sec:   2719
2023-03-14 18:16:59,825 - INFO - __main__ - Epoch  52, Step:  112100, Batch Loss:     5.099007, Lr: 0.000060, Tokens per sec:   2688
2023-03-14 18:17:19,625 - INFO - __main__ - Epoch  52, Step:  112200, Batch Loss:     5.281223, Lr: 0.000060, Tokens per sec:   2674
2023-03-14 18:17:39,370 - INFO - __main__ - Epoch  52, Step:  112300, Batch Loss:     5.596585, Lr: 0.000060, Tokens per sec:   2728
2023-03-14 18:17:59,424 - INFO - __main__ - Epoch  52, Step:  112400, Batch Loss:     5.114644, Lr: 0.000060, Tokens per sec:   2664
2023-03-14 18:18:18,397 - INFO - __main__ - Epoch  52, Step:  112500, Batch Loss:     5.074863, Lr: 0.000060, Tokens per sec:   2916
2023-03-14 18:18:36,774 - INFO - __main__ - Epoch  52, Step:  112600, Batch Loss:     7.218223, Lr: 0.000060, Tokens per sec:   2917
2023-03-14 18:18:55,462 - INFO - __main__ - Epoch  52, Step:  112700, Batch Loss:     4.402419, Lr: 0.000060, Tokens per sec:   2882
2023-03-14 18:19:15,398 - INFO - __main__ - Epoch  52, Step:  112800, Batch Loss:     3.513099, Lr: 0.000060, Tokens per sec:   2645
2023-03-14 18:19:35,358 - INFO - __main__ - Epoch  52, Step:  112900, Batch Loss:     4.394119, Lr: 0.000060, Tokens per sec:   2655
2023-03-14 18:19:55,243 - INFO - __main__ - Epoch  52, Step:  113000, Batch Loss:     8.477283, Lr: 0.000060, Tokens per sec:   2754
2023-03-14 18:20:15,197 - INFO - __main__ - Epoch  52, Step:  113100, Batch Loss:     5.173726, Lr: 0.000060, Tokens per sec:   2682
2023-03-14 18:20:35,154 - INFO - __main__ - Epoch  52, Step:  113200, Batch Loss:     5.228340, Lr: 0.000060, Tokens per sec:   2698
2023-03-14 18:20:55,059 - INFO - __main__ - Epoch  52, Step:  113300, Batch Loss:     5.119842, Lr: 0.000060, Tokens per sec:   2702
2023-03-14 18:20:56,679 - INFO - __main__ - Epoch  52: total training loss 11010.83
2023-03-14 18:20:56,680 - INFO - __main__ - Epoch 53
2023-03-14 18:21:15,337 - INFO - __main__ - Epoch  53, Step:  113400, Batch Loss:     4.020105, Lr: 0.000059, Tokens per sec:   2650
2023-03-14 18:21:35,267 - INFO - __main__ - Epoch  53, Step:  113500, Batch Loss:     5.192779, Lr: 0.000059, Tokens per sec:   2662
2023-03-14 18:21:55,146 - INFO - __main__ - Epoch  53, Step:  113600, Batch Loss:     5.814826, Lr: 0.000059, Tokens per sec:   2741
2023-03-14 18:22:14,236 - INFO - __main__ - Epoch  53, Step:  113700, Batch Loss:     4.228174, Lr: 0.000059, Tokens per sec:   2803
2023-03-14 18:22:32,349 - INFO - __main__ - Epoch  53, Step:  113800, Batch Loss:     4.994100, Lr: 0.000059, Tokens per sec:   2963
2023-03-14 18:22:51,432 - INFO - __main__ - Epoch  53, Step:  113900, Batch Loss:     4.294791, Lr: 0.000059, Tokens per sec:   2838
2023-03-14 18:23:09,400 - INFO - __main__ - Epoch  53, Step:  114000, Batch Loss:     4.799257, Lr: 0.000059, Tokens per sec:   3006
2023-03-14 18:23:27,396 - INFO - __main__ - Epoch  53, Step:  114100, Batch Loss:     5.892192, Lr: 0.000059, Tokens per sec:   3029
2023-03-14 18:23:46,885 - INFO - __main__ - Epoch  53, Step:  114200, Batch Loss:     5.809073, Lr: 0.000059, Tokens per sec:   2733
2023-03-14 18:24:06,795 - INFO - __main__ - Epoch  53, Step:  114300, Batch Loss:     5.529973, Lr: 0.000059, Tokens per sec:   2684
2023-03-14 18:24:26,717 - INFO - __main__ - Epoch  53, Step:  114400, Batch Loss:     4.789602, Lr: 0.000059, Tokens per sec:   2729
2023-03-14 18:24:46,629 - INFO - __main__ - Epoch  53, Step:  114500, Batch Loss:     3.360682, Lr: 0.000059, Tokens per sec:   2737
2023-03-14 18:25:06,586 - INFO - __main__ - Epoch  53, Step:  114600, Batch Loss:     4.302525, Lr: 0.000059, Tokens per sec:   2664
2023-03-14 18:25:26,556 - INFO - __main__ - Epoch  53, Step:  114700, Batch Loss:     3.776165, Lr: 0.000059, Tokens per sec:   2702
2023-03-14 18:25:46,512 - INFO - __main__ - Epoch  53, Step:  114800, Batch Loss:     4.784142, Lr: 0.000059, Tokens per sec:   2680
2023-03-14 18:26:06,441 - INFO - __main__ - Epoch  53, Step:  114900, Batch Loss:     4.571714, Lr: 0.000059, Tokens per sec:   2719
2023-03-14 18:26:26,374 - INFO - __main__ - Epoch  53, Step:  115000, Batch Loss:     5.552170, Lr: 0.000059, Tokens per sec:   2736
2023-03-14 18:26:46,330 - INFO - __main__ - Epoch  53, Step:  115100, Batch Loss:     5.959841, Lr: 0.000059, Tokens per sec:   2729
2023-03-14 18:27:06,247 - INFO - __main__ - Epoch  53, Step:  115200, Batch Loss:     4.676250, Lr: 0.000059, Tokens per sec:   2684
2023-03-14 18:27:25,803 - INFO - __main__ - Epoch  53, Step:  115300, Batch Loss:     3.895820, Lr: 0.000059, Tokens per sec:   2735
2023-03-14 18:27:45,733 - INFO - __main__ - Epoch  53, Step:  115400, Batch Loss:     3.208742, Lr: 0.000059, Tokens per sec:   2675
2023-03-14 18:28:03,149 - INFO - __main__ - Epoch  53: total training loss 10698.26
2023-03-14 18:28:03,151 - INFO - __main__ - Epoch 54
2023-03-14 18:28:05,992 - INFO - __main__ - Epoch  54, Step:  115500, Batch Loss:     5.266497, Lr: 0.000059, Tokens per sec:   2502
2023-03-14 18:28:25,618 - INFO - __main__ - Epoch  54, Step:  115600, Batch Loss:     4.154706, Lr: 0.000059, Tokens per sec:   2765
2023-03-14 18:28:44,846 - INFO - __main__ - Epoch  54, Step:  115700, Batch Loss:     3.410721, Lr: 0.000059, Tokens per sec:   2797
2023-03-14 18:29:04,864 - INFO - __main__ - Epoch  54, Step:  115800, Batch Loss:     4.081869, Lr: 0.000059, Tokens per sec:   2692
2023-03-14 18:29:24,808 - INFO - __main__ - Epoch  54, Step:  115900, Batch Loss:     3.912104, Lr: 0.000059, Tokens per sec:   2639
2023-03-14 18:29:43,859 - INFO - __main__ - Epoch  54, Step:  116000, Batch Loss:     4.084182, Lr: 0.000059, Tokens per sec:   2788
2023-03-14 18:30:03,692 - INFO - __main__ - Epoch  54, Step:  116100, Batch Loss:     4.538282, Lr: 0.000059, Tokens per sec:   2692
2023-03-14 18:30:23,370 - INFO - __main__ - Epoch  54, Step:  116200, Batch Loss:     3.260383, Lr: 0.000059, Tokens per sec:   2722
2023-03-14 18:30:43,117 - INFO - __main__ - Epoch  54, Step:  116300, Batch Loss:     6.736838, Lr: 0.000059, Tokens per sec:   2705
2023-03-14 18:31:03,086 - INFO - __main__ - Epoch  54, Step:  116400, Batch Loss:     7.107227, Lr: 0.000059, Tokens per sec:   2774
2023-03-14 18:31:21,144 - INFO - __main__ - Epoch  54, Step:  116500, Batch Loss:     6.000496, Lr: 0.000059, Tokens per sec:   2981
2023-03-14 18:31:40,055 - INFO - __main__ - Epoch  54, Step:  116600, Batch Loss:     3.524130, Lr: 0.000059, Tokens per sec:   2816
2023-03-14 18:32:00,029 - INFO - __main__ - Epoch  54, Step:  116700, Batch Loss:     4.553854, Lr: 0.000059, Tokens per sec:   2725
2023-03-14 18:32:19,985 - INFO - __main__ - Epoch  54, Step:  116800, Batch Loss:     4.796052, Lr: 0.000059, Tokens per sec:   2710
2023-03-14 18:32:39,896 - INFO - __main__ - Epoch  54, Step:  116900, Batch Loss:     4.226986, Lr: 0.000059, Tokens per sec:   2736
2023-03-14 18:32:59,832 - INFO - __main__ - Epoch  54, Step:  117000, Batch Loss:     5.458211, Lr: 0.000059, Tokens per sec:   2726
2023-03-14 18:33:19,782 - INFO - __main__ - Epoch  54, Step:  117100, Batch Loss:     4.193838, Lr: 0.000059, Tokens per sec:   2670
2023-03-14 18:33:39,643 - INFO - __main__ - Epoch  54, Step:  117200, Batch Loss:     5.054544, Lr: 0.000059, Tokens per sec:   2702
2023-03-14 18:33:59,556 - INFO - __main__ - Epoch  54, Step:  117300, Batch Loss:     6.395965, Lr: 0.000059, Tokens per sec:   2683
2023-03-14 18:34:19,325 - INFO - __main__ - Epoch  54, Step:  117400, Batch Loss:     3.883201, Lr: 0.000059, Tokens per sec:   2741
2023-03-14 18:34:39,289 - INFO - __main__ - Epoch  54, Step:  117500, Batch Loss:     6.104029, Lr: 0.000059, Tokens per sec:   2718
2023-03-14 18:34:59,137 - INFO - __main__ - Epoch  54, Step:  117600, Batch Loss:     5.914863, Lr: 0.000059, Tokens per sec:   2709
2023-03-14 18:35:10,996 - INFO - __main__ - Epoch  54: total training loss 10439.63
2023-03-14 18:35:10,997 - INFO - __main__ - Epoch 55
2023-03-14 18:35:17,978 - INFO - __main__ - Epoch  55, Step:  117700, Batch Loss:     7.432269, Lr: 0.000058, Tokens per sec:   2662
2023-03-14 18:35:37,403 - INFO - __main__ - Epoch  55, Step:  117800, Batch Loss:     5.437652, Lr: 0.000058, Tokens per sec:   2744
2023-03-14 18:35:57,417 - INFO - __main__ - Epoch  55, Step:  117900, Batch Loss:     3.136718, Lr: 0.000058, Tokens per sec:   2683
2023-03-14 18:36:17,312 - INFO - __main__ - Epoch  55, Step:  118000, Batch Loss:     4.147609, Lr: 0.000058, Tokens per sec:   2690
2023-03-14 18:36:37,260 - INFO - __main__ - Epoch  55, Step:  118100, Batch Loss:     3.872334, Lr: 0.000058, Tokens per sec:   2726
2023-03-14 18:36:57,224 - INFO - __main__ - Epoch  55, Step:  118200, Batch Loss:     5.518564, Lr: 0.000058, Tokens per sec:   2673
2023-03-14 18:37:17,228 - INFO - __main__ - Epoch  55, Step:  118300, Batch Loss:     4.461786, Lr: 0.000058, Tokens per sec:   2703
2023-03-14 18:37:37,197 - INFO - __main__ - Epoch  55, Step:  118400, Batch Loss:     3.922456, Lr: 0.000058, Tokens per sec:   2700
2023-03-14 18:37:57,179 - INFO - __main__ - Epoch  55, Step:  118500, Batch Loss:     5.898568, Lr: 0.000058, Tokens per sec:   2682
2023-03-14 18:38:17,108 - INFO - __main__ - Epoch  55, Step:  118600, Batch Loss:     3.469946, Lr: 0.000058, Tokens per sec:   2735
2023-03-14 18:38:37,039 - INFO - __main__ - Epoch  55, Step:  118700, Batch Loss:     2.651537, Lr: 0.000058, Tokens per sec:   2749
2023-03-14 18:38:56,976 - INFO - __main__ - Epoch  55, Step:  118800, Batch Loss:     5.664287, Lr: 0.000058, Tokens per sec:   2706
2023-03-14 18:39:16,892 - INFO - __main__ - Epoch  55, Step:  118900, Batch Loss:     4.271502, Lr: 0.000058, Tokens per sec:   2721
2023-03-14 18:39:36,826 - INFO - __main__ - Epoch  55, Step:  119000, Batch Loss:     2.711994, Lr: 0.000058, Tokens per sec:   2701
2023-03-14 18:39:56,456 - INFO - __main__ - Epoch  55, Step:  119100, Batch Loss:     4.534339, Lr: 0.000058, Tokens per sec:   2742
2023-03-14 18:40:14,394 - INFO - __main__ - Epoch  55, Step:  119200, Batch Loss:     5.079831, Lr: 0.000058, Tokens per sec:   2977
2023-03-14 18:40:32,380 - INFO - __main__ - Epoch  55, Step:  119300, Batch Loss:     3.913683, Lr: 0.000058, Tokens per sec:   2990
2023-03-14 18:40:51,742 - INFO - __main__ - Epoch  55, Step:  119400, Batch Loss:     5.970901, Lr: 0.000058, Tokens per sec:   2747
2023-03-14 18:41:11,694 - INFO - __main__ - Epoch  55, Step:  119500, Batch Loss:     3.458207, Lr: 0.000058, Tokens per sec:   2688
2023-03-14 18:41:31,728 - INFO - __main__ - Epoch  55, Step:  119600, Batch Loss:     3.996414, Lr: 0.000058, Tokens per sec:   2671
2023-03-14 18:41:51,220 - INFO - __main__ - Epoch  55, Step:  119700, Batch Loss:     5.508621, Lr: 0.000058, Tokens per sec:   2771
2023-03-14 18:42:11,346 - INFO - __main__ - Epoch  55, Step:  119800, Batch Loss:     5.271214, Lr: 0.000058, Tokens per sec:   2666
2023-03-14 18:42:20,420 - INFO - __main__ - Epoch  55: total training loss 10126.25
2023-03-14 18:42:20,421 - INFO - __main__ - Epoch 56
2023-03-14 18:42:31,613 - INFO - __main__ - Epoch  56, Step:  119900, Batch Loss:     5.357204, Lr: 0.000058, Tokens per sec:   2666
2023-03-14 18:42:50,408 - INFO - __main__ - Epoch  56, Step:  120000, Batch Loss:     4.510003, Lr: 0.000058, Tokens per sec:   2868
2023-03-14 18:43:09,572 - INFO - __main__ - Epoch  56, Step:  120100, Batch Loss:     4.804579, Lr: 0.000058, Tokens per sec:   2844
2023-03-14 18:43:29,460 - INFO - __main__ - Epoch  56, Step:  120200, Batch Loss:     4.148123, Lr: 0.000058, Tokens per sec:   2723
2023-03-14 18:43:49,638 - INFO - __main__ - Epoch  56, Step:  120300, Batch Loss:     5.158412, Lr: 0.000058, Tokens per sec:   2665
2023-03-14 18:44:09,563 - INFO - __main__ - Epoch  56, Step:  120400, Batch Loss:     5.163116, Lr: 0.000058, Tokens per sec:   2679
2023-03-14 18:44:29,597 - INFO - __main__ - Epoch  56, Step:  120500, Batch Loss:     3.825325, Lr: 0.000058, Tokens per sec:   2709
2023-03-14 18:44:49,893 - INFO - __main__ - Epoch  56, Step:  120600, Batch Loss:     4.303887, Lr: 0.000058, Tokens per sec:   2647
2023-03-14 18:45:10,205 - INFO - __main__ - Epoch  56, Step:  120700, Batch Loss:     3.917692, Lr: 0.000058, Tokens per sec:   2620
2023-03-14 18:45:29,943 - INFO - __main__ - Epoch  56, Step:  120800, Batch Loss:     3.501969, Lr: 0.000058, Tokens per sec:   2742
2023-03-14 18:45:49,927 - INFO - __main__ - Epoch  56, Step:  120900, Batch Loss:     4.914168, Lr: 0.000058, Tokens per sec:   2730
2023-03-14 18:46:10,228 - INFO - __main__ - Epoch  56, Step:  121000, Batch Loss:     3.075114, Lr: 0.000058, Tokens per sec:   2664
2023-03-14 18:46:30,180 - INFO - __main__ - Epoch  56, Step:  121100, Batch Loss:     5.357780, Lr: 0.000058, Tokens per sec:   2734
2023-03-14 18:46:50,215 - INFO - __main__ - Epoch  56, Step:  121200, Batch Loss:     5.096274, Lr: 0.000058, Tokens per sec:   2628
2023-03-14 18:47:10,237 - INFO - __main__ - Epoch  56, Step:  121300, Batch Loss:     4.715378, Lr: 0.000058, Tokens per sec:   2694
2023-03-14 18:47:30,308 - INFO - __main__ - Epoch  56, Step:  121400, Batch Loss:     4.804871, Lr: 0.000058, Tokens per sec:   2663
2023-03-14 18:47:50,394 - INFO - __main__ - Epoch  56, Step:  121500, Batch Loss:     6.341635, Lr: 0.000058, Tokens per sec:   2661
2023-03-14 18:48:10,585 - INFO - __main__ - Epoch  56, Step:  121600, Batch Loss:     4.341394, Lr: 0.000058, Tokens per sec:   2635
2023-03-14 18:48:30,546 - INFO - __main__ - Epoch  56, Step:  121700, Batch Loss:     4.633668, Lr: 0.000058, Tokens per sec:   2675
2023-03-14 18:48:50,762 - INFO - __main__ - Epoch  56, Step:  121800, Batch Loss:     3.093722, Lr: 0.000058, Tokens per sec:   2682
2023-03-14 18:49:10,795 - INFO - __main__ - Epoch  56, Step:  121900, Batch Loss:     3.965131, Lr: 0.000058, Tokens per sec:   2672
2023-03-14 18:49:30,817 - INFO - __main__ - Epoch  56, Step:  122000, Batch Loss:     6.676257, Lr: 0.000058, Tokens per sec:   2698
2023-03-14 18:49:35,622 - INFO - __main__ - Epoch  56: total training loss 9861.42
2023-03-14 18:49:35,623 - INFO - __main__ - Epoch 57
2023-03-14 18:49:51,276 - INFO - __main__ - Epoch  57, Step:  122100, Batch Loss:     6.056636, Lr: 0.000057, Tokens per sec:   2631
2023-03-14 18:50:11,434 - INFO - __main__ - Epoch  57, Step:  122200, Batch Loss:     6.108160, Lr: 0.000057, Tokens per sec:   2644
2023-03-14 18:50:31,552 - INFO - __main__ - Epoch  57, Step:  122300, Batch Loss:     4.046566, Lr: 0.000057, Tokens per sec:   2622
2023-03-14 18:50:51,765 - INFO - __main__ - Epoch  57, Step:  122400, Batch Loss:     4.706907, Lr: 0.000057, Tokens per sec:   2707
2023-03-14 18:51:11,084 - INFO - __main__ - Epoch  57, Step:  122500, Batch Loss:     4.619746, Lr: 0.000057, Tokens per sec:   2797
2023-03-14 18:51:30,258 - INFO - __main__ - Epoch  57, Step:  122600, Batch Loss:     4.859268, Lr: 0.000057, Tokens per sec:   2786
2023-03-14 18:51:49,732 - INFO - __main__ - Epoch  57, Step:  122700, Batch Loss:     4.033839, Lr: 0.000057, Tokens per sec:   2740
2023-03-14 18:52:09,207 - INFO - __main__ - Epoch  57, Step:  122800, Batch Loss:     3.719839, Lr: 0.000057, Tokens per sec:   2750
2023-03-14 18:52:28,635 - INFO - __main__ - Epoch  57, Step:  122900, Batch Loss:     4.135292, Lr: 0.000057, Tokens per sec:   2803
2023-03-14 18:52:47,871 - INFO - __main__ - Epoch  57, Step:  123000, Batch Loss:     4.766733, Lr: 0.000057, Tokens per sec:   2787
2023-03-14 18:53:07,287 - INFO - __main__ - Epoch  57, Step:  123100, Batch Loss:     4.712147, Lr: 0.000057, Tokens per sec:   2727
2023-03-14 18:53:26,673 - INFO - __main__ - Epoch  57, Step:  123200, Batch Loss:     5.543293, Lr: 0.000057, Tokens per sec:   2792
2023-03-14 18:53:45,755 - INFO - __main__ - Epoch  57, Step:  123300, Batch Loss:     4.415075, Lr: 0.000057, Tokens per sec:   2762
2023-03-14 18:54:05,284 - INFO - __main__ - Epoch  57, Step:  123400, Batch Loss:     4.183453, Lr: 0.000057, Tokens per sec:   2791
2023-03-14 18:54:24,615 - INFO - __main__ - Epoch  57, Step:  123500, Batch Loss:     4.924723, Lr: 0.000057, Tokens per sec:   2735
2023-03-14 18:54:44,829 - INFO - __main__ - Epoch  57, Step:  123600, Batch Loss:     4.071355, Lr: 0.000057, Tokens per sec:   2739
2023-03-14 18:55:04,854 - INFO - __main__ - Epoch  57, Step:  123700, Batch Loss:     4.413663, Lr: 0.000057, Tokens per sec:   2707
2023-03-14 18:55:24,538 - INFO - __main__ - Epoch  57, Step:  123800, Batch Loss:     4.847975, Lr: 0.000057, Tokens per sec:   2738
2023-03-14 18:55:43,098 - INFO - __main__ - Epoch  57, Step:  123900, Batch Loss:     5.503973, Lr: 0.000057, Tokens per sec:   2897
2023-03-14 18:56:01,360 - INFO - __main__ - Epoch  57, Step:  124000, Batch Loss:     5.065598, Lr: 0.000057, Tokens per sec:   2904
2023-03-14 18:56:19,509 - INFO - __main__ - Epoch  57, Step:  124100, Batch Loss:     3.837846, Lr: 0.000057, Tokens per sec:   3065
2023-03-14 18:56:38,048 - INFO - __main__ - Epoch  57, Step:  124200, Batch Loss:     5.969640, Lr: 0.000057, Tokens per sec:   2909
2023-03-14 18:56:38,759 - INFO - __main__ - Epoch  57: total training loss 9599.43
2023-03-14 18:56:38,760 - INFO - __main__ - Epoch 58
2023-03-14 18:56:58,341 - INFO - __main__ - Epoch  58, Step:  124300, Batch Loss:     4.838499, Lr: 0.000056, Tokens per sec:   2645
2023-03-14 18:57:18,474 - INFO - __main__ - Epoch  58, Step:  124400, Batch Loss:     2.962841, Lr: 0.000056, Tokens per sec:   2644
2023-03-14 18:57:38,738 - INFO - __main__ - Epoch  58, Step:  124500, Batch Loss:     3.940614, Lr: 0.000056, Tokens per sec:   2627
2023-03-14 18:57:59,168 - INFO - __main__ - Epoch  58, Step:  124600, Batch Loss:     3.633923, Lr: 0.000056, Tokens per sec:   2662
2023-03-14 18:58:19,494 - INFO - __main__ - Epoch  58, Step:  124700, Batch Loss:     3.625763, Lr: 0.000056, Tokens per sec:   2605
2023-03-14 18:58:39,219 - INFO - __main__ - Epoch  58, Step:  124800, Batch Loss:     4.793041, Lr: 0.000056, Tokens per sec:   2738
2023-03-14 18:58:59,309 - INFO - __main__ - Epoch  58, Step:  124900, Batch Loss:     3.407677, Lr: 0.000056, Tokens per sec:   2651
2023-03-14 18:59:19,405 - INFO - __main__ - Epoch  58, Step:  125000, Batch Loss:     4.764954, Lr: 0.000056, Tokens per sec:   2672
2023-03-14 18:59:38,861 - INFO - __main__ - Epoch  58, Step:  125100, Batch Loss:     4.261024, Lr: 0.000056, Tokens per sec:   2788
2023-03-14 18:59:57,453 - INFO - __main__ - Epoch  58, Step:  125200, Batch Loss:     4.684191, Lr: 0.000056, Tokens per sec:   2911
2023-03-14 19:00:17,566 - INFO - __main__ - Epoch  58, Step:  125300, Batch Loss:     4.415537, Lr: 0.000056, Tokens per sec:   2692
2023-03-14 19:00:38,109 - INFO - __main__ - Epoch  58, Step:  125400, Batch Loss:     4.169847, Lr: 0.000056, Tokens per sec:   2601
2023-03-14 19:00:57,955 - INFO - __main__ - Epoch  58, Step:  125500, Batch Loss:     4.363629, Lr: 0.000056, Tokens per sec:   2749
2023-03-14 19:01:25,573 - INFO - __main__ - Epoch  58, Step:  125600, Batch Loss:     4.189427, Lr: 0.000056, Tokens per sec:   1986
2023-03-14 19:02:03,678 - INFO - __main__ - Epoch  58, Step:  125700, Batch Loss:     5.598870, Lr: 0.000056, Tokens per sec:   1435
2023-03-14 19:02:41,747 - INFO - __main__ - Epoch  58, Step:  125800, Batch Loss:     5.039421, Lr: 0.000056, Tokens per sec:   1426
2023-03-14 19:03:19,643 - INFO - __main__ - Epoch  58, Step:  125900, Batch Loss:     5.217111, Lr: 0.000056, Tokens per sec:   1428
2023-03-14 19:03:56,914 - INFO - __main__ - Epoch  58, Step:  126000, Batch Loss:     4.826900, Lr: 0.000056, Tokens per sec:   1434
2023-03-14 19:04:34,881 - INFO - __main__ - Epoch  58, Step:  126100, Batch Loss:     5.469686, Lr: 0.000056, Tokens per sec:   1395
2023-03-14 19:05:13,026 - INFO - __main__ - Epoch  58, Step:  126200, Batch Loss:     5.008804, Lr: 0.000056, Tokens per sec:   1392
2023-03-14 19:05:51,052 - INFO - __main__ - Epoch  58, Step:  126300, Batch Loss:     4.723343, Lr: 0.000056, Tokens per sec:   1410
2023-03-14 19:06:22,013 - INFO - __main__ - Epoch  58: total training loss 9389.50
2023-03-14 19:06:22,014 - INFO - __main__ - Epoch 59
2023-03-14 19:06:29,176 - INFO - __main__ - Epoch  59, Step:  126400, Batch Loss:     4.502866, Lr: 0.000056, Tokens per sec:   1285
2023-03-14 19:07:07,144 - INFO - __main__ - Epoch  59, Step:  126500, Batch Loss:     4.494612, Lr: 0.000056, Tokens per sec:   1406
2023-03-14 19:07:45,169 - INFO - __main__ - Epoch  59, Step:  126600, Batch Loss:     4.199938, Lr: 0.000056, Tokens per sec:   1418
2023-03-14 19:08:23,601 - INFO - __main__ - Epoch  59, Step:  126700, Batch Loss:     3.567900, Lr: 0.000056, Tokens per sec:   1401
2023-03-14 19:09:01,941 - INFO - __main__ - Epoch  59, Step:  126800, Batch Loss:     4.978289, Lr: 0.000056, Tokens per sec:   1435
2023-03-14 19:09:40,038 - INFO - __main__ - Epoch  59, Step:  126900, Batch Loss:     3.376232, Lr: 0.000056, Tokens per sec:   1386
2023-03-14 19:10:18,408 - INFO - __main__ - Epoch  59, Step:  127000, Batch Loss:     4.203806, Lr: 0.000056, Tokens per sec:   1399
2023-03-14 19:10:56,549 - INFO - __main__ - Epoch  59, Step:  127100, Batch Loss:     6.346722, Lr: 0.000056, Tokens per sec:   1404
2023-03-14 19:11:35,111 - INFO - __main__ - Epoch  59, Step:  127200, Batch Loss:     4.515744, Lr: 0.000056, Tokens per sec:   1419
2023-03-14 19:12:13,141 - INFO - __main__ - Epoch  59, Step:  127300, Batch Loss:     4.542440, Lr: 0.000056, Tokens per sec:   1413
2023-03-14 19:12:50,765 - INFO - __main__ - Epoch  59, Step:  127400, Batch Loss:     4.599210, Lr: 0.000056, Tokens per sec:   1425
2023-03-14 19:13:28,764 - INFO - __main__ - Epoch  59, Step:  127500, Batch Loss:     3.707227, Lr: 0.000056, Tokens per sec:   1399
2023-03-14 19:14:06,399 - INFO - __main__ - Epoch  59, Step:  127600, Batch Loss:     4.184949, Lr: 0.000056, Tokens per sec:   1425
2023-03-14 19:14:44,536 - INFO - __main__ - Epoch  59, Step:  127700, Batch Loss:     4.083588, Lr: 0.000056, Tokens per sec:   1424
2023-03-14 19:15:22,711 - INFO - __main__ - Epoch  59, Step:  127800, Batch Loss:     4.804513, Lr: 0.000056, Tokens per sec:   1416
2023-03-14 19:16:00,154 - INFO - __main__ - Epoch  59, Step:  127900, Batch Loss:     4.923215, Lr: 0.000056, Tokens per sec:   1450
2023-03-14 19:16:38,319 - INFO - __main__ - Epoch  59, Step:  128000, Batch Loss:     5.101256, Lr: 0.000056, Tokens per sec:   1431
2023-03-14 19:17:16,701 - INFO - __main__ - Epoch  59, Step:  128100, Batch Loss:     3.683508, Lr: 0.000056, Tokens per sec:   1384
2023-03-14 19:17:54,947 - INFO - __main__ - Epoch  59, Step:  128200, Batch Loss:     5.607225, Lr: 0.000056, Tokens per sec:   1435
2023-03-14 19:18:33,058 - INFO - __main__ - Epoch  59, Step:  128300, Batch Loss:     3.674187, Lr: 0.000056, Tokens per sec:   1422
2023-03-14 19:19:11,157 - INFO - __main__ - Epoch  59, Step:  128400, Batch Loss:     2.886291, Lr: 0.000056, Tokens per sec:   1394
2023-03-14 19:19:49,239 - INFO - __main__ - Epoch  59, Step:  128500, Batch Loss:     4.647150, Lr: 0.000056, Tokens per sec:   1407
2023-03-14 19:20:12,374 - INFO - __main__ - Epoch  59: total training loss 9170.55
2023-03-14 19:20:12,375 - INFO - __main__ - Epoch 60
2023-03-14 19:20:27,474 - INFO - __main__ - Epoch  60, Step:  128600, Batch Loss:     3.997180, Lr: 0.000055, Tokens per sec:   1354
2023-03-14 19:21:05,619 - INFO - __main__ - Epoch  60, Step:  128700, Batch Loss:     4.524079, Lr: 0.000055, Tokens per sec:   1421
2023-03-14 19:21:43,865 - INFO - __main__ - Epoch  60, Step:  128800, Batch Loss:     4.956689, Lr: 0.000055, Tokens per sec:   1423
2023-03-14 19:22:21,939 - INFO - __main__ - Epoch  60, Step:  128900, Batch Loss:     3.265528, Lr: 0.000055, Tokens per sec:   1414
2023-03-14 19:23:00,271 - INFO - __main__ - Epoch  60, Step:  129000, Batch Loss:     4.202543, Lr: 0.000055, Tokens per sec:   1362
2023-03-14 19:23:38,379 - INFO - __main__ - Epoch  60, Step:  129100, Batch Loss:     3.958536, Lr: 0.000055, Tokens per sec:   1440
2023-03-14 19:24:16,519 - INFO - __main__ - Epoch  60, Step:  129200, Batch Loss:     3.620156, Lr: 0.000055, Tokens per sec:   1390
2023-03-14 19:24:54,581 - INFO - __main__ - Epoch  60, Step:  129300, Batch Loss:     4.329580, Lr: 0.000055, Tokens per sec:   1420
2023-03-14 19:25:32,757 - INFO - __main__ - Epoch  60, Step:  129400, Batch Loss:     4.710462, Lr: 0.000055, Tokens per sec:   1415
2023-03-14 19:26:10,370 - INFO - __main__ - Epoch  60, Step:  129500, Batch Loss:     4.958292, Lr: 0.000055, Tokens per sec:   1440
2023-03-14 19:26:48,694 - INFO - __main__ - Epoch  60, Step:  129600, Batch Loss:     6.051070, Lr: 0.000055, Tokens per sec:   1424
2023-03-14 19:27:27,009 - INFO - __main__ - Epoch  60, Step:  129700, Batch Loss:     3.049413, Lr: 0.000055, Tokens per sec:   1395
2023-03-14 19:28:05,351 - INFO - __main__ - Epoch  60, Step:  129800, Batch Loss:     5.479820, Lr: 0.000055, Tokens per sec:   1405
2023-03-14 19:28:43,485 - INFO - __main__ - Epoch  60, Step:  129900, Batch Loss:     3.966869, Lr: 0.000055, Tokens per sec:   1410
2023-03-14 19:29:21,665 - INFO - __main__ - Epoch  60, Step:  130000, Batch Loss:     5.401702, Lr: 0.000055, Tokens per sec:   1411
2023-03-14 19:29:59,498 - INFO - __main__ - Epoch  60, Step:  130100, Batch Loss:     5.242745, Lr: 0.000055, Tokens per sec:   1432
2023-03-14 19:30:36,940 - INFO - __main__ - Epoch  60, Step:  130200, Batch Loss:     4.460351, Lr: 0.000055, Tokens per sec:   1449
2023-03-14 19:31:14,142 - INFO - __main__ - Epoch  60, Step:  130300, Batch Loss:     4.228360, Lr: 0.000055, Tokens per sec:   1431
2023-03-14 19:31:51,619 - INFO - __main__ - Epoch  60, Step:  130400, Batch Loss:     3.472620, Lr: 0.000055, Tokens per sec:   1413
2023-03-14 19:32:29,726 - INFO - __main__ - Epoch  60, Step:  130500, Batch Loss:     3.661092, Lr: 0.000055, Tokens per sec:   1425
2023-03-14 19:33:07,364 - INFO - __main__ - Epoch  60, Step:  130600, Batch Loss:     3.697504, Lr: 0.000055, Tokens per sec:   1438
2023-03-14 19:33:44,436 - INFO - __main__ - Epoch  60, Step:  130700, Batch Loss:     4.097884, Lr: 0.000055, Tokens per sec:   1460
2023-03-14 19:33:59,739 - INFO - __main__ - Epoch  60: total training loss 8980.36
2023-03-14 19:33:59,740 - INFO - __main__ - Epoch 61
2023-03-14 19:34:22,847 - INFO - __main__ - Epoch  61, Step:  130800, Batch Loss:     2.273819, Lr: 0.000055, Tokens per sec:   1400
2023-03-14 19:35:00,914 - INFO - __main__ - Epoch  61, Step:  130900, Batch Loss:     3.375189, Lr: 0.000055, Tokens per sec:   1408
2023-03-14 19:35:38,809 - INFO - __main__ - Epoch  61, Step:  131000, Batch Loss:     2.895429, Lr: 0.000055, Tokens per sec:   1414
2023-03-14 19:36:17,124 - INFO - __main__ - Epoch  61, Step:  131100, Batch Loss:     3.206742, Lr: 0.000055, Tokens per sec:   1396
2023-03-14 19:36:55,217 - INFO - __main__ - Epoch  61, Step:  131200, Batch Loss:     3.765571, Lr: 0.000055, Tokens per sec:   1434
2023-03-14 19:37:33,357 - INFO - __main__ - Epoch  61, Step:  131300, Batch Loss:     2.866001, Lr: 0.000055, Tokens per sec:   1424
2023-03-14 19:38:10,544 - INFO - __main__ - Epoch  61, Step:  131400, Batch Loss:     6.230742, Lr: 0.000055, Tokens per sec:   1472
2023-03-14 19:38:48,643 - INFO - __main__ - Epoch  61, Step:  131500, Batch Loss:     4.069938, Lr: 0.000055, Tokens per sec:   1435
2023-03-14 19:39:26,981 - INFO - __main__ - Epoch  61, Step:  131600, Batch Loss:     4.621341, Lr: 0.000055, Tokens per sec:   1423
2023-03-14 19:40:05,285 - INFO - __main__ - Epoch  61, Step:  131700, Batch Loss:     5.500271, Lr: 0.000055, Tokens per sec:   1395
2023-03-14 19:40:32,200 - INFO - __main__ - Epoch  61, Step:  131800, Batch Loss:     6.475175, Lr: 0.000055, Tokens per sec:   1988
2023-03-14 19:40:58,097 - INFO - __main__ - Epoch  61, Step:  131900, Batch Loss:     2.975256, Lr: 0.000055, Tokens per sec:   2073
2023-03-14 19:41:24,201 - INFO - __main__ - Epoch  61, Step:  132000, Batch Loss:     4.355518, Lr: 0.000055, Tokens per sec:   2022
2023-03-14 19:41:50,047 - INFO - __main__ - Epoch  61, Step:  132100, Batch Loss:     3.760706, Lr: 0.000055, Tokens per sec:   2069
2023-03-14 19:42:16,320 - INFO - __main__ - Epoch  61, Step:  132200, Batch Loss:     4.741535, Lr: 0.000055, Tokens per sec:   2038
2023-03-14 19:42:41,637 - INFO - __main__ - Epoch  61, Step:  132300, Batch Loss:     3.747043, Lr: 0.000055, Tokens per sec:   2146
2023-03-14 19:43:06,575 - INFO - __main__ - Epoch  61, Step:  132400, Batch Loss:     3.561040, Lr: 0.000055, Tokens per sec:   2175
2023-03-14 19:43:31,699 - INFO - __main__ - Epoch  61, Step:  132500, Batch Loss:     4.676728, Lr: 0.000055, Tokens per sec:   2140
2023-03-14 19:43:55,543 - INFO - __main__ - Epoch  61, Step:  132600, Batch Loss:     3.585188, Lr: 0.000055, Tokens per sec:   2257
2023-03-14 19:44:15,781 - INFO - __main__ - Epoch  61, Step:  132700, Batch Loss:     4.670606, Lr: 0.000055, Tokens per sec:   2664
2023-03-14 19:44:34,446 - INFO - __main__ - Epoch  61, Step:  132800, Batch Loss:     5.158781, Lr: 0.000055, Tokens per sec:   2853
2023-03-14 19:44:53,166 - INFO - __main__ - Epoch  61, Step:  132900, Batch Loss:     5.040623, Lr: 0.000055, Tokens per sec:   2844
2023-03-14 19:44:57,104 - INFO - __main__ - Epoch  61: total training loss 8766.58
2023-03-14 19:44:57,105 - INFO - __main__ - Epoch 62
2023-03-14 19:45:13,351 - INFO - __main__ - Epoch  62, Step:  133000, Batch Loss:     4.652927, Lr: 0.000054, Tokens per sec:   2761
2023-03-14 19:45:33,533 - INFO - __main__ - Epoch  62, Step:  133100, Batch Loss:     4.886590, Lr: 0.000054, Tokens per sec:   2680
2023-03-14 19:45:52,989 - INFO - __main__ - Epoch  62, Step:  133200, Batch Loss:     2.240072, Lr: 0.000054, Tokens per sec:   2747
2023-03-14 19:46:12,091 - INFO - __main__ - Epoch  62, Step:  133300, Batch Loss:     4.670770, Lr: 0.000054, Tokens per sec:   2857
2023-03-14 19:46:30,868 - INFO - __main__ - Epoch  62, Step:  133400, Batch Loss:     3.450986, Lr: 0.000054, Tokens per sec:   2873
2023-03-14 19:46:51,047 - INFO - __main__ - Epoch  62, Step:  133500, Batch Loss:     3.944439, Lr: 0.000054, Tokens per sec:   2639
2023-03-14 19:47:11,246 - INFO - __main__ - Epoch  62, Step:  133600, Batch Loss:     6.060786, Lr: 0.000054, Tokens per sec:   2647
2023-03-14 19:47:31,262 - INFO - __main__ - Epoch  62, Step:  133700, Batch Loss:     3.558945, Lr: 0.000054, Tokens per sec:   2653
2023-03-14 19:47:50,459 - INFO - __main__ - Epoch  62, Step:  133800, Batch Loss:     4.097102, Lr: 0.000054, Tokens per sec:   2777
2023-03-14 19:48:10,622 - INFO - __main__ - Epoch  62, Step:  133900, Batch Loss:     2.863273, Lr: 0.000054, Tokens per sec:   2680
2023-03-14 19:48:30,912 - INFO - __main__ - Epoch  62, Step:  134000, Batch Loss:     3.626715, Lr: 0.000054, Tokens per sec:   2652
2023-03-14 19:48:51,111 - INFO - __main__ - Epoch  62, Step:  134100, Batch Loss:     3.885679, Lr: 0.000054, Tokens per sec:   2648
2023-03-14 19:49:10,820 - INFO - __main__ - Epoch  62, Step:  134200, Batch Loss:     4.172519, Lr: 0.000054, Tokens per sec:   2714
2023-03-14 19:49:43,274 - INFO - __main__ - Epoch  62, Step:  134300, Batch Loss:     4.460076, Lr: 0.000054, Tokens per sec:   1662
2023-03-14 19:50:21,140 - INFO - __main__ - Epoch  62, Step:  134400, Batch Loss:     3.614985, Lr: 0.000054, Tokens per sec:   1417
2023-03-14 19:50:59,303 - INFO - __main__ - Epoch  62, Step:  134500, Batch Loss:     4.198259, Lr: 0.000054, Tokens per sec:   1407
2023-03-14 19:51:37,696 - INFO - __main__ - Epoch  62, Step:  134600, Batch Loss:     5.627139, Lr: 0.000054, Tokens per sec:   1421
2023-03-14 19:52:15,938 - INFO - __main__ - Epoch  62, Step:  134700, Batch Loss:     4.052044, Lr: 0.000054, Tokens per sec:   1420
2023-03-14 19:52:54,135 - INFO - __main__ - Epoch  62, Step:  134800, Batch Loss:     3.724040, Lr: 0.000054, Tokens per sec:   1402
2023-03-14 19:53:32,180 - INFO - __main__ - Epoch  62, Step:  134900, Batch Loss:     3.746482, Lr: 0.000054, Tokens per sec:   1421
2023-03-14 19:54:09,919 - INFO - __main__ - Epoch  62, Step:  135000, Batch Loss:     4.790609, Lr: 0.000054, Tokens per sec:   1418
2023-03-14 19:54:47,321 - INFO - __main__ - Epoch  62: total training loss 8558.20
2023-03-14 19:54:47,322 - INFO - __main__ - Epoch 63
2023-03-14 19:54:48,399 - INFO - __main__ - Epoch  63, Step:  135100, Batch Loss:     2.845460, Lr: 0.000054, Tokens per sec:   1004
2023-03-14 19:55:25,889 - INFO - __main__ - Epoch  63, Step:  135200, Batch Loss:     3.241519, Lr: 0.000054, Tokens per sec:   1430
2023-03-14 19:56:02,903 - INFO - __main__ - Epoch  63, Step:  135300, Batch Loss:     4.656249, Lr: 0.000054, Tokens per sec:   1466
2023-03-14 19:56:41,154 - INFO - __main__ - Epoch  63, Step:  135400, Batch Loss:     3.239700, Lr: 0.000054, Tokens per sec:   1421
2023-03-14 19:57:18,832 - INFO - __main__ - Epoch  63, Step:  135500, Batch Loss:     4.538569, Lr: 0.000054, Tokens per sec:   1416
2023-03-14 19:57:57,140 - INFO - __main__ - Epoch  63, Step:  135600, Batch Loss:     2.999468, Lr: 0.000054, Tokens per sec:   1411
2023-03-14 19:58:35,145 - INFO - __main__ - Epoch  63, Step:  135700, Batch Loss:     2.781048, Lr: 0.000054, Tokens per sec:   1397
2023-03-14 19:59:12,901 - INFO - __main__ - Epoch  63, Step:  135800, Batch Loss:     2.542169, Lr: 0.000054, Tokens per sec:   1409
2023-03-14 19:59:51,014 - INFO - __main__ - Epoch  63, Step:  135900, Batch Loss:     3.707994, Lr: 0.000054, Tokens per sec:   1421
2023-03-14 20:00:29,329 - INFO - __main__ - Epoch  63, Step:  136000, Batch Loss:     3.624749, Lr: 0.000054, Tokens per sec:   1406
2023-03-14 20:01:07,681 - INFO - __main__ - Epoch  63, Step:  136100, Batch Loss:     4.279928, Lr: 0.000054, Tokens per sec:   1398
2023-03-14 20:01:45,905 - INFO - __main__ - Epoch  63, Step:  136200, Batch Loss:     2.651082, Lr: 0.000054, Tokens per sec:   1417
2023-03-14 20:02:24,268 - INFO - __main__ - Epoch  63, Step:  136300, Batch Loss:     3.929723, Lr: 0.000054, Tokens per sec:   1395
2023-03-14 20:03:02,626 - INFO - __main__ - Epoch  63, Step:  136400, Batch Loss:     3.793404, Lr: 0.000054, Tokens per sec:   1427
2023-03-14 20:03:40,860 - INFO - __main__ - Epoch  63, Step:  136500, Batch Loss:     3.687046, Lr: 0.000054, Tokens per sec:   1416
2023-03-14 20:04:19,105 - INFO - __main__ - Epoch  63, Step:  136600, Batch Loss:     4.581616, Lr: 0.000054, Tokens per sec:   1403
2023-03-14 20:04:57,222 - INFO - __main__ - Epoch  63, Step:  136700, Batch Loss:     4.306651, Lr: 0.000054, Tokens per sec:   1403
2023-03-14 20:05:35,011 - INFO - __main__ - Epoch  63, Step:  136800, Batch Loss:     3.112291, Lr: 0.000054, Tokens per sec:   1419
2023-03-14 20:06:12,720 - INFO - __main__ - Epoch  63, Step:  136900, Batch Loss:     3.364186, Lr: 0.000054, Tokens per sec:   1435
2023-03-14 20:06:50,987 - INFO - __main__ - Epoch  63, Step:  137000, Batch Loss:     3.849014, Lr: 0.000054, Tokens per sec:   1417
2023-03-14 20:07:29,121 - INFO - __main__ - Epoch  63, Step:  137100, Batch Loss:     3.882998, Lr: 0.000054, Tokens per sec:   1412
2023-03-14 20:08:07,307 - INFO - __main__ - Epoch  63, Step:  137200, Batch Loss:     5.088032, Lr: 0.000054, Tokens per sec:   1410
2023-03-14 20:08:36,483 - INFO - __main__ - Epoch  63: total training loss 8334.66
2023-03-14 20:08:36,484 - INFO - __main__ - Epoch 64
2023-03-14 20:08:45,569 - INFO - __main__ - Epoch  64, Step:  137300, Batch Loss:     3.096097, Lr: 0.000053, Tokens per sec:   1337
2023-03-14 20:09:22,837 - INFO - __main__ - Epoch  64, Step:  137400, Batch Loss:     2.182508, Lr: 0.000053, Tokens per sec:   1454
2023-03-14 20:10:00,853 - INFO - __main__ - Epoch  64, Step:  137500, Batch Loss:     2.673728, Lr: 0.000053, Tokens per sec:   1419
2023-03-14 20:10:39,129 - INFO - __main__ - Epoch  64, Step:  137600, Batch Loss:     4.719516, Lr: 0.000053, Tokens per sec:   1409
2023-03-14 20:11:16,285 - INFO - __main__ - Epoch  64, Step:  137700, Batch Loss:     3.808752, Lr: 0.000053, Tokens per sec:   1434
2023-03-14 20:11:53,890 - INFO - __main__ - Epoch  64, Step:  137800, Batch Loss:     3.982886, Lr: 0.000053, Tokens per sec:   1426
2023-03-14 20:12:31,960 - INFO - __main__ - Epoch  64, Step:  137900, Batch Loss:     2.998135, Lr: 0.000053, Tokens per sec:   1455
2023-03-14 20:13:09,925 - INFO - __main__ - Epoch  64, Step:  138000, Batch Loss:     4.037253, Lr: 0.000053, Tokens per sec:   1414
2023-03-14 20:13:48,078 - INFO - __main__ - Epoch  64, Step:  138100, Batch Loss:     4.142072, Lr: 0.000053, Tokens per sec:   1397
2023-03-14 20:14:26,200 - INFO - __main__ - Epoch  64, Step:  138200, Batch Loss:     3.682830, Lr: 0.000053, Tokens per sec:   1409
2023-03-14 20:15:03,975 - INFO - __main__ - Epoch  64, Step:  138300, Batch Loss:     4.301701, Lr: 0.000053, Tokens per sec:   1415
2023-03-14 20:15:42,392 - INFO - __main__ - Epoch  64, Step:  138400, Batch Loss:     3.508671, Lr: 0.000053, Tokens per sec:   1405
2023-03-14 20:16:20,348 - INFO - __main__ - Epoch  64, Step:  138500, Batch Loss:     3.811089, Lr: 0.000053, Tokens per sec:   1437
2023-03-14 20:16:58,434 - INFO - __main__ - Epoch  64, Step:  138600, Batch Loss:     5.127954, Lr: 0.000053, Tokens per sec:   1418
2023-03-14 20:17:36,536 - INFO - __main__ - Epoch  64, Step:  138700, Batch Loss:     3.140531, Lr: 0.000053, Tokens per sec:   1374
2023-03-14 20:18:13,741 - INFO - __main__ - Epoch  64, Step:  138800, Batch Loss:     4.215120, Lr: 0.000053, Tokens per sec:   1439
2023-03-14 20:18:50,792 - INFO - __main__ - Epoch  64, Step:  138900, Batch Loss:     2.653025, Lr: 0.000053, Tokens per sec:   1477
2023-03-14 20:19:27,980 - INFO - __main__ - Epoch  64, Step:  139000, Batch Loss:     3.711996, Lr: 0.000053, Tokens per sec:   1458
2023-03-14 20:20:04,972 - INFO - __main__ - Epoch  64, Step:  139100, Batch Loss:     2.582149, Lr: 0.000053, Tokens per sec:   1466
2023-03-14 20:20:40,908 - INFO - __main__ - Epoch  64, Step:  139200, Batch Loss:     4.305999, Lr: 0.000053, Tokens per sec:   1504
2023-03-14 20:21:17,921 - INFO - __main__ - Epoch  64, Step:  139300, Batch Loss:     4.222849, Lr: 0.000053, Tokens per sec:   1433
2023-03-14 20:21:54,256 - INFO - __main__ - Epoch  64, Step:  139400, Batch Loss:     5.282705, Lr: 0.000053, Tokens per sec:   1498
2023-03-14 20:22:14,451 - INFO - __main__ - Epoch  64: total training loss 8177.15
2023-03-14 20:22:14,451 - INFO - __main__ - Epoch 65
2023-03-14 20:22:31,012 - INFO - __main__ - Epoch  65, Step:  139500, Batch Loss:     3.305391, Lr: 0.000053, Tokens per sec:   1437
2023-03-14 20:23:07,299 - INFO - __main__ - Epoch  65, Step:  139600, Batch Loss:     3.725632, Lr: 0.000053, Tokens per sec:   1473
2023-03-14 20:23:43,728 - INFO - __main__ - Epoch  65, Step:  139700, Batch Loss:     3.042819, Lr: 0.000053, Tokens per sec:   1468
2023-03-14 20:24:20,745 - INFO - __main__ - Epoch  65, Step:  139800, Batch Loss:     3.533983, Lr: 0.000053, Tokens per sec:   1457
2023-03-14 20:24:58,732 - INFO - __main__ - Epoch  65, Step:  139900, Batch Loss:     4.096869, Lr: 0.000053, Tokens per sec:   1409
2023-03-14 20:25:36,861 - INFO - __main__ - Epoch  65, Step:  140000, Batch Loss:     3.778863, Lr: 0.000053, Tokens per sec:   1403
2023-03-14 20:26:15,274 - INFO - __main__ - Epoch  65, Step:  140100, Batch Loss:     3.872513, Lr: 0.000053, Tokens per sec:   1414
2023-03-14 20:26:53,563 - INFO - __main__ - Epoch  65, Step:  140200, Batch Loss:     2.719929, Lr: 0.000053, Tokens per sec:   1392
2023-03-14 20:27:31,664 - INFO - __main__ - Epoch  65, Step:  140300, Batch Loss:     4.039610, Lr: 0.000053, Tokens per sec:   1405
2023-03-14 20:28:09,546 - INFO - __main__ - Epoch  65, Step:  140400, Batch Loss:     3.829326, Lr: 0.000053, Tokens per sec:   1412
2023-03-14 20:28:47,493 - INFO - __main__ - Epoch  65, Step:  140500, Batch Loss:     2.676949, Lr: 0.000053, Tokens per sec:   1416
2023-03-14 20:29:12,255 - INFO - __main__ - Epoch  65, Step:  140600, Batch Loss:     3.184180, Lr: 0.000053, Tokens per sec:   2181
2023-03-14 20:29:38,785 - INFO - __main__ - Epoch  65, Step:  140700, Batch Loss:     3.122004, Lr: 0.000053, Tokens per sec:   2059
2023-03-14 20:30:05,069 - INFO - __main__ - Epoch  65, Step:  140800, Batch Loss:     4.138956, Lr: 0.000053, Tokens per sec:   2000
2023-03-14 20:30:31,602 - INFO - __main__ - Epoch  65, Step:  140900, Batch Loss:     3.988018, Lr: 0.000053, Tokens per sec:   2046
2023-03-14 20:30:58,062 - INFO - __main__ - Epoch  65, Step:  141000, Batch Loss:     4.001269, Lr: 0.000053, Tokens per sec:   2066
2023-03-14 20:31:24,508 - INFO - __main__ - Epoch  65, Step:  141100, Batch Loss:     4.511781, Lr: 0.000053, Tokens per sec:   2046
2023-03-14 20:31:49,689 - INFO - __main__ - Epoch  65, Step:  141200, Batch Loss:     4.352047, Lr: 0.000053, Tokens per sec:   2123
2023-03-14 20:32:14,740 - INFO - __main__ - Epoch  65, Step:  141300, Batch Loss:     3.998969, Lr: 0.000053, Tokens per sec:   2190
2023-03-14 20:32:35,498 - INFO - __main__ - Epoch  65, Step:  141400, Batch Loss:     4.710083, Lr: 0.000053, Tokens per sec:   2590
2023-03-14 20:32:54,829 - INFO - __main__ - Epoch  65, Step:  141500, Batch Loss:     3.264731, Lr: 0.000053, Tokens per sec:   2792
2023-03-14 20:33:14,754 - INFO - __main__ - Epoch  65, Step:  141600, Batch Loss:     4.196096, Lr: 0.000053, Tokens per sec:   2707
2023-03-14 20:33:21,888 - INFO - __main__ - Epoch  65: total training loss 7980.97
2023-03-14 20:33:21,889 - INFO - __main__ - Epoch 66
2023-03-14 20:33:35,242 - INFO - __main__ - Epoch  66, Step:  141700, Batch Loss:     3.335686, Lr: 0.000052, Tokens per sec:   2607
2023-03-14 20:33:55,352 - INFO - __main__ - Epoch  66, Step:  141800, Batch Loss:     4.589395, Lr: 0.000052, Tokens per sec:   2684
2023-03-14 20:34:15,547 - INFO - __main__ - Epoch  66, Step:  141900, Batch Loss:     2.212449, Lr: 0.000052, Tokens per sec:   2703
2023-03-14 20:34:35,754 - INFO - __main__ - Epoch  66, Step:  142000, Batch Loss:     3.846992, Lr: 0.000052, Tokens per sec:   2710
2023-03-14 20:34:55,824 - INFO - __main__ - Epoch  66, Step:  142100, Batch Loss:     2.985227, Lr: 0.000052, Tokens per sec:   2682
2023-03-14 20:35:15,980 - INFO - __main__ - Epoch  66, Step:  142200, Batch Loss:     3.495541, Lr: 0.000052, Tokens per sec:   2696
2023-03-14 20:35:36,091 - INFO - __main__ - Epoch  66, Step:  142300, Batch Loss:     3.650563, Lr: 0.000052, Tokens per sec:   2698
2023-03-14 20:35:54,541 - INFO - __main__ - Epoch  66, Step:  142400, Batch Loss:     3.501751, Lr: 0.000052, Tokens per sec:   2883
2023-03-14 20:36:13,057 - INFO - __main__ - Epoch  66, Step:  142500, Batch Loss:     4.102969, Lr: 0.000052, Tokens per sec:   2946
2023-03-14 20:36:33,152 - INFO - __main__ - Epoch  66, Step:  142600, Batch Loss:     4.200769, Lr: 0.000052, Tokens per sec:   2686
2023-03-14 20:36:53,189 - INFO - __main__ - Epoch  66, Step:  142700, Batch Loss:     2.855043, Lr: 0.000052, Tokens per sec:   2676
2023-03-14 20:37:13,232 - INFO - __main__ - Epoch  66, Step:  142800, Batch Loss:     4.290336, Lr: 0.000052, Tokens per sec:   2707
2023-03-14 20:37:32,105 - INFO - __main__ - Epoch  66, Step:  142900, Batch Loss:     3.663351, Lr: 0.000052, Tokens per sec:   2835
2023-03-14 20:37:51,655 - INFO - __main__ - Epoch  66, Step:  143000, Batch Loss:     5.550609, Lr: 0.000052, Tokens per sec:   2736
2023-03-14 20:38:11,190 - INFO - __main__ - Epoch  66, Step:  143100, Batch Loss:     4.871804, Lr: 0.000052, Tokens per sec:   2730
2023-03-14 20:38:30,503 - INFO - __main__ - Epoch  66, Step:  143200, Batch Loss:     4.765646, Lr: 0.000052, Tokens per sec:   2839
2023-03-14 20:38:50,096 - INFO - __main__ - Epoch  66, Step:  143300, Batch Loss:     3.140795, Lr: 0.000052, Tokens per sec:   2694
2023-03-14 20:39:09,971 - INFO - __main__ - Epoch  66, Step:  143400, Batch Loss:     3.385726, Lr: 0.000052, Tokens per sec:   2725
2023-03-14 20:39:30,512 - INFO - __main__ - Epoch  66, Step:  143500, Batch Loss:     4.086638, Lr: 0.000052, Tokens per sec:   2586
2023-03-14 20:39:51,507 - INFO - __main__ - Epoch  66, Step:  143600, Batch Loss:     5.205621, Lr: 0.000052, Tokens per sec:   2504
2023-03-14 20:40:11,503 - INFO - __main__ - Epoch  66, Step:  143700, Batch Loss:     2.863209, Lr: 0.000052, Tokens per sec:   2729
2023-03-14 20:40:31,586 - INFO - __main__ - Epoch  66, Step:  143800, Batch Loss:     3.294371, Lr: 0.000052, Tokens per sec:   2645
2023-03-14 20:40:34,486 - INFO - __main__ - Epoch  66: total training loss 7793.65
2023-03-14 20:40:34,487 - INFO - __main__ - Epoch 67
2023-03-14 20:40:52,127 - INFO - __main__ - Epoch  67, Step:  143900, Batch Loss:     3.838377, Lr: 0.000052, Tokens per sec:   2635
2023-03-14 20:41:12,115 - INFO - __main__ - Epoch  67, Step:  144000, Batch Loss:     2.336699, Lr: 0.000052, Tokens per sec:   2743
2023-03-14 20:41:32,165 - INFO - __main__ - Epoch  67, Step:  144100, Batch Loss:     3.560970, Lr: 0.000052, Tokens per sec:   2690
2023-03-14 20:41:52,385 - INFO - __main__ - Epoch  67, Step:  144200, Batch Loss:     3.047633, Lr: 0.000052, Tokens per sec:   2630
2023-03-14 20:42:12,791 - INFO - __main__ - Epoch  67, Step:  144300, Batch Loss:     3.985012, Lr: 0.000052, Tokens per sec:   2638
2023-03-14 20:42:33,401 - INFO - __main__ - Epoch  67, Step:  144400, Batch Loss:     3.131457, Lr: 0.000052, Tokens per sec:   2574
2023-03-14 20:42:53,785 - INFO - __main__ - Epoch  67, Step:  144500, Batch Loss:     4.099231, Lr: 0.000052, Tokens per sec:   2604
2023-03-14 20:43:13,656 - INFO - __main__ - Epoch  67, Step:  144600, Batch Loss:     2.091415, Lr: 0.000052, Tokens per sec:   2691
2023-03-14 20:43:33,518 - INFO - __main__ - Epoch  67, Step:  144700, Batch Loss:     2.655740, Lr: 0.000052, Tokens per sec:   2743
2023-03-14 20:43:53,536 - INFO - __main__ - Epoch  67, Step:  144800, Batch Loss:     3.132973, Lr: 0.000052, Tokens per sec:   2690
2023-03-14 20:44:13,282 - INFO - __main__ - Epoch  67, Step:  144900, Batch Loss:     2.770645, Lr: 0.000052, Tokens per sec:   2731
2023-03-14 20:44:32,929 - INFO - __main__ - Epoch  67, Step:  145000, Batch Loss:     4.786920, Lr: 0.000052, Tokens per sec:   2773
2023-03-14 20:44:53,697 - INFO - __main__ - Epoch  67, Step:  145100, Batch Loss:     3.237068, Lr: 0.000052, Tokens per sec:   2580
2023-03-14 20:45:14,881 - INFO - __main__ - Epoch  67, Step:  145200, Batch Loss:     2.991638, Lr: 0.000052, Tokens per sec:   2504
2023-03-14 20:45:36,099 - INFO - __main__ - Epoch  67, Step:  145300, Batch Loss:     2.926104, Lr: 0.000052, Tokens per sec:   2523
2023-03-14 20:45:57,906 - INFO - __main__ - Epoch  67, Step:  145400, Batch Loss:     4.948731, Lr: 0.000052, Tokens per sec:   2488
2023-03-14 20:46:19,340 - INFO - __main__ - Epoch  67, Step:  145500, Batch Loss:     3.745323, Lr: 0.000052, Tokens per sec:   2515
2023-03-14 20:46:40,827 - INFO - __main__ - Epoch  67, Step:  145600, Batch Loss:     3.595938, Lr: 0.000052, Tokens per sec:   2525
2023-03-14 20:47:02,001 - INFO - __main__ - Epoch  67, Step:  145700, Batch Loss:     3.951958, Lr: 0.000052, Tokens per sec:   2548
2023-03-14 20:47:22,320 - INFO - __main__ - Epoch  67, Step:  145800, Batch Loss:     3.534749, Lr: 0.000052, Tokens per sec:   2645
2023-03-14 20:47:43,251 - INFO - __main__ - Epoch  67, Step:  145900, Batch Loss:     5.503017, Lr: 0.000052, Tokens per sec:   2591
2023-03-14 20:48:02,582 - INFO - __main__ - Epoch  67: total training loss 7632.01
2023-03-14 20:48:02,582 - INFO - __main__ - Epoch 68
2023-03-14 20:48:04,316 - INFO - __main__ - Epoch  68, Step:  146000, Batch Loss:     3.261319, Lr: 0.000051, Tokens per sec:   2360
2023-03-14 20:48:25,471 - INFO - __main__ - Epoch  68, Step:  146100, Batch Loss:     4.696274, Lr: 0.000051, Tokens per sec:   2549
2023-03-14 20:48:46,318 - INFO - __main__ - Epoch  68, Step:  146200, Batch Loss:     3.520468, Lr: 0.000051, Tokens per sec:   2597
2023-03-14 20:49:07,243 - INFO - __main__ - Epoch  68, Step:  146300, Batch Loss:     2.539525, Lr: 0.000051, Tokens per sec:   2563
2023-03-14 20:49:27,838 - INFO - __main__ - Epoch  68, Step:  146400, Batch Loss:     2.214224, Lr: 0.000051, Tokens per sec:   2588
2023-03-14 20:49:48,634 - INFO - __main__ - Epoch  68, Step:  146500, Batch Loss:     3.697366, Lr: 0.000051, Tokens per sec:   2588
2023-03-14 20:50:09,316 - INFO - __main__ - Epoch  68, Step:  146600, Batch Loss:     4.653400, Lr: 0.000051, Tokens per sec:   2618
2023-03-14 20:50:30,336 - INFO - __main__ - Epoch  68, Step:  146700, Batch Loss:     3.064358, Lr: 0.000051, Tokens per sec:   2590
2023-03-14 20:50:51,167 - INFO - __main__ - Epoch  68, Step:  146800, Batch Loss:     3.788874, Lr: 0.000051, Tokens per sec:   2558
2023-03-14 20:51:11,943 - INFO - __main__ - Epoch  68, Step:  146900, Batch Loss:     2.831035, Lr: 0.000051, Tokens per sec:   2595
2023-03-14 20:51:32,793 - INFO - __main__ - Epoch  68, Step:  147000, Batch Loss:     3.614458, Lr: 0.000051, Tokens per sec:   2567
2023-03-14 20:51:53,422 - INFO - __main__ - Epoch  68, Step:  147100, Batch Loss:     3.040134, Lr: 0.000051, Tokens per sec:   2605
2023-03-14 20:52:13,889 - INFO - __main__ - Epoch  68, Step:  147200, Batch Loss:     3.478890, Lr: 0.000051, Tokens per sec:   2654
2023-03-14 20:52:34,273 - INFO - __main__ - Epoch  68, Step:  147300, Batch Loss:     3.926192, Lr: 0.000051, Tokens per sec:   2651
2023-03-14 20:52:55,108 - INFO - __main__ - Epoch  68, Step:  147400, Batch Loss:     3.588586, Lr: 0.000051, Tokens per sec:   2519
2023-03-14 20:53:15,867 - INFO - __main__ - Epoch  68, Step:  147500, Batch Loss:     4.186647, Lr: 0.000051, Tokens per sec:   2623
2023-03-14 20:53:36,665 - INFO - __main__ - Epoch  68, Step:  147600, Batch Loss:     3.858465, Lr: 0.000051, Tokens per sec:   2638
2023-03-14 20:53:57,028 - INFO - __main__ - Epoch  68, Step:  147700, Batch Loss:     3.221053, Lr: 0.000051, Tokens per sec:   2625
2023-03-14 20:54:17,834 - INFO - __main__ - Epoch  68, Step:  147800, Batch Loss:     3.123520, Lr: 0.000051, Tokens per sec:   2593
2023-03-14 20:54:38,375 - INFO - __main__ - Epoch  68, Step:  147900, Batch Loss:     3.098814, Lr: 0.000051, Tokens per sec:   2588
2023-03-14 20:54:58,831 - INFO - __main__ - Epoch  68, Step:  148000, Batch Loss:     2.052444, Lr: 0.000051, Tokens per sec:   2634
2023-03-14 20:55:19,069 - INFO - __main__ - Epoch  68, Step:  148100, Batch Loss:     3.428022, Lr: 0.000051, Tokens per sec:   2661
2023-03-14 20:55:34,069 - INFO - __main__ - Epoch  68: total training loss 7429.92
2023-03-14 20:55:34,070 - INFO - __main__ - Epoch 69
2023-03-14 20:55:40,038 - INFO - __main__ - Epoch  69, Step:  148200, Batch Loss:     2.609382, Lr: 0.000050, Tokens per sec:   2542
