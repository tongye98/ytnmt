2023-03-13 23:50:44,211 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-13 23:51:46,808 - INFO - data - average code tokens = 109.28999515442095
2023-03-13 23:51:46,808 - INFO - data - average ast tokens = 188.85505342888476
2023-03-13 23:51:46,808 - INFO - data - average text tokens = 15.993139680191783
2023-03-13 23:51:46,808 - INFO - data - average position tokens = 188.85505342888476
2023-03-13 23:51:46,808 - INFO - data - average ast edges = 375.7101068577695
2023-03-13 23:51:58,849 - INFO - data - code vocab length = 26684
2023-03-13 23:51:58,849 - INFO - data - text vocab length = 13207
2023-03-13 23:51:58,849 - INFO - data - position vocab length = 20587
2023-03-13 23:52:08,193 - INFO - model - Build Model...
2023-03-13 23:52:08,743 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (2): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (3): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (4): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (5): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-13 23:52:08,748 - INFO - model - Total parameters number: 91563520
2023-03-13 23:52:08,748 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-13 23:52:08,748 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.weight               [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.bias                 [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.weight               [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.bias                 [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.weight [512, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.bias [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,749 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.weight               [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.bias                 [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.weight [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.bias [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.weight [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.bias [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.weight [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.bias [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.weight [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.bias [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.weight               [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.bias                 [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.weight [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.bias [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.weight [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.bias [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.weight [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.bias [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.weight [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.bias [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,750 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-13 23:52:08,751 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,752 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-13 23:52:08,753 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-13 23:52:08,754 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-13 23:52:08,754 - INFO - model - The model is built.
2023-03-13 23:52:08,755 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-13 23:52:08,755 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-13 23:52:09,765 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-13 23:52:09,765 - INFO - __main__ - Scheduler = StepLR
2023-03-13 23:52:09,766 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-13 23:52:09,766 - INFO - __main__ - Epoch 1
2023-03-13 23:52:30,632 - INFO - __main__ - Epoch   1, Step:     100, Batch Loss:    90.196007, Lr: 0.000100, Tokens per sec:   2538
2023-03-13 23:52:50,258 - INFO - __main__ - Epoch   1, Step:     200, Batch Loss:    84.849762, Lr: 0.000100, Tokens per sec:   2707
2023-03-13 23:53:10,091 - INFO - __main__ - Epoch   1, Step:     300, Batch Loss:    74.636436, Lr: 0.000100, Tokens per sec:   2770
2023-03-13 23:53:29,898 - INFO - __main__ - Epoch   1, Step:     400, Batch Loss:    72.338028, Lr: 0.000100, Tokens per sec:   2742
2023-03-13 23:53:49,799 - INFO - __main__ - Epoch   1, Step:     500, Batch Loss:    68.078056, Lr: 0.000100, Tokens per sec:   2730
2023-03-13 23:54:09,678 - INFO - __main__ - Epoch   1, Step:     600, Batch Loss:    89.625336, Lr: 0.000100, Tokens per sec:   2710
2023-03-13 23:54:29,542 - INFO - __main__ - Epoch   1, Step:     700, Batch Loss:    78.371277, Lr: 0.000100, Tokens per sec:   2700
2023-03-13 23:54:49,403 - INFO - __main__ - Epoch   1, Step:     800, Batch Loss:    71.252548, Lr: 0.000100, Tokens per sec:   2714
2023-03-13 23:55:09,331 - INFO - __main__ - Epoch   1, Step:     900, Batch Loss:    73.359703, Lr: 0.000100, Tokens per sec:   2742
2023-03-13 23:55:29,267 - INFO - __main__ - Epoch   1, Step:    1000, Batch Loss:    82.151764, Lr: 0.000100, Tokens per sec:   2691
2023-03-13 23:55:49,202 - INFO - __main__ - Epoch   1, Step:    1100, Batch Loss:    80.563522, Lr: 0.000100, Tokens per sec:   2651
2023-03-13 23:56:08,982 - INFO - __main__ - Epoch   1, Step:    1200, Batch Loss:    69.341316, Lr: 0.000100, Tokens per sec:   2716
2023-03-13 23:56:28,839 - INFO - __main__ - Epoch   1, Step:    1300, Batch Loss:    73.420609, Lr: 0.000100, Tokens per sec:   2742
2023-03-13 23:56:48,730 - INFO - __main__ - Epoch   1, Step:    1400, Batch Loss:    75.890755, Lr: 0.000100, Tokens per sec:   2664
2023-03-13 23:57:08,599 - INFO - __main__ - Epoch   1, Step:    1500, Batch Loss:    88.965057, Lr: 0.000100, Tokens per sec:   2688
2023-03-13 23:57:28,488 - INFO - __main__ - Epoch   1, Step:    1600, Batch Loss:    96.978508, Lr: 0.000100, Tokens per sec:   2722
2023-03-13 23:57:48,386 - INFO - __main__ - Epoch   1, Step:    1700, Batch Loss:    86.829437, Lr: 0.000100, Tokens per sec:   2750
2023-03-13 23:58:08,280 - INFO - __main__ - Epoch   1, Step:    1800, Batch Loss:    85.759224, Lr: 0.000100, Tokens per sec:   2725
2023-03-13 23:58:28,170 - INFO - __main__ - Epoch   1, Step:    1900, Batch Loss:    90.281204, Lr: 0.000100, Tokens per sec:   2706
2023-03-13 23:58:48,065 - INFO - __main__ - Epoch   1, Step:    2000, Batch Loss:    70.570641, Lr: 0.000100, Tokens per sec:   2717
2023-03-13 23:59:07,896 - INFO - __main__ - Epoch   1, Step:    2100, Batch Loss:    72.223671, Lr: 0.000100, Tokens per sec:   2690
2023-03-13 23:59:23,613 - INFO - __main__ - Epoch   1: total training loss 171143.05
2023-03-13 23:59:23,614 - INFO - __main__ - Epoch 2
2023-03-13 23:59:28,013 - INFO - __main__ - Epoch   2, Step:    2200, Batch Loss:    70.645981, Lr: 0.000099, Tokens per sec:   2547
2023-03-13 23:59:47,969 - INFO - __main__ - Epoch   2, Step:    2300, Batch Loss:    64.094391, Lr: 0.000099, Tokens per sec:   2682
2023-03-14 00:00:07,465 - INFO - __main__ - Epoch   2, Step:    2400, Batch Loss:    60.980736, Lr: 0.000099, Tokens per sec:   2714
2023-03-14 00:00:27,333 - INFO - __main__ - Epoch   2, Step:    2500, Batch Loss:    49.764355, Lr: 0.000099, Tokens per sec:   2748
2023-03-14 00:00:46,310 - INFO - __main__ - Epoch   2, Step:    2600, Batch Loss:    62.317635, Lr: 0.000099, Tokens per sec:   2848
2023-03-14 00:01:06,184 - INFO - __main__ - Epoch   2, Step:    2700, Batch Loss:    58.474739, Lr: 0.000099, Tokens per sec:   2703
2023-03-14 00:01:26,115 - INFO - __main__ - Epoch   2, Step:    2800, Batch Loss:    71.738556, Lr: 0.000099, Tokens per sec:   2722
2023-03-14 00:01:46,049 - INFO - __main__ - Epoch   2, Step:    2900, Batch Loss:    70.485985, Lr: 0.000099, Tokens per sec:   2703
2023-03-14 00:02:05,967 - INFO - __main__ - Epoch   2, Step:    3000, Batch Loss:    67.387001, Lr: 0.000099, Tokens per sec:   2704
2023-03-14 00:02:25,894 - INFO - __main__ - Epoch   2, Step:    3100, Batch Loss:    57.223686, Lr: 0.000099, Tokens per sec:   2706
2023-03-14 00:02:45,828 - INFO - __main__ - Epoch   2, Step:    3200, Batch Loss:    53.868687, Lr: 0.000099, Tokens per sec:   2719
2023-03-14 00:03:05,770 - INFO - __main__ - Epoch   2, Step:    3300, Batch Loss:    60.375713, Lr: 0.000099, Tokens per sec:   2670
2023-03-14 00:03:25,699 - INFO - __main__ - Epoch   2, Step:    3400, Batch Loss:    65.138741, Lr: 0.000099, Tokens per sec:   2698
2023-03-14 00:03:45,600 - INFO - __main__ - Epoch   2, Step:    3500, Batch Loss:    61.951702, Lr: 0.000099, Tokens per sec:   2700
2023-03-14 00:04:05,524 - INFO - __main__ - Epoch   2, Step:    3600, Batch Loss:    49.632015, Lr: 0.000099, Tokens per sec:   2700
2023-03-14 00:04:25,340 - INFO - __main__ - Epoch   2, Step:    3700, Batch Loss:    58.022575, Lr: 0.000099, Tokens per sec:   2782
2023-03-14 00:04:43,508 - INFO - __main__ - Epoch   2, Step:    3800, Batch Loss:    78.605774, Lr: 0.000099, Tokens per sec:   2952
2023-03-14 00:05:01,576 - INFO - __main__ - Epoch   2, Step:    3900, Batch Loss:    56.804447, Lr: 0.000099, Tokens per sec:   3010
2023-03-14 00:05:19,809 - INFO - __main__ - Epoch   2, Step:    4000, Batch Loss:    53.524681, Lr: 0.000099, Tokens per sec:   2951
2023-03-14 00:05:38,885 - INFO - __main__ - Epoch   2, Step:    4100, Batch Loss:    51.439098, Lr: 0.000099, Tokens per sec:   2746
2023-03-14 00:05:57,308 - INFO - __main__ - Epoch   2, Step:    4200, Batch Loss:    67.846405, Lr: 0.000099, Tokens per sec:   2892
2023-03-14 00:06:16,349 - INFO - __main__ - Epoch   2, Step:    4300, Batch Loss:    79.085709, Lr: 0.000099, Tokens per sec:   2859
2023-03-14 00:06:27,404 - INFO - __main__ - Epoch   2: total training loss 139184.19
2023-03-14 00:08:51,879 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 00:08:51,880 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 16.75283213737249, rouge_l = 34.80191725168568, meteor = 0
2023-03-14 00:08:52,992 - INFO - __main__ - Example #0
2023-03-14 00:08:52,992 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 00:08:52,992 - INFO - __main__ - 	Hypothesis: return the hashcode for this object .
2023-03-14 00:08:52,992 - INFO - __main__ - Example #1
2023-03-14 00:08:52,992 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 00:08:52,992 - INFO - __main__ - 	Hypothesis: call when the start of the start of the start of the start of the start .
2023-03-14 00:08:52,992 - INFO - __main__ - Example #2
2023-03-14 00:08:52,992 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 00:08:52,992 - INFO - __main__ - 	Hypothesis: update the property change listener .
2023-03-14 00:08:52,999 - INFO - __main__ - Validation time = 142.09240198135376s.
2023-03-14 00:08:52,999 - INFO - __main__ - Epoch 3
2023-03-14 00:09:00,923 - INFO - __main__ - Epoch   3, Step:    4400, Batch Loss:    70.188065, Lr: 0.000098, Tokens per sec:   2880
2023-03-14 00:09:19,284 - INFO - __main__ - Epoch   3, Step:    4500, Batch Loss:    61.957634, Lr: 0.000098, Tokens per sec:   2930
2023-03-14 00:09:37,971 - INFO - __main__ - Epoch   3, Step:    4600, Batch Loss:    46.840935, Lr: 0.000098, Tokens per sec:   2862
2023-03-14 00:09:56,351 - INFO - __main__ - Epoch   3, Step:    4700, Batch Loss:    53.860325, Lr: 0.000098, Tokens per sec:   2970
2023-03-14 00:10:14,725 - INFO - __main__ - Epoch   3, Step:    4800, Batch Loss:    69.147675, Lr: 0.000098, Tokens per sec:   2969
2023-03-14 00:10:33,548 - INFO - __main__ - Epoch   3, Step:    4900, Batch Loss:    54.518982, Lr: 0.000098, Tokens per sec:   2841
2023-03-14 00:10:52,301 - INFO - __main__ - Epoch   3, Step:    5000, Batch Loss:    43.790916, Lr: 0.000098, Tokens per sec:   2877
2023-03-14 00:11:11,123 - INFO - __main__ - Epoch   3, Step:    5100, Batch Loss:    60.808666, Lr: 0.000098, Tokens per sec:   2862
2023-03-14 00:11:30,079 - INFO - __main__ - Epoch   3, Step:    5200, Batch Loss:    57.602779, Lr: 0.000098, Tokens per sec:   2864
2023-03-14 00:11:48,765 - INFO - __main__ - Epoch   3, Step:    5300, Batch Loss:    53.476116, Lr: 0.000098, Tokens per sec:   2842
2023-03-14 00:12:07,293 - INFO - __main__ - Epoch   3, Step:    5400, Batch Loss:    54.791477, Lr: 0.000098, Tokens per sec:   2905
2023-03-14 00:12:25,527 - INFO - __main__ - Epoch   3, Step:    5500, Batch Loss:    68.693733, Lr: 0.000098, Tokens per sec:   2903
2023-03-14 00:12:44,793 - INFO - __main__ - Epoch   3, Step:    5600, Batch Loss:    52.235149, Lr: 0.000098, Tokens per sec:   2797
2023-03-14 00:13:03,766 - INFO - __main__ - Epoch   3, Step:    5700, Batch Loss:    42.769100, Lr: 0.000098, Tokens per sec:   2855
2023-03-14 00:13:22,143 - INFO - __main__ - Epoch   3, Step:    5800, Batch Loss:    74.155792, Lr: 0.000098, Tokens per sec:   2905
2023-03-14 00:13:41,436 - INFO - __main__ - Epoch   3, Step:    5900, Batch Loss:    59.163036, Lr: 0.000098, Tokens per sec:   2787
2023-03-14 00:13:59,852 - INFO - __main__ - Epoch   3, Step:    6000, Batch Loss:    60.673794, Lr: 0.000098, Tokens per sec:   2952
2023-03-14 00:14:18,678 - INFO - __main__ - Epoch   3, Step:    6100, Batch Loss:    43.274475, Lr: 0.000098, Tokens per sec:   2837
2023-03-14 00:14:37,675 - INFO - __main__ - Epoch   3, Step:    6200, Batch Loss:    54.012348, Lr: 0.000098, Tokens per sec:   2854
2023-03-14 00:14:56,374 - INFO - __main__ - Epoch   3, Step:    6300, Batch Loss:    62.334049, Lr: 0.000098, Tokens per sec:   2873
2023-03-14 00:15:15,316 - INFO - __main__ - Epoch   3, Step:    6400, Batch Loss:    48.638775, Lr: 0.000098, Tokens per sec:   2872
2023-03-14 00:15:34,642 - INFO - __main__ - Epoch   3, Step:    6500, Batch Loss:    59.732861, Lr: 0.000098, Tokens per sec:   2771
2023-03-14 00:15:41,703 - INFO - __main__ - Epoch   3: total training loss 124389.61
2023-03-14 00:15:41,703 - INFO - __main__ - Epoch 4
2023-03-14 00:15:53,945 - INFO - __main__ - Epoch   4, Step:    6600, Batch Loss:    43.929081, Lr: 0.000097, Tokens per sec:   2814
2023-03-14 00:16:12,559 - INFO - __main__ - Epoch   4, Step:    6700, Batch Loss:    57.290989, Lr: 0.000097, Tokens per sec:   2879
2023-03-14 00:16:31,364 - INFO - __main__ - Epoch   4, Step:    6800, Batch Loss:    54.396736, Lr: 0.000097, Tokens per sec:   2887
2023-03-14 00:16:50,250 - INFO - __main__ - Epoch   4, Step:    6900, Batch Loss:    53.874912, Lr: 0.000097, Tokens per sec:   2863
2023-03-14 00:17:09,547 - INFO - __main__ - Epoch   4, Step:    7000, Batch Loss:    53.621723, Lr: 0.000097, Tokens per sec:   2753
2023-03-14 00:17:28,134 - INFO - __main__ - Epoch   4, Step:    7100, Batch Loss:    63.902859, Lr: 0.000097, Tokens per sec:   2953
2023-03-14 00:17:46,903 - INFO - __main__ - Epoch   4, Step:    7200, Batch Loss:    46.568378, Lr: 0.000097, Tokens per sec:   2823
2023-03-14 00:18:05,390 - INFO - __main__ - Epoch   4, Step:    7300, Batch Loss:    57.968285, Lr: 0.000097, Tokens per sec:   2948
2023-03-14 00:18:23,730 - INFO - __main__ - Epoch   4, Step:    7400, Batch Loss:    52.965855, Lr: 0.000097, Tokens per sec:   2921
2023-03-14 00:18:42,856 - INFO - __main__ - Epoch   4, Step:    7500, Batch Loss:    47.071476, Lr: 0.000097, Tokens per sec:   2869
2023-03-14 00:19:01,794 - INFO - __main__ - Epoch   4, Step:    7600, Batch Loss:    57.154366, Lr: 0.000097, Tokens per sec:   2868
2023-03-14 00:19:20,805 - INFO - __main__ - Epoch   4, Step:    7700, Batch Loss:    50.063610, Lr: 0.000097, Tokens per sec:   2780
2023-03-14 00:19:39,238 - INFO - __main__ - Epoch   4, Step:    7800, Batch Loss:    51.369205, Lr: 0.000097, Tokens per sec:   2900
2023-03-14 00:19:58,247 - INFO - __main__ - Epoch   4, Step:    7900, Batch Loss:    63.590786, Lr: 0.000097, Tokens per sec:   2807
2023-03-14 00:20:16,616 - INFO - __main__ - Epoch   4, Step:    8000, Batch Loss:    57.640816, Lr: 0.000097, Tokens per sec:   2929
2023-03-14 00:20:35,464 - INFO - __main__ - Epoch   4, Step:    8100, Batch Loss:    55.794605, Lr: 0.000097, Tokens per sec:   2872
2023-03-14 00:20:54,091 - INFO - __main__ - Epoch   4, Step:    8200, Batch Loss:    43.323101, Lr: 0.000097, Tokens per sec:   2865
2023-03-14 00:21:12,463 - INFO - __main__ - Epoch   4, Step:    8300, Batch Loss:    41.594460, Lr: 0.000097, Tokens per sec:   2917
2023-03-14 00:21:30,798 - INFO - __main__ - Epoch   4, Step:    8400, Batch Loss:    46.891945, Lr: 0.000097, Tokens per sec:   2926
2023-03-14 00:21:49,571 - INFO - __main__ - Epoch   4, Step:    8500, Batch Loss:    52.731636, Lr: 0.000097, Tokens per sec:   2913
2023-03-14 00:22:08,808 - INFO - __main__ - Epoch   4, Step:    8600, Batch Loss:    38.953899, Lr: 0.000097, Tokens per sec:   2758
2023-03-14 00:22:27,599 - INFO - __main__ - Epoch   4, Step:    8700, Batch Loss:    42.362247, Lr: 0.000097, Tokens per sec:   2874
2023-03-14 00:22:30,628 - INFO - __main__ - Epoch   4: total training loss 113552.25
2023-03-14 00:25:32,224 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 00:25:32,224 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 20.894367772312513, rouge_l = 39.04950727741647, meteor = 0
2023-03-14 00:25:33,380 - INFO - __main__ - Example #0
2023-03-14 00:25:33,380 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 00:25:33,380 - INFO - __main__ - 	Hypothesis: return a hashcode for this object .
2023-03-14 00:25:33,380 - INFO - __main__ - Example #1
2023-03-14 00:25:33,380 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 00:25:33,380 - INFO - __main__ - 	Hypothesis: call when the start of the start of the start of the start of the start .
2023-03-14 00:25:33,380 - INFO - __main__ - Example #2
2023-03-14 00:25:33,380 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 00:25:33,380 - INFO - __main__ - 	Hypothesis: this method be call by the bind property change event . this method be call when a bind property be change .
2023-03-14 00:25:33,386 - INFO - __main__ - Validation time = 179.21711492538452s.
2023-03-14 00:25:33,386 - INFO - __main__ - Epoch 5
2023-03-14 00:25:50,378 - INFO - __main__ - Epoch   5, Step:    8800, Batch Loss:    50.082394, Lr: 0.000096, Tokens per sec:   2639
2023-03-14 00:26:09,016 - INFO - __main__ - Epoch   5, Step:    8900, Batch Loss:    55.062511, Lr: 0.000096, Tokens per sec:   2913
2023-03-14 00:26:28,693 - INFO - __main__ - Epoch   5, Step:    9000, Batch Loss:    48.913822, Lr: 0.000096, Tokens per sec:   2743
2023-03-14 00:26:48,598 - INFO - __main__ - Epoch   5, Step:    9100, Batch Loss:    42.600872, Lr: 0.000096, Tokens per sec:   2656
2023-03-14 00:27:08,529 - INFO - __main__ - Epoch   5, Step:    9200, Batch Loss:    56.539604, Lr: 0.000096, Tokens per sec:   2733
2023-03-14 00:27:28,429 - INFO - __main__ - Epoch   5, Step:    9300, Batch Loss:    44.473953, Lr: 0.000096, Tokens per sec:   2669
2023-03-14 00:27:48,382 - INFO - __main__ - Epoch   5, Step:    9400, Batch Loss:    40.435658, Lr: 0.000096, Tokens per sec:   2693
2023-03-14 00:28:08,314 - INFO - __main__ - Epoch   5, Step:    9500, Batch Loss:    42.215160, Lr: 0.000096, Tokens per sec:   2673
2023-03-14 00:28:26,594 - INFO - __main__ - Epoch   5, Step:    9600, Batch Loss:    63.668423, Lr: 0.000096, Tokens per sec:   2917
2023-03-14 00:28:45,418 - INFO - __main__ - Epoch   5, Step:    9700, Batch Loss:    51.187031, Lr: 0.000096, Tokens per sec:   2913
2023-03-14 00:29:05,215 - INFO - __main__ - Epoch   5, Step:    9800, Batch Loss:    36.371155, Lr: 0.000096, Tokens per sec:   2717
2023-03-14 00:29:24,839 - INFO - __main__ - Epoch   5, Step:    9900, Batch Loss:    49.739285, Lr: 0.000096, Tokens per sec:   2697
2023-03-14 00:29:44,925 - INFO - __main__ - Epoch   5, Step:   10000, Batch Loss:    49.182335, Lr: 0.000096, Tokens per sec:   2694
2023-03-14 00:30:04,359 - INFO - __main__ - Epoch   5, Step:   10100, Batch Loss:    54.179794, Lr: 0.000096, Tokens per sec:   2808
2023-03-14 00:30:23,274 - INFO - __main__ - Epoch   5, Step:   10200, Batch Loss:    50.584427, Lr: 0.000096, Tokens per sec:   2866
2023-03-14 00:30:43,221 - INFO - __main__ - Epoch   5, Step:   10300, Batch Loss:    49.020332, Lr: 0.000096, Tokens per sec:   2719
2023-03-14 00:31:03,143 - INFO - __main__ - Epoch   5, Step:   10400, Batch Loss:    54.552380, Lr: 0.000096, Tokens per sec:   2707
2023-03-14 00:31:23,074 - INFO - __main__ - Epoch   5, Step:   10500, Batch Loss:    49.486958, Lr: 0.000096, Tokens per sec:   2728
2023-03-14 00:31:42,629 - INFO - __main__ - Epoch   5, Step:   10600, Batch Loss:    37.543941, Lr: 0.000096, Tokens per sec:   2769
2023-03-14 00:32:00,558 - INFO - __main__ - Epoch   5, Step:   10700, Batch Loss:    46.779270, Lr: 0.000096, Tokens per sec:   2982
2023-03-14 00:32:19,371 - INFO - __main__ - Epoch   5, Step:   10800, Batch Loss:    39.028111, Lr: 0.000096, Tokens per sec:   2871
2023-03-14 00:32:38,162 - INFO - __main__ - Epoch   5: total training loss 104716.12
2023-03-14 00:32:38,162 - INFO - __main__ - Epoch 6
2023-03-14 00:32:39,425 - INFO - __main__ - Epoch   6, Step:   10900, Batch Loss:    51.778893, Lr: 0.000095, Tokens per sec:   2395
2023-03-14 00:32:58,410 - INFO - __main__ - Epoch   6, Step:   11000, Batch Loss:    31.818235, Lr: 0.000095, Tokens per sec:   2885
2023-03-14 00:33:16,426 - INFO - __main__ - Epoch   6, Step:   11100, Batch Loss:    50.189049, Lr: 0.000095, Tokens per sec:   3017
2023-03-14 00:33:35,564 - INFO - __main__ - Epoch   6, Step:   11200, Batch Loss:    41.592613, Lr: 0.000095, Tokens per sec:   2815
2023-03-14 00:33:54,511 - INFO - __main__ - Epoch   6, Step:   11300, Batch Loss:    63.402607, Lr: 0.000095, Tokens per sec:   2836
2023-03-14 00:34:12,546 - INFO - __main__ - Epoch   6, Step:   11400, Batch Loss:    35.151264, Lr: 0.000095, Tokens per sec:   2995
2023-03-14 00:34:30,871 - INFO - __main__ - Epoch   6, Step:   11500, Batch Loss:    32.183681, Lr: 0.000095, Tokens per sec:   2925
2023-03-14 00:34:48,923 - INFO - __main__ - Epoch   6, Step:   11600, Batch Loss:    36.649654, Lr: 0.000095, Tokens per sec:   2943
2023-03-14 00:35:06,962 - INFO - __main__ - Epoch   6, Step:   11700, Batch Loss:    48.108700, Lr: 0.000095, Tokens per sec:   2974
2023-03-14 00:35:25,899 - INFO - __main__ - Epoch   6, Step:   11800, Batch Loss:    35.848965, Lr: 0.000095, Tokens per sec:   2823
2023-03-14 00:35:45,841 - INFO - __main__ - Epoch   6, Step:   11900, Batch Loss:    48.276466, Lr: 0.000095, Tokens per sec:   2682
2023-03-14 00:36:05,961 - INFO - __main__ - Epoch   6, Step:   12000, Batch Loss:    48.321918, Lr: 0.000095, Tokens per sec:   2672
2023-03-14 00:36:25,929 - INFO - __main__ - Epoch   6, Step:   12100, Batch Loss:    40.901482, Lr: 0.000095, Tokens per sec:   2683
2023-03-14 00:36:45,851 - INFO - __main__ - Epoch   6, Step:   12200, Batch Loss:    35.707806, Lr: 0.000095, Tokens per sec:   2719
2023-03-14 00:37:05,821 - INFO - __main__ - Epoch   6, Step:   12300, Batch Loss:    56.018478, Lr: 0.000095, Tokens per sec:   2745
2023-03-14 00:37:25,658 - INFO - __main__ - Epoch   6, Step:   12400, Batch Loss:    37.041344, Lr: 0.000095, Tokens per sec:   2710
2023-03-14 00:37:43,646 - INFO - __main__ - Epoch   6, Step:   12500, Batch Loss:    41.675869, Lr: 0.000095, Tokens per sec:   2961
2023-03-14 00:38:03,128 - INFO - __main__ - Epoch   6, Step:   12600, Batch Loss:    36.910122, Lr: 0.000095, Tokens per sec:   2778
2023-03-14 00:38:23,034 - INFO - __main__ - Epoch   6, Step:   12700, Batch Loss:    54.334084, Lr: 0.000095, Tokens per sec:   2702
2023-03-14 00:38:42,487 - INFO - __main__ - Epoch   6, Step:   12800, Batch Loss:    47.333824, Lr: 0.000095, Tokens per sec:   2747
2023-03-14 00:39:02,423 - INFO - __main__ - Epoch   6, Step:   12900, Batch Loss:    39.278130, Lr: 0.000095, Tokens per sec:   2677
2023-03-14 00:39:22,378 - INFO - __main__ - Epoch   6, Step:   13000, Batch Loss:    45.835579, Lr: 0.000095, Tokens per sec:   2714
2023-03-14 00:39:37,204 - INFO - __main__ - Epoch   6: total training loss 97076.59
2023-03-14 00:42:56,746 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 00:42:56,746 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 24.482772272449385, rouge_l = 42.53323927483007, meteor = 0
2023-03-14 00:42:57,864 - INFO - __main__ - Example #0
2023-03-14 00:42:57,864 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 00:42:57,864 - INFO - __main__ - 	Hypothesis: return a hash code for this object .
2023-03-14 00:42:57,864 - INFO - __main__ - Example #1
2023-03-14 00:42:57,864 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 00:42:57,864 - INFO - __main__ - 	Hypothesis: call when the start of the start of the start .
2023-03-14 00:42:57,864 - INFO - __main__ - Example #2
2023-03-14 00:42:57,864 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 00:42:57,864 - INFO - __main__ - 	Hypothesis: update the bind property change .
2023-03-14 00:42:57,870 - INFO - __main__ - Validation time = 196.7185297012329s.
2023-03-14 00:42:57,870 - INFO - __main__ - Epoch 7
2023-03-14 00:43:03,291 - INFO - __main__ - Epoch   7, Step:   13100, Batch Loss:    34.909660, Lr: 0.000094, Tokens per sec:   2661
2023-03-14 00:43:23,199 - INFO - __main__ - Epoch   7, Step:   13200, Batch Loss:    42.208206, Lr: 0.000094, Tokens per sec:   2697
2023-03-14 00:43:43,143 - INFO - __main__ - Epoch   7, Step:   13300, Batch Loss:    48.641613, Lr: 0.000094, Tokens per sec:   2730
2023-03-14 00:44:03,174 - INFO - __main__ - Epoch   7, Step:   13400, Batch Loss:    46.932701, Lr: 0.000094, Tokens per sec:   2668
2023-03-14 00:44:22,557 - INFO - __main__ - Epoch   7, Step:   13500, Batch Loss:    38.950115, Lr: 0.000094, Tokens per sec:   2783
2023-03-14 00:44:42,568 - INFO - __main__ - Epoch   7, Step:   13600, Batch Loss:    32.502266, Lr: 0.000094, Tokens per sec:   2669
2023-03-14 00:45:02,477 - INFO - __main__ - Epoch   7, Step:   13700, Batch Loss:    39.120438, Lr: 0.000094, Tokens per sec:   2673
2023-03-14 00:45:22,431 - INFO - __main__ - Epoch   7, Step:   13800, Batch Loss:    54.072056, Lr: 0.000094, Tokens per sec:   2654
2023-03-14 00:45:42,391 - INFO - __main__ - Epoch   7, Step:   13900, Batch Loss:    36.837109, Lr: 0.000094, Tokens per sec:   2729
2023-03-14 00:46:02,428 - INFO - __main__ - Epoch   7, Step:   14000, Batch Loss:    47.678406, Lr: 0.000094, Tokens per sec:   2711
2023-03-14 00:46:21,370 - INFO - __main__ - Epoch   7, Step:   14100, Batch Loss:    40.929386, Lr: 0.000094, Tokens per sec:   2837
2023-03-14 00:46:39,846 - INFO - __main__ - Epoch   7, Step:   14200, Batch Loss:    30.560003, Lr: 0.000094, Tokens per sec:   2946
2023-03-14 00:46:59,370 - INFO - __main__ - Epoch   7, Step:   14300, Batch Loss:    39.017853, Lr: 0.000094, Tokens per sec:   2754
2023-03-14 00:47:17,413 - INFO - __main__ - Epoch   7, Step:   14400, Batch Loss:    46.797619, Lr: 0.000094, Tokens per sec:   3013
2023-03-14 00:47:37,299 - INFO - __main__ - Epoch   7, Step:   14500, Batch Loss:    48.569481, Lr: 0.000094, Tokens per sec:   2751
2023-03-14 00:47:57,291 - INFO - __main__ - Epoch   7, Step:   14600, Batch Loss:    46.668331, Lr: 0.000094, Tokens per sec:   2694
2023-03-14 00:48:17,134 - INFO - __main__ - Epoch   7, Step:   14700, Batch Loss:    41.373272, Lr: 0.000094, Tokens per sec:   2698
2023-03-14 00:48:37,014 - INFO - __main__ - Epoch   7, Step:   14800, Batch Loss:    44.394356, Lr: 0.000094, Tokens per sec:   2698
2023-03-14 00:48:56,984 - INFO - __main__ - Epoch   7, Step:   14900, Batch Loss:    41.164421, Lr: 0.000094, Tokens per sec:   2684
2023-03-14 00:49:16,909 - INFO - __main__ - Epoch   7, Step:   15000, Batch Loss:    36.581928, Lr: 0.000094, Tokens per sec:   2680
2023-03-14 00:49:35,682 - INFO - __main__ - Epoch   7, Step:   15100, Batch Loss:    28.760344, Lr: 0.000094, Tokens per sec:   2842
2023-03-14 00:49:55,294 - INFO - __main__ - Epoch   7, Step:   15200, Batch Loss:    41.205704, Lr: 0.000094, Tokens per sec:   2717
2023-03-14 00:50:05,873 - INFO - __main__ - Epoch   7: total training loss 90314.03
2023-03-14 00:50:05,873 - INFO - __main__ - Epoch 8
2023-03-14 00:50:15,450 - INFO - __main__ - Epoch   8, Step:   15300, Batch Loss:    42.835056, Lr: 0.000093, Tokens per sec:   2589
2023-03-14 00:50:34,445 - INFO - __main__ - Epoch   8, Step:   15400, Batch Loss:    41.301521, Lr: 0.000093, Tokens per sec:   2879
2023-03-14 00:50:54,318 - INFO - __main__ - Epoch   8, Step:   15500, Batch Loss:    54.134758, Lr: 0.000093, Tokens per sec:   2709
2023-03-14 00:51:13,134 - INFO - __main__ - Epoch   8, Step:   15600, Batch Loss:    43.856071, Lr: 0.000093, Tokens per sec:   2892
2023-03-14 00:51:31,393 - INFO - __main__ - Epoch   8, Step:   15700, Batch Loss:    40.279835, Lr: 0.000093, Tokens per sec:   2981
2023-03-14 00:51:49,422 - INFO - __main__ - Epoch   8, Step:   15800, Batch Loss:    30.892061, Lr: 0.000093, Tokens per sec:   2976
2023-03-14 00:52:07,391 - INFO - __main__ - Epoch   8, Step:   15900, Batch Loss:    54.588383, Lr: 0.000093, Tokens per sec:   2993
2023-03-14 00:52:25,386 - INFO - __main__ - Epoch   8, Step:   16000, Batch Loss:    37.214600, Lr: 0.000093, Tokens per sec:   2956
2023-03-14 00:52:44,453 - INFO - __main__ - Epoch   8, Step:   16100, Batch Loss:    41.171482, Lr: 0.000093, Tokens per sec:   2837
2023-03-14 00:53:02,576 - INFO - __main__ - Epoch   8, Step:   16200, Batch Loss:    41.208542, Lr: 0.000093, Tokens per sec:   2994
2023-03-14 00:53:20,686 - INFO - __main__ - Epoch   8, Step:   16300, Batch Loss:    36.246899, Lr: 0.000093, Tokens per sec:   2918
2023-03-14 00:53:40,565 - INFO - __main__ - Epoch   8, Step:   16400, Batch Loss:    31.305887, Lr: 0.000093, Tokens per sec:   2731
2023-03-14 00:54:00,341 - INFO - __main__ - Epoch   8, Step:   16500, Batch Loss:    43.089649, Lr: 0.000093, Tokens per sec:   2732
2023-03-14 00:54:18,696 - INFO - __main__ - Epoch   8, Step:   16600, Batch Loss:    44.302174, Lr: 0.000093, Tokens per sec:   2920
2023-03-14 00:54:38,098 - INFO - __main__ - Epoch   8, Step:   16700, Batch Loss:    46.989693, Lr: 0.000093, Tokens per sec:   2731
2023-03-14 00:54:57,056 - INFO - __main__ - Epoch   8, Step:   16800, Batch Loss:    40.931488, Lr: 0.000093, Tokens per sec:   2848
2023-03-14 00:55:15,034 - INFO - __main__ - Epoch   8, Step:   16900, Batch Loss:    31.651049, Lr: 0.000093, Tokens per sec:   3000
2023-03-14 00:55:34,363 - INFO - __main__ - Epoch   8, Step:   17000, Batch Loss:    45.982975, Lr: 0.000093, Tokens per sec:   2809
2023-03-14 00:55:52,811 - INFO - __main__ - Epoch   8, Step:   17100, Batch Loss:    51.077709, Lr: 0.000093, Tokens per sec:   2897
2023-03-14 00:56:12,780 - INFO - __main__ - Epoch   8, Step:   17200, Batch Loss:    35.450874, Lr: 0.000093, Tokens per sec:   2704
2023-03-14 00:56:32,746 - INFO - __main__ - Epoch   8, Step:   17300, Batch Loss:    43.625427, Lr: 0.000093, Tokens per sec:   2670
2023-03-14 00:56:52,703 - INFO - __main__ - Epoch   8, Step:   17400, Batch Loss:    46.261734, Lr: 0.000093, Tokens per sec:   2719
2023-03-14 00:56:58,577 - INFO - __main__ - Epoch   8: total training loss 84235.79
2023-03-14 01:00:31,403 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 01:00:31,404 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 27.11579370288823, rouge_l = 44.52647545152218, meteor = 0
2023-03-14 01:00:32,534 - INFO - __main__ - Delete test_dir3/4358.ckpt
2023-03-14 01:00:32,632 - INFO - __main__ - Example #0
2023-03-14 01:00:32,632 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 01:00:32,632 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 01:00:32,632 - INFO - __main__ - Example #1
2023-03-14 01:00:32,632 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 01:00:32,632 - INFO - __main__ - 	Hypothesis: call when the start of the view be start .
2023-03-14 01:00:32,632 - INFO - __main__ - Example #2
2023-03-14 01:00:32,632 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 01:00:32,632 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this method be call when a bind property change event be change .
2023-03-14 01:00:32,638 - INFO - __main__ - Validation time = 210.21036028862s.
2023-03-14 01:00:32,638 - INFO - __main__ - Epoch 9
2023-03-14 01:00:46,430 - INFO - __main__ - Epoch   9, Step:   17500, Batch Loss:    35.644348, Lr: 0.000092, Tokens per sec:   2638
2023-03-14 01:01:06,388 - INFO - __main__ - Epoch   9, Step:   17600, Batch Loss:    30.633320, Lr: 0.000092, Tokens per sec:   2694
2023-03-14 01:01:26,309 - INFO - __main__ - Epoch   9, Step:   17700, Batch Loss:    31.647120, Lr: 0.000092, Tokens per sec:   2739
2023-03-14 01:01:45,981 - INFO - __main__ - Epoch   9, Step:   17800, Batch Loss:    24.885015, Lr: 0.000092, Tokens per sec:   2758
2023-03-14 01:02:04,537 - INFO - __main__ - Epoch   9, Step:   17900, Batch Loss:    32.103764, Lr: 0.000092, Tokens per sec:   2878
2023-03-14 01:02:24,040 - INFO - __main__ - Epoch   9, Step:   18000, Batch Loss:    34.964622, Lr: 0.000092, Tokens per sec:   2754
2023-03-14 01:02:44,021 - INFO - __main__ - Epoch   9, Step:   18100, Batch Loss:    43.654259, Lr: 0.000092, Tokens per sec:   2708
2023-03-14 01:03:03,996 - INFO - __main__ - Epoch   9, Step:   18200, Batch Loss:    25.699091, Lr: 0.000092, Tokens per sec:   2765
2023-03-14 01:03:23,960 - INFO - __main__ - Epoch   9, Step:   18300, Batch Loss:    21.473610, Lr: 0.000092, Tokens per sec:   2696
2023-03-14 01:03:43,820 - INFO - __main__ - Epoch   9, Step:   18400, Batch Loss:    42.645893, Lr: 0.000092, Tokens per sec:   2690
2023-03-14 01:04:03,470 - INFO - __main__ - Epoch   9, Step:   18500, Batch Loss:    41.751129, Lr: 0.000092, Tokens per sec:   2756
2023-03-14 01:04:23,447 - INFO - __main__ - Epoch   9, Step:   18600, Batch Loss:    53.729115, Lr: 0.000092, Tokens per sec:   2690
2023-03-14 01:04:43,399 - INFO - __main__ - Epoch   9, Step:   18700, Batch Loss:    21.024149, Lr: 0.000092, Tokens per sec:   2714
2023-03-14 01:05:03,395 - INFO - __main__ - Epoch   9, Step:   18800, Batch Loss:    42.875492, Lr: 0.000092, Tokens per sec:   2679
2023-03-14 01:05:23,348 - INFO - __main__ - Epoch   9, Step:   18900, Batch Loss:    35.962708, Lr: 0.000092, Tokens per sec:   2687
2023-03-14 01:05:43,299 - INFO - __main__ - Epoch   9, Step:   19000, Batch Loss:    41.631374, Lr: 0.000092, Tokens per sec:   2691
2023-03-14 01:06:03,174 - INFO - __main__ - Epoch   9, Step:   19100, Batch Loss:    39.270432, Lr: 0.000092, Tokens per sec:   2734
2023-03-14 01:06:23,109 - INFO - __main__ - Epoch   9, Step:   19200, Batch Loss:    40.940952, Lr: 0.000092, Tokens per sec:   2681
2023-03-14 01:06:43,149 - INFO - __main__ - Epoch   9, Step:   19300, Batch Loss:    27.657919, Lr: 0.000092, Tokens per sec:   2650
2023-03-14 01:07:03,018 - INFO - __main__ - Epoch   9, Step:   19400, Batch Loss:    34.435345, Lr: 0.000092, Tokens per sec:   2725
2023-03-14 01:07:22,875 - INFO - __main__ - Epoch   9, Step:   19500, Batch Loss:    33.185863, Lr: 0.000092, Tokens per sec:   2679
2023-03-14 01:07:40,826 - INFO - __main__ - Epoch   9, Step:   19600, Batch Loss:    33.327362, Lr: 0.000092, Tokens per sec:   2979
2023-03-14 01:07:42,824 - INFO - __main__ - Epoch   9: total training loss 78708.68
2023-03-14 01:07:42,825 - INFO - __main__ - Epoch 10
2023-03-14 01:08:00,287 - INFO - __main__ - Epoch  10, Step:   19700, Batch Loss:    32.524643, Lr: 0.000091, Tokens per sec:   2740
2023-03-14 01:08:18,218 - INFO - __main__ - Epoch  10, Step:   19800, Batch Loss:    37.391178, Lr: 0.000091, Tokens per sec:   3026
2023-03-14 01:08:37,531 - INFO - __main__ - Epoch  10, Step:   19900, Batch Loss:    29.924963, Lr: 0.000091, Tokens per sec:   2839
2023-03-14 01:08:56,085 - INFO - __main__ - Epoch  10, Step:   20000, Batch Loss:    31.102755, Lr: 0.000091, Tokens per sec:   2866
2023-03-14 01:09:15,658 - INFO - __main__ - Epoch  10, Step:   20100, Batch Loss:    34.480526, Lr: 0.000091, Tokens per sec:   2785
2023-03-14 01:09:33,672 - INFO - __main__ - Epoch  10, Step:   20200, Batch Loss:    36.520367, Lr: 0.000091, Tokens per sec:   3009
2023-03-14 01:09:51,803 - INFO - __main__ - Epoch  10, Step:   20300, Batch Loss:    37.869556, Lr: 0.000091, Tokens per sec:   2993
2023-03-14 01:10:09,906 - INFO - __main__ - Epoch  10, Step:   20400, Batch Loss:    34.552898, Lr: 0.000091, Tokens per sec:   2916
2023-03-14 01:10:29,713 - INFO - __main__ - Epoch  10, Step:   20500, Batch Loss:    23.958908, Lr: 0.000091, Tokens per sec:   2741
2023-03-14 01:10:49,248 - INFO - __main__ - Epoch  10, Step:   20600, Batch Loss:    42.352287, Lr: 0.000091, Tokens per sec:   2785
2023-03-14 01:11:07,860 - INFO - __main__ - Epoch  10, Step:   20700, Batch Loss:    42.403973, Lr: 0.000091, Tokens per sec:   2872
2023-03-14 01:11:27,486 - INFO - __main__ - Epoch  10, Step:   20800, Batch Loss:    42.387150, Lr: 0.000091, Tokens per sec:   2720
2023-03-14 01:11:47,389 - INFO - __main__ - Epoch  10, Step:   20900, Batch Loss:    29.741524, Lr: 0.000091, Tokens per sec:   2690
2023-03-14 01:12:06,484 - INFO - __main__ - Epoch  10, Step:   21000, Batch Loss:    30.293657, Lr: 0.000091, Tokens per sec:   2843
2023-03-14 01:12:24,505 - INFO - __main__ - Epoch  10, Step:   21100, Batch Loss:    37.706196, Lr: 0.000091, Tokens per sec:   2943
2023-03-14 01:12:44,505 - INFO - __main__ - Epoch  10, Step:   21200, Batch Loss:    22.908844, Lr: 0.000091, Tokens per sec:   2672
2023-03-14 01:13:04,017 - INFO - __main__ - Epoch  10, Step:   21300, Batch Loss:    34.996918, Lr: 0.000091, Tokens per sec:   2739
2023-03-14 01:13:23,472 - INFO - __main__ - Epoch  10, Step:   21400, Batch Loss:    40.991245, Lr: 0.000091, Tokens per sec:   2781
2023-03-14 01:13:42,010 - INFO - __main__ - Epoch  10, Step:   21500, Batch Loss:    35.878704, Lr: 0.000091, Tokens per sec:   2897
2023-03-14 01:14:01,927 - INFO - __main__ - Epoch  10, Step:   21600, Batch Loss:    15.110545, Lr: 0.000091, Tokens per sec:   2673
2023-03-14 01:14:20,354 - INFO - __main__ - Epoch  10, Step:   21700, Batch Loss:    37.302673, Lr: 0.000091, Tokens per sec:   2983
2023-03-14 01:14:36,752 - INFO - __main__ - Epoch  10: total training loss 73712.24
2023-03-14 01:18:09,349 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 01:18:09,350 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 29.03074380981718, rouge_l = 45.88693282827569, meteor = 0
2023-03-14 01:18:10,481 - INFO - __main__ - Delete test_dir3/8716.ckpt
2023-03-14 01:18:10,624 - INFO - __main__ - Example #0
2023-03-14 01:18:10,624 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 01:18:10,624 - INFO - __main__ - 	Hypothesis: return a hashcode for this principal .
2023-03-14 01:18:10,624 - INFO - __main__ - Example #1
2023-03-14 01:18:10,624 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 01:18:10,624 - INFO - __main__ - 	Hypothesis: call when the view be start .
2023-03-14 01:18:10,624 - INFO - __main__ - Example #2
2023-03-14 01:18:10,624 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 01:18:10,624 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui component subclass of jtext component subclass .
2023-03-14 01:18:10,630 - INFO - __main__ - Validation time = 210.01223850250244s.
2023-03-14 01:18:10,630 - INFO - __main__ - Epoch 11
2023-03-14 01:18:12,861 - INFO - __main__ - Epoch  11, Step:   21800, Batch Loss:    26.258141, Lr: 0.000090, Tokens per sec:   2314
2023-03-14 01:18:31,533 - INFO - __main__ - Epoch  11, Step:   21900, Batch Loss:    28.307873, Lr: 0.000090, Tokens per sec:   2910
2023-03-14 01:18:49,525 - INFO - __main__ - Epoch  11, Step:   22000, Batch Loss:    34.352764, Lr: 0.000090, Tokens per sec:   2923
2023-03-14 01:19:09,090 - INFO - __main__ - Epoch  11, Step:   22100, Batch Loss:    34.641090, Lr: 0.000090, Tokens per sec:   2754
2023-03-14 01:19:27,221 - INFO - __main__ - Epoch  11, Step:   22200, Batch Loss:    24.957750, Lr: 0.000090, Tokens per sec:   2960
2023-03-14 01:19:46,596 - INFO - __main__ - Epoch  11, Step:   22300, Batch Loss:    25.420166, Lr: 0.000090, Tokens per sec:   2783
2023-03-14 01:20:05,105 - INFO - __main__ - Epoch  11, Step:   22400, Batch Loss:    27.996534, Lr: 0.000090, Tokens per sec:   2907
2023-03-14 01:20:23,286 - INFO - __main__ - Epoch  11, Step:   22500, Batch Loss:    27.728653, Lr: 0.000090, Tokens per sec:   2990
2023-03-14 01:20:42,866 - INFO - __main__ - Epoch  11, Step:   22600, Batch Loss:    26.823153, Lr: 0.000090, Tokens per sec:   2731
2023-03-14 01:21:01,670 - INFO - __main__ - Epoch  11, Step:   22700, Batch Loss:    35.556137, Lr: 0.000090, Tokens per sec:   2905
2023-03-14 01:21:20,290 - INFO - __main__ - Epoch  11, Step:   22800, Batch Loss:    28.196997, Lr: 0.000090, Tokens per sec:   2946
2023-03-14 01:21:39,949 - INFO - __main__ - Epoch  11, Step:   22900, Batch Loss:    37.165581, Lr: 0.000090, Tokens per sec:   2698
2023-03-14 01:21:59,867 - INFO - __main__ - Epoch  11, Step:   23000, Batch Loss:    23.922344, Lr: 0.000090, Tokens per sec:   2724
2023-03-14 01:22:19,840 - INFO - __main__ - Epoch  11, Step:   23100, Batch Loss:    24.242714, Lr: 0.000090, Tokens per sec:   2689
2023-03-14 01:22:39,557 - INFO - __main__ - Epoch  11, Step:   23200, Batch Loss:    34.240757, Lr: 0.000090, Tokens per sec:   2746
2023-03-14 01:22:58,197 - INFO - __main__ - Epoch  11, Step:   23300, Batch Loss:    29.206327, Lr: 0.000090, Tokens per sec:   2855
2023-03-14 01:23:16,323 - INFO - __main__ - Epoch  11, Step:   23400, Batch Loss:    31.131975, Lr: 0.000090, Tokens per sec:   2950
2023-03-14 01:23:34,344 - INFO - __main__ - Epoch  11, Step:   23500, Batch Loss:    35.852623, Lr: 0.000090, Tokens per sec:   3000
2023-03-14 01:23:52,199 - INFO - __main__ - Epoch  11, Step:   23600, Batch Loss:    29.680332, Lr: 0.000090, Tokens per sec:   3045
2023-03-14 01:24:10,146 - INFO - __main__ - Epoch  11, Step:   23700, Batch Loss:    26.259840, Lr: 0.000090, Tokens per sec:   2934
2023-03-14 01:24:28,919 - INFO - __main__ - Epoch  11, Step:   23800, Batch Loss:    34.885136, Lr: 0.000090, Tokens per sec:   2854
2023-03-14 01:24:48,860 - INFO - __main__ - Epoch  11, Step:   23900, Batch Loss:    33.509888, Lr: 0.000090, Tokens per sec:   2738
2023-03-14 01:25:02,644 - INFO - __main__ - Epoch  11: total training loss 69114.90
2023-03-14 01:25:02,645 - INFO - __main__ - Epoch 12
2023-03-14 01:25:09,092 - INFO - __main__ - Epoch  12, Step:   24000, Batch Loss:    24.029694, Lr: 0.000090, Tokens per sec:   2579
2023-03-14 01:25:29,000 - INFO - __main__ - Epoch  12, Step:   24100, Batch Loss:    31.277363, Lr: 0.000090, Tokens per sec:   2688
2023-03-14 01:25:49,097 - INFO - __main__ - Epoch  12, Step:   24200, Batch Loss:    29.684332, Lr: 0.000090, Tokens per sec:   2640
2023-03-14 01:26:09,231 - INFO - __main__ - Epoch  12, Step:   24300, Batch Loss:    32.791714, Lr: 0.000090, Tokens per sec:   2668
2023-03-14 01:26:29,222 - INFO - __main__ - Epoch  12, Step:   24400, Batch Loss:    30.670980, Lr: 0.000090, Tokens per sec:   2688
2023-03-14 01:26:48,343 - INFO - __main__ - Epoch  12, Step:   24500, Batch Loss:    28.032919, Lr: 0.000090, Tokens per sec:   2834
2023-03-14 01:27:06,379 - INFO - __main__ - Epoch  12, Step:   24600, Batch Loss:    21.657969, Lr: 0.000090, Tokens per sec:   2953
2023-03-14 01:27:25,226 - INFO - __main__ - Epoch  12, Step:   24700, Batch Loss:    24.193840, Lr: 0.000090, Tokens per sec:   2864
2023-03-14 01:27:43,216 - INFO - __main__ - Epoch  12, Step:   24800, Batch Loss:    30.310839, Lr: 0.000090, Tokens per sec:   3003
2023-03-14 01:28:01,201 - INFO - __main__ - Epoch  12, Step:   24900, Batch Loss:    32.781281, Lr: 0.000090, Tokens per sec:   3020
2023-03-14 01:28:19,538 - INFO - __main__ - Epoch  12, Step:   25000, Batch Loss:    15.882320, Lr: 0.000090, Tokens per sec:   2913
2023-03-14 01:28:37,571 - INFO - __main__ - Epoch  12, Step:   25100, Batch Loss:    35.512768, Lr: 0.000090, Tokens per sec:   2979
2023-03-14 01:28:57,355 - INFO - __main__ - Epoch  12, Step:   25200, Batch Loss:    30.603910, Lr: 0.000090, Tokens per sec:   2755
2023-03-14 01:29:15,772 - INFO - __main__ - Epoch  12, Step:   25300, Batch Loss:    23.597801, Lr: 0.000090, Tokens per sec:   2884
2023-03-14 01:29:33,842 - INFO - __main__ - Epoch  12, Step:   25400, Batch Loss:    24.569088, Lr: 0.000090, Tokens per sec:   3020
2023-03-14 01:29:53,797 - INFO - __main__ - Epoch  12, Step:   25500, Batch Loss:    23.640236, Lr: 0.000090, Tokens per sec:   2674
2023-03-14 01:30:11,768 - INFO - __main__ - Epoch  12, Step:   25600, Batch Loss:    28.695469, Lr: 0.000090, Tokens per sec:   2968
2023-03-14 01:30:30,663 - INFO - __main__ - Epoch  12, Step:   25700, Batch Loss:    41.950005, Lr: 0.000090, Tokens per sec:   2866
2023-03-14 01:30:50,619 - INFO - __main__ - Epoch  12, Step:   25800, Batch Loss:    31.097662, Lr: 0.000090, Tokens per sec:   2716
2023-03-14 01:31:10,550 - INFO - __main__ - Epoch  12, Step:   25900, Batch Loss:    37.905865, Lr: 0.000090, Tokens per sec:   2708
2023-03-14 01:31:30,483 - INFO - __main__ - Epoch  12, Step:   26000, Batch Loss:    26.514189, Lr: 0.000090, Tokens per sec:   2687
2023-03-14 01:31:50,410 - INFO - __main__ - Epoch  12, Step:   26100, Batch Loss:    23.930185, Lr: 0.000090, Tokens per sec:   2755
2023-03-14 01:32:00,058 - INFO - __main__ - Epoch  12: total training loss 64864.11
2023-03-14 01:35:39,151 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 01:35:39,151 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 31.02411710766222, rouge_l = 47.22492937154941, meteor = 0
2023-03-14 01:35:40,324 - INFO - __main__ - Delete test_dir3/13074.ckpt
2023-03-14 01:35:40,460 - INFO - __main__ - Example #0
2023-03-14 01:35:40,460 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 01:35:40,460 - INFO - __main__ - 	Hypothesis: return a hash code for this principal .
2023-03-14 01:35:40,460 - INFO - __main__ - Example #1
2023-03-14 01:35:40,460 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 01:35:40,460 - INFO - __main__ - 	Hypothesis: call when the view be start .
2023-03-14 01:35:40,460 - INFO - __main__ - Example #2
2023-03-14 01:35:40,460 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 01:35:40,460 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this component
2023-03-14 01:35:40,467 - INFO - __main__ - Validation time = 216.15330743789673s.
2023-03-14 01:35:40,467 - INFO - __main__ - Epoch 13
2023-03-14 01:35:51,106 - INFO - __main__ - Epoch  13, Step:   26200, Batch Loss:    19.780203, Lr: 0.000089, Tokens per sec:   2639
2023-03-14 01:36:11,002 - INFO - __main__ - Epoch  13, Step:   26300, Batch Loss:    22.093525, Lr: 0.000089, Tokens per sec:   2695
2023-03-14 01:36:30,951 - INFO - __main__ - Epoch  13, Step:   26400, Batch Loss:    28.811733, Lr: 0.000089, Tokens per sec:   2698
2023-03-14 01:36:50,661 - INFO - __main__ - Epoch  13, Step:   26500, Batch Loss:    20.491596, Lr: 0.000089, Tokens per sec:   2717
2023-03-14 01:37:09,416 - INFO - __main__ - Epoch  13, Step:   26600, Batch Loss:    30.358681, Lr: 0.000089, Tokens per sec:   2874
2023-03-14 01:37:29,398 - INFO - __main__ - Epoch  13, Step:   26700, Batch Loss:    21.248993, Lr: 0.000089, Tokens per sec:   2717
2023-03-14 01:37:49,093 - INFO - __main__ - Epoch  13, Step:   26800, Batch Loss:    31.205696, Lr: 0.000089, Tokens per sec:   2762
2023-03-14 01:38:07,139 - INFO - __main__ - Epoch  13, Step:   26900, Batch Loss:    28.239347, Lr: 0.000089, Tokens per sec:   2975
2023-03-14 01:38:25,222 - INFO - __main__ - Epoch  13, Step:   27000, Batch Loss:    27.144833, Lr: 0.000089, Tokens per sec:   2997
2023-03-14 01:38:43,326 - INFO - __main__ - Epoch  13, Step:   27100, Batch Loss:    22.697268, Lr: 0.000089, Tokens per sec:   3000
2023-03-14 01:39:03,217 - INFO - __main__ - Epoch  13, Step:   27200, Batch Loss:    27.516499, Lr: 0.000089, Tokens per sec:   2728
2023-03-14 01:39:23,097 - INFO - __main__ - Epoch  13, Step:   27300, Batch Loss:    35.943031, Lr: 0.000089, Tokens per sec:   2703
2023-03-14 01:39:42,991 - INFO - __main__ - Epoch  13, Step:   27400, Batch Loss:    32.339256, Lr: 0.000089, Tokens per sec:   2679
2023-03-14 01:40:01,580 - INFO - __main__ - Epoch  13, Step:   27500, Batch Loss:    23.732685, Lr: 0.000089, Tokens per sec:   2904
2023-03-14 01:40:21,221 - INFO - __main__ - Epoch  13, Step:   27600, Batch Loss:    31.506765, Lr: 0.000089, Tokens per sec:   2742
2023-03-14 01:40:40,129 - INFO - __main__ - Epoch  13, Step:   27700, Batch Loss:    25.429876, Lr: 0.000089, Tokens per sec:   2866
2023-03-14 01:40:59,407 - INFO - __main__ - Epoch  13, Step:   27800, Batch Loss:    30.168211, Lr: 0.000089, Tokens per sec:   2817
2023-03-14 01:41:18,600 - INFO - __main__ - Epoch  13, Step:   27900, Batch Loss:    21.866539, Lr: 0.000089, Tokens per sec:   2808
2023-03-14 01:41:38,108 - INFO - __main__ - Epoch  13, Step:   28000, Batch Loss:    22.660055, Lr: 0.000089, Tokens per sec:   2707
2023-03-14 01:41:57,823 - INFO - __main__ - Epoch  13, Step:   28100, Batch Loss:    25.128431, Lr: 0.000089, Tokens per sec:   2665
2023-03-14 01:42:17,713 - INFO - __main__ - Epoch  13, Step:   28200, Batch Loss:    32.121265, Lr: 0.000089, Tokens per sec:   2695
2023-03-14 01:42:36,659 - INFO - __main__ - Epoch  13, Step:   28300, Batch Loss:    25.071127, Lr: 0.000089, Tokens per sec:   2844
2023-03-14 01:42:41,534 - INFO - __main__ - Epoch  13: total training loss 60920.61
2023-03-14 01:42:41,534 - INFO - __main__ - Epoch 14
2023-03-14 01:42:56,323 - INFO - __main__ - Epoch  14, Step:   28400, Batch Loss:    23.382788, Lr: 0.000088, Tokens per sec:   2678
2023-03-14 01:43:15,301 - INFO - __main__ - Epoch  14, Step:   28500, Batch Loss:    31.568104, Lr: 0.000088, Tokens per sec:   2855
2023-03-14 01:43:35,181 - INFO - __main__ - Epoch  14, Step:   28600, Batch Loss:    26.927620, Lr: 0.000088, Tokens per sec:   2703
2023-03-14 01:43:55,040 - INFO - __main__ - Epoch  14, Step:   28700, Batch Loss:    24.324415, Lr: 0.000088, Tokens per sec:   2699
2023-03-14 01:44:14,068 - INFO - __main__ - Epoch  14, Step:   28800, Batch Loss:    27.038372, Lr: 0.000088, Tokens per sec:   2821
2023-03-14 01:44:32,153 - INFO - __main__ - Epoch  14, Step:   28900, Batch Loss:    32.099781, Lr: 0.000088, Tokens per sec:   2947
2023-03-14 01:44:50,215 - INFO - __main__ - Epoch  14, Step:   29000, Batch Loss:    28.735302, Lr: 0.000088, Tokens per sec:   3010
2023-03-14 01:45:08,287 - INFO - __main__ - Epoch  14, Step:   29100, Batch Loss:    20.520901, Lr: 0.000088, Tokens per sec:   2972
2023-03-14 01:45:26,785 - INFO - __main__ - Epoch  14, Step:   29200, Batch Loss:    24.212135, Lr: 0.000088, Tokens per sec:   2924
2023-03-14 01:45:44,835 - INFO - __main__ - Epoch  14, Step:   29300, Batch Loss:    19.924711, Lr: 0.000088, Tokens per sec:   2955
2023-03-14 01:46:02,967 - INFO - __main__ - Epoch  14, Step:   29400, Batch Loss:    31.125523, Lr: 0.000088, Tokens per sec:   2964
2023-03-14 01:46:21,103 - INFO - __main__ - Epoch  14, Step:   29500, Batch Loss:    27.439917, Lr: 0.000088, Tokens per sec:   2980
2023-03-14 01:46:41,043 - INFO - __main__ - Epoch  14, Step:   29600, Batch Loss:    25.706799, Lr: 0.000088, Tokens per sec:   2709
2023-03-14 01:46:59,111 - INFO - __main__ - Epoch  14, Step:   29700, Batch Loss:    27.870213, Lr: 0.000088, Tokens per sec:   2953
2023-03-14 01:47:17,126 - INFO - __main__ - Epoch  14, Step:   29800, Batch Loss:    27.693085, Lr: 0.000088, Tokens per sec:   2965
2023-03-14 01:47:35,579 - INFO - __main__ - Epoch  14, Step:   29900, Batch Loss:    32.252758, Lr: 0.000088, Tokens per sec:   2963
2023-03-14 01:47:55,555 - INFO - __main__ - Epoch  14, Step:   30000, Batch Loss:    22.793480, Lr: 0.000088, Tokens per sec:   2656
2023-03-14 01:48:15,573 - INFO - __main__ - Epoch  14, Step:   30100, Batch Loss:    28.134743, Lr: 0.000088, Tokens per sec:   2724
2023-03-14 01:48:33,662 - INFO - __main__ - Epoch  14, Step:   30200, Batch Loss:    25.443533, Lr: 0.000088, Tokens per sec:   3012
2023-03-14 01:48:51,674 - INFO - __main__ - Epoch  14, Step:   30300, Batch Loss:    23.660738, Lr: 0.000088, Tokens per sec:   2989
2023-03-14 01:49:09,731 - INFO - __main__ - Epoch  14, Step:   30400, Batch Loss:    28.434555, Lr: 0.000088, Tokens per sec:   2964
2023-03-14 01:49:27,792 - INFO - __main__ - Epoch  14, Step:   30500, Batch Loss:    33.423504, Lr: 0.000088, Tokens per sec:   2962
2023-03-14 01:49:28,900 - INFO - __main__ - Epoch  14: total training loss 57310.91
2023-03-14 01:53:08,258 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 01:53:08,259 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 32.72081739002394, rouge_l = 48.703708805805896, meteor = 0
2023-03-14 01:53:09,354 - INFO - __main__ - Delete test_dir3/17432.ckpt
2023-03-14 01:53:09,500 - INFO - __main__ - Example #0
2023-03-14 01:53:09,500 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 01:53:09,500 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 01:53:09,500 - INFO - __main__ - Example #1
2023-03-14 01:53:09,500 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 01:53:09,500 - INFO - __main__ - 	Hypothesis: call when the user start .
2023-03-14 01:53:09,500 - INFO - __main__ - Example #2
2023-03-14 01:53:09,500 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 01:53:09,500 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this method be a bind property change on the associate jtext component .
2023-03-14 01:53:09,507 - INFO - __main__ - Validation time = 216.70873999595642s.
2023-03-14 01:53:09,508 - INFO - __main__ - Epoch 15
2023-03-14 01:53:28,266 - INFO - __main__ - Epoch  15, Step:   30600, Batch Loss:    19.479076, Lr: 0.000087, Tokens per sec:   2680
2023-03-14 01:53:48,197 - INFO - __main__ - Epoch  15, Step:   30700, Batch Loss:    23.044020, Lr: 0.000087, Tokens per sec:   2704
2023-03-14 01:54:08,127 - INFO - __main__ - Epoch  15, Step:   30800, Batch Loss:    14.212197, Lr: 0.000087, Tokens per sec:   2648
2023-03-14 01:54:27,489 - INFO - __main__ - Epoch  15, Step:   30900, Batch Loss:    24.183613, Lr: 0.000087, Tokens per sec:   2728
2023-03-14 01:54:46,282 - INFO - __main__ - Epoch  15, Step:   31000, Batch Loss:    26.627539, Lr: 0.000087, Tokens per sec:   2892
2023-03-14 01:55:05,957 - INFO - __main__ - Epoch  15, Step:   31100, Batch Loss:    24.935556, Lr: 0.000087, Tokens per sec:   2759
2023-03-14 01:55:25,585 - INFO - __main__ - Epoch  15, Step:   31200, Batch Loss:    24.213001, Lr: 0.000087, Tokens per sec:   2765
2023-03-14 01:55:44,247 - INFO - __main__ - Epoch  15, Step:   31300, Batch Loss:    34.290871, Lr: 0.000087, Tokens per sec:   2883
2023-03-14 01:56:02,235 - INFO - __main__ - Epoch  15, Step:   31400, Batch Loss:    26.398073, Lr: 0.000087, Tokens per sec:   3005
2023-03-14 01:56:20,900 - INFO - __main__ - Epoch  15, Step:   31500, Batch Loss:    25.003185, Lr: 0.000087, Tokens per sec:   2894
2023-03-14 01:56:40,841 - INFO - __main__ - Epoch  15, Step:   31600, Batch Loss:    32.194012, Lr: 0.000087, Tokens per sec:   2673
2023-03-14 01:56:59,235 - INFO - __main__ - Epoch  15, Step:   31700, Batch Loss:    35.723904, Lr: 0.000087, Tokens per sec:   2933
2023-03-14 01:57:17,728 - INFO - __main__ - Epoch  15, Step:   31800, Batch Loss:    26.776585, Lr: 0.000087, Tokens per sec:   2898
2023-03-14 01:57:36,656 - INFO - __main__ - Epoch  15, Step:   31900, Batch Loss:    27.264238, Lr: 0.000087, Tokens per sec:   2851
2023-03-14 01:57:54,815 - INFO - __main__ - Epoch  15, Step:   32000, Batch Loss:    18.175182, Lr: 0.000087, Tokens per sec:   2988
2023-03-14 01:58:12,979 - INFO - __main__ - Epoch  15, Step:   32100, Batch Loss:    20.127947, Lr: 0.000087, Tokens per sec:   2954
2023-03-14 01:58:31,602 - INFO - __main__ - Epoch  15, Step:   32200, Batch Loss:    28.388144, Lr: 0.000087, Tokens per sec:   2915
2023-03-14 01:58:49,557 - INFO - __main__ - Epoch  15, Step:   32300, Batch Loss:    20.062599, Lr: 0.000087, Tokens per sec:   2992
2023-03-14 01:59:08,067 - INFO - __main__ - Epoch  15, Step:   32400, Batch Loss:    25.017632, Lr: 0.000087, Tokens per sec:   2934
2023-03-14 01:59:26,397 - INFO - __main__ - Epoch  15, Step:   32500, Batch Loss:    23.932384, Lr: 0.000087, Tokens per sec:   2959
2023-03-14 01:59:45,466 - INFO - __main__ - Epoch  15, Step:   32600, Batch Loss:    29.232557, Lr: 0.000087, Tokens per sec:   2835
2023-03-14 02:00:01,640 - INFO - __main__ - Epoch  15: total training loss 53930.34
2023-03-14 02:00:01,642 - INFO - __main__ - Epoch 16
2023-03-14 02:00:04,597 - INFO - __main__ - Epoch  16, Step:   32700, Batch Loss:    23.379009, Lr: 0.000086, Tokens per sec:   2818
2023-03-14 02:00:22,678 - INFO - __main__ - Epoch  16, Step:   32800, Batch Loss:    24.260489, Lr: 0.000086, Tokens per sec:   2970
2023-03-14 02:00:41,792 - INFO - __main__ - Epoch  16, Step:   32900, Batch Loss:    21.874861, Lr: 0.000086, Tokens per sec:   2821
2023-03-14 02:01:00,975 - INFO - __main__ - Epoch  16, Step:   33000, Batch Loss:    32.439957, Lr: 0.000086, Tokens per sec:   2822
2023-03-14 02:01:20,175 - INFO - __main__ - Epoch  16, Step:   33100, Batch Loss:    24.313000, Lr: 0.000086, Tokens per sec:   2789
2023-03-14 02:01:39,603 - INFO - __main__ - Epoch  16, Step:   33200, Batch Loss:    20.684845, Lr: 0.000086, Tokens per sec:   2797
2023-03-14 02:01:59,484 - INFO - __main__ - Epoch  16, Step:   33300, Batch Loss:    23.025652, Lr: 0.000086, Tokens per sec:   2662
2023-03-14 02:02:18,889 - INFO - __main__ - Epoch  16, Step:   33400, Batch Loss:    21.295132, Lr: 0.000086, Tokens per sec:   2773
2023-03-14 02:02:38,850 - INFO - __main__ - Epoch  16, Step:   33500, Batch Loss:    19.774897, Lr: 0.000086, Tokens per sec:   2695
2023-03-14 02:02:57,664 - INFO - __main__ - Epoch  16, Step:   33600, Batch Loss:    26.305470, Lr: 0.000086, Tokens per sec:   2852
2023-03-14 02:03:15,697 - INFO - __main__ - Epoch  16, Step:   33700, Batch Loss:    21.070072, Lr: 0.000086, Tokens per sec:   2971
2023-03-14 02:03:34,562 - INFO - __main__ - Epoch  16, Step:   33800, Batch Loss:    22.459379, Lr: 0.000086, Tokens per sec:   2859
2023-03-14 02:03:52,524 - INFO - __main__ - Epoch  16, Step:   33900, Batch Loss:    30.174019, Lr: 0.000086, Tokens per sec:   3037
2023-03-14 02:04:10,525 - INFO - __main__ - Epoch  16, Step:   34000, Batch Loss:    16.152063, Lr: 0.000086, Tokens per sec:   2912
2023-03-14 02:04:30,489 - INFO - __main__ - Epoch  16, Step:   34100, Batch Loss:    25.938427, Lr: 0.000086, Tokens per sec:   2716
2023-03-14 02:04:50,383 - INFO - __main__ - Epoch  16, Step:   34200, Batch Loss:    25.845617, Lr: 0.000086, Tokens per sec:   2729
2023-03-14 02:05:10,336 - INFO - __main__ - Epoch  16, Step:   34300, Batch Loss:    31.940418, Lr: 0.000086, Tokens per sec:   2661
2023-03-14 02:05:28,715 - INFO - __main__ - Epoch  16, Step:   34400, Batch Loss:    20.438431, Lr: 0.000086, Tokens per sec:   2925
2023-03-14 02:05:46,780 - INFO - __main__ - Epoch  16, Step:   34500, Batch Loss:    18.802086, Lr: 0.000086, Tokens per sec:   3043
2023-03-14 02:06:05,333 - INFO - __main__ - Epoch  16, Step:   34600, Batch Loss:    25.735069, Lr: 0.000086, Tokens per sec:   2918
2023-03-14 02:06:24,940 - INFO - __main__ - Epoch  16, Step:   34700, Batch Loss:    19.790789, Lr: 0.000086, Tokens per sec:   2758
2023-03-14 02:06:44,989 - INFO - __main__ - Epoch  16, Step:   34800, Batch Loss:    17.372751, Lr: 0.000086, Tokens per sec:   2682
2023-03-14 02:06:57,791 - INFO - __main__ - Epoch  16: total training loss 50862.90
2023-03-14 02:10:38,254 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 02:10:38,254 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 34.32501042675714, rouge_l = 49.71249058816392, meteor = 0
2023-03-14 02:10:39,400 - INFO - __main__ - Delete test_dir3/21790.ckpt
2023-03-14 02:10:39,528 - INFO - __main__ - Example #0
2023-03-14 02:10:39,528 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 02:10:39,528 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 02:10:39,528 - INFO - __main__ - Example #1
2023-03-14 02:10:39,528 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 02:10:39,528 - INFO - __main__ - 	Hypothesis: call when the view be start .
2023-03-14 02:10:39,529 - INFO - __main__ - Example #2
2023-03-14 02:10:39,529 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 02:10:39,529 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change the bind property of the ui bind property of jtext component subclass . this method
2023-03-14 02:10:39,534 - INFO - __main__ - Validation time = 217.45939660072327s.
2023-03-14 02:10:39,535 - INFO - __main__ - Epoch 17
2023-03-14 02:10:46,924 - INFO - __main__ - Epoch  17, Step:   34900, Batch Loss:    16.351213, Lr: 0.000085, Tokens per sec:   2620
2023-03-14 02:11:06,858 - INFO - __main__ - Epoch  17, Step:   35000, Batch Loss:    15.771618, Lr: 0.000085, Tokens per sec:   2672
2023-03-14 02:11:26,795 - INFO - __main__ - Epoch  17, Step:   35100, Batch Loss:    14.738537, Lr: 0.000085, Tokens per sec:   2658
2023-03-14 02:11:46,750 - INFO - __main__ - Epoch  17, Step:   35200, Batch Loss:    17.400139, Lr: 0.000085, Tokens per sec:   2710
2023-03-14 02:12:06,630 - INFO - __main__ - Epoch  17, Step:   35300, Batch Loss:    24.835516, Lr: 0.000085, Tokens per sec:   2708
2023-03-14 02:12:26,495 - INFO - __main__ - Epoch  17, Step:   35400, Batch Loss:    22.996016, Lr: 0.000085, Tokens per sec:   2683
2023-03-14 02:12:46,449 - INFO - __main__ - Epoch  17, Step:   35500, Batch Loss:    15.579900, Lr: 0.000085, Tokens per sec:   2692
2023-03-14 02:13:04,899 - INFO - __main__ - Epoch  17, Step:   35600, Batch Loss:    22.595844, Lr: 0.000085, Tokens per sec:   2967
2023-03-14 02:13:23,602 - INFO - __main__ - Epoch  17, Step:   35700, Batch Loss:    21.129583, Lr: 0.000085, Tokens per sec:   2965
2023-03-14 02:13:42,048 - INFO - __main__ - Epoch  17, Step:   35800, Batch Loss:    32.798298, Lr: 0.000085, Tokens per sec:   2957
2023-03-14 02:14:01,992 - INFO - __main__ - Epoch  17, Step:   35900, Batch Loss:    20.230127, Lr: 0.000085, Tokens per sec:   2706
2023-03-14 02:14:21,985 - INFO - __main__ - Epoch  17, Step:   36000, Batch Loss:    14.048160, Lr: 0.000085, Tokens per sec:   2665
2023-03-14 02:14:41,979 - INFO - __main__ - Epoch  17, Step:   36100, Batch Loss:    23.668697, Lr: 0.000085, Tokens per sec:   2694
2023-03-14 02:15:01,946 - INFO - __main__ - Epoch  17, Step:   36200, Batch Loss:    28.091307, Lr: 0.000085, Tokens per sec:   2720
2023-03-14 02:15:21,801 - INFO - __main__ - Epoch  17, Step:   36300, Batch Loss:    21.050896, Lr: 0.000085, Tokens per sec:   2740
2023-03-14 02:15:40,414 - INFO - __main__ - Epoch  17, Step:   36400, Batch Loss:    22.625965, Lr: 0.000085, Tokens per sec:   2854
2023-03-14 02:15:58,330 - INFO - __main__ - Epoch  17, Step:   36500, Batch Loss:    31.024178, Lr: 0.000085, Tokens per sec:   2980
2023-03-14 02:16:16,536 - INFO - __main__ - Epoch  17, Step:   36600, Batch Loss:    26.518335, Lr: 0.000085, Tokens per sec:   2975
2023-03-14 02:16:34,531 - INFO - __main__ - Epoch  17, Step:   36700, Batch Loss:    22.457745, Lr: 0.000085, Tokens per sec:   2963
2023-03-14 02:16:52,541 - INFO - __main__ - Epoch  17, Step:   36800, Batch Loss:    16.328821, Lr: 0.000085, Tokens per sec:   2947
2023-03-14 02:17:10,994 - INFO - __main__ - Epoch  17, Step:   36900, Batch Loss:    23.727474, Lr: 0.000085, Tokens per sec:   2955
2023-03-14 02:17:29,562 - INFO - __main__ - Epoch  17, Step:   37000, Batch Loss:    23.006432, Lr: 0.000085, Tokens per sec:   2886
2023-03-14 02:17:37,305 - INFO - __main__ - Epoch  17: total training loss 47911.64
2023-03-14 02:17:37,306 - INFO - __main__ - Epoch 18
2023-03-14 02:17:48,928 - INFO - __main__ - Epoch  18, Step:   37100, Batch Loss:    14.914881, Lr: 0.000084, Tokens per sec:   2601
2023-03-14 02:18:08,863 - INFO - __main__ - Epoch  18, Step:   37200, Batch Loss:    15.782629, Lr: 0.000084, Tokens per sec:   2716
2023-03-14 02:18:28,157 - INFO - __main__ - Epoch  18, Step:   37300, Batch Loss:    27.348001, Lr: 0.000084, Tokens per sec:   2782
2023-03-14 02:18:47,698 - INFO - __main__ - Epoch  18, Step:   37400, Batch Loss:    24.859123, Lr: 0.000084, Tokens per sec:   2781
2023-03-14 02:19:07,747 - INFO - __main__ - Epoch  18, Step:   37500, Batch Loss:    24.050810, Lr: 0.000084, Tokens per sec:   2685
2023-03-14 02:19:27,760 - INFO - __main__ - Epoch  18, Step:   37600, Batch Loss:    18.036451, Lr: 0.000084, Tokens per sec:   2659
2023-03-14 02:19:47,730 - INFO - __main__ - Epoch  18, Step:   37700, Batch Loss:    28.744757, Lr: 0.000084, Tokens per sec:   2715
2023-03-14 02:20:06,448 - INFO - __main__ - Epoch  18, Step:   37800, Batch Loss:    17.489813, Lr: 0.000084, Tokens per sec:   2851
2023-03-14 02:20:25,383 - INFO - __main__ - Epoch  18, Step:   37900, Batch Loss:    27.531338, Lr: 0.000084, Tokens per sec:   2865
2023-03-14 02:20:45,402 - INFO - __main__ - Epoch  18, Step:   38000, Batch Loss:    14.284040, Lr: 0.000084, Tokens per sec:   2682
2023-03-14 02:21:03,800 - INFO - __main__ - Epoch  18, Step:   38100, Batch Loss:    12.366856, Lr: 0.000084, Tokens per sec:   2910
2023-03-14 02:21:21,850 - INFO - __main__ - Epoch  18, Step:   38200, Batch Loss:    20.398447, Lr: 0.000084, Tokens per sec:   2979
2023-03-14 02:21:39,935 - INFO - __main__ - Epoch  18, Step:   38300, Batch Loss:    21.995787, Lr: 0.000084, Tokens per sec:   2957
2023-03-14 02:21:58,019 - INFO - __main__ - Epoch  18, Step:   38400, Batch Loss:    19.538902, Lr: 0.000084, Tokens per sec:   2976
2023-03-14 02:22:17,112 - INFO - __main__ - Epoch  18, Step:   38500, Batch Loss:    19.400661, Lr: 0.000084, Tokens per sec:   2824
2023-03-14 02:22:35,122 - INFO - __main__ - Epoch  18, Step:   38600, Batch Loss:    26.018932, Lr: 0.000084, Tokens per sec:   3003
2023-03-14 02:22:53,155 - INFO - __main__ - Epoch  18, Step:   38700, Batch Loss:    24.003424, Lr: 0.000084, Tokens per sec:   2975
2023-03-14 02:23:11,134 - INFO - __main__ - Epoch  18, Step:   38800, Batch Loss:    19.351330, Lr: 0.000084, Tokens per sec:   2990
2023-03-14 02:23:29,262 - INFO - __main__ - Epoch  18, Step:   38900, Batch Loss:    17.134182, Lr: 0.000084, Tokens per sec:   2935
2023-03-14 02:23:47,353 - INFO - __main__ - Epoch  18, Step:   39000, Batch Loss:    16.636055, Lr: 0.000084, Tokens per sec:   2997
2023-03-14 02:24:05,415 - INFO - __main__ - Epoch  18, Step:   39100, Batch Loss:    18.093964, Lr: 0.000084, Tokens per sec:   3052
2023-03-14 02:24:23,473 - INFO - __main__ - Epoch  18, Step:   39200, Batch Loss:    23.380154, Lr: 0.000084, Tokens per sec:   2959
2023-03-14 02:24:27,475 - INFO - __main__ - Epoch  18: total training loss 45324.90
2023-03-14 02:28:08,173 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 02:28:08,174 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 35.737075207809575, rouge_l = 50.779818356869896, meteor = 0
2023-03-14 02:28:09,289 - INFO - __main__ - Delete test_dir3/26148.ckpt
2023-03-14 02:28:09,427 - INFO - __main__ - Example #0
2023-03-14 02:28:09,427 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 02:28:09,427 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 02:28:09,427 - INFO - __main__ - Example #1
2023-03-14 02:28:09,427 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 02:28:09,427 - INFO - __main__ - 	Hypothesis: call when the start of the view be start .
2023-03-14 02:28:09,427 - INFO - __main__ - Example #2
2023-03-14 02:28:09,427 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 02:28:09,427 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 02:28:09,434 - INFO - __main__ - Validation time = 217.97457480430603s.
2023-03-14 02:28:09,434 - INFO - __main__ - Epoch 19
2023-03-14 02:28:25,258 - INFO - __main__ - Epoch  19, Step:   39300, Batch Loss:    20.059956, Lr: 0.000083, Tokens per sec:   2652
2023-03-14 02:28:45,121 - INFO - __main__ - Epoch  19, Step:   39400, Batch Loss:    21.922356, Lr: 0.000083, Tokens per sec:   2691
2023-03-14 02:29:05,010 - INFO - __main__ - Epoch  19, Step:   39500, Batch Loss:    11.788652, Lr: 0.000083, Tokens per sec:   2706
2023-03-14 02:29:24,724 - INFO - __main__ - Epoch  19, Step:   39600, Batch Loss:    24.204390, Lr: 0.000083, Tokens per sec:   2767
2023-03-14 02:29:42,717 - INFO - __main__ - Epoch  19, Step:   39700, Batch Loss:    18.895884, Lr: 0.000083, Tokens per sec:   2969
2023-03-14 02:30:01,913 - INFO - __main__ - Epoch  19, Step:   39800, Batch Loss:    19.814764, Lr: 0.000083, Tokens per sec:   2821
2023-03-14 02:30:21,322 - INFO - __main__ - Epoch  19, Step:   39900, Batch Loss:    17.356022, Lr: 0.000083, Tokens per sec:   2757
2023-03-14 02:30:40,532 - INFO - __main__ - Epoch  19, Step:   40000, Batch Loss:    19.845280, Lr: 0.000083, Tokens per sec:   2794
2023-03-14 02:30:59,050 - INFO - __main__ - Epoch  19, Step:   40100, Batch Loss:    21.571569, Lr: 0.000083, Tokens per sec:   2963
2023-03-14 02:31:17,119 - INFO - __main__ - Epoch  19, Step:   40200, Batch Loss:    18.106760, Lr: 0.000083, Tokens per sec:   3011
2023-03-14 02:31:35,056 - INFO - __main__ - Epoch  19, Step:   40300, Batch Loss:    22.405407, Lr: 0.000083, Tokens per sec:   2968
2023-03-14 02:31:53,324 - INFO - __main__ - Epoch  19, Step:   40400, Batch Loss:    24.667894, Lr: 0.000083, Tokens per sec:   2922
2023-03-14 02:32:11,301 - INFO - __main__ - Epoch  19, Step:   40500, Batch Loss:    20.296551, Lr: 0.000083, Tokens per sec:   2993
2023-03-14 02:32:30,535 - INFO - __main__ - Epoch  19, Step:   40600, Batch Loss:    22.287300, Lr: 0.000083, Tokens per sec:   2789
2023-03-14 02:32:50,412 - INFO - __main__ - Epoch  19, Step:   40700, Batch Loss:    27.596687, Lr: 0.000083, Tokens per sec:   2721
2023-03-14 02:33:10,424 - INFO - __main__ - Epoch  19, Step:   40800, Batch Loss:    23.750383, Lr: 0.000083, Tokens per sec:   2679
2023-03-14 02:33:30,118 - INFO - __main__ - Epoch  19, Step:   40900, Batch Loss:    20.977602, Lr: 0.000083, Tokens per sec:   2723
2023-03-14 02:33:48,912 - INFO - __main__ - Epoch  19, Step:   41000, Batch Loss:    21.638489, Lr: 0.000083, Tokens per sec:   2815
2023-03-14 02:34:08,248 - INFO - __main__ - Epoch  19, Step:   41100, Batch Loss:    20.598230, Lr: 0.000083, Tokens per sec:   2752
2023-03-14 02:34:27,621 - INFO - __main__ - Epoch  19, Step:   41200, Batch Loss:    15.007386, Lr: 0.000083, Tokens per sec:   2835
2023-03-14 02:34:46,376 - INFO - __main__ - Epoch  19, Step:   41300, Batch Loss:    18.811413, Lr: 0.000083, Tokens per sec:   2906
2023-03-14 02:35:06,298 - INFO - __main__ - Epoch  19, Step:   41400, Batch Loss:    22.010286, Lr: 0.000083, Tokens per sec:   2702
2023-03-14 02:35:06,532 - INFO - __main__ - Epoch  19: total training loss 42814.91
2023-03-14 02:35:06,533 - INFO - __main__ - Epoch 20
2023-03-14 02:35:26,359 - INFO - __main__ - Epoch  20, Step:   41500, Batch Loss:    14.712610, Lr: 0.000083, Tokens per sec:   2671
2023-03-14 02:35:45,506 - INFO - __main__ - Epoch  20, Step:   41600, Batch Loss:    14.189229, Lr: 0.000083, Tokens per sec:   2846
2023-03-14 02:36:05,472 - INFO - __main__ - Epoch  20, Step:   41700, Batch Loss:    18.828840, Lr: 0.000083, Tokens per sec:   2687
2023-03-14 02:36:24,680 - INFO - __main__ - Epoch  20, Step:   41800, Batch Loss:    21.281492, Lr: 0.000083, Tokens per sec:   2827
2023-03-14 02:36:42,655 - INFO - __main__ - Epoch  20, Step:   41900, Batch Loss:    15.120598, Lr: 0.000083, Tokens per sec:   2984
2023-03-14 02:37:02,194 - INFO - __main__ - Epoch  20, Step:   42000, Batch Loss:    20.417063, Lr: 0.000083, Tokens per sec:   2772
2023-03-14 02:37:22,107 - INFO - __main__ - Epoch  20, Step:   42100, Batch Loss:    22.472048, Lr: 0.000083, Tokens per sec:   2742
2023-03-14 02:37:42,020 - INFO - __main__ - Epoch  20, Step:   42200, Batch Loss:    15.444701, Lr: 0.000083, Tokens per sec:   2687
2023-03-14 02:38:01,984 - INFO - __main__ - Epoch  20, Step:   42300, Batch Loss:    22.070656, Lr: 0.000083, Tokens per sec:   2671
2023-03-14 02:38:21,866 - INFO - __main__ - Epoch  20, Step:   42400, Batch Loss:    15.026186, Lr: 0.000083, Tokens per sec:   2712
2023-03-14 02:38:41,834 - INFO - __main__ - Epoch  20, Step:   42500, Batch Loss:    23.049982, Lr: 0.000083, Tokens per sec:   2672
2023-03-14 02:39:01,058 - INFO - __main__ - Epoch  20, Step:   42600, Batch Loss:    17.283850, Lr: 0.000083, Tokens per sec:   2783
2023-03-14 02:39:20,950 - INFO - __main__ - Epoch  20, Step:   42700, Batch Loss:    17.323526, Lr: 0.000083, Tokens per sec:   2706
2023-03-14 02:39:40,789 - INFO - __main__ - Epoch  20, Step:   42800, Batch Loss:    17.529808, Lr: 0.000083, Tokens per sec:   2766
2023-03-14 02:40:00,722 - INFO - __main__ - Epoch  20, Step:   42900, Batch Loss:    18.462713, Lr: 0.000083, Tokens per sec:   2733
2023-03-14 02:40:19,669 - INFO - __main__ - Epoch  20, Step:   43000, Batch Loss:    14.764434, Lr: 0.000083, Tokens per sec:   2805
2023-03-14 02:40:37,663 - INFO - __main__ - Epoch  20, Step:   43100, Batch Loss:    20.448494, Lr: 0.000083, Tokens per sec:   3013
2023-03-14 02:40:55,679 - INFO - __main__ - Epoch  20, Step:   43200, Batch Loss:    20.141043, Lr: 0.000083, Tokens per sec:   2967
2023-03-14 02:41:13,791 - INFO - __main__ - Epoch  20, Step:   43300, Batch Loss:    19.873854, Lr: 0.000083, Tokens per sec:   2960
2023-03-14 02:41:31,829 - INFO - __main__ - Epoch  20, Step:   43400, Batch Loss:    18.082991, Lr: 0.000083, Tokens per sec:   2950
2023-03-14 02:41:50,032 - INFO - __main__ - Epoch  20, Step:   43500, Batch Loss:    26.881876, Lr: 0.000083, Tokens per sec:   2938
2023-03-14 02:42:04,573 - INFO - __main__ - Epoch  20: total training loss 40474.14
2023-03-14 02:45:56,586 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 02:45:56,586 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 36.64708227453678, rouge_l = 51.36679233757048, meteor = 0
2023-03-14 02:45:57,716 - INFO - __main__ - Delete test_dir3/30506.ckpt
2023-03-14 02:45:57,849 - INFO - __main__ - Example #0
2023-03-14 02:45:57,849 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 02:45:57,850 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 02:45:57,850 - INFO - __main__ - Example #1
2023-03-14 02:45:57,850 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 02:45:57,850 - INFO - __main__ - 	Hypothesis: call when the view begin of the entity be start .
2023-03-14 02:45:57,850 - INFO - __main__ - Example #2
2023-03-14 02:45:57,850 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 02:45:57,850 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 02:45:57,857 - INFO - __main__ - Validation time = 229.2033622264862s.
2023-03-14 02:45:57,858 - INFO - __main__ - Epoch 21
2023-03-14 02:46:02,104 - INFO - __main__ - Epoch  21, Step:   43600, Batch Loss:    14.710236, Lr: 0.000082, Tokens per sec:   2475
2023-03-14 02:46:22,052 - INFO - __main__ - Epoch  21, Step:   43700, Batch Loss:    16.491638, Lr: 0.000082, Tokens per sec:   2741
2023-03-14 02:46:41,928 - INFO - __main__ - Epoch  21, Step:   43800, Batch Loss:    14.838137, Lr: 0.000082, Tokens per sec:   2708
2023-03-14 02:47:02,261 - INFO - __main__ - Epoch  21, Step:   43900, Batch Loss:    13.204611, Lr: 0.000082, Tokens per sec:   2624
2023-03-14 02:47:22,138 - INFO - __main__ - Epoch  21, Step:   44000, Batch Loss:    15.751569, Lr: 0.000082, Tokens per sec:   2715
2023-03-14 02:47:42,026 - INFO - __main__ - Epoch  21, Step:   44100, Batch Loss:    13.689814, Lr: 0.000082, Tokens per sec:   2711
2023-03-14 02:48:00,420 - INFO - __main__ - Epoch  21, Step:   44200, Batch Loss:    14.423379, Lr: 0.000082, Tokens per sec:   2877
2023-03-14 02:48:19,877 - INFO - __main__ - Epoch  21, Step:   44300, Batch Loss:    15.771910, Lr: 0.000082, Tokens per sec:   2724
2023-03-14 02:48:38,238 - INFO - __main__ - Epoch  21, Step:   44400, Batch Loss:    14.817826, Lr: 0.000082, Tokens per sec:   2950
2023-03-14 02:48:56,610 - INFO - __main__ - Epoch  21, Step:   44500, Batch Loss:    15.968934, Lr: 0.000082, Tokens per sec:   2938
2023-03-14 02:49:16,574 - INFO - __main__ - Epoch  21, Step:   44600, Batch Loss:    21.939201, Lr: 0.000082, Tokens per sec:   2703
2023-03-14 02:49:36,490 - INFO - __main__ - Epoch  21, Step:   44700, Batch Loss:    15.204332, Lr: 0.000082, Tokens per sec:   2730
2023-03-14 02:49:56,368 - INFO - __main__ - Epoch  21, Step:   44800, Batch Loss:    23.533014, Lr: 0.000082, Tokens per sec:   2673
2023-03-14 02:50:16,185 - INFO - __main__ - Epoch  21, Step:   44900, Batch Loss:    23.229082, Lr: 0.000082, Tokens per sec:   2751
2023-03-14 02:50:36,118 - INFO - __main__ - Epoch  21, Step:   45000, Batch Loss:    22.709925, Lr: 0.000082, Tokens per sec:   2718
2023-03-14 02:50:55,986 - INFO - __main__ - Epoch  21, Step:   45100, Batch Loss:    15.462163, Lr: 0.000082, Tokens per sec:   2713
2023-03-14 02:51:15,829 - INFO - __main__ - Epoch  21, Step:   45200, Batch Loss:    19.679041, Lr: 0.000082, Tokens per sec:   2751
2023-03-14 02:51:35,462 - INFO - __main__ - Epoch  21, Step:   45300, Batch Loss:    16.977377, Lr: 0.000082, Tokens per sec:   2749
2023-03-14 02:51:53,585 - INFO - __main__ - Epoch  21, Step:   45400, Batch Loss:    21.701654, Lr: 0.000082, Tokens per sec:   2923
2023-03-14 02:52:12,935 - INFO - __main__ - Epoch  21, Step:   45500, Batch Loss:    20.639538, Lr: 0.000082, Tokens per sec:   2779
2023-03-14 02:52:31,999 - INFO - __main__ - Epoch  21, Step:   45600, Batch Loss:    15.865015, Lr: 0.000082, Tokens per sec:   2811
2023-03-14 02:52:49,993 - INFO - __main__ - Epoch  21, Step:   45700, Batch Loss:    20.744823, Lr: 0.000082, Tokens per sec:   2997
2023-03-14 02:53:00,806 - INFO - __main__ - Epoch  21: total training loss 38359.24
2023-03-14 02:53:00,807 - INFO - __main__ - Epoch 22
2023-03-14 02:53:09,193 - INFO - __main__ - Epoch  22, Step:   45800, Batch Loss:    15.568706, Lr: 0.000081, Tokens per sec:   2659
2023-03-14 02:53:28,365 - INFO - __main__ - Epoch  22, Step:   45900, Batch Loss:    14.874646, Lr: 0.000081, Tokens per sec:   2790
2023-03-14 02:53:46,586 - INFO - __main__ - Epoch  22, Step:   46000, Batch Loss:    17.506361, Lr: 0.000081, Tokens per sec:   2959
2023-03-14 02:54:04,519 - INFO - __main__ - Epoch  22, Step:   46100, Batch Loss:    14.978217, Lr: 0.000081, Tokens per sec:   3002
2023-03-14 02:54:22,592 - INFO - __main__ - Epoch  22, Step:   46200, Batch Loss:    14.485352, Lr: 0.000081, Tokens per sec:   3038
2023-03-14 02:54:41,512 - INFO - __main__ - Epoch  22, Step:   46300, Batch Loss:    19.667995, Lr: 0.000081, Tokens per sec:   2888
2023-03-14 02:55:00,967 - INFO - __main__ - Epoch  22, Step:   46400, Batch Loss:    17.466009, Lr: 0.000081, Tokens per sec:   2771
2023-03-14 02:55:20,909 - INFO - __main__ - Epoch  22, Step:   46500, Batch Loss:    20.637300, Lr: 0.000081, Tokens per sec:   2690
2023-03-14 02:55:39,917 - INFO - __main__ - Epoch  22, Step:   46600, Batch Loss:    20.716290, Lr: 0.000081, Tokens per sec:   2787
2023-03-14 02:55:59,258 - INFO - __main__ - Epoch  22, Step:   46700, Batch Loss:    20.046381, Lr: 0.000081, Tokens per sec:   2779
2023-03-14 02:56:18,093 - INFO - __main__ - Epoch  22, Step:   46800, Batch Loss:    15.123168, Lr: 0.000081, Tokens per sec:   2812
2023-03-14 02:56:37,614 - INFO - __main__ - Epoch  22, Step:   46900, Batch Loss:    16.700315, Lr: 0.000081, Tokens per sec:   2754
2023-03-14 02:56:57,579 - INFO - __main__ - Epoch  22, Step:   47000, Batch Loss:    22.223127, Lr: 0.000081, Tokens per sec:   2713
2023-03-14 02:57:16,193 - INFO - __main__ - Epoch  22, Step:   47100, Batch Loss:    16.069063, Lr: 0.000081, Tokens per sec:   2869
2023-03-14 02:57:34,250 - INFO - __main__ - Epoch  22, Step:   47200, Batch Loss:    12.025767, Lr: 0.000081, Tokens per sec:   2980
2023-03-14 02:57:52,262 - INFO - __main__ - Epoch  22, Step:   47300, Batch Loss:    12.750268, Lr: 0.000081, Tokens per sec:   2966
2023-03-14 02:58:10,293 - INFO - __main__ - Epoch  22, Step:   47400, Batch Loss:    15.221361, Lr: 0.000081, Tokens per sec:   2957
2023-03-14 02:58:28,339 - INFO - __main__ - Epoch  22, Step:   47500, Batch Loss:    22.687763, Lr: 0.000081, Tokens per sec:   3010
2023-03-14 02:58:47,760 - INFO - __main__ - Epoch  22, Step:   47600, Batch Loss:    18.121216, Lr: 0.000081, Tokens per sec:   2786
2023-03-14 02:59:07,567 - INFO - __main__ - Epoch  22, Step:   47700, Batch Loss:    20.627323, Lr: 0.000081, Tokens per sec:   2743
2023-03-14 02:59:27,485 - INFO - __main__ - Epoch  22, Step:   47800, Batch Loss:    17.454226, Lr: 0.000081, Tokens per sec:   2702
2023-03-14 02:59:47,226 - INFO - __main__ - Epoch  22, Step:   47900, Batch Loss:    14.115602, Lr: 0.000081, Tokens per sec:   2719
2023-03-14 02:59:54,816 - INFO - __main__ - Epoch  22: total training loss 36379.95
2023-03-14 03:03:46,318 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 03:03:46,318 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 37.53277567051762, rouge_l = 52.08777158780072, meteor = 0
2023-03-14 03:03:47,440 - INFO - __main__ - Delete test_dir3/34864.ckpt
2023-03-14 03:03:47,581 - INFO - __main__ - Example #0
2023-03-14 03:03:47,582 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 03:03:47,582 - INFO - __main__ - 	Hypothesis: return a hash code value for this string constant .
2023-03-14 03:03:47,582 - INFO - __main__ - Example #1
2023-03-14 03:03:47,582 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 03:03:47,582 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 03:03:47,582 - INFO - __main__ - Example #2
2023-03-14 03:03:47,582 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 03:03:47,582 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 03:03:47,589 - INFO - __main__ - Validation time = 228.6734049320221s.
2023-03-14 03:03:47,589 - INFO - __main__ - Epoch 23
2023-03-14 03:04:00,195 - INFO - __main__ - Epoch  23, Step:   48000, Batch Loss:    14.632421, Lr: 0.000080, Tokens per sec:   2645
2023-03-14 03:04:20,168 - INFO - __main__ - Epoch  23, Step:   48100, Batch Loss:    14.592663, Lr: 0.000080, Tokens per sec:   2700
2023-03-14 03:04:40,093 - INFO - __main__ - Epoch  23, Step:   48200, Batch Loss:    13.891931, Lr: 0.000080, Tokens per sec:   2743
2023-03-14 03:04:59,976 - INFO - __main__ - Epoch  23, Step:   48300, Batch Loss:    12.460948, Lr: 0.000080, Tokens per sec:   2686
2023-03-14 03:05:18,087 - INFO - __main__ - Epoch  23, Step:   48400, Batch Loss:    11.809099, Lr: 0.000080, Tokens per sec:   3015
2023-03-14 03:05:37,602 - INFO - __main__ - Epoch  23, Step:   48500, Batch Loss:    17.760864, Lr: 0.000080, Tokens per sec:   2773
2023-03-14 03:05:55,853 - INFO - __main__ - Epoch  23, Step:   48600, Batch Loss:    17.242849, Lr: 0.000080, Tokens per sec:   2964
2023-03-14 03:06:15,587 - INFO - __main__ - Epoch  23, Step:   48700, Batch Loss:    16.039015, Lr: 0.000080, Tokens per sec:   2735
2023-03-14 03:06:34,780 - INFO - __main__ - Epoch  23, Step:   48800, Batch Loss:    12.199402, Lr: 0.000080, Tokens per sec:   2790
2023-03-14 03:06:54,255 - INFO - __main__ - Epoch  23, Step:   48900, Batch Loss:    13.914357, Lr: 0.000080, Tokens per sec:   2771
2023-03-14 03:07:13,969 - INFO - __main__ - Epoch  23, Step:   49000, Batch Loss:    16.004745, Lr: 0.000080, Tokens per sec:   2735
2023-03-14 03:07:32,857 - INFO - __main__ - Epoch  23, Step:   49100, Batch Loss:    17.676947, Lr: 0.000080, Tokens per sec:   2836
2023-03-14 03:07:50,958 - INFO - __main__ - Epoch  23, Step:   49200, Batch Loss:    10.051829, Lr: 0.000080, Tokens per sec:   2980
2023-03-14 03:08:08,930 - INFO - __main__ - Epoch  23, Step:   49300, Batch Loss:    19.741585, Lr: 0.000080, Tokens per sec:   2998
2023-03-14 03:08:28,689 - INFO - __main__ - Epoch  23, Step:   49400, Batch Loss:    17.544340, Lr: 0.000080, Tokens per sec:   2763
2023-03-14 03:08:48,380 - INFO - __main__ - Epoch  23, Step:   49500, Batch Loss:    16.775637, Lr: 0.000080, Tokens per sec:   2732
2023-03-14 03:09:08,192 - INFO - __main__ - Epoch  23, Step:   49600, Batch Loss:    11.345516, Lr: 0.000080, Tokens per sec:   2737
2023-03-14 03:09:28,090 - INFO - __main__ - Epoch  23, Step:   49700, Batch Loss:    17.816427, Lr: 0.000080, Tokens per sec:   2679
2023-03-14 03:09:48,104 - INFO - __main__ - Epoch  23, Step:   49800, Batch Loss:    16.610447, Lr: 0.000080, Tokens per sec:   2705
2023-03-14 03:10:08,083 - INFO - __main__ - Epoch  23, Step:   49900, Batch Loss:    16.317875, Lr: 0.000080, Tokens per sec:   2662
2023-03-14 03:10:27,285 - INFO - __main__ - Epoch  23, Step:   50000, Batch Loss:    14.580598, Lr: 0.000080, Tokens per sec:   2754
2023-03-14 03:10:45,272 - INFO - __main__ - Epoch  23, Step:   50100, Batch Loss:    15.545020, Lr: 0.000080, Tokens per sec:   2941
2023-03-14 03:10:48,367 - INFO - __main__ - Epoch  23: total training loss 34431.97
2023-03-14 03:10:48,368 - INFO - __main__ - Epoch 24
2023-03-14 03:11:05,235 - INFO - __main__ - Epoch  24, Step:   50200, Batch Loss:    11.404657, Lr: 0.000079, Tokens per sec:   2675
2023-03-14 03:11:24,844 - INFO - __main__ - Epoch  24, Step:   50300, Batch Loss:    19.003187, Lr: 0.000079, Tokens per sec:   2735
2023-03-14 03:11:43,080 - INFO - __main__ - Epoch  24, Step:   50400, Batch Loss:    18.499907, Lr: 0.000079, Tokens per sec:   2987
2023-03-14 03:12:03,091 - INFO - __main__ - Epoch  24, Step:   50500, Batch Loss:    10.698860, Lr: 0.000079, Tokens per sec:   2667
2023-03-14 03:12:23,061 - INFO - __main__ - Epoch  24, Step:   50600, Batch Loss:    14.635789, Lr: 0.000079, Tokens per sec:   2711
2023-03-14 03:12:41,359 - INFO - __main__ - Epoch  24, Step:   50700, Batch Loss:    13.384706, Lr: 0.000079, Tokens per sec:   2930
2023-03-14 03:13:00,840 - INFO - __main__ - Epoch  24, Step:   50800, Batch Loss:    13.393278, Lr: 0.000079, Tokens per sec:   2787
2023-03-14 03:13:20,769 - INFO - __main__ - Epoch  24, Step:   50900, Batch Loss:    12.529204, Lr: 0.000079, Tokens per sec:   2650
2023-03-14 03:13:39,225 - INFO - __main__ - Epoch  24, Step:   51000, Batch Loss:    12.822015, Lr: 0.000079, Tokens per sec:   2881
2023-03-14 03:13:59,105 - INFO - __main__ - Epoch  24, Step:   51100, Batch Loss:    14.323496, Lr: 0.000079, Tokens per sec:   2715
2023-03-14 03:14:19,028 - INFO - __main__ - Epoch  24, Step:   51200, Batch Loss:    14.423618, Lr: 0.000079, Tokens per sec:   2740
2023-03-14 03:14:38,986 - INFO - __main__ - Epoch  24, Step:   51300, Batch Loss:    10.872581, Lr: 0.000079, Tokens per sec:   2713
2023-03-14 03:14:58,917 - INFO - __main__ - Epoch  24, Step:   51400, Batch Loss:     8.766504, Lr: 0.000079, Tokens per sec:   2716
2023-03-14 03:15:18,941 - INFO - __main__ - Epoch  24, Step:   51500, Batch Loss:    12.471828, Lr: 0.000079, Tokens per sec:   2700
2023-03-14 03:15:38,840 - INFO - __main__ - Epoch  24, Step:   51600, Batch Loss:    20.099003, Lr: 0.000079, Tokens per sec:   2705
2023-03-14 03:15:58,676 - INFO - __main__ - Epoch  24, Step:   51700, Batch Loss:    15.972266, Lr: 0.000079, Tokens per sec:   2678
2023-03-14 03:16:17,892 - INFO - __main__ - Epoch  24, Step:   51800, Batch Loss:    14.202600, Lr: 0.000079, Tokens per sec:   2765
2023-03-14 03:16:37,611 - INFO - __main__ - Epoch  24, Step:   51900, Batch Loss:    15.360428, Lr: 0.000079, Tokens per sec:   2775
2023-03-14 03:16:55,923 - INFO - __main__ - Epoch  24, Step:   52000, Batch Loss:    16.013847, Lr: 0.000079, Tokens per sec:   2927
2023-03-14 03:17:14,100 - INFO - __main__ - Epoch  24, Step:   52100, Batch Loss:    10.450728, Lr: 0.000079, Tokens per sec:   2964
2023-03-14 03:17:32,823 - INFO - __main__ - Epoch  24, Step:   52200, Batch Loss:    12.071076, Lr: 0.000079, Tokens per sec:   2871
2023-03-14 03:17:50,140 - INFO - __main__ - Epoch  24: total training loss 32725.13
2023-03-14 03:21:40,907 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 03:21:40,907 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 38.69455933527249, rouge_l = 52.63025937217019, meteor = 0
2023-03-14 03:21:42,048 - INFO - __main__ - Delete test_dir3/39222.ckpt
2023-03-14 03:21:42,186 - INFO - __main__ - Example #0
2023-03-14 03:21:42,186 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 03:21:42,186 - INFO - __main__ - 	Hypothesis: return a hashcode for this principal .
2023-03-14 03:21:42,186 - INFO - __main__ - Example #1
2023-03-14 03:21:42,186 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 03:21:42,186 - INFO - __main__ - 	Hypothesis: call when the view be start .
2023-03-14 03:21:42,186 - INFO - __main__ - Example #2
2023-03-14 03:21:42,186 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 03:21:42,186 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change the ui . this be use to update the jtext component subclass . this method
2023-03-14 03:21:42,193 - INFO - __main__ - Validation time = 227.96345043182373s.
2023-03-14 03:21:42,194 - INFO - __main__ - Epoch 25
2023-03-14 03:21:43,192 - INFO - __main__ - Epoch  25, Step:   52300, Batch Loss:    10.977423, Lr: 0.000079, Tokens per sec:   2169
2023-03-14 03:22:01,238 - INFO - __main__ - Epoch  25, Step:   52400, Batch Loss:    13.912421, Lr: 0.000079, Tokens per sec:   2985
2023-03-14 03:22:21,071 - INFO - __main__ - Epoch  25, Step:   52500, Batch Loss:    11.354407, Lr: 0.000079, Tokens per sec:   2715
2023-03-14 03:22:41,028 - INFO - __main__ - Epoch  25, Step:   52600, Batch Loss:    13.572516, Lr: 0.000079, Tokens per sec:   2648
2023-03-14 03:23:01,038 - INFO - __main__ - Epoch  25, Step:   52700, Batch Loss:    17.892054, Lr: 0.000079, Tokens per sec:   2687
2023-03-14 03:23:19,883 - INFO - __main__ - Epoch  25, Step:   52800, Batch Loss:    15.117715, Lr: 0.000079, Tokens per sec:   2848
2023-03-14 03:23:39,587 - INFO - __main__ - Epoch  25, Step:   52900, Batch Loss:    19.417576, Lr: 0.000079, Tokens per sec:   2716
2023-03-14 03:23:58,051 - INFO - __main__ - Epoch  25, Step:   53000, Batch Loss:    14.048923, Lr: 0.000079, Tokens per sec:   2867
2023-03-14 03:24:16,262 - INFO - __main__ - Epoch  25, Step:   53100, Batch Loss:    21.948650, Lr: 0.000079, Tokens per sec:   3018
2023-03-14 03:24:34,438 - INFO - __main__ - Epoch  25, Step:   53200, Batch Loss:    11.047522, Lr: 0.000079, Tokens per sec:   2951
2023-03-14 03:24:52,477 - INFO - __main__ - Epoch  25, Step:   53300, Batch Loss:    11.945212, Lr: 0.000079, Tokens per sec:   2957
2023-03-14 03:25:11,001 - INFO - __main__ - Epoch  25, Step:   53400, Batch Loss:    20.601160, Lr: 0.000079, Tokens per sec:   2953
2023-03-14 03:25:30,936 - INFO - __main__ - Epoch  25, Step:   53500, Batch Loss:    17.584993, Lr: 0.000079, Tokens per sec:   2626
2023-03-14 03:25:50,399 - INFO - __main__ - Epoch  25, Step:   53600, Batch Loss:     8.759290, Lr: 0.000079, Tokens per sec:   2709
2023-03-14 03:26:10,185 - INFO - __main__ - Epoch  25, Step:   53700, Batch Loss:    13.176560, Lr: 0.000079, Tokens per sec:   2753
2023-03-14 03:26:30,232 - INFO - __main__ - Epoch  25, Step:   53800, Batch Loss:    18.234692, Lr: 0.000079, Tokens per sec:   2668
2023-03-14 03:26:50,189 - INFO - __main__ - Epoch  25, Step:   53900, Batch Loss:    15.765427, Lr: 0.000079, Tokens per sec:   2736
2023-03-14 03:27:10,060 - INFO - __main__ - Epoch  25, Step:   54000, Batch Loss:    17.429115, Lr: 0.000079, Tokens per sec:   2775
2023-03-14 03:27:30,001 - INFO - __main__ - Epoch  25, Step:   54100, Batch Loss:    14.790371, Lr: 0.000079, Tokens per sec:   2707
2023-03-14 03:27:49,931 - INFO - __main__ - Epoch  25, Step:   54200, Batch Loss:    14.128902, Lr: 0.000079, Tokens per sec:   2734
2023-03-14 03:28:09,948 - INFO - __main__ - Epoch  25, Step:   54300, Batch Loss:    25.494419, Lr: 0.000079, Tokens per sec:   2696
2023-03-14 03:28:30,017 - INFO - __main__ - Epoch  25, Step:   54400, Batch Loss:    15.913206, Lr: 0.000079, Tokens per sec:   2696
2023-03-14 03:28:44,974 - INFO - __main__ - Epoch  25: total training loss 31055.46
2023-03-14 03:28:44,974 - INFO - __main__ - Epoch 26
2023-03-14 03:28:50,192 - INFO - __main__ - Epoch  26, Step:   54500, Batch Loss:    11.107488, Lr: 0.000078, Tokens per sec:   2629
2023-03-14 03:29:08,954 - INFO - __main__ - Epoch  26, Step:   54600, Batch Loss:     7.013093, Lr: 0.000078, Tokens per sec:   2818
2023-03-14 03:29:28,166 - INFO - __main__ - Epoch  26, Step:   54700, Batch Loss:    10.639565, Lr: 0.000078, Tokens per sec:   2807
2023-03-14 03:29:47,810 - INFO - __main__ - Epoch  26, Step:   54800, Batch Loss:    11.858890, Lr: 0.000078, Tokens per sec:   2782
2023-03-14 03:30:07,465 - INFO - __main__ - Epoch  26, Step:   54900, Batch Loss:    15.229503, Lr: 0.000078, Tokens per sec:   2734
2023-03-14 03:30:27,435 - INFO - __main__ - Epoch  26, Step:   55000, Batch Loss:    11.736465, Lr: 0.000078, Tokens per sec:   2673
2023-03-14 03:30:47,291 - INFO - __main__ - Epoch  26, Step:   55100, Batch Loss:     9.198574, Lr: 0.000078, Tokens per sec:   2686
2023-03-14 03:31:07,223 - INFO - __main__ - Epoch  26, Step:   55200, Batch Loss:    16.198376, Lr: 0.000078, Tokens per sec:   2691
2023-03-14 03:31:27,103 - INFO - __main__ - Epoch  26, Step:   55300, Batch Loss:    13.762121, Lr: 0.000078, Tokens per sec:   2710
2023-03-14 03:31:45,556 - INFO - __main__ - Epoch  26, Step:   55400, Batch Loss:    11.879993, Lr: 0.000078, Tokens per sec:   2894
2023-03-14 03:32:04,276 - INFO - __main__ - Epoch  26, Step:   55500, Batch Loss:    11.913238, Lr: 0.000078, Tokens per sec:   2907
2023-03-14 03:32:23,522 - INFO - __main__ - Epoch  26, Step:   55600, Batch Loss:    12.431873, Lr: 0.000078, Tokens per sec:   2792
2023-03-14 03:32:41,455 - INFO - __main__ - Epoch  26, Step:   55700, Batch Loss:     9.782747, Lr: 0.000078, Tokens per sec:   3011
2023-03-14 03:32:59,411 - INFO - __main__ - Epoch  26, Step:   55800, Batch Loss:    13.409228, Lr: 0.000078, Tokens per sec:   2990
2023-03-14 03:33:17,366 - INFO - __main__ - Epoch  26, Step:   55900, Batch Loss:    17.020130, Lr: 0.000078, Tokens per sec:   2987
2023-03-14 03:33:35,339 - INFO - __main__ - Epoch  26, Step:   56000, Batch Loss:    17.641830, Lr: 0.000078, Tokens per sec:   2968
2023-03-14 03:33:53,397 - INFO - __main__ - Epoch  26, Step:   56100, Batch Loss:    11.898284, Lr: 0.000078, Tokens per sec:   3017
2023-03-14 03:34:11,472 - INFO - __main__ - Epoch  26, Step:   56200, Batch Loss:    14.684690, Lr: 0.000078, Tokens per sec:   3014
2023-03-14 03:34:30,580 - INFO - __main__ - Epoch  26, Step:   56300, Batch Loss:    14.018253, Lr: 0.000078, Tokens per sec:   2831
2023-03-14 03:34:48,820 - INFO - __main__ - Epoch  26, Step:   56400, Batch Loss:    19.623217, Lr: 0.000078, Tokens per sec:   2962
2023-03-14 03:35:06,809 - INFO - __main__ - Epoch  26, Step:   56500, Batch Loss:    12.377383, Lr: 0.000078, Tokens per sec:   2981
2023-03-14 03:35:24,924 - INFO - __main__ - Epoch  26, Step:   56600, Batch Loss:    13.845301, Lr: 0.000078, Tokens per sec:   2981
2023-03-14 03:35:35,578 - INFO - __main__ - Epoch  26: total training loss 29539.63
2023-03-14 03:39:22,904 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 03:39:22,904 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 39.54333283436144, rouge_l = 53.25905158397662, meteor = 0
2023-03-14 03:39:24,034 - INFO - __main__ - Delete test_dir3/43580.ckpt
2023-03-14 03:39:24,173 - INFO - __main__ - Example #0
2023-03-14 03:39:24,173 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 03:39:24,173 - INFO - __main__ - 	Hypothesis: return a hashcode for this string constant .
2023-03-14 03:39:24,173 - INFO - __main__ - Example #1
2023-03-14 03:39:24,173 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 03:39:24,173 - INFO - __main__ - 	Hypothesis: call when the activity start .
2023-03-14 03:39:24,173 - INFO - __main__ - Example #2
2023-03-14 03:39:24,173 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 03:39:24,173 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change the ui of jtext component subclass . this method get call when the associate jtext
2023-03-14 03:39:24,180 - INFO - __main__ - Validation time = 224.21814632415771s.
2023-03-14 03:39:24,180 - INFO - __main__ - Epoch 27
2023-03-14 03:39:33,620 - INFO - __main__ - Epoch  27, Step:   56700, Batch Loss:    12.278028, Lr: 0.000077, Tokens per sec:   2656
2023-03-14 03:39:53,565 - INFO - __main__ - Epoch  27, Step:   56800, Batch Loss:    13.256947, Lr: 0.000077, Tokens per sec:   2709
2023-03-14 03:40:13,273 - INFO - __main__ - Epoch  27, Step:   56900, Batch Loss:    10.763019, Lr: 0.000077, Tokens per sec:   2703
2023-03-14 03:40:33,169 - INFO - __main__ - Epoch  27, Step:   57000, Batch Loss:    14.396383, Lr: 0.000077, Tokens per sec:   2700
2023-03-14 03:40:53,021 - INFO - __main__ - Epoch  27, Step:   57100, Batch Loss:     8.642339, Lr: 0.000077, Tokens per sec:   2733
2023-03-14 03:41:11,763 - INFO - __main__ - Epoch  27, Step:   57200, Batch Loss:    15.578450, Lr: 0.000077, Tokens per sec:   2847
2023-03-14 03:41:30,914 - INFO - __main__ - Epoch  27, Step:   57300, Batch Loss:    17.430487, Lr: 0.000077, Tokens per sec:   2833
2023-03-14 03:41:50,879 - INFO - __main__ - Epoch  27, Step:   57400, Batch Loss:    17.188871, Lr: 0.000077, Tokens per sec:   2767
2023-03-14 03:42:10,699 - INFO - __main__ - Epoch  27, Step:   57500, Batch Loss:    13.130376, Lr: 0.000077, Tokens per sec:   2685
2023-03-14 03:42:29,797 - INFO - __main__ - Epoch  27, Step:   57600, Batch Loss:     7.152997, Lr: 0.000077, Tokens per sec:   2842
2023-03-14 03:42:47,748 - INFO - __main__ - Epoch  27, Step:   57700, Batch Loss:    14.400826, Lr: 0.000077, Tokens per sec:   3074
2023-03-14 03:43:06,831 - INFO - __main__ - Epoch  27, Step:   57800, Batch Loss:    10.383961, Lr: 0.000077, Tokens per sec:   2805
2023-03-14 03:43:26,796 - INFO - __main__ - Epoch  27, Step:   57900, Batch Loss:     9.082977, Lr: 0.000077, Tokens per sec:   2694
2023-03-14 03:43:46,728 - INFO - __main__ - Epoch  27, Step:   58000, Batch Loss:    13.758542, Lr: 0.000077, Tokens per sec:   2699
2023-03-14 03:44:06,649 - INFO - __main__ - Epoch  27, Step:   58100, Batch Loss:    15.458255, Lr: 0.000077, Tokens per sec:   2701
2023-03-14 03:44:26,578 - INFO - __main__ - Epoch  27, Step:   58200, Batch Loss:    12.692585, Lr: 0.000077, Tokens per sec:   2688
2023-03-14 03:44:46,144 - INFO - __main__ - Epoch  27, Step:   58300, Batch Loss:    16.168648, Lr: 0.000077, Tokens per sec:   2782
2023-03-14 03:45:06,045 - INFO - __main__ - Epoch  27, Step:   58400, Batch Loss:    11.770081, Lr: 0.000077, Tokens per sec:   2690
2023-03-14 03:45:25,958 - INFO - __main__ - Epoch  27, Step:   58500, Batch Loss:    11.441943, Lr: 0.000077, Tokens per sec:   2648
2023-03-14 03:45:45,925 - INFO - __main__ - Epoch  27, Step:   58600, Batch Loss:    10.541867, Lr: 0.000077, Tokens per sec:   2667
2023-03-14 03:46:05,830 - INFO - __main__ - Epoch  27, Step:   58700, Batch Loss:    15.167973, Lr: 0.000077, Tokens per sec:   2675
2023-03-14 03:46:25,754 - INFO - __main__ - Epoch  27, Step:   58800, Batch Loss:    13.951624, Lr: 0.000077, Tokens per sec:   2692
2023-03-14 03:46:32,380 - INFO - __main__ - Epoch  27: total training loss 28086.57
2023-03-14 03:46:32,380 - INFO - __main__ - Epoch 28
2023-03-14 03:46:45,975 - INFO - __main__ - Epoch  28, Step:   58900, Batch Loss:    14.960471, Lr: 0.000076, Tokens per sec:   2631
2023-03-14 03:47:05,867 - INFO - __main__ - Epoch  28, Step:   59000, Batch Loss:     9.441534, Lr: 0.000076, Tokens per sec:   2726
2023-03-14 03:47:25,479 - INFO - __main__ - Epoch  28, Step:   59100, Batch Loss:    14.800447, Lr: 0.000076, Tokens per sec:   2745
2023-03-14 03:47:45,375 - INFO - __main__ - Epoch  28, Step:   59200, Batch Loss:    12.828570, Lr: 0.000076, Tokens per sec:   2739
2023-03-14 03:48:04,431 - INFO - __main__ - Epoch  28, Step:   59300, Batch Loss:     9.966045, Lr: 0.000076, Tokens per sec:   2823
2023-03-14 03:48:22,896 - INFO - __main__ - Epoch  28, Step:   59400, Batch Loss:    12.858739, Lr: 0.000076, Tokens per sec:   2912
2023-03-14 03:48:40,846 - INFO - __main__ - Epoch  28, Step:   59500, Batch Loss:     9.856689, Lr: 0.000076, Tokens per sec:   3009
2023-03-14 03:48:58,770 - INFO - __main__ - Epoch  28, Step:   59600, Batch Loss:    11.989629, Lr: 0.000076, Tokens per sec:   3042
2023-03-14 03:49:17,066 - INFO - __main__ - Epoch  28, Step:   59700, Batch Loss:    13.749249, Lr: 0.000076, Tokens per sec:   2932
2023-03-14 03:49:36,419 - INFO - __main__ - Epoch  28, Step:   59800, Batch Loss:    10.276575, Lr: 0.000076, Tokens per sec:   2783
2023-03-14 03:49:54,434 - INFO - __main__ - Epoch  28, Step:   59900, Batch Loss:    16.086153, Lr: 0.000076, Tokens per sec:   2916
2023-03-14 03:50:12,414 - INFO - __main__ - Epoch  28, Step:   60000, Batch Loss:    12.257239, Lr: 0.000076, Tokens per sec:   3009
2023-03-14 03:50:30,365 - INFO - __main__ - Epoch  28, Step:   60100, Batch Loss:    14.749613, Lr: 0.000076, Tokens per sec:   3051
2023-03-14 03:50:48,321 - INFO - __main__ - Epoch  28, Step:   60200, Batch Loss:    14.289280, Lr: 0.000076, Tokens per sec:   3011
2023-03-14 03:51:06,252 - INFO - __main__ - Epoch  28, Step:   60300, Batch Loss:    11.104103, Lr: 0.000076, Tokens per sec:   2917
2023-03-14 03:51:24,224 - INFO - __main__ - Epoch  28, Step:   60400, Batch Loss:    11.633942, Lr: 0.000076, Tokens per sec:   3012
2023-03-14 03:51:42,184 - INFO - __main__ - Epoch  28, Step:   60500, Batch Loss:    14.951491, Lr: 0.000076, Tokens per sec:   2982
2023-03-14 03:52:00,976 - INFO - __main__ - Epoch  28, Step:   60600, Batch Loss:    12.162388, Lr: 0.000076, Tokens per sec:   2881
2023-03-14 03:52:20,924 - INFO - __main__ - Epoch  28, Step:   60700, Batch Loss:    13.272214, Lr: 0.000076, Tokens per sec:   2707
2023-03-14 03:52:39,366 - INFO - __main__ - Epoch  28, Step:   60800, Batch Loss:    16.004900, Lr: 0.000076, Tokens per sec:   2958
2023-03-14 03:52:59,016 - INFO - __main__ - Epoch  28, Step:   60900, Batch Loss:    11.349335, Lr: 0.000076, Tokens per sec:   2719
2023-03-14 03:53:18,896 - INFO - __main__ - Epoch  28, Step:   61000, Batch Loss:    14.179739, Lr: 0.000076, Tokens per sec:   2686
2023-03-14 03:53:21,045 - INFO - __main__ - Epoch  28: total training loss 26677.57
2023-03-14 03:57:07,924 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 03:57:07,925 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 40.31338020575044, rouge_l = 53.85343190269939, meteor = 0
2023-03-14 03:57:09,057 - INFO - __main__ - Delete test_dir3/47938.ckpt
2023-03-14 03:57:09,191 - INFO - __main__ - Example #0
2023-03-14 03:57:09,191 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 03:57:09,191 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 03:57:09,191 - INFO - __main__ - Example #1
2023-03-14 03:57:09,191 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 03:57:09,191 - INFO - __main__ - 	Hypothesis: call when the bean start .
2023-03-14 03:57:09,191 - INFO - __main__ - Example #2
2023-03-14 03:57:09,191 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 03:57:09,191 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 03:57:09,198 - INFO - __main__ - Validation time = 224.05926370620728s.
2023-03-14 03:57:09,198 - INFO - __main__ - Epoch 29
2023-03-14 03:57:26,950 - INFO - __main__ - Epoch  29, Step:   61100, Batch Loss:     7.690646, Lr: 0.000075, Tokens per sec:   2664
2023-03-14 03:57:46,866 - INFO - __main__ - Epoch  29, Step:   61200, Batch Loss:    10.595725, Lr: 0.000075, Tokens per sec:   2725
2023-03-14 03:58:06,719 - INFO - __main__ - Epoch  29, Step:   61300, Batch Loss:    11.439504, Lr: 0.000075, Tokens per sec:   2643
2023-03-14 03:58:26,591 - INFO - __main__ - Epoch  29, Step:   61400, Batch Loss:    15.091321, Lr: 0.000075, Tokens per sec:   2701
2023-03-14 03:58:46,650 - INFO - __main__ - Epoch  29, Step:   61500, Batch Loss:     8.566786, Lr: 0.000075, Tokens per sec:   2728
2023-03-14 03:59:06,653 - INFO - __main__ - Epoch  29, Step:   61600, Batch Loss:    14.668810, Lr: 0.000075, Tokens per sec:   2732
2023-03-14 03:59:26,493 - INFO - __main__ - Epoch  29, Step:   61700, Batch Loss:    11.611067, Lr: 0.000075, Tokens per sec:   2738
2023-03-14 03:59:44,874 - INFO - __main__ - Epoch  29, Step:   61800, Batch Loss:    10.381287, Lr: 0.000075, Tokens per sec:   2964
2023-03-14 04:00:02,674 - INFO - __main__ - Epoch  29, Step:   61900, Batch Loss:    14.438165, Lr: 0.000075, Tokens per sec:   3072
2023-03-14 04:00:21,928 - INFO - __main__ - Epoch  29, Step:   62000, Batch Loss:    11.852553, Lr: 0.000075, Tokens per sec:   2739
2023-03-14 04:00:41,426 - INFO - __main__ - Epoch  29, Step:   62100, Batch Loss:    12.021810, Lr: 0.000075, Tokens per sec:   2723
2023-03-14 04:01:01,285 - INFO - __main__ - Epoch  29, Step:   62200, Batch Loss:    10.023200, Lr: 0.000075, Tokens per sec:   2736
2023-03-14 04:01:21,238 - INFO - __main__ - Epoch  29, Step:   62300, Batch Loss:     7.932746, Lr: 0.000075, Tokens per sec:   2683
2023-03-14 04:01:40,783 - INFO - __main__ - Epoch  29, Step:   62400, Batch Loss:    12.491474, Lr: 0.000075, Tokens per sec:   2773
2023-03-14 04:02:00,663 - INFO - __main__ - Epoch  29, Step:   62500, Batch Loss:    13.439346, Lr: 0.000075, Tokens per sec:   2702
2023-03-14 04:02:19,260 - INFO - __main__ - Epoch  29, Step:   62600, Batch Loss:    15.003936, Lr: 0.000075, Tokens per sec:   2877
2023-03-14 04:02:38,355 - INFO - __main__ - Epoch  29, Step:   62700, Batch Loss:    13.482072, Lr: 0.000075, Tokens per sec:   2829
2023-03-14 04:02:56,468 - INFO - __main__ - Epoch  29, Step:   62800, Batch Loss:    14.756195, Lr: 0.000075, Tokens per sec:   2931
2023-03-14 04:03:16,144 - INFO - __main__ - Epoch  29, Step:   62900, Batch Loss:    15.243316, Lr: 0.000075, Tokens per sec:   2721
2023-03-14 04:03:34,195 - INFO - __main__ - Epoch  29, Step:   63000, Batch Loss:    13.074051, Lr: 0.000075, Tokens per sec:   2956
2023-03-14 04:03:54,144 - INFO - __main__ - Epoch  29, Step:   63100, Batch Loss:    14.684084, Lr: 0.000075, Tokens per sec:   2672
2023-03-14 04:04:12,241 - INFO - __main__ - Epoch  29: total training loss 25419.70
2023-03-14 04:04:12,242 - INFO - __main__ - Epoch 30
2023-03-14 04:04:14,320 - INFO - __main__ - Epoch  30, Step:   63200, Batch Loss:     9.104038, Lr: 0.000075, Tokens per sec:   2155
2023-03-14 04:04:34,216 - INFO - __main__ - Epoch  30, Step:   63300, Batch Loss:     9.644945, Lr: 0.000075, Tokens per sec:   2755
2023-03-14 04:04:54,049 - INFO - __main__ - Epoch  30, Step:   63400, Batch Loss:     9.539440, Lr: 0.000075, Tokens per sec:   2700
2023-03-14 04:05:13,771 - INFO - __main__ - Epoch  30, Step:   63500, Batch Loss:    13.058101, Lr: 0.000075, Tokens per sec:   2727
2023-03-14 04:05:33,577 - INFO - __main__ - Epoch  30, Step:   63600, Batch Loss:    10.391699, Lr: 0.000075, Tokens per sec:   2718
2023-03-14 04:05:53,373 - INFO - __main__ - Epoch  30, Step:   63700, Batch Loss:    12.637692, Lr: 0.000075, Tokens per sec:   2752
2023-03-14 04:06:13,266 - INFO - __main__ - Epoch  30, Step:   63800, Batch Loss:    13.470436, Lr: 0.000075, Tokens per sec:   2743
2023-03-14 04:06:33,081 - INFO - __main__ - Epoch  30, Step:   63900, Batch Loss:     9.668714, Lr: 0.000075, Tokens per sec:   2719
2023-03-14 04:06:52,858 - INFO - __main__ - Epoch  30, Step:   64000, Batch Loss:     9.706147, Lr: 0.000075, Tokens per sec:   2722
2023-03-14 04:07:12,616 - INFO - __main__ - Epoch  30, Step:   64100, Batch Loss:     8.841360, Lr: 0.000075, Tokens per sec:   2746
2023-03-14 04:07:32,394 - INFO - __main__ - Epoch  30, Step:   64200, Batch Loss:    12.594862, Lr: 0.000075, Tokens per sec:   2713
2023-03-14 04:07:52,215 - INFO - __main__ - Epoch  30, Step:   64300, Batch Loss:    13.834792, Lr: 0.000075, Tokens per sec:   2673
2023-03-14 04:08:11,917 - INFO - __main__ - Epoch  30, Step:   64400, Batch Loss:    11.302537, Lr: 0.000075, Tokens per sec:   2718
2023-03-14 04:08:31,597 - INFO - __main__ - Epoch  30, Step:   64500, Batch Loss:    15.365314, Lr: 0.000075, Tokens per sec:   2748
2023-03-14 04:08:49,336 - INFO - __main__ - Epoch  30, Step:   64600, Batch Loss:    11.712408, Lr: 0.000075, Tokens per sec:   3029
2023-03-14 04:09:06,928 - INFO - __main__ - Epoch  30, Step:   64700, Batch Loss:     9.949448, Lr: 0.000075, Tokens per sec:   2983
2023-03-14 04:09:24,649 - INFO - __main__ - Epoch  30, Step:   64800, Batch Loss:     7.334551, Lr: 0.000075, Tokens per sec:   3031
2023-03-14 04:09:42,310 - INFO - __main__ - Epoch  30, Step:   64900, Batch Loss:    15.457751, Lr: 0.000075, Tokens per sec:   3114
2023-03-14 04:10:01,599 - INFO - __main__ - Epoch  30, Step:   65000, Batch Loss:    13.615644, Lr: 0.000075, Tokens per sec:   2766
2023-03-14 04:10:21,331 - INFO - __main__ - Epoch  30, Step:   65100, Batch Loss:    10.129217, Lr: 0.000075, Tokens per sec:   2736
2023-03-14 04:10:41,058 - INFO - __main__ - Epoch  30, Step:   65200, Batch Loss:    11.912898, Lr: 0.000075, Tokens per sec:   2691
2023-03-14 04:11:00,856 - INFO - __main__ - Epoch  30, Step:   65300, Batch Loss:    14.170398, Lr: 0.000075, Tokens per sec:   2755
2023-03-14 04:11:14,705 - INFO - __main__ - Epoch  30: total training loss 24251.93
2023-03-14 04:15:03,689 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 04:15:03,690 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 40.98949182070896, rouge_l = 54.24708717363688, meteor = 0
2023-03-14 04:15:04,798 - INFO - __main__ - Delete test_dir3/52296.ckpt
2023-03-14 04:15:04,914 - INFO - __main__ - Example #0
2023-03-14 04:15:04,914 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 04:15:04,914 - INFO - __main__ - 	Hypothesis: return a hashcode for this string constant .
2023-03-14 04:15:04,914 - INFO - __main__ - Example #1
2023-03-14 04:15:04,914 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 04:15:04,914 - INFO - __main__ - 	Hypothesis: call when the start of the view be start .
2023-03-14 04:15:04,914 - INFO - __main__ - Example #2
2023-03-14 04:15:04,914 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 04:15:04,914 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 04:15:04,920 - INFO - __main__ - Validation time = 225.7996699810028s.
2023-03-14 04:15:04,920 - INFO - __main__ - Epoch 31
2023-03-14 04:15:10,534 - INFO - __main__ - Epoch  31, Step:   65400, Batch Loss:    11.180488, Lr: 0.000074, Tokens per sec:   2859
2023-03-14 04:15:29,794 - INFO - __main__ - Epoch  31, Step:   65500, Batch Loss:     7.738264, Lr: 0.000074, Tokens per sec:   2801
2023-03-14 04:15:49,545 - INFO - __main__ - Epoch  31, Step:   65600, Batch Loss:    10.954910, Lr: 0.000074, Tokens per sec:   2734
2023-03-14 04:16:09,358 - INFO - __main__ - Epoch  31, Step:   65700, Batch Loss:     8.567815, Lr: 0.000074, Tokens per sec:   2722
2023-03-14 04:16:29,088 - INFO - __main__ - Epoch  31, Step:   65800, Batch Loss:     9.559402, Lr: 0.000074, Tokens per sec:   2684
2023-03-14 04:16:48,820 - INFO - __main__ - Epoch  31, Step:   65900, Batch Loss:     8.327536, Lr: 0.000074, Tokens per sec:   2700
2023-03-14 04:17:08,597 - INFO - __main__ - Epoch  31, Step:   66000, Batch Loss:    10.623676, Lr: 0.000074, Tokens per sec:   2700
2023-03-14 04:17:28,388 - INFO - __main__ - Epoch  31, Step:   66100, Batch Loss:    10.810900, Lr: 0.000074, Tokens per sec:   2728
2023-03-14 04:17:48,162 - INFO - __main__ - Epoch  31, Step:   66200, Batch Loss:     6.952757, Lr: 0.000074, Tokens per sec:   2732
2023-03-14 04:18:07,918 - INFO - __main__ - Epoch  31, Step:   66300, Batch Loss:    11.713362, Lr: 0.000074, Tokens per sec:   2675
2023-03-14 04:18:27,727 - INFO - __main__ - Epoch  31, Step:   66400, Batch Loss:    11.369205, Lr: 0.000074, Tokens per sec:   2773
2023-03-14 04:18:47,499 - INFO - __main__ - Epoch  31, Step:   66500, Batch Loss:     7.673500, Lr: 0.000074, Tokens per sec:   2737
2023-03-14 04:19:07,244 - INFO - __main__ - Epoch  31, Step:   66600, Batch Loss:    11.404062, Lr: 0.000074, Tokens per sec:   2704
2023-03-14 04:19:27,025 - INFO - __main__ - Epoch  31, Step:   66700, Batch Loss:     8.968315, Lr: 0.000074, Tokens per sec:   2714
2023-03-14 04:19:46,745 - INFO - __main__ - Epoch  31, Step:   66800, Batch Loss:     9.313418, Lr: 0.000074, Tokens per sec:   2737
2023-03-14 04:20:06,483 - INFO - __main__ - Epoch  31, Step:   66900, Batch Loss:    10.373258, Lr: 0.000074, Tokens per sec:   2721
2023-03-14 04:20:26,270 - INFO - __main__ - Epoch  31, Step:   67000, Batch Loss:    12.328427, Lr: 0.000074, Tokens per sec:   2789
2023-03-14 04:20:45,967 - INFO - __main__ - Epoch  31, Step:   67100, Batch Loss:    11.534995, Lr: 0.000074, Tokens per sec:   2736
2023-03-14 04:21:05,711 - INFO - __main__ - Epoch  31, Step:   67200, Batch Loss:    10.727812, Lr: 0.000074, Tokens per sec:   2714
2023-03-14 04:21:25,494 - INFO - __main__ - Epoch  31, Step:   67300, Batch Loss:    15.042774, Lr: 0.000074, Tokens per sec:   2741
2023-03-14 04:21:45,198 - INFO - __main__ - Epoch  31, Step:   67400, Batch Loss:     9.757493, Lr: 0.000074, Tokens per sec:   2719
2023-03-14 04:22:04,977 - INFO - __main__ - Epoch  31, Step:   67500, Batch Loss:    10.127885, Lr: 0.000074, Tokens per sec:   2699
2023-03-14 04:22:14,702 - INFO - __main__ - Epoch  31: total training loss 23224.81
2023-03-14 04:22:14,702 - INFO - __main__ - Epoch 32
2023-03-14 04:22:25,103 - INFO - __main__ - Epoch  32, Step:   67600, Batch Loss:     7.411794, Lr: 0.000073, Tokens per sec:   2623
2023-03-14 04:22:44,829 - INFO - __main__ - Epoch  32, Step:   67700, Batch Loss:     8.863929, Lr: 0.000073, Tokens per sec:   2754
2023-03-14 04:23:04,566 - INFO - __main__ - Epoch  32, Step:   67800, Batch Loss:     9.106791, Lr: 0.000073, Tokens per sec:   2759
2023-03-14 04:23:24,330 - INFO - __main__ - Epoch  32, Step:   67900, Batch Loss:     8.921086, Lr: 0.000073, Tokens per sec:   2727
2023-03-14 04:23:44,056 - INFO - __main__ - Epoch  32, Step:   68000, Batch Loss:     7.556196, Lr: 0.000073, Tokens per sec:   2648
2023-03-14 04:24:03,844 - INFO - __main__ - Epoch  32, Step:   68100, Batch Loss:     9.256096, Lr: 0.000073, Tokens per sec:   2742
2023-03-14 04:24:23,600 - INFO - __main__ - Epoch  32, Step:   68200, Batch Loss:     7.574066, Lr: 0.000073, Tokens per sec:   2724
2023-03-14 04:24:43,336 - INFO - __main__ - Epoch  32, Step:   68300, Batch Loss:    14.079024, Lr: 0.000073, Tokens per sec:   2701
2023-03-14 04:25:03,110 - INFO - __main__ - Epoch  32, Step:   68400, Batch Loss:     9.132473, Lr: 0.000073, Tokens per sec:   2703
2023-03-14 04:25:22,818 - INFO - __main__ - Epoch  32, Step:   68500, Batch Loss:    10.428567, Lr: 0.000073, Tokens per sec:   2755
2023-03-14 04:25:42,633 - INFO - __main__ - Epoch  32, Step:   68600, Batch Loss:    11.289387, Lr: 0.000073, Tokens per sec:   2732
2023-03-14 04:26:02,372 - INFO - __main__ - Epoch  32, Step:   68700, Batch Loss:    10.903242, Lr: 0.000073, Tokens per sec:   2660
2023-03-14 04:26:22,145 - INFO - __main__ - Epoch  32, Step:   68800, Batch Loss:    11.970001, Lr: 0.000073, Tokens per sec:   2754
2023-03-14 04:26:41,914 - INFO - __main__ - Epoch  32, Step:   68900, Batch Loss:     8.801279, Lr: 0.000073, Tokens per sec:   2737
2023-03-14 04:27:01,671 - INFO - __main__ - Epoch  32, Step:   69000, Batch Loss:    14.287432, Lr: 0.000073, Tokens per sec:   2729
2023-03-14 04:27:21,360 - INFO - __main__ - Epoch  32, Step:   69100, Batch Loss:     9.532460, Lr: 0.000073, Tokens per sec:   2734
2023-03-14 04:27:41,054 - INFO - __main__ - Epoch  32, Step:   69200, Batch Loss:     7.247879, Lr: 0.000073, Tokens per sec:   2799
2023-03-14 04:28:00,728 - INFO - __main__ - Epoch  32, Step:   69300, Batch Loss:    12.080170, Lr: 0.000073, Tokens per sec:   2745
2023-03-14 04:28:20,576 - INFO - __main__ - Epoch  32, Step:   69400, Batch Loss:     9.528761, Lr: 0.000073, Tokens per sec:   2718
2023-03-14 04:28:40,353 - INFO - __main__ - Epoch  32, Step:   69500, Batch Loss:     8.587040, Lr: 0.000073, Tokens per sec:   2722
2023-03-14 04:29:00,150 - INFO - __main__ - Epoch  32, Step:   69600, Batch Loss:    10.907666, Lr: 0.000073, Tokens per sec:   2743
2023-03-14 04:29:19,906 - INFO - __main__ - Epoch  32, Step:   69700, Batch Loss:     9.494372, Lr: 0.000073, Tokens per sec:   2648
2023-03-14 04:29:25,507 - INFO - __main__ - Epoch  32: total training loss 22226.03
2023-03-14 04:33:11,038 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 04:33:11,039 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 41.598285535167506, rouge_l = 54.54434852414477, meteor = 0
2023-03-14 04:33:12,170 - INFO - __main__ - Delete test_dir3/56654.ckpt
2023-03-14 04:33:12,312 - INFO - __main__ - Example #0
2023-03-14 04:33:12,313 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 04:33:12,313 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 04:33:12,313 - INFO - __main__ - Example #1
2023-03-14 04:33:12,313 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 04:33:12,313 - INFO - __main__ - 	Hypothesis: call when the bean be start .
2023-03-14 04:33:12,313 - INFO - __main__ - Example #2
2023-03-14 04:33:12,313 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 04:33:12,313 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 04:33:12,319 - INFO - __main__ - Validation time = 222.71222066879272s.
2023-03-14 04:33:12,320 - INFO - __main__ - Epoch 33
2023-03-14 04:33:26,842 - INFO - __main__ - Epoch  33, Step:   69800, Batch Loss:     7.169662, Lr: 0.000072, Tokens per sec:   2683
2023-03-14 04:33:46,556 - INFO - __main__ - Epoch  33, Step:   69900, Batch Loss:     9.942012, Lr: 0.000072, Tokens per sec:   2714
2023-03-14 04:34:06,376 - INFO - __main__ - Epoch  33, Step:   70000, Batch Loss:     7.778522, Lr: 0.000072, Tokens per sec:   2731
2023-03-14 04:34:26,085 - INFO - __main__ - Epoch  33, Step:   70100, Batch Loss:     7.397740, Lr: 0.000072, Tokens per sec:   2667
2023-03-14 04:34:45,903 - INFO - __main__ - Epoch  33, Step:   70200, Batch Loss:    12.165798, Lr: 0.000072, Tokens per sec:   2747
2023-03-14 04:35:05,637 - INFO - __main__ - Epoch  33, Step:   70300, Batch Loss:     7.507034, Lr: 0.000072, Tokens per sec:   2721
2023-03-14 04:35:25,393 - INFO - __main__ - Epoch  33, Step:   70400, Batch Loss:     6.577232, Lr: 0.000072, Tokens per sec:   2714
2023-03-14 04:35:45,113 - INFO - __main__ - Epoch  33, Step:   70500, Batch Loss:     7.481878, Lr: 0.000072, Tokens per sec:   2721
2023-03-14 04:36:04,811 - INFO - __main__ - Epoch  33, Step:   70600, Batch Loss:     6.345289, Lr: 0.000072, Tokens per sec:   2776
2023-03-14 04:36:24,596 - INFO - __main__ - Epoch  33, Step:   70700, Batch Loss:    12.275783, Lr: 0.000072, Tokens per sec:   2713
2023-03-14 04:36:44,307 - INFO - __main__ - Epoch  33, Step:   70800, Batch Loss:    13.171812, Lr: 0.000072, Tokens per sec:   2716
2023-03-14 04:37:04,017 - INFO - __main__ - Epoch  33, Step:   70900, Batch Loss:    11.650394, Lr: 0.000072, Tokens per sec:   2711
2023-03-14 04:37:23,802 - INFO - __main__ - Epoch  33, Step:   71000, Batch Loss:     8.063093, Lr: 0.000072, Tokens per sec:   2695
2023-03-14 04:37:43,555 - INFO - __main__ - Epoch  33, Step:   71100, Batch Loss:     7.960173, Lr: 0.000072, Tokens per sec:   2705
2023-03-14 04:38:03,260 - INFO - __main__ - Epoch  33, Step:   71200, Batch Loss:     9.415055, Lr: 0.000072, Tokens per sec:   2720
2023-03-14 04:38:23,035 - INFO - __main__ - Epoch  33, Step:   71300, Batch Loss:     9.886685, Lr: 0.000072, Tokens per sec:   2739
2023-03-14 04:38:42,805 - INFO - __main__ - Epoch  33, Step:   71400, Batch Loss:     8.502018, Lr: 0.000072, Tokens per sec:   2706
2023-03-14 04:39:02,578 - INFO - __main__ - Epoch  33, Step:   71500, Batch Loss:    10.698034, Lr: 0.000072, Tokens per sec:   2733
2023-03-14 04:39:22,334 - INFO - __main__ - Epoch  33, Step:   71600, Batch Loss:     9.448105, Lr: 0.000072, Tokens per sec:   2757
2023-03-14 04:39:42,111 - INFO - __main__ - Epoch  33, Step:   71700, Batch Loss:    12.003914, Lr: 0.000072, Tokens per sec:   2721
2023-03-14 04:40:01,860 - INFO - __main__ - Epoch  33, Step:   71800, Batch Loss:    12.394433, Lr: 0.000072, Tokens per sec:   2823
2023-03-14 04:40:21,635 - INFO - __main__ - Epoch  33, Step:   71900, Batch Loss:     9.829041, Lr: 0.000072, Tokens per sec:   2706
2023-03-14 04:40:23,069 - INFO - __main__ - Epoch  33: total training loss 21238.74
2023-03-14 04:40:23,069 - INFO - __main__ - Epoch 34
2023-03-14 04:40:41,732 - INFO - __main__ - Epoch  34, Step:   72000, Batch Loss:    12.338315, Lr: 0.000072, Tokens per sec:   2673
2023-03-14 04:41:01,459 - INFO - __main__ - Epoch  34, Step:   72100, Batch Loss:     8.357237, Lr: 0.000072, Tokens per sec:   2735
2023-03-14 04:41:21,191 - INFO - __main__ - Epoch  34, Step:   72200, Batch Loss:     7.375324, Lr: 0.000072, Tokens per sec:   2732
2023-03-14 04:41:40,967 - INFO - __main__ - Epoch  34, Step:   72300, Batch Loss:     6.947790, Lr: 0.000072, Tokens per sec:   2729
2023-03-14 04:42:00,715 - INFO - __main__ - Epoch  34, Step:   72400, Batch Loss:    13.099083, Lr: 0.000072, Tokens per sec:   2753
2023-03-14 04:42:20,379 - INFO - __main__ - Epoch  34, Step:   72500, Batch Loss:     8.895398, Lr: 0.000072, Tokens per sec:   2669
2023-03-14 04:42:40,136 - INFO - __main__ - Epoch  34, Step:   72600, Batch Loss:    10.712879, Lr: 0.000072, Tokens per sec:   2805
2023-03-14 04:42:59,822 - INFO - __main__ - Epoch  34, Step:   72700, Batch Loss:     6.990089, Lr: 0.000072, Tokens per sec:   2677
2023-03-14 04:43:19,596 - INFO - __main__ - Epoch  34, Step:   72800, Batch Loss:     9.208572, Lr: 0.000072, Tokens per sec:   2722
2023-03-14 04:43:39,409 - INFO - __main__ - Epoch  34, Step:   72900, Batch Loss:     8.264949, Lr: 0.000072, Tokens per sec:   2691
2023-03-14 04:43:59,158 - INFO - __main__ - Epoch  34, Step:   73000, Batch Loss:     7.139328, Lr: 0.000072, Tokens per sec:   2710
2023-03-14 04:44:18,956 - INFO - __main__ - Epoch  34, Step:   73100, Batch Loss:     9.177248, Lr: 0.000072, Tokens per sec:   2700
2023-03-14 04:44:38,762 - INFO - __main__ - Epoch  34, Step:   73200, Batch Loss:     8.304110, Lr: 0.000072, Tokens per sec:   2743
2023-03-14 04:44:58,509 - INFO - __main__ - Epoch  34, Step:   73300, Batch Loss:     8.542560, Lr: 0.000072, Tokens per sec:   2769
2023-03-14 04:45:18,239 - INFO - __main__ - Epoch  34, Step:   73400, Batch Loss:     9.413674, Lr: 0.000072, Tokens per sec:   2719
2023-03-14 04:45:38,071 - INFO - __main__ - Epoch  34, Step:   73500, Batch Loss:    11.012634, Lr: 0.000072, Tokens per sec:   2766
2023-03-14 04:45:57,863 - INFO - __main__ - Epoch  34, Step:   73600, Batch Loss:     7.239619, Lr: 0.000072, Tokens per sec:   2675
2023-03-14 04:46:17,602 - INFO - __main__ - Epoch  34, Step:   73700, Batch Loss:     7.515692, Lr: 0.000072, Tokens per sec:   2733
2023-03-14 04:46:37,312 - INFO - __main__ - Epoch  34, Step:   73800, Batch Loss:     8.630940, Lr: 0.000072, Tokens per sec:   2730
2023-03-14 04:46:57,056 - INFO - __main__ - Epoch  34, Step:   73900, Batch Loss:    12.515060, Lr: 0.000072, Tokens per sec:   2729
2023-03-14 04:47:16,752 - INFO - __main__ - Epoch  34, Step:   74000, Batch Loss:    11.408678, Lr: 0.000072, Tokens per sec:   2747
2023-03-14 04:47:33,821 - INFO - __main__ - Epoch  34: total training loss 20387.68
2023-03-14 04:51:22,665 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 04:51:22,666 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 42.06315902684487, rouge_l = 55.02501475226169, meteor = 0
2023-03-14 04:51:23,765 - INFO - __main__ - Delete test_dir3/61012.ckpt
2023-03-14 04:51:23,901 - INFO - __main__ - Example #0
2023-03-14 04:51:23,902 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 04:51:23,902 - INFO - __main__ - 	Hypothesis: return a hashcode for the principal .
2023-03-14 04:51:23,902 - INFO - __main__ - Example #1
2023-03-14 04:51:23,902 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 04:51:23,902 - INFO - __main__ - 	Hypothesis: call when the view be start .
2023-03-14 04:51:23,902 - INFO - __main__ - Example #2
2023-03-14 04:51:23,902 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 04:51:23,902 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 04:51:23,907 - INFO - __main__ - Validation time = 225.99164748191833s.
2023-03-14 04:51:23,908 - INFO - __main__ - Epoch 35
2023-03-14 04:51:26,957 - INFO - __main__ - Epoch  35, Step:   74100, Batch Loss:     9.609052, Lr: 0.000071, Tokens per sec:   2517
2023-03-14 04:51:46,826 - INFO - __main__ - Epoch  35, Step:   74200, Batch Loss:     8.420305, Lr: 0.000071, Tokens per sec:   2757
2023-03-14 04:52:06,526 - INFO - __main__ - Epoch  35, Step:   74300, Batch Loss:     7.770985, Lr: 0.000071, Tokens per sec:   2748
2023-03-14 04:52:26,257 - INFO - __main__ - Epoch  35, Step:   74400, Batch Loss:     8.239651, Lr: 0.000071, Tokens per sec:   2680
2023-03-14 04:52:46,093 - INFO - __main__ - Epoch  35, Step:   74500, Batch Loss:    10.539012, Lr: 0.000071, Tokens per sec:   2735
2023-03-14 04:53:03,967 - INFO - __main__ - Epoch  35, Step:   74600, Batch Loss:     7.492137, Lr: 0.000071, Tokens per sec:   3028
2023-03-14 04:53:21,626 - INFO - __main__ - Epoch  35, Step:   74700, Batch Loss:     9.259025, Lr: 0.000071, Tokens per sec:   3023
2023-03-14 04:53:39,296 - INFO - __main__ - Epoch  35, Step:   74800, Batch Loss:     8.098079, Lr: 0.000071, Tokens per sec:   3019
2023-03-14 04:53:57,007 - INFO - __main__ - Epoch  35, Step:   74900, Batch Loss:     8.449293, Lr: 0.000071, Tokens per sec:   3087
2023-03-14 04:54:14,715 - INFO - __main__ - Epoch  35, Step:   75000, Batch Loss:     7.903553, Lr: 0.000071, Tokens per sec:   3029
2023-03-14 04:54:32,395 - INFO - __main__ - Epoch  35, Step:   75100, Batch Loss:     9.998545, Lr: 0.000071, Tokens per sec:   3078
2023-03-14 04:54:50,096 - INFO - __main__ - Epoch  35, Step:   75200, Batch Loss:     8.579441, Lr: 0.000071, Tokens per sec:   3026
2023-03-14 04:55:07,767 - INFO - __main__ - Epoch  35, Step:   75300, Batch Loss:     8.506267, Lr: 0.000071, Tokens per sec:   3056
2023-03-14 04:55:25,427 - INFO - __main__ - Epoch  35, Step:   75400, Batch Loss:     9.705304, Lr: 0.000071, Tokens per sec:   3061
2023-03-14 04:55:43,146 - INFO - __main__ - Epoch  35, Step:   75500, Batch Loss:    10.300598, Lr: 0.000071, Tokens per sec:   3031
2023-03-14 04:56:02,529 - INFO - __main__ - Epoch  35, Step:   75600, Batch Loss:    11.839526, Lr: 0.000071, Tokens per sec:   2790
2023-03-14 04:56:22,326 - INFO - __main__ - Epoch  35, Step:   75700, Batch Loss:     8.656613, Lr: 0.000071, Tokens per sec:   2725
2023-03-14 04:56:42,120 - INFO - __main__ - Epoch  35, Step:   75800, Batch Loss:    11.900655, Lr: 0.000071, Tokens per sec:   2700
2023-03-14 04:57:01,872 - INFO - __main__ - Epoch  35, Step:   75900, Batch Loss:    13.032687, Lr: 0.000071, Tokens per sec:   2687
2023-03-14 04:57:21,646 - INFO - __main__ - Epoch  35, Step:   76000, Batch Loss:     8.986646, Lr: 0.000071, Tokens per sec:   2718
2023-03-14 04:57:41,372 - INFO - __main__ - Epoch  35, Step:   76100, Batch Loss:    11.350301, Lr: 0.000071, Tokens per sec:   2710
2023-03-14 04:58:01,092 - INFO - __main__ - Epoch  35, Step:   76200, Batch Loss:    12.785043, Lr: 0.000071, Tokens per sec:   2707
2023-03-14 04:58:13,984 - INFO - __main__ - Epoch  35: total training loss 19462.37
2023-03-14 04:58:13,985 - INFO - __main__ - Epoch 36
2023-03-14 04:58:21,249 - INFO - __main__ - Epoch  36, Step:   76300, Batch Loss:     8.535130, Lr: 0.000070, Tokens per sec:   2700
2023-03-14 04:58:40,999 - INFO - __main__ - Epoch  36, Step:   76400, Batch Loss:     9.426000, Lr: 0.000070, Tokens per sec:   2711
2023-03-14 04:59:00,712 - INFO - __main__ - Epoch  36, Step:   76500, Batch Loss:    10.452535, Lr: 0.000070, Tokens per sec:   2772
2023-03-14 04:59:20,468 - INFO - __main__ - Epoch  36, Step:   76600, Batch Loss:     7.443776, Lr: 0.000070, Tokens per sec:   2753
2023-03-14 04:59:40,243 - INFO - __main__ - Epoch  36, Step:   76700, Batch Loss:     7.283871, Lr: 0.000070, Tokens per sec:   2703
2023-03-14 05:00:00,006 - INFO - __main__ - Epoch  36, Step:   76800, Batch Loss:    11.857697, Lr: 0.000070, Tokens per sec:   2760
2023-03-14 05:00:19,728 - INFO - __main__ - Epoch  36, Step:   76900, Batch Loss:     6.413038, Lr: 0.000070, Tokens per sec:   2701
2023-03-14 05:00:39,418 - INFO - __main__ - Epoch  36, Step:   77000, Batch Loss:     7.097997, Lr: 0.000070, Tokens per sec:   2772
2023-03-14 05:00:59,180 - INFO - __main__ - Epoch  36, Step:   77100, Batch Loss:     4.816730, Lr: 0.000070, Tokens per sec:   2740
2023-03-14 05:01:19,005 - INFO - __main__ - Epoch  36, Step:   77200, Batch Loss:     9.695508, Lr: 0.000070, Tokens per sec:   2678
2023-03-14 05:01:38,809 - INFO - __main__ - Epoch  36, Step:   77300, Batch Loss:     9.229827, Lr: 0.000070, Tokens per sec:   2719
2023-03-14 05:01:57,312 - INFO - __main__ - Epoch  36, Step:   77400, Batch Loss:    11.838163, Lr: 0.000070, Tokens per sec:   2904
2023-03-14 05:02:15,023 - INFO - __main__ - Epoch  36, Step:   77500, Batch Loss:     9.543884, Lr: 0.000070, Tokens per sec:   2983
2023-03-14 05:02:32,753 - INFO - __main__ - Epoch  36, Step:   77600, Batch Loss:     8.183340, Lr: 0.000070, Tokens per sec:   3037
2023-03-14 05:02:52,035 - INFO - __main__ - Epoch  36, Step:   77700, Batch Loss:    12.053568, Lr: 0.000070, Tokens per sec:   2770
2023-03-14 05:03:11,686 - INFO - __main__ - Epoch  36, Step:   77800, Batch Loss:     6.242676, Lr: 0.000070, Tokens per sec:   2734
2023-03-14 05:03:31,397 - INFO - __main__ - Epoch  36, Step:   77900, Batch Loss:     9.239445, Lr: 0.000070, Tokens per sec:   2726
2023-03-14 05:03:51,141 - INFO - __main__ - Epoch  36, Step:   78000, Batch Loss:     7.650849, Lr: 0.000070, Tokens per sec:   2728
2023-03-14 05:04:10,876 - INFO - __main__ - Epoch  36, Step:   78100, Batch Loss:     9.977412, Lr: 0.000070, Tokens per sec:   2739
2023-03-14 05:04:30,628 - INFO - __main__ - Epoch  36, Step:   78200, Batch Loss:     6.783102, Lr: 0.000070, Tokens per sec:   2695
2023-03-14 05:04:49,835 - INFO - __main__ - Epoch  36, Step:   78300, Batch Loss:     9.431306, Lr: 0.000070, Tokens per sec:   2815
2023-03-14 05:05:07,553 - INFO - __main__ - Epoch  36, Step:   78400, Batch Loss:     9.563590, Lr: 0.000070, Tokens per sec:   3042
2023-03-14 05:05:16,317 - INFO - __main__ - Epoch  36: total training loss 18712.43
2023-03-14 05:09:03,510 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 05:09:03,510 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 42.55935096022228, rouge_l = 55.26293642433624, meteor = 0
2023-03-14 05:09:04,619 - INFO - __main__ - Delete test_dir3/65370.ckpt
2023-03-14 05:09:04,754 - INFO - __main__ - Example #0
2023-03-14 05:09:04,754 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 05:09:04,754 - INFO - __main__ - 	Hypothesis: return a hashcode for this string constant .
2023-03-14 05:09:04,754 - INFO - __main__ - Example #1
2023-03-14 05:09:04,754 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 05:09:04,754 - INFO - __main__ - 	Hypothesis: call when the host be start .
2023-03-14 05:09:04,754 - INFO - __main__ - Example #2
2023-03-14 05:09:04,754 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 05:09:04,754 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 05:09:04,760 - INFO - __main__ - Validation time = 224.03536820411682s.
2023-03-14 05:09:04,760 - INFO - __main__ - Epoch 37
2023-03-14 05:09:16,139 - INFO - __main__ - Epoch  37, Step:   78500, Batch Loss:     9.372932, Lr: 0.000070, Tokens per sec:   2690
2023-03-14 05:09:35,908 - INFO - __main__ - Epoch  37, Step:   78600, Batch Loss:     6.710920, Lr: 0.000070, Tokens per sec:   2678
2023-03-14 05:09:55,643 - INFO - __main__ - Epoch  37, Step:   78700, Batch Loss:     8.398793, Lr: 0.000070, Tokens per sec:   2710
2023-03-14 05:10:15,399 - INFO - __main__ - Epoch  37, Step:   78800, Batch Loss:     7.964123, Lr: 0.000070, Tokens per sec:   2725
2023-03-14 05:10:35,179 - INFO - __main__ - Epoch  37, Step:   78900, Batch Loss:     7.761780, Lr: 0.000070, Tokens per sec:   2704
2023-03-14 05:10:54,983 - INFO - __main__ - Epoch  37, Step:   79000, Batch Loss:     8.975986, Lr: 0.000070, Tokens per sec:   2745
2023-03-14 05:11:14,150 - INFO - __main__ - Epoch  37, Step:   79100, Batch Loss:     6.349026, Lr: 0.000070, Tokens per sec:   2763
2023-03-14 05:11:33,905 - INFO - __main__ - Epoch  37, Step:   79200, Batch Loss:     6.757972, Lr: 0.000070, Tokens per sec:   2724
2023-03-14 05:11:53,650 - INFO - __main__ - Epoch  37, Step:   79300, Batch Loss:     7.819919, Lr: 0.000070, Tokens per sec:   2726
2023-03-14 05:12:13,418 - INFO - __main__ - Epoch  37, Step:   79400, Batch Loss:     6.595897, Lr: 0.000070, Tokens per sec:   2742
2023-03-14 05:12:33,132 - INFO - __main__ - Epoch  37, Step:   79500, Batch Loss:    10.902874, Lr: 0.000070, Tokens per sec:   2692
2023-03-14 05:12:52,914 - INFO - __main__ - Epoch  37, Step:   79600, Batch Loss:    10.937626, Lr: 0.000070, Tokens per sec:   2713
2023-03-14 05:13:12,685 - INFO - __main__ - Epoch  37, Step:   79700, Batch Loss:     7.072397, Lr: 0.000070, Tokens per sec:   2738
2023-03-14 05:13:32,473 - INFO - __main__ - Epoch  37, Step:   79800, Batch Loss:     8.058023, Lr: 0.000070, Tokens per sec:   2719
2023-03-14 05:13:52,266 - INFO - __main__ - Epoch  37, Step:   79900, Batch Loss:     9.801872, Lr: 0.000070, Tokens per sec:   2696
2023-03-14 05:14:11,969 - INFO - __main__ - Epoch  37, Step:   80000, Batch Loss:     6.626864, Lr: 0.000070, Tokens per sec:   2746
2023-03-14 05:14:31,640 - INFO - __main__ - Epoch  37, Step:   80100, Batch Loss:     6.607741, Lr: 0.000070, Tokens per sec:   2794
2023-03-14 05:14:51,386 - INFO - __main__ - Epoch  37, Step:   80200, Batch Loss:     7.394435, Lr: 0.000070, Tokens per sec:   2754
2023-03-14 05:15:11,110 - INFO - __main__ - Epoch  37, Step:   80300, Batch Loss:     8.007641, Lr: 0.000070, Tokens per sec:   2772
2023-03-14 05:15:30,837 - INFO - __main__ - Epoch  37, Step:   80400, Batch Loss:     7.473268, Lr: 0.000070, Tokens per sec:   2702
2023-03-14 05:15:50,649 - INFO - __main__ - Epoch  37, Step:   80500, Batch Loss:     9.601155, Lr: 0.000070, Tokens per sec:   2741
2023-03-14 05:16:10,376 - INFO - __main__ - Epoch  37, Step:   80600, Batch Loss:     8.730510, Lr: 0.000070, Tokens per sec:   2740
2023-03-14 05:16:14,969 - INFO - __main__ - Epoch  37: total training loss 17987.46
2023-03-14 05:16:14,970 - INFO - __main__ - Epoch 38
2023-03-14 05:16:30,396 - INFO - __main__ - Epoch  38, Step:   80700, Batch Loss:     7.441516, Lr: 0.000069, Tokens per sec:   2683
2023-03-14 05:16:50,140 - INFO - __main__ - Epoch  38, Step:   80800, Batch Loss:     4.846228, Lr: 0.000069, Tokens per sec:   2713
2023-03-14 05:17:09,847 - INFO - __main__ - Epoch  38, Step:   80900, Batch Loss:     6.669876, Lr: 0.000069, Tokens per sec:   2736
2023-03-14 05:17:29,582 - INFO - __main__ - Epoch  38, Step:   81000, Batch Loss:     7.371463, Lr: 0.000069, Tokens per sec:   2712
2023-03-14 05:17:49,368 - INFO - __main__ - Epoch  38, Step:   81100, Batch Loss:     8.433798, Lr: 0.000069, Tokens per sec:   2711
2023-03-14 05:18:09,190 - INFO - __main__ - Epoch  38, Step:   81200, Batch Loss:     6.068367, Lr: 0.000069, Tokens per sec:   2739
2023-03-14 05:18:28,944 - INFO - __main__ - Epoch  38, Step:   81300, Batch Loss:     9.736167, Lr: 0.000069, Tokens per sec:   2721
2023-03-14 05:18:48,636 - INFO - __main__ - Epoch  38, Step:   81400, Batch Loss:     7.665355, Lr: 0.000069, Tokens per sec:   2768
2023-03-14 05:19:08,351 - INFO - __main__ - Epoch  38, Step:   81500, Batch Loss:     6.728329, Lr: 0.000069, Tokens per sec:   2742
2023-03-14 05:19:28,075 - INFO - __main__ - Epoch  38, Step:   81600, Batch Loss:     7.794556, Lr: 0.000069, Tokens per sec:   2741
2023-03-14 05:19:47,760 - INFO - __main__ - Epoch  38, Step:   81700, Batch Loss:     7.673662, Lr: 0.000069, Tokens per sec:   2689
2023-03-14 05:20:06,955 - INFO - __main__ - Epoch  38, Step:   81800, Batch Loss:     8.118983, Lr: 0.000069, Tokens per sec:   2810
2023-03-14 05:20:26,698 - INFO - __main__ - Epoch  38, Step:   81900, Batch Loss:     6.489014, Lr: 0.000069, Tokens per sec:   2693
2023-03-14 05:20:46,466 - INFO - __main__ - Epoch  38, Step:   82000, Batch Loss:     6.366082, Lr: 0.000069, Tokens per sec:   2685
2023-03-14 05:21:06,194 - INFO - __main__ - Epoch  38, Step:   82100, Batch Loss:     7.676850, Lr: 0.000069, Tokens per sec:   2735
2023-03-14 05:21:25,994 - INFO - __main__ - Epoch  38, Step:   82200, Batch Loss:    10.421417, Lr: 0.000069, Tokens per sec:   2728
2023-03-14 05:21:45,777 - INFO - __main__ - Epoch  38, Step:   82300, Batch Loss:     7.606351, Lr: 0.000069, Tokens per sec:   2756
2023-03-14 05:22:05,510 - INFO - __main__ - Epoch  38, Step:   82400, Batch Loss:    10.206226, Lr: 0.000069, Tokens per sec:   2740
2023-03-14 05:22:25,289 - INFO - __main__ - Epoch  38, Step:   82500, Batch Loss:     7.054002, Lr: 0.000069, Tokens per sec:   2739
2023-03-14 05:22:44,999 - INFO - __main__ - Epoch  38, Step:   82600, Batch Loss:     9.680803, Lr: 0.000069, Tokens per sec:   2692
2023-03-14 05:23:04,712 - INFO - __main__ - Epoch  38, Step:   82700, Batch Loss:     9.445295, Lr: 0.000069, Tokens per sec:   2733
2023-03-14 05:23:23,159 - INFO - __main__ - Epoch  38, Step:   82800, Batch Loss:     8.476524, Lr: 0.000069, Tokens per sec:   2949
2023-03-14 05:23:23,578 - INFO - __main__ - Epoch  38: total training loss 17343.04
2023-03-14 05:27:11,337 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 05:27:11,338 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 42.80440036167374, rouge_l = 55.49906732200893, meteor = 0
2023-03-14 05:27:12,433 - INFO - __main__ - Delete test_dir3/69728.ckpt
2023-03-14 05:27:12,566 - INFO - __main__ - Example #0
2023-03-14 05:27:12,566 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 05:27:12,566 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 05:27:12,566 - INFO - __main__ - Example #1
2023-03-14 05:27:12,566 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 05:27:12,566 - INFO - __main__ - 	Hypothesis: call when the host be start .
2023-03-14 05:27:12,566 - INFO - __main__ - Example #2
2023-03-14 05:27:12,566 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 05:27:12,566 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 05:27:12,572 - INFO - __main__ - Validation time = 224.90208339691162s.
2023-03-14 05:27:12,572 - INFO - __main__ - Epoch 39
2023-03-14 05:27:32,190 - INFO - __main__ - Epoch  39, Step:   82900, Batch Loss:     7.588450, Lr: 0.000068, Tokens per sec:   2653
2023-03-14 05:27:51,929 - INFO - __main__ - Epoch  39, Step:   83000, Batch Loss:     7.060693, Lr: 0.000068, Tokens per sec:   2717
2023-03-14 05:28:11,669 - INFO - __main__ - Epoch  39, Step:   83100, Batch Loss:     6.799443, Lr: 0.000068, Tokens per sec:   2766
2023-03-14 05:28:31,453 - INFO - __main__ - Epoch  39, Step:   83200, Batch Loss:     5.817940, Lr: 0.000068, Tokens per sec:   2716
2023-03-14 05:28:51,249 - INFO - __main__ - Epoch  39, Step:   83300, Batch Loss:     7.275162, Lr: 0.000068, Tokens per sec:   2756
2023-03-14 05:29:10,980 - INFO - __main__ - Epoch  39, Step:   83400, Batch Loss:     8.370496, Lr: 0.000068, Tokens per sec:   2765
2023-03-14 05:29:30,732 - INFO - __main__ - Epoch  39, Step:   83500, Batch Loss:     7.623085, Lr: 0.000068, Tokens per sec:   2693
2023-03-14 05:29:50,472 - INFO - __main__ - Epoch  39, Step:   83600, Batch Loss:     6.968967, Lr: 0.000068, Tokens per sec:   2736
2023-03-14 05:30:10,177 - INFO - __main__ - Epoch  39, Step:   83700, Batch Loss:     9.886868, Lr: 0.000068, Tokens per sec:   2731
2023-03-14 05:30:29,925 - INFO - __main__ - Epoch  39, Step:   83800, Batch Loss:     8.208436, Lr: 0.000068, Tokens per sec:   2728
2023-03-14 05:30:49,712 - INFO - __main__ - Epoch  39, Step:   83900, Batch Loss:     9.702279, Lr: 0.000068, Tokens per sec:   2750
2023-03-14 05:31:09,457 - INFO - __main__ - Epoch  39, Step:   84000, Batch Loss:     8.893970, Lr: 0.000068, Tokens per sec:   2751
2023-03-14 05:31:29,235 - INFO - __main__ - Epoch  39, Step:   84100, Batch Loss:     8.566644, Lr: 0.000068, Tokens per sec:   2722
2023-03-14 05:31:48,971 - INFO - __main__ - Epoch  39, Step:   84200, Batch Loss:     5.709919, Lr: 0.000068, Tokens per sec:   2715
2023-03-14 05:32:08,666 - INFO - __main__ - Epoch  39, Step:   84300, Batch Loss:     8.426315, Lr: 0.000068, Tokens per sec:   2687
2023-03-14 05:32:28,414 - INFO - __main__ - Epoch  39, Step:   84400, Batch Loss:    10.104169, Lr: 0.000068, Tokens per sec:   2738
2023-03-14 05:32:48,138 - INFO - __main__ - Epoch  39, Step:   84500, Batch Loss:     7.674213, Lr: 0.000068, Tokens per sec:   2714
2023-03-14 05:33:07,794 - INFO - __main__ - Epoch  39, Step:   84600, Batch Loss:     7.665202, Lr: 0.000068, Tokens per sec:   2710
2023-03-14 05:33:27,229 - INFO - __main__ - Epoch  39, Step:   84700, Batch Loss:     7.052985, Lr: 0.000068, Tokens per sec:   2789
2023-03-14 05:33:46,998 - INFO - __main__ - Epoch  39, Step:   84800, Batch Loss:    10.082527, Lr: 0.000068, Tokens per sec:   2755
2023-03-14 05:34:06,732 - INFO - __main__ - Epoch  39, Step:   84900, Batch Loss:     8.719433, Lr: 0.000068, Tokens per sec:   2689
2023-03-14 05:34:22,734 - INFO - __main__ - Epoch  39: total training loss 16671.53
2023-03-14 05:34:22,735 - INFO - __main__ - Epoch 40
2023-03-14 05:34:26,724 - INFO - __main__ - Epoch  40, Step:   85000, Batch Loss:     5.241462, Lr: 0.000068, Tokens per sec:   2522
2023-03-14 05:34:46,479 - INFO - __main__ - Epoch  40, Step:   85100, Batch Loss:     5.821842, Lr: 0.000068, Tokens per sec:   2695
2023-03-14 05:35:06,251 - INFO - __main__ - Epoch  40, Step:   85200, Batch Loss:     7.596982, Lr: 0.000068, Tokens per sec:   2707
2023-03-14 05:35:25,975 - INFO - __main__ - Epoch  40, Step:   85300, Batch Loss:     5.406792, Lr: 0.000068, Tokens per sec:   2712
2023-03-14 05:35:45,776 - INFO - __main__ - Epoch  40, Step:   85400, Batch Loss:     5.123710, Lr: 0.000068, Tokens per sec:   2714
2023-03-14 05:36:05,511 - INFO - __main__ - Epoch  40, Step:   85500, Batch Loss:     5.628670, Lr: 0.000068, Tokens per sec:   2716
2023-03-14 05:36:25,290 - INFO - __main__ - Epoch  40, Step:   85600, Batch Loss:     6.460775, Lr: 0.000068, Tokens per sec:   2706
2023-03-14 05:36:44,949 - INFO - __main__ - Epoch  40, Step:   85700, Batch Loss:     7.571951, Lr: 0.000068, Tokens per sec:   2756
2023-03-14 05:37:04,753 - INFO - __main__ - Epoch  40, Step:   85800, Batch Loss:    12.056544, Lr: 0.000068, Tokens per sec:   2754
2023-03-14 05:37:24,543 - INFO - __main__ - Epoch  40, Step:   85900, Batch Loss:     6.380620, Lr: 0.000068, Tokens per sec:   2703
2023-03-14 05:37:44,261 - INFO - __main__ - Epoch  40, Step:   86000, Batch Loss:     8.648642, Lr: 0.000068, Tokens per sec:   2738
2023-03-14 05:38:03,996 - INFO - __main__ - Epoch  40, Step:   86100, Batch Loss:     8.902184, Lr: 0.000068, Tokens per sec:   2775
2023-03-14 05:38:23,801 - INFO - __main__ - Epoch  40, Step:   86200, Batch Loss:     9.450457, Lr: 0.000068, Tokens per sec:   2720
2023-03-14 05:38:42,488 - INFO - __main__ - Epoch  40, Step:   86300, Batch Loss:     4.715394, Lr: 0.000068, Tokens per sec:   2897
2023-03-14 05:39:01,409 - INFO - __main__ - Epoch  40, Step:   86400, Batch Loss:     7.986190, Lr: 0.000068, Tokens per sec:   2816
2023-03-14 05:39:21,151 - INFO - __main__ - Epoch  40, Step:   86500, Batch Loss:     7.441071, Lr: 0.000068, Tokens per sec:   2722
2023-03-14 05:39:40,803 - INFO - __main__ - Epoch  40, Step:   86600, Batch Loss:     6.626618, Lr: 0.000068, Tokens per sec:   2718
2023-03-14 05:40:00,474 - INFO - __main__ - Epoch  40, Step:   86700, Batch Loss:     7.579198, Lr: 0.000068, Tokens per sec:   2726
2023-03-14 05:40:20,179 - INFO - __main__ - Epoch  40, Step:   86800, Batch Loss:     8.947416, Lr: 0.000068, Tokens per sec:   2800
2023-03-14 05:40:39,901 - INFO - __main__ - Epoch  40, Step:   86900, Batch Loss:     7.623475, Lr: 0.000068, Tokens per sec:   2747
2023-03-14 05:40:59,680 - INFO - __main__ - Epoch  40, Step:   87000, Batch Loss:     8.617587, Lr: 0.000068, Tokens per sec:   2700
2023-03-14 05:41:19,393 - INFO - __main__ - Epoch  40, Step:   87100, Batch Loss:     4.509930, Lr: 0.000068, Tokens per sec:   2700
2023-03-14 05:41:31,286 - INFO - __main__ - Epoch  40: total training loss 16010.61
2023-03-14 05:45:19,495 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 05:45:19,496 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 43.1815758527905, rouge_l = 55.76527918514148, meteor = 0
2023-03-14 05:45:20,580 - INFO - __main__ - Delete test_dir3/74086.ckpt
2023-03-14 05:45:20,708 - INFO - __main__ - Example #0
2023-03-14 05:45:20,708 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 05:45:20,708 - INFO - __main__ - 	Hypothesis: return a hashcode for this string constant .
2023-03-14 05:45:20,708 - INFO - __main__ - Example #1
2023-03-14 05:45:20,708 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 05:45:20,708 - INFO - __main__ - 	Hypothesis: call when the activity start .
2023-03-14 05:45:20,708 - INFO - __main__ - Example #2
2023-03-14 05:45:20,708 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 05:45:20,708 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 05:45:20,716 - INFO - __main__ - Validation time = 225.34904980659485s.
2023-03-14 05:45:20,716 - INFO - __main__ - Epoch 41
2023-03-14 05:45:28,934 - INFO - __main__ - Epoch  41, Step:   87200, Batch Loss:     7.358488, Lr: 0.000067, Tokens per sec:   2596
2023-03-14 05:45:48,739 - INFO - __main__ - Epoch  41, Step:   87300, Batch Loss:     7.435971, Lr: 0.000067, Tokens per sec:   2729
2023-03-14 05:46:08,485 - INFO - __main__ - Epoch  41, Step:   87400, Batch Loss:     7.620999, Lr: 0.000067, Tokens per sec:   2750
2023-03-14 05:46:28,202 - INFO - __main__ - Epoch  41, Step:   87500, Batch Loss:     5.432852, Lr: 0.000067, Tokens per sec:   2707
2023-03-14 05:46:47,968 - INFO - __main__ - Epoch  41, Step:   87600, Batch Loss:     7.661962, Lr: 0.000067, Tokens per sec:   2769
2023-03-14 05:47:07,728 - INFO - __main__ - Epoch  41, Step:   87700, Batch Loss:     5.850573, Lr: 0.000067, Tokens per sec:   2679
2023-03-14 05:47:27,446 - INFO - __main__ - Epoch  41, Step:   87800, Batch Loss:     3.826153, Lr: 0.000067, Tokens per sec:   2781
2023-03-14 05:47:47,235 - INFO - __main__ - Epoch  41, Step:   87900, Batch Loss:     6.970970, Lr: 0.000067, Tokens per sec:   2720
2023-03-14 05:48:07,029 - INFO - __main__ - Epoch  41, Step:   88000, Batch Loss:     5.741225, Lr: 0.000067, Tokens per sec:   2713
2023-03-14 05:48:26,764 - INFO - __main__ - Epoch  41, Step:   88100, Batch Loss:     4.627433, Lr: 0.000067, Tokens per sec:   2687
2023-03-14 05:48:46,563 - INFO - __main__ - Epoch  41, Step:   88200, Batch Loss:     6.453734, Lr: 0.000067, Tokens per sec:   2714
2023-03-14 05:49:06,287 - INFO - __main__ - Epoch  41, Step:   88300, Batch Loss:     5.370433, Lr: 0.000067, Tokens per sec:   2673
2023-03-14 05:49:26,019 - INFO - __main__ - Epoch  41, Step:   88400, Batch Loss:     9.197376, Lr: 0.000067, Tokens per sec:   2745
2023-03-14 05:49:46,161 - INFO - __main__ - Epoch  41, Step:   88500, Batch Loss:     7.717353, Lr: 0.000067, Tokens per sec:   2651
2023-03-14 05:50:05,908 - INFO - __main__ - Epoch  41, Step:   88600, Batch Loss:     7.826948, Lr: 0.000067, Tokens per sec:   2722
2023-03-14 05:50:25,648 - INFO - __main__ - Epoch  41, Step:   88700, Batch Loss:     6.895213, Lr: 0.000067, Tokens per sec:   2749
2023-03-14 05:50:45,404 - INFO - __main__ - Epoch  41, Step:   88800, Batch Loss:     7.636034, Lr: 0.000067, Tokens per sec:   2700
2023-03-14 05:51:05,222 - INFO - __main__ - Epoch  41, Step:   88900, Batch Loss:     5.986639, Lr: 0.000067, Tokens per sec:   2771
2023-03-14 05:51:25,056 - INFO - __main__ - Epoch  41, Step:   89000, Batch Loss:     4.069249, Lr: 0.000067, Tokens per sec:   2713
2023-03-14 05:51:44,827 - INFO - __main__ - Epoch  41, Step:   89100, Batch Loss:     3.731939, Lr: 0.000067, Tokens per sec:   2758
2023-03-14 05:52:04,589 - INFO - __main__ - Epoch  41, Step:   89200, Batch Loss:     7.266079, Lr: 0.000067, Tokens per sec:   2716
2023-03-14 05:52:24,298 - INFO - __main__ - Epoch  41, Step:   89300, Batch Loss:     7.853662, Lr: 0.000067, Tokens per sec:   2725
2023-03-14 05:52:32,054 - INFO - __main__ - Epoch  41: total training loss 15516.06
2023-03-14 05:52:32,055 - INFO - __main__ - Epoch 42
2023-03-14 05:52:44,423 - INFO - __main__ - Epoch  42, Step:   89400, Batch Loss:     5.442430, Lr: 0.000066, Tokens per sec:   2611
2023-03-14 05:53:04,108 - INFO - __main__ - Epoch  42, Step:   89500, Batch Loss:     3.411123, Lr: 0.000066, Tokens per sec:   2708
2023-03-14 05:53:23,825 - INFO - __main__ - Epoch  42, Step:   89600, Batch Loss:     6.832701, Lr: 0.000066, Tokens per sec:   2698
2023-03-14 05:53:43,574 - INFO - __main__ - Epoch  42, Step:   89700, Batch Loss:     5.919896, Lr: 0.000066, Tokens per sec:   2729
2023-03-14 05:54:03,274 - INFO - __main__ - Epoch  42, Step:   89800, Batch Loss:     7.993271, Lr: 0.000066, Tokens per sec:   2722
2023-03-14 05:54:23,016 - INFO - __main__ - Epoch  42, Step:   89900, Batch Loss:     7.200835, Lr: 0.000066, Tokens per sec:   2774
2023-03-14 05:54:42,765 - INFO - __main__ - Epoch  42, Step:   90000, Batch Loss:     5.136418, Lr: 0.000066, Tokens per sec:   2755
2023-03-14 05:55:02,489 - INFO - __main__ - Epoch  42, Step:   90100, Batch Loss:     7.475691, Lr: 0.000066, Tokens per sec:   2723
2023-03-14 05:55:22,280 - INFO - __main__ - Epoch  42, Step:   90200, Batch Loss:     5.839242, Lr: 0.000066, Tokens per sec:   2677
2023-03-14 05:55:42,073 - INFO - __main__ - Epoch  42, Step:   90300, Batch Loss:     4.381783, Lr: 0.000066, Tokens per sec:   2781
2023-03-14 05:56:01,731 - INFO - __main__ - Epoch  42, Step:   90400, Batch Loss:     4.353676, Lr: 0.000066, Tokens per sec:   2702
2023-03-14 05:56:21,480 - INFO - __main__ - Epoch  42, Step:   90500, Batch Loss:     6.900250, Lr: 0.000066, Tokens per sec:   2722
2023-03-14 05:56:41,260 - INFO - __main__ - Epoch  42, Step:   90600, Batch Loss:     8.950448, Lr: 0.000066, Tokens per sec:   2697
2023-03-14 05:57:00,974 - INFO - __main__ - Epoch  42, Step:   90700, Batch Loss:     4.932458, Lr: 0.000066, Tokens per sec:   2732
2023-03-14 05:57:20,728 - INFO - __main__ - Epoch  42, Step:   90800, Batch Loss:     7.308255, Lr: 0.000066, Tokens per sec:   2733
2023-03-14 05:57:40,522 - INFO - __main__ - Epoch  42, Step:   90900, Batch Loss:     6.812817, Lr: 0.000066, Tokens per sec:   2709
2023-03-14 05:58:00,268 - INFO - __main__ - Epoch  42, Step:   91000, Batch Loss:     6.623495, Lr: 0.000066, Tokens per sec:   2717
2023-03-14 05:58:20,028 - INFO - __main__ - Epoch  42, Step:   91100, Batch Loss:     6.738239, Lr: 0.000066, Tokens per sec:   2768
2023-03-14 05:58:39,736 - INFO - __main__ - Epoch  42, Step:   91200, Batch Loss:     8.523906, Lr: 0.000066, Tokens per sec:   2718
2023-03-14 05:58:59,532 - INFO - __main__ - Epoch  42, Step:   91300, Batch Loss:     6.026904, Lr: 0.000066, Tokens per sec:   2695
2023-03-14 05:59:19,266 - INFO - __main__ - Epoch  42, Step:   91400, Batch Loss:    10.587008, Lr: 0.000066, Tokens per sec:   2738
2023-03-14 05:59:39,038 - INFO - __main__ - Epoch  42, Step:   91500, Batch Loss:     7.712600, Lr: 0.000066, Tokens per sec:   2777
2023-03-14 05:59:42,625 - INFO - __main__ - Epoch  42: total training loss 14985.78
2023-03-14 06:03:28,922 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 06:03:28,922 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 43.5207279092327, rouge_l = 55.995897621580916, meteor = 0
2023-03-14 06:03:30,015 - INFO - __main__ - Delete test_dir3/78444.ckpt
2023-03-14 06:03:30,156 - INFO - __main__ - Example #0
2023-03-14 06:03:30,156 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 06:03:30,156 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 06:03:30,156 - INFO - __main__ - Example #1
2023-03-14 06:03:30,156 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 06:03:30,156 - INFO - __main__ - 	Hypothesis: call when the view be start .
2023-03-14 06:03:30,156 - INFO - __main__ - Example #2
2023-03-14 06:03:30,156 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 06:03:30,156 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this function
2023-03-14 06:03:30,163 - INFO - __main__ - Validation time = 223.43139743804932s.
2023-03-14 06:03:30,163 - INFO - __main__ - Epoch 43
2023-03-14 06:03:45,117 - INFO - __main__ - Epoch  43, Step:   91600, Batch Loss:     6.881030, Lr: 0.000066, Tokens per sec:   2961
2023-03-14 06:04:02,846 - INFO - __main__ - Epoch  43, Step:   91700, Batch Loss:     4.979557, Lr: 0.000066, Tokens per sec:   3028
2023-03-14 06:04:20,587 - INFO - __main__ - Epoch  43, Step:   91800, Batch Loss:     3.532007, Lr: 0.000066, Tokens per sec:   3000
2023-03-14 06:04:38,279 - INFO - __main__ - Epoch  43, Step:   91900, Batch Loss:     4.987948, Lr: 0.000066, Tokens per sec:   3057
2023-03-14 06:04:55,993 - INFO - __main__ - Epoch  43, Step:   92000, Batch Loss:     7.024173, Lr: 0.000066, Tokens per sec:   3096
2023-03-14 06:05:13,634 - INFO - __main__ - Epoch  43, Step:   92100, Batch Loss:     3.616504, Lr: 0.000066, Tokens per sec:   3073
2023-03-14 06:05:32,154 - INFO - __main__ - Epoch  43, Step:   92200, Batch Loss:     6.794551, Lr: 0.000066, Tokens per sec:   2918
2023-03-14 06:05:51,901 - INFO - __main__ - Epoch  43, Step:   92300, Batch Loss:     6.592088, Lr: 0.000066, Tokens per sec:   2711
2023-03-14 06:06:11,601 - INFO - __main__ - Epoch  43, Step:   92400, Batch Loss:     6.285176, Lr: 0.000066, Tokens per sec:   2715
2023-03-14 06:06:31,356 - INFO - __main__ - Epoch  43, Step:   92500, Batch Loss:     8.973452, Lr: 0.000066, Tokens per sec:   2757
2023-03-14 06:06:51,057 - INFO - __main__ - Epoch  43, Step:   92600, Batch Loss:     6.165087, Lr: 0.000066, Tokens per sec:   2717
2023-03-14 06:07:10,768 - INFO - __main__ - Epoch  43, Step:   92700, Batch Loss:     5.689334, Lr: 0.000066, Tokens per sec:   2722
2023-03-14 06:07:30,506 - INFO - __main__ - Epoch  43, Step:   92800, Batch Loss:     6.041118, Lr: 0.000066, Tokens per sec:   2728
2023-03-14 06:07:50,263 - INFO - __main__ - Epoch  43, Step:   92900, Batch Loss:     5.627021, Lr: 0.000066, Tokens per sec:   2774
2023-03-14 06:08:09,999 - INFO - __main__ - Epoch  43, Step:   93000, Batch Loss:     5.988989, Lr: 0.000066, Tokens per sec:   2722
2023-03-14 06:08:29,734 - INFO - __main__ - Epoch  43, Step:   93100, Batch Loss:     8.768411, Lr: 0.000066, Tokens per sec:   2729
2023-03-14 06:08:49,446 - INFO - __main__ - Epoch  43, Step:   93200, Batch Loss:     7.224362, Lr: 0.000066, Tokens per sec:   2707
2023-03-14 06:09:09,120 - INFO - __main__ - Epoch  43, Step:   93300, Batch Loss:     6.538005, Lr: 0.000066, Tokens per sec:   2709
2023-03-14 06:09:28,873 - INFO - __main__ - Epoch  43, Step:   93400, Batch Loss:     5.645153, Lr: 0.000066, Tokens per sec:   2687
2023-03-14 06:09:48,629 - INFO - __main__ - Epoch  43, Step:   93500, Batch Loss:     9.379117, Lr: 0.000066, Tokens per sec:   2744
2023-03-14 06:10:08,338 - INFO - __main__ - Epoch  43, Step:   93600, Batch Loss:     5.071953, Lr: 0.000066, Tokens per sec:   2660
2023-03-14 06:10:27,565 - INFO - __main__ - Epoch  43: total training loss 14415.49
2023-03-14 06:10:27,566 - INFO - __main__ - Epoch 44
2023-03-14 06:10:28,459 - INFO - __main__ - Epoch  44, Step:   93700, Batch Loss:     5.797163, Lr: 0.000065, Tokens per sec:   1648
2023-03-14 06:10:48,292 - INFO - __main__ - Epoch  44, Step:   93800, Batch Loss:     5.515250, Lr: 0.000065, Tokens per sec:   2738
2023-03-14 06:11:08,059 - INFO - __main__ - Epoch  44, Step:   93900, Batch Loss:     4.797182, Lr: 0.000065, Tokens per sec:   2747
2023-03-14 06:11:27,789 - INFO - __main__ - Epoch  44, Step:   94000, Batch Loss:     5.001945, Lr: 0.000065, Tokens per sec:   2673
2023-03-14 06:11:47,548 - INFO - __main__ - Epoch  44, Step:   94100, Batch Loss:     5.627639, Lr: 0.000065, Tokens per sec:   2693
2023-03-14 06:12:07,237 - INFO - __main__ - Epoch  44, Step:   94200, Batch Loss:     4.582989, Lr: 0.000065, Tokens per sec:   2690
2023-03-14 06:12:26,912 - INFO - __main__ - Epoch  44, Step:   94300, Batch Loss:     8.430124, Lr: 0.000065, Tokens per sec:   2749
2023-03-14 06:12:45,623 - INFO - __main__ - Epoch  44, Step:   94400, Batch Loss:     6.959275, Lr: 0.000065, Tokens per sec:   2853
2023-03-14 06:13:03,304 - INFO - __main__ - Epoch  44, Step:   94500, Batch Loss:     9.302087, Lr: 0.000065, Tokens per sec:   3042
2023-03-14 06:13:22,973 - INFO - __main__ - Epoch  44, Step:   94600, Batch Loss:     6.615380, Lr: 0.000065, Tokens per sec:   2733
2023-03-14 06:13:42,696 - INFO - __main__ - Epoch  44, Step:   94700, Batch Loss:     4.665093, Lr: 0.000065, Tokens per sec:   2736
2023-03-14 06:14:02,467 - INFO - __main__ - Epoch  44, Step:   94800, Batch Loss:     8.366330, Lr: 0.000065, Tokens per sec:   2746
2023-03-14 06:14:22,218 - INFO - __main__ - Epoch  44, Step:   94900, Batch Loss:     9.729838, Lr: 0.000065, Tokens per sec:   2781
2023-03-14 06:14:41,993 - INFO - __main__ - Epoch  44, Step:   95000, Batch Loss:     7.939288, Lr: 0.000065, Tokens per sec:   2751
2023-03-14 06:15:01,766 - INFO - __main__ - Epoch  44, Step:   95100, Batch Loss:     5.512736, Lr: 0.000065, Tokens per sec:   2763
2023-03-14 06:15:21,523 - INFO - __main__ - Epoch  44, Step:   95200, Batch Loss:     6.478586, Lr: 0.000065, Tokens per sec:   2726
2023-03-14 06:15:41,231 - INFO - __main__ - Epoch  44, Step:   95300, Batch Loss:     7.130912, Lr: 0.000065, Tokens per sec:   2744
2023-03-14 06:16:00,924 - INFO - __main__ - Epoch  44, Step:   95400, Batch Loss:     6.241893, Lr: 0.000065, Tokens per sec:   2737
2023-03-14 06:16:20,657 - INFO - __main__ - Epoch  44, Step:   95500, Batch Loss:     8.004670, Lr: 0.000065, Tokens per sec:   2692
2023-03-14 06:16:40,383 - INFO - __main__ - Epoch  44, Step:   95600, Batch Loss:     8.064131, Lr: 0.000065, Tokens per sec:   2698
2023-03-14 06:17:00,118 - INFO - __main__ - Epoch  44, Step:   95700, Batch Loss:     7.253890, Lr: 0.000065, Tokens per sec:   2736
2023-03-14 06:17:19,799 - INFO - __main__ - Epoch  44, Step:   95800, Batch Loss:     6.456865, Lr: 0.000065, Tokens per sec:   2693
2023-03-14 06:17:34,863 - INFO - __main__ - Epoch  44: total training loss 14002.03
2023-03-14 06:21:20,209 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 06:21:20,209 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 43.74457943694324, rouge_l = 56.23332189482739, meteor = 0
2023-03-14 06:21:21,311 - INFO - __main__ - Delete test_dir3/82802.ckpt
2023-03-14 06:21:21,443 - INFO - __main__ - Example #0
2023-03-14 06:21:21,444 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 06:21:21,444 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 06:21:21,444 - INFO - __main__ - Example #1
2023-03-14 06:21:21,444 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 06:21:21,444 - INFO - __main__ - 	Hypothesis: call when the activity start .
2023-03-14 06:21:21,444 - INFO - __main__ - Example #2
2023-03-14 06:21:21,444 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 06:21:21,444 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 06:21:21,451 - INFO - __main__ - Validation time = 222.5202763080597s.
2023-03-14 06:21:21,451 - INFO - __main__ - Epoch 45
2023-03-14 06:21:26,500 - INFO - __main__ - Epoch  45, Step:   95900, Batch Loss:     4.973837, Lr: 0.000064, Tokens per sec:   2482
2023-03-14 06:21:46,238 - INFO - __main__ - Epoch  45, Step:   96000, Batch Loss:     5.481899, Lr: 0.000064, Tokens per sec:   2712
2023-03-14 06:22:06,014 - INFO - __main__ - Epoch  45, Step:   96100, Batch Loss:     5.790885, Lr: 0.000064, Tokens per sec:   2706
2023-03-14 06:22:25,799 - INFO - __main__ - Epoch  45, Step:   96200, Batch Loss:     6.818393, Lr: 0.000064, Tokens per sec:   2738
2023-03-14 06:22:45,614 - INFO - __main__ - Epoch  45, Step:   96300, Batch Loss:     4.660292, Lr: 0.000064, Tokens per sec:   2702
2023-03-14 06:23:05,399 - INFO - __main__ - Epoch  45, Step:   96400, Batch Loss:     6.839869, Lr: 0.000064, Tokens per sec:   2731
2023-03-14 06:23:25,090 - INFO - __main__ - Epoch  45, Step:   96500, Batch Loss:     5.490613, Lr: 0.000064, Tokens per sec:   2695
2023-03-14 06:23:44,848 - INFO - __main__ - Epoch  45, Step:   96600, Batch Loss:     6.568545, Lr: 0.000064, Tokens per sec:   2717
2023-03-14 06:24:04,578 - INFO - __main__ - Epoch  45, Step:   96700, Batch Loss:     6.572082, Lr: 0.000064, Tokens per sec:   2730
2023-03-14 06:24:24,410 - INFO - __main__ - Epoch  45, Step:   96800, Batch Loss:     6.812198, Lr: 0.000064, Tokens per sec:   2747
2023-03-14 06:24:42,538 - INFO - __main__ - Epoch  45, Step:   96900, Batch Loss:     5.965971, Lr: 0.000064, Tokens per sec:   3002
2023-03-14 06:25:00,178 - INFO - __main__ - Epoch  45, Step:   97000, Batch Loss:     7.124168, Lr: 0.000064, Tokens per sec:   3002
2023-03-14 06:25:19,891 - INFO - __main__ - Epoch  45, Step:   97100, Batch Loss:     7.250953, Lr: 0.000064, Tokens per sec:   2724
2023-03-14 06:25:38,610 - INFO - __main__ - Epoch  45, Step:   97200, Batch Loss:     6.147834, Lr: 0.000064, Tokens per sec:   2947
2023-03-14 06:25:57,797 - INFO - __main__ - Epoch  45, Step:   97300, Batch Loss:     5.391675, Lr: 0.000064, Tokens per sec:   2802
2023-03-14 06:26:17,590 - INFO - __main__ - Epoch  45, Step:   97400, Batch Loss:     6.116399, Lr: 0.000064, Tokens per sec:   2717
2023-03-14 06:26:37,388 - INFO - __main__ - Epoch  45, Step:   97500, Batch Loss:     3.999244, Lr: 0.000064, Tokens per sec:   2698
2023-03-14 06:26:57,165 - INFO - __main__ - Epoch  45, Step:   97600, Batch Loss:     5.194324, Lr: 0.000064, Tokens per sec:   2685
2023-03-14 06:27:16,967 - INFO - __main__ - Epoch  45, Step:   97700, Batch Loss:     6.152375, Lr: 0.000064, Tokens per sec:   2715
2023-03-14 06:27:36,709 - INFO - __main__ - Epoch  45, Step:   97800, Batch Loss:     7.952886, Lr: 0.000064, Tokens per sec:   2712
2023-03-14 06:27:56,468 - INFO - __main__ - Epoch  45, Step:   97900, Batch Loss:     9.771087, Lr: 0.000064, Tokens per sec:   2792
2023-03-14 06:28:16,259 - INFO - __main__ - Epoch  45, Step:   98000, Batch Loss:     6.540953, Lr: 0.000064, Tokens per sec:   2754
2023-03-14 06:28:27,104 - INFO - __main__ - Epoch  45: total training loss 13532.85
2023-03-14 06:28:27,105 - INFO - __main__ - Epoch 46
2023-03-14 06:28:36,267 - INFO - __main__ - Epoch  46, Step:   98100, Batch Loss:     6.932293, Lr: 0.000064, Tokens per sec:   2634
2023-03-14 06:28:56,033 - INFO - __main__ - Epoch  46, Step:   98200, Batch Loss:     5.510009, Lr: 0.000064, Tokens per sec:   2731
2023-03-14 06:29:15,715 - INFO - __main__ - Epoch  46, Step:   98300, Batch Loss:     4.853687, Lr: 0.000064, Tokens per sec:   2753
2023-03-14 06:29:35,515 - INFO - __main__ - Epoch  46, Step:   98400, Batch Loss:     6.930713, Lr: 0.000064, Tokens per sec:   2717
2023-03-14 06:29:55,393 - INFO - __main__ - Epoch  46, Step:   98500, Batch Loss:     4.906763, Lr: 0.000064, Tokens per sec:   2735
2023-03-14 06:30:15,171 - INFO - __main__ - Epoch  46, Step:   98600, Batch Loss:     5.268197, Lr: 0.000064, Tokens per sec:   2713
2023-03-14 06:30:34,922 - INFO - __main__ - Epoch  46, Step:   98700, Batch Loss:     5.505841, Lr: 0.000064, Tokens per sec:   2730
2023-03-14 06:30:54,689 - INFO - __main__ - Epoch  46, Step:   98800, Batch Loss:     4.285941, Lr: 0.000064, Tokens per sec:   2768
2023-03-14 06:31:14,450 - INFO - __main__ - Epoch  46, Step:   98900, Batch Loss:     6.410321, Lr: 0.000064, Tokens per sec:   2671
2023-03-14 06:31:34,149 - INFO - __main__ - Epoch  46, Step:   99000, Batch Loss:     6.534870, Lr: 0.000064, Tokens per sec:   2701
2023-03-14 06:31:53,819 - INFO - __main__ - Epoch  46, Step:   99100, Batch Loss:     6.320685, Lr: 0.000064, Tokens per sec:   2736
2023-03-14 06:32:13,624 - INFO - __main__ - Epoch  46, Step:   99200, Batch Loss:     6.068585, Lr: 0.000064, Tokens per sec:   2781
2023-03-14 06:32:33,404 - INFO - __main__ - Epoch  46, Step:   99300, Batch Loss:     4.860096, Lr: 0.000064, Tokens per sec:   2690
2023-03-14 06:32:53,118 - INFO - __main__ - Epoch  46, Step:   99400, Batch Loss:     8.418468, Lr: 0.000064, Tokens per sec:   2760
2023-03-14 06:33:12,951 - INFO - __main__ - Epoch  46, Step:   99500, Batch Loss:     6.099566, Lr: 0.000064, Tokens per sec:   2697
2023-03-14 06:33:32,704 - INFO - __main__ - Epoch  46, Step:   99600, Batch Loss:     7.355653, Lr: 0.000064, Tokens per sec:   2731
2023-03-14 06:33:52,436 - INFO - __main__ - Epoch  46, Step:   99700, Batch Loss:     7.014955, Lr: 0.000064, Tokens per sec:   2748
2023-03-14 06:34:12,133 - INFO - __main__ - Epoch  46, Step:   99800, Batch Loss:     6.819818, Lr: 0.000064, Tokens per sec:   2722
2023-03-14 06:34:31,841 - INFO - __main__ - Epoch  46, Step:   99900, Batch Loss:     5.859143, Lr: 0.000064, Tokens per sec:   2725
2023-03-14 06:34:51,600 - INFO - __main__ - Epoch  46, Step:  100000, Batch Loss:     6.691317, Lr: 0.000064, Tokens per sec:   2714
2023-03-14 06:35:11,297 - INFO - __main__ - Epoch  46, Step:  100100, Batch Loss:     5.396920, Lr: 0.000064, Tokens per sec:   2680
2023-03-14 06:35:31,087 - INFO - __main__ - Epoch  46, Step:  100200, Batch Loss:     6.345304, Lr: 0.000064, Tokens per sec:   2748
2023-03-14 06:35:37,871 - INFO - __main__ - Epoch  46: total training loss 13098.32
2023-03-14 06:39:24,785 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 06:39:24,786 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 43.8594514523467, rouge_l = 56.32436198147482, meteor = 0
2023-03-14 06:39:25,883 - INFO - __main__ - Delete test_dir3/87160.ckpt
2023-03-14 06:39:26,019 - INFO - __main__ - Example #0
2023-03-14 06:39:26,020 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 06:39:26,020 - INFO - __main__ - 	Hypothesis: return a hashcode value for this string constant object .
2023-03-14 06:39:26,020 - INFO - __main__ - Example #1
2023-03-14 06:39:26,020 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 06:39:26,020 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 06:39:26,020 - INFO - __main__ - Example #2
2023-03-14 06:39:26,020 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 06:39:26,020 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this bind
2023-03-14 06:39:26,026 - INFO - __main__ - Validation time = 224.05486488342285s.
2023-03-14 06:39:26,026 - INFO - __main__ - Epoch 47
2023-03-14 06:39:38,022 - INFO - __main__ - Epoch  47, Step:  100300, Batch Loss:     7.284678, Lr: 0.000063, Tokens per sec:   2936
2023-03-14 06:39:55,734 - INFO - __main__ - Epoch  47, Step:  100400, Batch Loss:     6.347108, Lr: 0.000063, Tokens per sec:   3028
2023-03-14 06:40:13,370 - INFO - __main__ - Epoch  47, Step:  100500, Batch Loss:     3.595675, Lr: 0.000063, Tokens per sec:   3087
2023-03-14 06:40:31,040 - INFO - __main__ - Epoch  47, Step:  100600, Batch Loss:     3.904868, Lr: 0.000063, Tokens per sec:   3056
2023-03-14 06:40:50,564 - INFO - __main__ - Epoch  47, Step:  100700, Batch Loss:     8.486189, Lr: 0.000063, Tokens per sec:   2764
2023-03-14 06:41:10,343 - INFO - __main__ - Epoch  47, Step:  100800, Batch Loss:     4.385815, Lr: 0.000063, Tokens per sec:   2734
2023-03-14 06:41:30,055 - INFO - __main__ - Epoch  47, Step:  100900, Batch Loss:     7.003320, Lr: 0.000063, Tokens per sec:   2738
2023-03-14 06:41:49,510 - INFO - __main__ - Epoch  47, Step:  101000, Batch Loss:     5.422737, Lr: 0.000063, Tokens per sec:   2725
2023-03-14 06:42:07,504 - INFO - __main__ - Epoch  47, Step:  101100, Batch Loss:     5.248038, Lr: 0.000063, Tokens per sec:   3006
2023-03-14 06:42:27,253 - INFO - __main__ - Epoch  47, Step:  101200, Batch Loss:     5.704497, Lr: 0.000063, Tokens per sec:   2766
2023-03-14 06:42:46,991 - INFO - __main__ - Epoch  47, Step:  101300, Batch Loss:     6.424846, Lr: 0.000063, Tokens per sec:   2690
2023-03-14 06:43:06,700 - INFO - __main__ - Epoch  47, Step:  101400, Batch Loss:     7.274530, Lr: 0.000063, Tokens per sec:   2739
2023-03-14 06:43:26,457 - INFO - __main__ - Epoch  47, Step:  101500, Batch Loss:     4.677446, Lr: 0.000063, Tokens per sec:   2738
2023-03-14 06:43:46,158 - INFO - __main__ - Epoch  47, Step:  101600, Batch Loss:     6.207638, Lr: 0.000063, Tokens per sec:   2722
2023-03-14 06:44:05,964 - INFO - __main__ - Epoch  47, Step:  101700, Batch Loss:     7.539534, Lr: 0.000063, Tokens per sec:   2675
2023-03-14 06:44:24,102 - INFO - __main__ - Epoch  47, Step:  101800, Batch Loss:     5.061114, Lr: 0.000063, Tokens per sec:   2993
2023-03-14 06:44:41,866 - INFO - __main__ - Epoch  47, Step:  101900, Batch Loss:     4.996052, Lr: 0.000063, Tokens per sec:   2987
2023-03-14 06:44:59,553 - INFO - __main__ - Epoch  47, Step:  102000, Batch Loss:     7.418462, Lr: 0.000063, Tokens per sec:   3021
2023-03-14 06:45:17,254 - INFO - __main__ - Epoch  47, Step:  102100, Batch Loss:     6.417717, Lr: 0.000063, Tokens per sec:   3079
2023-03-14 06:45:35,001 - INFO - __main__ - Epoch  47, Step:  102200, Batch Loss:     6.420135, Lr: 0.000063, Tokens per sec:   2992
2023-03-14 06:45:52,715 - INFO - __main__ - Epoch  47, Step:  102300, Batch Loss:     2.695367, Lr: 0.000063, Tokens per sec:   3050
2023-03-14 06:46:10,498 - INFO - __main__ - Epoch  47, Step:  102400, Batch Loss:     7.712385, Lr: 0.000063, Tokens per sec:   3069
2023-03-14 06:46:12,857 - INFO - __main__ - Epoch  47: total training loss 12763.47
2023-03-14 06:46:12,858 - INFO - __main__ - Epoch 48
2023-03-14 06:46:28,621 - INFO - __main__ - Epoch  48, Step:  102500, Batch Loss:     6.430730, Lr: 0.000062, Tokens per sec:   2956
2023-03-14 06:46:46,339 - INFO - __main__ - Epoch  48, Step:  102600, Batch Loss:     6.170681, Lr: 0.000062, Tokens per sec:   3043
2023-03-14 06:47:04,130 - INFO - __main__ - Epoch  48, Step:  102700, Batch Loss:     4.639746, Lr: 0.000062, Tokens per sec:   3025
2023-03-14 06:47:21,806 - INFO - __main__ - Epoch  48, Step:  102800, Batch Loss:     7.032305, Lr: 0.000062, Tokens per sec:   3040
2023-03-14 06:47:39,537 - INFO - __main__ - Epoch  48, Step:  102900, Batch Loss:     6.060342, Lr: 0.000062, Tokens per sec:   3044
2023-03-14 06:47:57,871 - INFO - __main__ - Epoch  48, Step:  103000, Batch Loss:     4.649737, Lr: 0.000062, Tokens per sec:   2954
2023-03-14 06:48:17,579 - INFO - __main__ - Epoch  48, Step:  103100, Batch Loss:     4.784907, Lr: 0.000062, Tokens per sec:   2704
2023-03-14 06:48:36,414 - INFO - __main__ - Epoch  48, Step:  103200, Batch Loss:     3.841723, Lr: 0.000062, Tokens per sec:   2857
2023-03-14 06:48:54,033 - INFO - __main__ - Epoch  48, Step:  103300, Batch Loss:     5.536252, Lr: 0.000062, Tokens per sec:   3042
2023-03-14 06:49:11,774 - INFO - __main__ - Epoch  48, Step:  103400, Batch Loss:     5.147665, Lr: 0.000062, Tokens per sec:   3037
2023-03-14 06:49:31,037 - INFO - __main__ - Epoch  48, Step:  103500, Batch Loss:     6.194588, Lr: 0.000062, Tokens per sec:   2784
2023-03-14 06:49:50,740 - INFO - __main__ - Epoch  48, Step:  103600, Batch Loss:     5.432410, Lr: 0.000062, Tokens per sec:   2706
2023-03-14 06:50:10,477 - INFO - __main__ - Epoch  48, Step:  103700, Batch Loss:     5.842614, Lr: 0.000062, Tokens per sec:   2776
2023-03-14 06:50:30,184 - INFO - __main__ - Epoch  48, Step:  103800, Batch Loss:     7.614978, Lr: 0.000062, Tokens per sec:   2738
2023-03-14 06:50:49,898 - INFO - __main__ - Epoch  48, Step:  103900, Batch Loss:     5.818748, Lr: 0.000062, Tokens per sec:   2699
2023-03-14 06:51:09,592 - INFO - __main__ - Epoch  48, Step:  104000, Batch Loss:     5.123705, Lr: 0.000062, Tokens per sec:   2715
2023-03-14 06:51:29,273 - INFO - __main__ - Epoch  48, Step:  104100, Batch Loss:     5.695080, Lr: 0.000062, Tokens per sec:   2740
2023-03-14 06:51:48,898 - INFO - __main__ - Epoch  48, Step:  104200, Batch Loss:     6.067950, Lr: 0.000062, Tokens per sec:   2768
2023-03-14 06:52:08,584 - INFO - __main__ - Epoch  48, Step:  104300, Batch Loss:     4.334783, Lr: 0.000062, Tokens per sec:   2702
2023-03-14 06:52:28,305 - INFO - __main__ - Epoch  48, Step:  104400, Batch Loss:     6.500624, Lr: 0.000062, Tokens per sec:   2750
2023-03-14 06:52:48,072 - INFO - __main__ - Epoch  48, Step:  104500, Batch Loss:     7.239971, Lr: 0.000062, Tokens per sec:   2771
2023-03-14 06:53:06,254 - INFO - __main__ - Epoch  48: total training loss 12337.20
2023-03-14 06:56:53,538 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 06:56:53,538 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 43.86252855857745, rouge_l = 56.16988283119133, meteor = 0
2023-03-14 06:56:54,630 - INFO - __main__ - Delete test_dir3/91518.ckpt
2023-03-14 06:56:54,765 - INFO - __main__ - Example #0
2023-03-14 06:56:54,765 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 06:56:54,765 - INFO - __main__ - 	Hypothesis: return a hashcode for this string constant object .
2023-03-14 06:56:54,765 - INFO - __main__ - Example #1
2023-03-14 06:56:54,765 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 06:56:54,765 - INFO - __main__ - 	Hypothesis: call when the view be start .
2023-03-14 06:56:54,765 - INFO - __main__ - Example #2
2023-03-14 06:56:54,766 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 06:56:54,766 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this jtext
2023-03-14 06:56:54,772 - INFO - __main__ - Validation time = 224.4372091293335s.
2023-03-14 06:56:54,772 - INFO - __main__ - Epoch 49
2023-03-14 06:56:56,616 - INFO - __main__ - Epoch  49, Step:  104600, Batch Loss:     6.080740, Lr: 0.000062, Tokens per sec:   2297
2023-03-14 06:57:16,354 - INFO - __main__ - Epoch  49, Step:  104700, Batch Loss:     4.048566, Lr: 0.000062, Tokens per sec:   2710
2023-03-14 06:57:36,093 - INFO - __main__ - Epoch  49, Step:  104800, Batch Loss:     4.467490, Lr: 0.000062, Tokens per sec:   2738
2023-03-14 06:57:55,814 - INFO - __main__ - Epoch  49, Step:  104900, Batch Loss:     5.871576, Lr: 0.000062, Tokens per sec:   2747
2023-03-14 06:58:15,475 - INFO - __main__ - Epoch  49, Step:  105000, Batch Loss:     6.531227, Lr: 0.000062, Tokens per sec:   2761
2023-03-14 06:58:35,230 - INFO - __main__ - Epoch  49, Step:  105100, Batch Loss:     4.958842, Lr: 0.000062, Tokens per sec:   2708
2023-03-14 06:58:54,910 - INFO - __main__ - Epoch  49, Step:  105200, Batch Loss:     5.456385, Lr: 0.000062, Tokens per sec:   2702
2023-03-14 06:59:13,409 - INFO - __main__ - Epoch  49, Step:  105300, Batch Loss:     3.688146, Lr: 0.000062, Tokens per sec:   2925
2023-03-14 06:59:31,045 - INFO - __main__ - Epoch  49, Step:  105400, Batch Loss:     5.398347, Lr: 0.000062, Tokens per sec:   3087
2023-03-14 06:59:48,726 - INFO - __main__ - Epoch  49, Step:  105500, Batch Loss:     4.885928, Lr: 0.000062, Tokens per sec:   3035
2023-03-14 07:00:06,437 - INFO - __main__ - Epoch  49, Step:  105600, Batch Loss:     4.262217, Lr: 0.000062, Tokens per sec:   3042
2023-03-14 07:00:25,059 - INFO - __main__ - Epoch  49, Step:  105700, Batch Loss:     6.443208, Lr: 0.000062, Tokens per sec:   2913
2023-03-14 07:00:44,788 - INFO - __main__ - Epoch  49, Step:  105800, Batch Loss:     6.139048, Lr: 0.000062, Tokens per sec:   2748
2023-03-14 07:01:03,186 - INFO - __main__ - Epoch  49, Step:  105900, Batch Loss:     4.833384, Lr: 0.000062, Tokens per sec:   2960
2023-03-14 07:01:20,816 - INFO - __main__ - Epoch  49, Step:  106000, Batch Loss:     6.544791, Lr: 0.000062, Tokens per sec:   3016
2023-03-14 07:01:39,048 - INFO - __main__ - Epoch  49, Step:  106100, Batch Loss:     5.041551, Lr: 0.000062, Tokens per sec:   2956
2023-03-14 07:01:57,480 - INFO - __main__ - Epoch  49, Step:  106200, Batch Loss:     4.884512, Lr: 0.000062, Tokens per sec:   2930
2023-03-14 07:02:15,154 - INFO - __main__ - Epoch  49, Step:  106300, Batch Loss:     3.819019, Lr: 0.000062, Tokens per sec:   3050
2023-03-14 07:02:33,236 - INFO - __main__ - Epoch  49, Step:  106400, Batch Loss:     4.733378, Lr: 0.000062, Tokens per sec:   2971
2023-03-14 07:02:51,474 - INFO - __main__ - Epoch  49, Step:  106500, Batch Loss:     6.750495, Lr: 0.000062, Tokens per sec:   2927
2023-03-14 07:03:09,103 - INFO - __main__ - Epoch  49, Step:  106600, Batch Loss:     7.552557, Lr: 0.000062, Tokens per sec:   3100
2023-03-14 07:03:26,783 - INFO - __main__ - Epoch  49, Step:  106700, Batch Loss:     5.900712, Lr: 0.000062, Tokens per sec:   2950
2023-03-14 07:03:39,384 - INFO - __main__ - Epoch  49: total training loss 11953.94
2023-03-14 07:03:39,385 - INFO - __main__ - Epoch 50
2023-03-14 07:03:45,414 - INFO - __main__ - Epoch  50, Step:  106800, Batch Loss:     3.683209, Lr: 0.000061, Tokens per sec:   2633
2023-03-14 07:04:05,206 - INFO - __main__ - Epoch  50, Step:  106900, Batch Loss:     5.579484, Lr: 0.000061, Tokens per sec:   2724
2023-03-14 07:04:24,987 - INFO - __main__ - Epoch  50, Step:  107000, Batch Loss:     5.870899, Lr: 0.000061, Tokens per sec:   2792
2023-03-14 07:04:44,688 - INFO - __main__ - Epoch  50, Step:  107100, Batch Loss:     4.742321, Lr: 0.000061, Tokens per sec:   2738
2023-03-14 07:05:04,379 - INFO - __main__ - Epoch  50, Step:  107200, Batch Loss:     5.747642, Lr: 0.000061, Tokens per sec:   2713
2023-03-14 07:05:24,039 - INFO - __main__ - Epoch  50, Step:  107300, Batch Loss:     4.648001, Lr: 0.000061, Tokens per sec:   2721
2023-03-14 07:05:43,760 - INFO - __main__ - Epoch  50, Step:  107400, Batch Loss:     4.001863, Lr: 0.000061, Tokens per sec:   2709
2023-03-14 07:06:03,539 - INFO - __main__ - Epoch  50, Step:  107500, Batch Loss:     4.497612, Lr: 0.000061, Tokens per sec:   2748
2023-03-14 07:06:23,262 - INFO - __main__ - Epoch  50, Step:  107600, Batch Loss:     5.109404, Lr: 0.000061, Tokens per sec:   2738
2023-03-14 07:06:42,975 - INFO - __main__ - Epoch  50, Step:  107700, Batch Loss:     5.749063, Lr: 0.000061, Tokens per sec:   2749
2023-03-14 07:07:02,706 - INFO - __main__ - Epoch  50, Step:  107800, Batch Loss:     4.061790, Lr: 0.000061, Tokens per sec:   2680
2023-03-14 07:07:22,386 - INFO - __main__ - Epoch  50, Step:  107900, Batch Loss:     5.016100, Lr: 0.000061, Tokens per sec:   2720
2023-03-14 07:07:42,182 - INFO - __main__ - Epoch  50, Step:  108000, Batch Loss:     5.270112, Lr: 0.000061, Tokens per sec:   2754
2023-03-14 07:08:01,895 - INFO - __main__ - Epoch  50, Step:  108100, Batch Loss:     5.620621, Lr: 0.000061, Tokens per sec:   2746
2023-03-14 07:08:21,622 - INFO - __main__ - Epoch  50, Step:  108200, Batch Loss:     3.049652, Lr: 0.000061, Tokens per sec:   2701
2023-03-14 07:08:41,364 - INFO - __main__ - Epoch  50, Step:  108300, Batch Loss:     4.053731, Lr: 0.000061, Tokens per sec:   2749
2023-03-14 07:09:01,106 - INFO - __main__ - Epoch  50, Step:  108400, Batch Loss:     6.368547, Lr: 0.000061, Tokens per sec:   2708
2023-03-14 07:09:20,839 - INFO - __main__ - Epoch  50, Step:  108500, Batch Loss:     6.835017, Lr: 0.000061, Tokens per sec:   2739
2023-03-14 07:09:40,558 - INFO - __main__ - Epoch  50, Step:  108600, Batch Loss:     4.426744, Lr: 0.000061, Tokens per sec:   2713
2023-03-14 07:10:00,247 - INFO - __main__ - Epoch  50, Step:  108700, Batch Loss:     4.425182, Lr: 0.000061, Tokens per sec:   2723
2023-03-14 07:10:19,927 - INFO - __main__ - Epoch  50, Step:  108800, Batch Loss:     8.272352, Lr: 0.000061, Tokens per sec:   2731
2023-03-14 07:10:39,711 - INFO - __main__ - Epoch  50, Step:  108900, Batch Loss:     3.711250, Lr: 0.000061, Tokens per sec:   2728
2023-03-14 07:10:49,663 - INFO - __main__ - Epoch  50: total training loss 11631.03
2023-03-14 07:14:38,830 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 07:14:38,831 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 43.90575031340256, rouge_l = 56.100427497920556, meteor = 0
2023-03-14 07:14:39,926 - INFO - __main__ - Delete test_dir3/95876.ckpt
2023-03-14 07:14:40,063 - INFO - __main__ - Example #0
2023-03-14 07:14:40,063 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 07:14:40,063 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 07:14:40,063 - INFO - __main__ - Example #1
2023-03-14 07:14:40,063 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 07:14:40,063 - INFO - __main__ - 	Hypothesis: call when the activity be start . < p > by default , not start .
2023-03-14 07:14:40,063 - INFO - __main__ - Example #2
2023-03-14 07:14:40,063 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 07:14:40,063 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 07:14:40,070 - INFO - __main__ - Validation time = 225.96802425384521s.
2023-03-14 07:14:40,070 - INFO - __main__ - Epoch 51
2023-03-14 07:14:50,256 - INFO - __main__ - Epoch  51, Step:  109000, Batch Loss:     6.406507, Lr: 0.000061, Tokens per sec:   2629
2023-03-14 07:15:09,990 - INFO - __main__ - Epoch  51, Step:  109100, Batch Loss:     4.714449, Lr: 0.000061, Tokens per sec:   2685
2023-03-14 07:15:29,766 - INFO - __main__ - Epoch  51, Step:  109200, Batch Loss:     5.132525, Lr: 0.000061, Tokens per sec:   2730
2023-03-14 07:15:49,547 - INFO - __main__ - Epoch  51, Step:  109300, Batch Loss:     5.362877, Lr: 0.000061, Tokens per sec:   2753
2023-03-14 07:16:09,275 - INFO - __main__ - Epoch  51, Step:  109400, Batch Loss:     4.955381, Lr: 0.000061, Tokens per sec:   2742
2023-03-14 07:16:28,988 - INFO - __main__ - Epoch  51, Step:  109500, Batch Loss:     6.490153, Lr: 0.000061, Tokens per sec:   2742
2023-03-14 07:16:48,744 - INFO - __main__ - Epoch  51, Step:  109600, Batch Loss:     4.831284, Lr: 0.000061, Tokens per sec:   2700
2023-03-14 07:17:08,517 - INFO - __main__ - Epoch  51, Step:  109700, Batch Loss:     4.227905, Lr: 0.000061, Tokens per sec:   2700
2023-03-14 07:17:28,216 - INFO - __main__ - Epoch  51, Step:  109800, Batch Loss:     4.143109, Lr: 0.000061, Tokens per sec:   2741
2023-03-14 07:17:47,961 - INFO - __main__ - Epoch  51, Step:  109900, Batch Loss:     5.803945, Lr: 0.000061, Tokens per sec:   2739
2023-03-14 07:18:07,641 - INFO - __main__ - Epoch  51, Step:  110000, Batch Loss:     5.189420, Lr: 0.000061, Tokens per sec:   2747
2023-03-14 07:18:27,366 - INFO - __main__ - Epoch  51, Step:  110100, Batch Loss:     5.428228, Lr: 0.000061, Tokens per sec:   2747
2023-03-14 07:18:47,100 - INFO - __main__ - Epoch  51, Step:  110200, Batch Loss:     3.636507, Lr: 0.000061, Tokens per sec:   2729
2023-03-14 07:19:06,836 - INFO - __main__ - Epoch  51, Step:  110300, Batch Loss:     4.903264, Lr: 0.000061, Tokens per sec:   2798
2023-03-14 07:19:26,551 - INFO - __main__ - Epoch  51, Step:  110400, Batch Loss:     6.335376, Lr: 0.000061, Tokens per sec:   2677
2023-03-14 07:19:46,307 - INFO - __main__ - Epoch  51, Step:  110500, Batch Loss:     5.433859, Lr: 0.000061, Tokens per sec:   2670
2023-03-14 07:20:06,016 - INFO - __main__ - Epoch  51, Step:  110600, Batch Loss:     5.663413, Lr: 0.000061, Tokens per sec:   2778
2023-03-14 07:20:25,643 - INFO - __main__ - Epoch  51, Step:  110700, Batch Loss:     5.757577, Lr: 0.000061, Tokens per sec:   2745
2023-03-14 07:20:45,325 - INFO - __main__ - Epoch  51, Step:  110800, Batch Loss:     6.559474, Lr: 0.000061, Tokens per sec:   2734
2023-03-14 07:21:05,065 - INFO - __main__ - Epoch  51, Step:  110900, Batch Loss:     4.707536, Lr: 0.000061, Tokens per sec:   2705
2023-03-14 07:21:24,743 - INFO - __main__ - Epoch  51, Step:  111000, Batch Loss:     7.266447, Lr: 0.000061, Tokens per sec:   2730
2023-03-14 07:21:44,411 - INFO - __main__ - Epoch  51, Step:  111100, Batch Loss:     7.047968, Lr: 0.000061, Tokens per sec:   2743
2023-03-14 07:21:50,188 - INFO - __main__ - Epoch  51: total training loss 11338.71
2023-03-14 07:21:50,189 - INFO - __main__ - Epoch 52
2023-03-14 07:22:03,541 - INFO - __main__ - Epoch  52, Step:  111200, Batch Loss:     3.701501, Lr: 0.000060, Tokens per sec:   2922
2023-03-14 07:22:21,481 - INFO - __main__ - Epoch  52, Step:  111300, Batch Loss:     3.697236, Lr: 0.000060, Tokens per sec:   2950
2023-03-14 07:22:39,132 - INFO - __main__ - Epoch  52, Step:  111400, Batch Loss:     3.861313, Lr: 0.000060, Tokens per sec:   3049
2023-03-14 07:22:57,250 - INFO - __main__ - Epoch  52, Step:  111500, Batch Loss:     5.349592, Lr: 0.000060, Tokens per sec:   2994
2023-03-14 07:23:16,973 - INFO - __main__ - Epoch  52, Step:  111600, Batch Loss:     4.328683, Lr: 0.000060, Tokens per sec:   2687
2023-03-14 07:23:36,690 - INFO - __main__ - Epoch  52, Step:  111700, Batch Loss:     7.239761, Lr: 0.000060, Tokens per sec:   2752
2023-03-14 07:23:56,400 - INFO - __main__ - Epoch  52, Step:  111800, Batch Loss:     3.662479, Lr: 0.000060, Tokens per sec:   2802
2023-03-14 07:24:16,103 - INFO - __main__ - Epoch  52, Step:  111900, Batch Loss:     3.780912, Lr: 0.000060, Tokens per sec:   2754
2023-03-14 07:24:35,817 - INFO - __main__ - Epoch  52, Step:  112000, Batch Loss:     4.888433, Lr: 0.000060, Tokens per sec:   2718
2023-03-14 07:24:55,524 - INFO - __main__ - Epoch  52, Step:  112100, Batch Loss:     5.870182, Lr: 0.000060, Tokens per sec:   2740
2023-03-14 07:25:15,254 - INFO - __main__ - Epoch  52, Step:  112200, Batch Loss:     4.883121, Lr: 0.000060, Tokens per sec:   2683
2023-03-14 07:25:35,014 - INFO - __main__ - Epoch  52, Step:  112300, Batch Loss:     5.333928, Lr: 0.000060, Tokens per sec:   2726
2023-03-14 07:25:54,762 - INFO - __main__ - Epoch  52, Step:  112400, Batch Loss:     4.619593, Lr: 0.000060, Tokens per sec:   2705
2023-03-14 07:26:14,505 - INFO - __main__ - Epoch  52, Step:  112500, Batch Loss:     5.580592, Lr: 0.000060, Tokens per sec:   2803
2023-03-14 07:26:34,285 - INFO - __main__ - Epoch  52, Step:  112600, Batch Loss:     5.759792, Lr: 0.000060, Tokens per sec:   2710
2023-03-14 07:26:54,049 - INFO - __main__ - Epoch  52, Step:  112700, Batch Loss:     3.940546, Lr: 0.000060, Tokens per sec:   2725
2023-03-14 07:27:13,786 - INFO - __main__ - Epoch  52, Step:  112800, Batch Loss:     3.617592, Lr: 0.000060, Tokens per sec:   2671
2023-03-14 07:27:33,542 - INFO - __main__ - Epoch  52, Step:  112900, Batch Loss:     4.812212, Lr: 0.000060, Tokens per sec:   2682
2023-03-14 07:27:53,250 - INFO - __main__ - Epoch  52, Step:  113000, Batch Loss:     6.804067, Lr: 0.000060, Tokens per sec:   2779
2023-03-14 07:28:12,940 - INFO - __main__ - Epoch  52, Step:  113100, Batch Loss:     5.574729, Lr: 0.000060, Tokens per sec:   2718
2023-03-14 07:28:32,659 - INFO - __main__ - Epoch  52, Step:  113200, Batch Loss:     4.785078, Lr: 0.000060, Tokens per sec:   2731
2023-03-14 07:28:52,324 - INFO - __main__ - Epoch  52, Step:  113300, Batch Loss:     4.890424, Lr: 0.000060, Tokens per sec:   2735
2023-03-14 07:28:53,904 - INFO - __main__ - Epoch  52: total training loss 11001.44
2023-03-14 07:32:40,374 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 07:32:40,375 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 44.17510385389019, rouge_l = 56.37441590281129, meteor = 0
2023-03-14 07:32:41,502 - INFO - __main__ - Delete test_dir3/100234.ckpt
2023-03-14 07:32:41,639 - INFO - __main__ - Example #0
2023-03-14 07:32:41,639 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 07:32:41,639 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 07:32:41,639 - INFO - __main__ - Example #1
2023-03-14 07:32:41,639 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 07:32:41,640 - INFO - __main__ - 	Hypothesis: call when the activity start .
2023-03-14 07:32:41,640 - INFO - __main__ - Example #2
2023-03-14 07:32:41,640 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 07:32:41,640 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 07:32:41,647 - INFO - __main__ - Validation time = 223.6132411956787s.
2023-03-14 07:32:41,647 - INFO - __main__ - Epoch 53
2023-03-14 07:33:00,112 - INFO - __main__ - Epoch  53, Step:  113400, Batch Loss:     3.188572, Lr: 0.000059, Tokens per sec:   2676
2023-03-14 07:33:18,711 - INFO - __main__ - Epoch  53, Step:  113500, Batch Loss:     4.160601, Lr: 0.000059, Tokens per sec:   2852
2023-03-14 07:33:36,344 - INFO - __main__ - Epoch  53, Step:  113600, Batch Loss:     5.903320, Lr: 0.000059, Tokens per sec:   3090
2023-03-14 07:33:55,854 - INFO - __main__ - Epoch  53, Step:  113700, Batch Loss:     4.337113, Lr: 0.000059, Tokens per sec:   2742
2023-03-14 07:34:15,528 - INFO - __main__ - Epoch  53, Step:  113800, Batch Loss:     5.144156, Lr: 0.000059, Tokens per sec:   2728
2023-03-14 07:34:35,304 - INFO - __main__ - Epoch  53, Step:  113900, Batch Loss:     4.304892, Lr: 0.000059, Tokens per sec:   2739
2023-03-14 07:34:55,056 - INFO - __main__ - Epoch  53, Step:  114000, Batch Loss:     5.420745, Lr: 0.000059, Tokens per sec:   2734
2023-03-14 07:35:14,808 - INFO - __main__ - Epoch  53, Step:  114100, Batch Loss:     6.134115, Lr: 0.000059, Tokens per sec:   2759
2023-03-14 07:35:34,427 - INFO - __main__ - Epoch  53, Step:  114200, Batch Loss:     5.232953, Lr: 0.000059, Tokens per sec:   2715
2023-03-14 07:35:54,121 - INFO - __main__ - Epoch  53, Step:  114300, Batch Loss:     4.729460, Lr: 0.000059, Tokens per sec:   2713
2023-03-14 07:36:13,901 - INFO - __main__ - Epoch  53, Step:  114400, Batch Loss:     5.175246, Lr: 0.000059, Tokens per sec:   2749
2023-03-14 07:36:33,610 - INFO - __main__ - Epoch  53, Step:  114500, Batch Loss:     3.946596, Lr: 0.000059, Tokens per sec:   2765
2023-03-14 07:36:53,375 - INFO - __main__ - Epoch  53, Step:  114600, Batch Loss:     5.224996, Lr: 0.000059, Tokens per sec:   2690
2023-03-14 07:37:13,090 - INFO - __main__ - Epoch  53, Step:  114700, Batch Loss:     3.572058, Lr: 0.000059, Tokens per sec:   2737
2023-03-14 07:37:32,848 - INFO - __main__ - Epoch  53, Step:  114800, Batch Loss:     4.534201, Lr: 0.000059, Tokens per sec:   2707
2023-03-14 07:37:52,595 - INFO - __main__ - Epoch  53, Step:  114900, Batch Loss:     4.476706, Lr: 0.000059, Tokens per sec:   2744
2023-03-14 07:38:12,336 - INFO - __main__ - Epoch  53, Step:  115000, Batch Loss:     5.565541, Lr: 0.000059, Tokens per sec:   2763
2023-03-14 07:38:32,043 - INFO - __main__ - Epoch  53, Step:  115100, Batch Loss:     5.742349, Lr: 0.000059, Tokens per sec:   2763
2023-03-14 07:38:51,771 - INFO - __main__ - Epoch  53, Step:  115200, Batch Loss:     4.472371, Lr: 0.000059, Tokens per sec:   2710
2023-03-14 07:39:11,512 - INFO - __main__ - Epoch  53, Step:  115300, Batch Loss:     4.852664, Lr: 0.000059, Tokens per sec:   2709
2023-03-14 07:39:31,264 - INFO - __main__ - Epoch  53, Step:  115400, Batch Loss:     2.799464, Lr: 0.000059, Tokens per sec:   2699
2023-03-14 07:39:48,469 - INFO - __main__ - Epoch  53: total training loss 10684.81
2023-03-14 07:39:48,470 - INFO - __main__ - Epoch 54
2023-03-14 07:39:51,323 - INFO - __main__ - Epoch  54, Step:  115500, Batch Loss:     5.208178, Lr: 0.000059, Tokens per sec:   2491
2023-03-14 07:40:11,034 - INFO - __main__ - Epoch  54, Step:  115600, Batch Loss:     3.425893, Lr: 0.000059, Tokens per sec:   2753
2023-03-14 07:40:30,748 - INFO - __main__ - Epoch  54, Step:  115700, Batch Loss:     4.445876, Lr: 0.000059, Tokens per sec:   2728
2023-03-14 07:40:50,477 - INFO - __main__ - Epoch  54, Step:  115800, Batch Loss:     3.515955, Lr: 0.000059, Tokens per sec:   2731
2023-03-14 07:41:10,154 - INFO - __main__ - Epoch  54, Step:  115900, Batch Loss:     3.590503, Lr: 0.000059, Tokens per sec:   2675
2023-03-14 07:41:29,821 - INFO - __main__ - Epoch  54, Step:  116000, Batch Loss:     4.379460, Lr: 0.000059, Tokens per sec:   2701
2023-03-14 07:41:49,544 - INFO - __main__ - Epoch  54, Step:  116100, Batch Loss:     5.808719, Lr: 0.000059, Tokens per sec:   2707
2023-03-14 07:42:09,249 - INFO - __main__ - Epoch  54, Step:  116200, Batch Loss:     2.982209, Lr: 0.000059, Tokens per sec:   2718
2023-03-14 07:42:28,935 - INFO - __main__ - Epoch  54, Step:  116300, Batch Loss:     6.714796, Lr: 0.000059, Tokens per sec:   2713
2023-03-14 07:42:48,681 - INFO - __main__ - Epoch  54, Step:  116400, Batch Loss:     6.577393, Lr: 0.000059, Tokens per sec:   2805
2023-03-14 07:43:08,372 - INFO - __main__ - Epoch  54, Step:  116500, Batch Loss:     6.776432, Lr: 0.000059, Tokens per sec:   2734
2023-03-14 07:43:28,085 - INFO - __main__ - Epoch  54, Step:  116600, Batch Loss:     4.015224, Lr: 0.000059, Tokens per sec:   2701
2023-03-14 07:43:47,790 - INFO - __main__ - Epoch  54, Step:  116700, Batch Loss:     3.553124, Lr: 0.000059, Tokens per sec:   2762
2023-03-14 07:44:07,505 - INFO - __main__ - Epoch  54, Step:  116800, Batch Loss:     4.657813, Lr: 0.000059, Tokens per sec:   2743
2023-03-14 07:44:27,193 - INFO - __main__ - Epoch  54, Step:  116900, Batch Loss:     4.603225, Lr: 0.000059, Tokens per sec:   2767
2023-03-14 07:44:46,868 - INFO - __main__ - Epoch  54, Step:  117000, Batch Loss:     4.537333, Lr: 0.000059, Tokens per sec:   2762
2023-03-14 07:45:06,623 - INFO - __main__ - Epoch  54, Step:  117100, Batch Loss:     3.853540, Lr: 0.000059, Tokens per sec:   2696
2023-03-14 07:45:26,276 - INFO - __main__ - Epoch  54, Step:  117200, Batch Loss:     4.488851, Lr: 0.000059, Tokens per sec:   2731
2023-03-14 07:45:45,895 - INFO - __main__ - Epoch  54, Step:  117300, Batch Loss:     5.724736, Lr: 0.000059, Tokens per sec:   2723
2023-03-14 07:46:05,620 - INFO - __main__ - Epoch  54, Step:  117400, Batch Loss:     3.720821, Lr: 0.000059, Tokens per sec:   2747
2023-03-14 07:46:25,301 - INFO - __main__ - Epoch  54, Step:  117500, Batch Loss:     5.707128, Lr: 0.000059, Tokens per sec:   2757
2023-03-14 07:46:45,024 - INFO - __main__ - Epoch  54, Step:  117600, Batch Loss:     5.649758, Lr: 0.000059, Tokens per sec:   2726
2023-03-14 07:46:58,021 - INFO - __main__ - Epoch  54: total training loss 10408.87
2023-03-14 07:50:45,225 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 07:50:45,225 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 44.25110072352754, rouge_l = 56.36045466177878, meteor = 0
2023-03-14 07:50:46,336 - INFO - __main__ - Delete test_dir3/104592.ckpt
2023-03-14 07:50:46,469 - INFO - __main__ - Example #0
2023-03-14 07:50:46,469 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 07:50:46,469 - INFO - __main__ - 	Hypothesis: return a hash code for this set of attribute .
2023-03-14 07:50:46,469 - INFO - __main__ - Example #1
2023-03-14 07:50:46,469 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 07:50:46,469 - INFO - __main__ - 	Hypothesis: call when the activity start . < p > by default , this method be call .
2023-03-14 07:50:46,469 - INFO - __main__ - Example #2
2023-03-14 07:50:46,469 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 07:50:46,469 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 07:50:46,476 - INFO - __main__ - Validation time = 224.3552041053772s.
2023-03-14 07:50:46,476 - INFO - __main__ - Epoch 55
2023-03-14 07:50:53,491 - INFO - __main__ - Epoch  55, Step:  117700, Batch Loss:     7.135412, Lr: 0.000058, Tokens per sec:   2645
2023-03-14 07:51:11,664 - INFO - __main__ - Epoch  55, Step:  117800, Batch Loss:     4.908579, Lr: 0.000058, Tokens per sec:   2933
2023-03-14 07:51:29,357 - INFO - __main__ - Epoch  55, Step:  117900, Batch Loss:     3.017942, Lr: 0.000058, Tokens per sec:   3035
2023-03-14 07:51:47,994 - INFO - __main__ - Epoch  55, Step:  118000, Batch Loss:     3.427628, Lr: 0.000058, Tokens per sec:   2872
2023-03-14 07:52:07,692 - INFO - __main__ - Epoch  55, Step:  118100, Batch Loss:     3.056506, Lr: 0.000058, Tokens per sec:   2760
2023-03-14 07:52:25,511 - INFO - __main__ - Epoch  55, Step:  118200, Batch Loss:     5.175772, Lr: 0.000058, Tokens per sec:   2994
2023-03-14 07:52:44,210 - INFO - __main__ - Epoch  55, Step:  118300, Batch Loss:     4.438227, Lr: 0.000058, Tokens per sec:   2892
2023-03-14 07:53:03,933 - INFO - __main__ - Epoch  55, Step:  118400, Batch Loss:     4.023449, Lr: 0.000058, Tokens per sec:   2734
2023-03-14 07:53:23,725 - INFO - __main__ - Epoch  55, Step:  118500, Batch Loss:     5.366207, Lr: 0.000058, Tokens per sec:   2708
2023-03-14 07:53:43,402 - INFO - __main__ - Epoch  55, Step:  118600, Batch Loss:     3.305915, Lr: 0.000058, Tokens per sec:   2770
2023-03-14 07:54:03,179 - INFO - __main__ - Epoch  55, Step:  118700, Batch Loss:     3.053456, Lr: 0.000058, Tokens per sec:   2770
2023-03-14 07:54:22,907 - INFO - __main__ - Epoch  55, Step:  118800, Batch Loss:     5.455708, Lr: 0.000058, Tokens per sec:   2735
2023-03-14 07:54:42,561 - INFO - __main__ - Epoch  55, Step:  118900, Batch Loss:     4.089892, Lr: 0.000058, Tokens per sec:   2757
2023-03-14 07:55:02,312 - INFO - __main__ - Epoch  55, Step:  119000, Batch Loss:     3.267124, Lr: 0.000058, Tokens per sec:   2726
2023-03-14 07:55:22,024 - INFO - __main__ - Epoch  55, Step:  119100, Batch Loss:     4.710436, Lr: 0.000058, Tokens per sec:   2730
2023-03-14 07:55:42,100 - INFO - __main__ - Epoch  55, Step:  119200, Batch Loss:     5.223367, Lr: 0.000058, Tokens per sec:   2660
2023-03-14 07:56:01,899 - INFO - __main__ - Epoch  55, Step:  119300, Batch Loss:     5.109443, Lr: 0.000058, Tokens per sec:   2716
2023-03-14 07:56:21,536 - INFO - __main__ - Epoch  55, Step:  119400, Batch Loss:     5.298923, Lr: 0.000058, Tokens per sec:   2709
2023-03-14 07:56:41,176 - INFO - __main__ - Epoch  55, Step:  119500, Batch Loss:     3.401156, Lr: 0.000058, Tokens per sec:   2730
2023-03-14 07:57:00,881 - INFO - __main__ - Epoch  55, Step:  119600, Batch Loss:     3.447092, Lr: 0.000058, Tokens per sec:   2716
2023-03-14 07:57:20,589 - INFO - __main__ - Epoch  55, Step:  119700, Batch Loss:     4.605019, Lr: 0.000058, Tokens per sec:   2741
2023-03-14 07:57:40,386 - INFO - __main__ - Epoch  55, Step:  119800, Batch Loss:     5.486959, Lr: 0.000058, Tokens per sec:   2711
2023-03-14 07:57:49,317 - INFO - __main__ - Epoch  55: total training loss 10174.06
2023-03-14 07:57:49,318 - INFO - __main__ - Epoch 56
2023-03-14 07:58:00,446 - INFO - __main__ - Epoch  56, Step:  119900, Batch Loss:     5.004005, Lr: 0.000058, Tokens per sec:   2682
2023-03-14 07:58:20,161 - INFO - __main__ - Epoch  56, Step:  120000, Batch Loss:     4.208499, Lr: 0.000058, Tokens per sec:   2734
2023-03-14 07:58:39,782 - INFO - __main__ - Epoch  56, Step:  120100, Batch Loss:     5.067118, Lr: 0.000058, Tokens per sec:   2777
2023-03-14 07:58:59,538 - INFO - __main__ - Epoch  56, Step:  120200, Batch Loss:     5.762618, Lr: 0.000058, Tokens per sec:   2741
2023-03-14 07:59:19,312 - INFO - __main__ - Epoch  56, Step:  120300, Batch Loss:     4.871288, Lr: 0.000058, Tokens per sec:   2719
2023-03-14 07:59:39,006 - INFO - __main__ - Epoch  56, Step:  120400, Batch Loss:     5.054299, Lr: 0.000058, Tokens per sec:   2710
2023-03-14 07:59:58,708 - INFO - __main__ - Epoch  56, Step:  120500, Batch Loss:     4.558030, Lr: 0.000058, Tokens per sec:   2755
2023-03-14 08:00:18,414 - INFO - __main__ - Epoch  56, Step:  120600, Batch Loss:     4.485578, Lr: 0.000058, Tokens per sec:   2726
2023-03-14 08:00:38,119 - INFO - __main__ - Epoch  56, Step:  120700, Batch Loss:     3.090866, Lr: 0.000058, Tokens per sec:   2701
2023-03-14 08:00:57,778 - INFO - __main__ - Epoch  56, Step:  120800, Batch Loss:     3.656960, Lr: 0.000058, Tokens per sec:   2753
2023-03-14 08:01:17,504 - INFO - __main__ - Epoch  56, Step:  120900, Batch Loss:     4.600473, Lr: 0.000058, Tokens per sec:   2764
2023-03-14 08:01:37,278 - INFO - __main__ - Epoch  56, Step:  121000, Batch Loss:     2.675673, Lr: 0.000058, Tokens per sec:   2734
2023-03-14 08:01:56,944 - INFO - __main__ - Epoch  56, Step:  121100, Batch Loss:     4.964424, Lr: 0.000058, Tokens per sec:   2773
2023-03-14 08:02:16,710 - INFO - __main__ - Epoch  56, Step:  121200, Batch Loss:     4.721178, Lr: 0.000058, Tokens per sec:   2664
2023-03-14 08:02:36,490 - INFO - __main__ - Epoch  56, Step:  121300, Batch Loss:     5.470144, Lr: 0.000058, Tokens per sec:   2727
2023-03-14 08:02:56,204 - INFO - __main__ - Epoch  56, Step:  121400, Batch Loss:     5.253201, Lr: 0.000058, Tokens per sec:   2711
2023-03-14 08:03:15,927 - INFO - __main__ - Epoch  56, Step:  121500, Batch Loss:     6.121241, Lr: 0.000058, Tokens per sec:   2710
2023-03-14 08:03:35,637 - INFO - __main__ - Epoch  56, Step:  121600, Batch Loss:     6.128706, Lr: 0.000058, Tokens per sec:   2699
2023-03-14 08:03:55,346 - INFO - __main__ - Epoch  56, Step:  121700, Batch Loss:     4.614817, Lr: 0.000058, Tokens per sec:   2709
2023-03-14 08:04:15,056 - INFO - __main__ - Epoch  56, Step:  121800, Batch Loss:     3.338871, Lr: 0.000058, Tokens per sec:   2751
2023-03-14 08:04:34,791 - INFO - __main__ - Epoch  56, Step:  121900, Batch Loss:     3.498465, Lr: 0.000058, Tokens per sec:   2712
2023-03-14 08:04:54,461 - INFO - __main__ - Epoch  56, Step:  122000, Batch Loss:     6.726137, Lr: 0.000058, Tokens per sec:   2746
2023-03-14 08:04:59,204 - INFO - __main__ - Epoch  56: total training loss 9873.18
2023-03-14 08:08:45,646 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 08:08:45,647 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 44.39269084672374, rouge_l = 56.418981169271845, meteor = 0
2023-03-14 08:08:46,760 - INFO - __main__ - Delete test_dir3/108950.ckpt
2023-03-14 08:08:46,889 - INFO - __main__ - Example #0
2023-03-14 08:08:46,889 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 08:08:46,890 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 08:08:46,890 - INFO - __main__ - Example #1
2023-03-14 08:08:46,890 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 08:08:46,890 - INFO - __main__ - 	Hypothesis: call when the activity start .
2023-03-14 08:08:46,890 - INFO - __main__ - Example #2
2023-03-14 08:08:46,890 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 08:08:46,890 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 08:08:46,898 - INFO - __main__ - Validation time = 223.58254313468933s.
2023-03-14 08:08:46,898 - INFO - __main__ - Epoch 57
2023-03-14 08:09:02,165 - INFO - __main__ - Epoch  57, Step:  122100, Batch Loss:     5.791336, Lr: 0.000057, Tokens per sec:   2696
2023-03-14 08:09:21,832 - INFO - __main__ - Epoch  57, Step:  122200, Batch Loss:     5.723440, Lr: 0.000057, Tokens per sec:   2710
2023-03-14 08:09:41,594 - INFO - __main__ - Epoch  57, Step:  122300, Batch Loss:     3.851481, Lr: 0.000057, Tokens per sec:   2670
2023-03-14 08:10:01,341 - INFO - __main__ - Epoch  57, Step:  122400, Batch Loss:     4.871719, Lr: 0.000057, Tokens per sec:   2770
2023-03-14 08:10:21,142 - INFO - __main__ - Epoch  57, Step:  122500, Batch Loss:     3.642433, Lr: 0.000057, Tokens per sec:   2729
2023-03-14 08:10:40,890 - INFO - __main__ - Epoch  57, Step:  122600, Batch Loss:     4.554749, Lr: 0.000057, Tokens per sec:   2705
2023-03-14 08:11:00,561 - INFO - __main__ - Epoch  57, Step:  122700, Batch Loss:     3.972248, Lr: 0.000057, Tokens per sec:   2712
2023-03-14 08:11:20,316 - INFO - __main__ - Epoch  57, Step:  122800, Batch Loss:     3.353653, Lr: 0.000057, Tokens per sec:   2711
2023-03-14 08:11:40,035 - INFO - __main__ - Epoch  57, Step:  122900, Batch Loss:     4.404944, Lr: 0.000057, Tokens per sec:   2762
2023-03-14 08:11:59,670 - INFO - __main__ - Epoch  57, Step:  123000, Batch Loss:     4.640098, Lr: 0.000057, Tokens per sec:   2731
2023-03-14 08:12:19,441 - INFO - __main__ - Epoch  57, Step:  123100, Batch Loss:     4.633130, Lr: 0.000057, Tokens per sec:   2678
2023-03-14 08:12:39,217 - INFO - __main__ - Epoch  57, Step:  123200, Batch Loss:     5.695623, Lr: 0.000057, Tokens per sec:   2737
2023-03-14 08:12:58,890 - INFO - __main__ - Epoch  57, Step:  123300, Batch Loss:     4.741805, Lr: 0.000057, Tokens per sec:   2679
2023-03-14 08:13:18,568 - INFO - __main__ - Epoch  57, Step:  123400, Batch Loss:     3.414549, Lr: 0.000057, Tokens per sec:   2769
2023-03-14 08:13:38,196 - INFO - __main__ - Epoch  57, Step:  123500, Batch Loss:     4.241058, Lr: 0.000057, Tokens per sec:   2693
2023-03-14 08:13:57,969 - INFO - __main__ - Epoch  57, Step:  123600, Batch Loss:     3.412028, Lr: 0.000057, Tokens per sec:   2800
2023-03-14 08:14:17,742 - INFO - __main__ - Epoch  57, Step:  123700, Batch Loss:     4.956148, Lr: 0.000057, Tokens per sec:   2741
2023-03-14 08:14:37,452 - INFO - __main__ - Epoch  57, Step:  123800, Batch Loss:     4.579559, Lr: 0.000057, Tokens per sec:   2734
2023-03-14 08:14:57,145 - INFO - __main__ - Epoch  57, Step:  123900, Batch Loss:     4.926926, Lr: 0.000057, Tokens per sec:   2730
2023-03-14 08:15:16,863 - INFO - __main__ - Epoch  57, Step:  124000, Batch Loss:     6.030777, Lr: 0.000057, Tokens per sec:   2689
2023-03-14 08:15:36,614 - INFO - __main__ - Epoch  57, Step:  124100, Batch Loss:     4.158201, Lr: 0.000057, Tokens per sec:   2816
2023-03-14 08:15:56,302 - INFO - __main__ - Epoch  57, Step:  124200, Batch Loss:     6.028224, Lr: 0.000057, Tokens per sec:   2739
2023-03-14 08:15:56,937 - INFO - __main__ - Epoch  57: total training loss 9597.31
2023-03-14 08:15:56,938 - INFO - __main__ - Epoch 58
2023-03-14 08:16:16,363 - INFO - __main__ - Epoch  58, Step:  124300, Batch Loss:     3.819182, Lr: 0.000056, Tokens per sec:   2668
2023-03-14 08:16:36,057 - INFO - __main__ - Epoch  58, Step:  124400, Batch Loss:     3.138245, Lr: 0.000056, Tokens per sec:   2703
2023-03-14 08:16:55,808 - INFO - __main__ - Epoch  58, Step:  124500, Batch Loss:     4.507642, Lr: 0.000056, Tokens per sec:   2695
2023-03-14 08:17:15,517 - INFO - __main__ - Epoch  58, Step:  124600, Batch Loss:     3.446130, Lr: 0.000056, Tokens per sec:   2759
2023-03-14 08:17:35,270 - INFO - __main__ - Epoch  58, Step:  124700, Batch Loss:     3.881819, Lr: 0.000056, Tokens per sec:   2680
2023-03-14 08:17:54,953 - INFO - __main__ - Epoch  58, Step:  124800, Batch Loss:     5.725512, Lr: 0.000056, Tokens per sec:   2743
2023-03-14 08:18:14,630 - INFO - __main__ - Epoch  58, Step:  124900, Batch Loss:     3.589223, Lr: 0.000056, Tokens per sec:   2707
2023-03-14 08:18:34,309 - INFO - __main__ - Epoch  58, Step:  125000, Batch Loss:     5.639717, Lr: 0.000056, Tokens per sec:   2729
2023-03-14 08:18:53,957 - INFO - __main__ - Epoch  58, Step:  125100, Batch Loss:     3.831856, Lr: 0.000056, Tokens per sec:   2761
2023-03-14 08:19:13,727 - INFO - __main__ - Epoch  58, Step:  125200, Batch Loss:     4.082347, Lr: 0.000056, Tokens per sec:   2737
2023-03-14 08:19:33,450 - INFO - __main__ - Epoch  58, Step:  125300, Batch Loss:     5.528461, Lr: 0.000056, Tokens per sec:   2745
2023-03-14 08:19:53,122 - INFO - __main__ - Epoch  58, Step:  125400, Batch Loss:     4.301647, Lr: 0.000056, Tokens per sec:   2716
2023-03-14 08:20:12,798 - INFO - __main__ - Epoch  58, Step:  125500, Batch Loss:     5.447719, Lr: 0.000056, Tokens per sec:   2773
2023-03-14 08:20:32,442 - INFO - __main__ - Epoch  58, Step:  125600, Batch Loss:     5.385837, Lr: 0.000056, Tokens per sec:   2792
2023-03-14 08:20:52,217 - INFO - __main__ - Epoch  58, Step:  125700, Batch Loss:     6.196667, Lr: 0.000056, Tokens per sec:   2765
2023-03-14 08:21:11,946 - INFO - __main__ - Epoch  58, Step:  125800, Batch Loss:     4.547100, Lr: 0.000056, Tokens per sec:   2751
2023-03-14 08:21:31,594 - INFO - __main__ - Epoch  58, Step:  125900, Batch Loss:     4.068640, Lr: 0.000056, Tokens per sec:   2754
2023-03-14 08:21:51,299 - INFO - __main__ - Epoch  58, Step:  126000, Batch Loss:     4.946675, Lr: 0.000056, Tokens per sec:   2713
2023-03-14 08:22:11,029 - INFO - __main__ - Epoch  58, Step:  126100, Batch Loss:     6.335687, Lr: 0.000056, Tokens per sec:   2684
2023-03-14 08:22:30,727 - INFO - __main__ - Epoch  58, Step:  126200, Batch Loss:     5.569837, Lr: 0.000056, Tokens per sec:   2696
2023-03-14 08:22:50,459 - INFO - __main__ - Epoch  58, Step:  126300, Batch Loss:     4.937047, Lr: 0.000056, Tokens per sec:   2718
2023-03-14 08:23:06,646 - INFO - __main__ - Epoch  58: total training loss 9360.05
2023-03-14 08:26:52,128 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 08:26:52,129 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 44.63804397621604, rouge_l = 56.857872248497664, meteor = 0
2023-03-14 08:26:53,252 - INFO - __main__ - Delete test_dir3/113308.ckpt
2023-03-14 08:26:53,392 - INFO - __main__ - Example #0
2023-03-14 08:26:53,392 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 08:26:53,392 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 08:26:53,392 - INFO - __main__ - Example #1
2023-03-14 08:26:53,392 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 08:26:53,392 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 08:26:53,392 - INFO - __main__ - Example #2
2023-03-14 08:26:53,392 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 08:26:53,392 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 08:26:53,398 - INFO - __main__ - Validation time = 222.62984085083008s.
2023-03-14 08:26:53,399 - INFO - __main__ - Epoch 59
2023-03-14 08:26:57,259 - INFO - __main__ - Epoch  59, Step:  126400, Batch Loss:     4.464164, Lr: 0.000056, Tokens per sec:   2383
2023-03-14 08:27:17,044 - INFO - __main__ - Epoch  59, Step:  126500, Batch Loss:     4.340697, Lr: 0.000056, Tokens per sec:   2698
2023-03-14 08:27:36,782 - INFO - __main__ - Epoch  59, Step:  126600, Batch Loss:     3.081532, Lr: 0.000056, Tokens per sec:   2731
2023-03-14 08:27:56,563 - INFO - __main__ - Epoch  59, Step:  126700, Batch Loss:     3.020205, Lr: 0.000056, Tokens per sec:   2723
2023-03-14 08:28:16,346 - INFO - __main__ - Epoch  59, Step:  126800, Batch Loss:     4.492615, Lr: 0.000056, Tokens per sec:   2780
2023-03-14 08:28:36,067 - INFO - __main__ - Epoch  59, Step:  126900, Batch Loss:     4.225135, Lr: 0.000056, Tokens per sec:   2677
2023-03-14 08:28:55,842 - INFO - __main__ - Epoch  59, Step:  127000, Batch Loss:     4.037516, Lr: 0.000056, Tokens per sec:   2714
2023-03-14 08:29:15,599 - INFO - __main__ - Epoch  59, Step:  127100, Batch Loss:     5.631691, Lr: 0.000056, Tokens per sec:   2711
2023-03-14 08:29:35,394 - INFO - __main__ - Epoch  59, Step:  127200, Batch Loss:     4.637364, Lr: 0.000056, Tokens per sec:   2764
2023-03-14 08:29:55,081 - INFO - __main__ - Epoch  59, Step:  127300, Batch Loss:     4.156989, Lr: 0.000056, Tokens per sec:   2729
2023-03-14 08:30:14,798 - INFO - __main__ - Epoch  59, Step:  127400, Batch Loss:     4.684252, Lr: 0.000056, Tokens per sec:   2719
2023-03-14 08:30:34,520 - INFO - __main__ - Epoch  59, Step:  127500, Batch Loss:     3.346163, Lr: 0.000056, Tokens per sec:   2696
2023-03-14 08:30:54,274 - INFO - __main__ - Epoch  59, Step:  127600, Batch Loss:     4.392907, Lr: 0.000056, Tokens per sec:   2715
2023-03-14 08:31:14,032 - INFO - __main__ - Epoch  59, Step:  127700, Batch Loss:     3.752786, Lr: 0.000056, Tokens per sec:   2749
2023-03-14 08:31:33,732 - INFO - __main__ - Epoch  59, Step:  127800, Batch Loss:     4.403446, Lr: 0.000056, Tokens per sec:   2744
2023-03-14 08:31:53,483 - INFO - __main__ - Epoch  59, Step:  127900, Batch Loss:     5.595002, Lr: 0.000056, Tokens per sec:   2748
2023-03-14 08:32:13,180 - INFO - __main__ - Epoch  59, Step:  128000, Batch Loss:     5.370254, Lr: 0.000056, Tokens per sec:   2773
2023-03-14 08:32:32,449 - INFO - __main__ - Epoch  59, Step:  128100, Batch Loss:     3.418878, Lr: 0.000056, Tokens per sec:   2757
2023-03-14 08:32:50,139 - INFO - __main__ - Epoch  59, Step:  128200, Batch Loss:     5.696369, Lr: 0.000056, Tokens per sec:   3103
2023-03-14 08:33:09,697 - INFO - __main__ - Epoch  59, Step:  128300, Batch Loss:     4.669994, Lr: 0.000056, Tokens per sec:   2772
2023-03-14 08:33:29,413 - INFO - __main__ - Epoch  59, Step:  128400, Batch Loss:     2.704956, Lr: 0.000056, Tokens per sec:   2694
2023-03-14 08:33:47,558 - INFO - __main__ - Epoch  59, Step:  128500, Batch Loss:     4.512414, Lr: 0.000056, Tokens per sec:   2953
2023-03-14 08:33:59,532 - INFO - __main__ - Epoch  59: total training loss 9137.37
2023-03-14 08:33:59,533 - INFO - __main__ - Epoch 60
2023-03-14 08:34:07,321 - INFO - __main__ - Epoch  60, Step:  128600, Batch Loss:     4.125024, Lr: 0.000055, Tokens per sec:   2626
2023-03-14 08:34:24,916 - INFO - __main__ - Epoch  60, Step:  128700, Batch Loss:     4.573277, Lr: 0.000055, Tokens per sec:   3080
2023-03-14 08:34:42,564 - INFO - __main__ - Epoch  60, Step:  128800, Batch Loss:     4.685179, Lr: 0.000055, Tokens per sec:   3084
2023-03-14 08:35:00,207 - INFO - __main__ - Epoch  60, Step:  128900, Batch Loss:     3.361901, Lr: 0.000055, Tokens per sec:   3052
2023-03-14 08:35:17,898 - INFO - __main__ - Epoch  60, Step:  129000, Batch Loss:     3.358395, Lr: 0.000055, Tokens per sec:   2951
2023-03-14 08:35:37,010 - INFO - __main__ - Epoch  60, Step:  129100, Batch Loss:     4.179999, Lr: 0.000055, Tokens per sec:   2871
2023-03-14 08:35:56,680 - INFO - __main__ - Epoch  60, Step:  129200, Batch Loss:     3.852571, Lr: 0.000055, Tokens per sec:   2695
2023-03-14 08:36:16,341 - INFO - __main__ - Epoch  60, Step:  129300, Batch Loss:     5.066172, Lr: 0.000055, Tokens per sec:   2749
2023-03-14 08:36:36,077 - INFO - __main__ - Epoch  60, Step:  129400, Batch Loss:     4.945149, Lr: 0.000055, Tokens per sec:   2737
2023-03-14 08:36:55,790 - INFO - __main__ - Epoch  60, Step:  129500, Batch Loss:     6.214747, Lr: 0.000055, Tokens per sec:   2747
2023-03-14 08:37:15,611 - INFO - __main__ - Epoch  60, Step:  129600, Batch Loss:     5.958491, Lr: 0.000055, Tokens per sec:   2754
2023-03-14 08:37:35,346 - INFO - __main__ - Epoch  60, Step:  129700, Batch Loss:     3.061342, Lr: 0.000055, Tokens per sec:   2708
2023-03-14 08:37:55,133 - INFO - __main__ - Epoch  60, Step:  129800, Batch Loss:     5.832022, Lr: 0.000055, Tokens per sec:   2723
2023-03-14 08:38:14,871 - INFO - __main__ - Epoch  60, Step:  129900, Batch Loss:     4.045644, Lr: 0.000055, Tokens per sec:   2725
2023-03-14 08:38:34,582 - INFO - __main__ - Epoch  60, Step:  130000, Batch Loss:     4.870447, Lr: 0.000055, Tokens per sec:   2733
2023-03-14 08:38:54,263 - INFO - __main__ - Epoch  60, Step:  130100, Batch Loss:     5.203730, Lr: 0.000055, Tokens per sec:   2752
2023-03-14 08:39:13,911 - INFO - __main__ - Epoch  60, Step:  130200, Batch Loss:     4.552238, Lr: 0.000055, Tokens per sec:   2760
2023-03-14 08:39:33,558 - INFO - __main__ - Epoch  60, Step:  130300, Batch Loss:     4.800297, Lr: 0.000055, Tokens per sec:   2709
2023-03-14 08:39:53,208 - INFO - __main__ - Epoch  60, Step:  130400, Batch Loss:     3.457280, Lr: 0.000055, Tokens per sec:   2694
2023-03-14 08:40:12,908 - INFO - __main__ - Epoch  60, Step:  130500, Batch Loss:     3.469834, Lr: 0.000055, Tokens per sec:   2757
2023-03-14 08:40:32,639 - INFO - __main__ - Epoch  60, Step:  130600, Batch Loss:     3.005039, Lr: 0.000055, Tokens per sec:   2743
2023-03-14 08:40:52,377 - INFO - __main__ - Epoch  60, Step:  130700, Batch Loss:     4.350432, Lr: 0.000055, Tokens per sec:   2742
2023-03-14 08:41:00,299 - INFO - __main__ - Epoch  60: total training loss 8956.78
2023-03-14 08:44:45,835 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 44.61551379127028, rouge_l = 56.89815461784957, meteor = 0
2023-03-14 08:44:46,936 - INFO - __main__ - Delete test_dir3/117666.ckpt
2023-03-14 08:44:47,068 - INFO - __main__ - Example #0
2023-03-14 08:44:47,068 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 08:44:47,068 - INFO - __main__ - 	Hypothesis: return a hash code for this class constant object .
2023-03-14 08:44:47,068 - INFO - __main__ - Example #1
2023-03-14 08:44:47,068 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 08:44:47,068 - INFO - __main__ - 	Hypothesis: call when the activity start .
2023-03-14 08:44:47,068 - INFO - __main__ - Example #2
2023-03-14 08:44:47,068 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 08:44:47,068 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 08:44:47,074 - INFO - __main__ - Validation time = 222.3298716545105s.
2023-03-14 08:44:47,075 - INFO - __main__ - Epoch 61
2023-03-14 08:44:59,284 - INFO - __main__ - Epoch  61, Step:  130800, Batch Loss:     2.566085, Lr: 0.000055, Tokens per sec:   2649
2023-03-14 08:45:18,994 - INFO - __main__ - Epoch  61, Step:  130900, Batch Loss:     3.664206, Lr: 0.000055, Tokens per sec:   2720
2023-03-14 08:45:38,764 - INFO - __main__ - Epoch  61, Step:  131000, Batch Loss:     2.867788, Lr: 0.000055, Tokens per sec:   2711
2023-03-14 08:45:58,467 - INFO - __main__ - Epoch  61, Step:  131100, Batch Loss:     3.627409, Lr: 0.000055, Tokens per sec:   2714
2023-03-14 08:46:18,201 - INFO - __main__ - Epoch  61, Step:  131200, Batch Loss:     3.654273, Lr: 0.000055, Tokens per sec:   2767
2023-03-14 08:46:37,895 - INFO - __main__ - Epoch  61, Step:  131300, Batch Loss:     3.491289, Lr: 0.000055, Tokens per sec:   2758
2023-03-14 08:46:57,647 - INFO - __main__ - Epoch  61, Step:  131400, Batch Loss:     4.711156, Lr: 0.000055, Tokens per sec:   2771
2023-03-14 08:47:17,312 - INFO - __main__ - Epoch  61, Step:  131500, Batch Loss:     3.347276, Lr: 0.000055, Tokens per sec:   2781
2023-03-14 08:47:37,000 - INFO - __main__ - Epoch  61, Step:  131600, Batch Loss:     5.129963, Lr: 0.000055, Tokens per sec:   2770
2023-03-14 08:47:56,647 - INFO - __main__ - Epoch  61, Step:  131700, Batch Loss:     5.095941, Lr: 0.000055, Tokens per sec:   2720
2023-03-14 08:48:16,307 - INFO - __main__ - Epoch  61, Step:  131800, Batch Loss:     5.940196, Lr: 0.000055, Tokens per sec:   2722
2023-03-14 08:48:36,014 - INFO - __main__ - Epoch  61, Step:  131900, Batch Loss:     3.723737, Lr: 0.000055, Tokens per sec:   2724
2023-03-14 08:48:55,707 - INFO - __main__ - Epoch  61, Step:  132000, Batch Loss:     3.487042, Lr: 0.000055, Tokens per sec:   2681
2023-03-14 08:49:15,364 - INFO - __main__ - Epoch  61, Step:  132100, Batch Loss:     3.221146, Lr: 0.000055, Tokens per sec:   2720
2023-03-14 08:49:35,117 - INFO - __main__ - Epoch  61, Step:  132200, Batch Loss:     4.580524, Lr: 0.000055, Tokens per sec:   2711
2023-03-14 08:49:54,886 - INFO - __main__ - Epoch  61, Step:  132300, Batch Loss:     3.633523, Lr: 0.000055, Tokens per sec:   2748
2023-03-14 08:50:14,598 - INFO - __main__ - Epoch  61, Step:  132400, Batch Loss:     3.217833, Lr: 0.000055, Tokens per sec:   2752
2023-03-14 08:50:34,320 - INFO - __main__ - Epoch  61, Step:  132500, Batch Loss:     4.432695, Lr: 0.000055, Tokens per sec:   2726
2023-03-14 08:50:53,983 - INFO - __main__ - Epoch  61, Step:  132600, Batch Loss:     3.688872, Lr: 0.000055, Tokens per sec:   2737
2023-03-14 08:51:13,706 - INFO - __main__ - Epoch  61, Step:  132700, Batch Loss:     5.355561, Lr: 0.000055, Tokens per sec:   2733
2023-03-14 08:51:33,453 - INFO - __main__ - Epoch  61, Step:  132800, Batch Loss:     4.450210, Lr: 0.000055, Tokens per sec:   2697
2023-03-14 08:51:53,114 - INFO - __main__ - Epoch  61, Step:  132900, Batch Loss:     4.153113, Lr: 0.000055, Tokens per sec:   2708
2023-03-14 08:51:56,923 - INFO - __main__ - Epoch  61: total training loss 8733.60
2023-03-14 08:51:56,924 - INFO - __main__ - Epoch 62
2023-03-14 08:52:13,186 - INFO - __main__ - Epoch  62, Step:  133000, Batch Loss:     4.626297, Lr: 0.000054, Tokens per sec:   2758
2023-03-14 08:52:32,901 - INFO - __main__ - Epoch  62, Step:  133100, Batch Loss:     4.423276, Lr: 0.000054, Tokens per sec:   2743
2023-03-14 08:52:52,591 - INFO - __main__ - Epoch  62, Step:  133200, Batch Loss:     2.603207, Lr: 0.000054, Tokens per sec:   2714
2023-03-14 08:53:12,323 - INFO - __main__ - Epoch  62, Step:  133300, Batch Loss:     4.940160, Lr: 0.000054, Tokens per sec:   2766
2023-03-14 08:53:32,060 - INFO - __main__ - Epoch  62, Step:  133400, Batch Loss:     3.432257, Lr: 0.000054, Tokens per sec:   2733
2023-03-14 08:53:51,769 - INFO - __main__ - Epoch  62, Step:  133500, Batch Loss:     3.095368, Lr: 0.000054, Tokens per sec:   2702
2023-03-14 08:54:11,535 - INFO - __main__ - Epoch  62, Step:  133600, Batch Loss:     5.267133, Lr: 0.000054, Tokens per sec:   2705
2023-03-14 08:54:31,223 - INFO - __main__ - Epoch  62, Step:  133700, Batch Loss:     3.278489, Lr: 0.000054, Tokens per sec:   2697
2023-03-14 08:54:50,927 - INFO - __main__ - Epoch  62, Step:  133800, Batch Loss:     3.737342, Lr: 0.000054, Tokens per sec:   2705
2023-03-14 08:55:10,661 - INFO - __main__ - Epoch  62, Step:  133900, Batch Loss:     3.380526, Lr: 0.000054, Tokens per sec:   2738
2023-03-14 08:55:30,404 - INFO - __main__ - Epoch  62, Step:  134000, Batch Loss:     2.812839, Lr: 0.000054, Tokens per sec:   2725
2023-03-14 08:55:50,084 - INFO - __main__ - Epoch  62, Step:  134100, Batch Loss:     5.073225, Lr: 0.000054, Tokens per sec:   2717
2023-03-14 08:56:09,855 - INFO - __main__ - Epoch  62, Step:  134200, Batch Loss:     3.096921, Lr: 0.000054, Tokens per sec:   2705
2023-03-14 08:56:29,534 - INFO - __main__ - Epoch  62, Step:  134300, Batch Loss:     5.129735, Lr: 0.000054, Tokens per sec:   2741
2023-03-14 08:56:49,307 - INFO - __main__ - Epoch  62, Step:  134400, Batch Loss:     2.804482, Lr: 0.000054, Tokens per sec:   2713
2023-03-14 08:57:09,031 - INFO - __main__ - Epoch  62, Step:  134500, Batch Loss:     3.502201, Lr: 0.000054, Tokens per sec:   2722
2023-03-14 08:57:28,774 - INFO - __main__ - Epoch  62, Step:  134600, Batch Loss:     5.807224, Lr: 0.000054, Tokens per sec:   2763
2023-03-14 08:57:46,960 - INFO - __main__ - Epoch  62, Step:  134700, Batch Loss:     4.336091, Lr: 0.000054, Tokens per sec:   2985
2023-03-14 08:58:04,609 - INFO - __main__ - Epoch  62, Step:  134800, Batch Loss:     4.769639, Lr: 0.000054, Tokens per sec:   3034
2023-03-14 08:58:22,208 - INFO - __main__ - Epoch  62, Step:  134900, Batch Loss:     3.921471, Lr: 0.000054, Tokens per sec:   3071
2023-03-14 08:58:39,891 - INFO - __main__ - Epoch  62, Step:  135000, Batch Loss:     5.475217, Lr: 0.000054, Tokens per sec:   3027
2023-03-14 08:58:57,288 - INFO - __main__ - Epoch  62: total training loss 8565.86
2023-03-14 09:02:44,694 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 44.48185239687064, rouge_l = 56.610744457935056, meteor = 0
2023-03-14 09:02:45,798 - INFO - __main__ - Delete test_dir3/122024.ckpt
2023-03-14 09:02:45,934 - INFO - __main__ - Example #0
2023-03-14 09:02:45,934 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 09:02:45,935 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 09:02:45,935 - INFO - __main__ - Example #1
2023-03-14 09:02:45,935 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 09:02:45,935 - INFO - __main__ - 	Hypothesis: call when the activity start . < p > a start .
2023-03-14 09:02:45,935 - INFO - __main__ - Example #2
2023-03-14 09:02:45,935 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 09:02:45,935 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 09:02:45,945 - INFO - __main__ - Validation time = 224.55601525306702s.
2023-03-14 09:02:45,946 - INFO - __main__ - Epoch 63
2023-03-14 09:02:46,612 - INFO - __main__ - Epoch  63, Step:  135100, Batch Loss:     3.177975, Lr: 0.000054, Tokens per sec:   1613
2023-03-14 09:03:06,361 - INFO - __main__ - Epoch  63, Step:  135200, Batch Loss:     3.751141, Lr: 0.000054, Tokens per sec:   2715
2023-03-14 09:03:26,166 - INFO - __main__ - Epoch  63, Step:  135300, Batch Loss:     5.101526, Lr: 0.000054, Tokens per sec:   2740
2023-03-14 09:03:45,911 - INFO - __main__ - Epoch  63, Step:  135400, Batch Loss:     2.591728, Lr: 0.000054, Tokens per sec:   2752
2023-03-14 09:04:05,644 - INFO - __main__ - Epoch  63, Step:  135500, Batch Loss:     4.439038, Lr: 0.000054, Tokens per sec:   2703
2023-03-14 09:04:23,738 - INFO - __main__ - Epoch  63, Step:  135600, Batch Loss:     3.168151, Lr: 0.000054, Tokens per sec:   2988
2023-03-14 09:04:42,307 - INFO - __main__ - Epoch  63, Step:  135700, Batch Loss:     3.773748, Lr: 0.000054, Tokens per sec:   2860
2023-03-14 09:05:02,055 - INFO - __main__ - Epoch  63, Step:  135800, Batch Loss:     2.740725, Lr: 0.000054, Tokens per sec:   2693
2023-03-14 09:05:21,018 - INFO - __main__ - Epoch  63, Step:  135900, Batch Loss:     3.417329, Lr: 0.000054, Tokens per sec:   2856
2023-03-14 09:05:39,269 - INFO - __main__ - Epoch  63, Step:  136000, Batch Loss:     3.007504, Lr: 0.000054, Tokens per sec:   2951
2023-03-14 09:05:57,930 - INFO - __main__ - Epoch  63, Step:  136100, Batch Loss:     4.394861, Lr: 0.000054, Tokens per sec:   2874
2023-03-14 09:06:17,033 - INFO - __main__ - Epoch  63, Step:  136200, Batch Loss:     2.691686, Lr: 0.000054, Tokens per sec:   2835
2023-03-14 09:06:35,630 - INFO - __main__ - Epoch  63, Step:  136300, Batch Loss:     4.924703, Lr: 0.000054, Tokens per sec:   2879
2023-03-14 09:06:53,902 - INFO - __main__ - Epoch  63, Step:  136400, Batch Loss:     3.146554, Lr: 0.000054, Tokens per sec:   2997
2023-03-14 09:07:12,124 - INFO - __main__ - Epoch  63, Step:  136500, Batch Loss:     3.048481, Lr: 0.000054, Tokens per sec:   2971
2023-03-14 09:07:30,036 - INFO - __main__ - Epoch  63, Step:  136600, Batch Loss:     5.197221, Lr: 0.000054, Tokens per sec:   2995
2023-03-14 09:07:48,993 - INFO - __main__ - Epoch  63, Step:  136700, Batch Loss:     3.948926, Lr: 0.000054, Tokens per sec:   2821
2023-03-14 09:08:07,033 - INFO - __main__ - Epoch  63, Step:  136800, Batch Loss:     4.049688, Lr: 0.000054, Tokens per sec:   2972
2023-03-14 09:08:26,154 - INFO - __main__ - Epoch  63, Step:  136900, Batch Loss:     3.430807, Lr: 0.000054, Tokens per sec:   2830
2023-03-14 09:08:46,147 - INFO - __main__ - Epoch  63, Step:  137000, Batch Loss:     4.385146, Lr: 0.000054, Tokens per sec:   2713
2023-03-14 09:09:06,047 - INFO - __main__ - Epoch  63, Step:  137100, Batch Loss:     3.801311, Lr: 0.000054, Tokens per sec:   2706
2023-03-14 09:09:25,788 - INFO - __main__ - Epoch  63, Step:  137200, Batch Loss:     5.331295, Lr: 0.000054, Tokens per sec:   2728
2023-03-14 09:09:40,643 - INFO - __main__ - Epoch  63: total training loss 8326.94
2023-03-14 09:09:40,644 - INFO - __main__ - Epoch 64
2023-03-14 09:09:45,574 - INFO - __main__ - Epoch  64, Step:  137300, Batch Loss:     3.044850, Lr: 0.000053, Tokens per sec:   2466
2023-03-14 09:10:05,298 - INFO - __main__ - Epoch  64, Step:  137400, Batch Loss:     3.037017, Lr: 0.000053, Tokens per sec:   2748
2023-03-14 09:10:25,204 - INFO - __main__ - Epoch  64, Step:  137500, Batch Loss:     3.175086, Lr: 0.000053, Tokens per sec:   2710
2023-03-14 09:10:43,933 - INFO - __main__ - Epoch  64, Step:  137600, Batch Loss:     4.891260, Lr: 0.000053, Tokens per sec:   2879
2023-03-14 09:11:03,019 - INFO - __main__ - Epoch  64, Step:  137700, Batch Loss:     5.003139, Lr: 0.000053, Tokens per sec:   2792
2023-03-14 09:11:23,006 - INFO - __main__ - Epoch  64, Step:  137800, Batch Loss:     3.691051, Lr: 0.000053, Tokens per sec:   2683
2023-03-14 09:11:42,966 - INFO - __main__ - Epoch  64, Step:  137900, Batch Loss:     3.558247, Lr: 0.000053, Tokens per sec:   2775
2023-03-14 09:12:02,915 - INFO - __main__ - Epoch  64, Step:  138000, Batch Loss:     4.191695, Lr: 0.000053, Tokens per sec:   2691
2023-03-14 09:12:22,916 - INFO - __main__ - Epoch  64, Step:  138100, Batch Loss:     4.145053, Lr: 0.000053, Tokens per sec:   2664
2023-03-14 09:12:42,461 - INFO - __main__ - Epoch  64, Step:  138200, Batch Loss:     4.199197, Lr: 0.000053, Tokens per sec:   2749
2023-03-14 09:13:02,142 - INFO - __main__ - Epoch  64, Step:  138300, Batch Loss:     3.940297, Lr: 0.000053, Tokens per sec:   2716
2023-03-14 09:13:21,724 - INFO - __main__ - Epoch  64, Step:  138400, Batch Loss:     3.360116, Lr: 0.000053, Tokens per sec:   2756
2023-03-14 09:13:41,731 - INFO - __main__ - Epoch  64, Step:  138500, Batch Loss:     3.662934, Lr: 0.000053, Tokens per sec:   2726
2023-03-14 09:14:01,721 - INFO - __main__ - Epoch  64, Step:  138600, Batch Loss:     4.232875, Lr: 0.000053, Tokens per sec:   2702
2023-03-14 09:14:21,666 - INFO - __main__ - Epoch  64, Step:  138700, Batch Loss:     3.667782, Lr: 0.000053, Tokens per sec:   2624
2023-03-14 09:14:41,631 - INFO - __main__ - Epoch  64, Step:  138800, Batch Loss:     2.641017, Lr: 0.000053, Tokens per sec:   2682
2023-03-14 09:15:01,567 - INFO - __main__ - Epoch  64, Step:  138900, Batch Loss:     2.128597, Lr: 0.000053, Tokens per sec:   2746
2023-03-14 09:15:21,507 - INFO - __main__ - Epoch  64, Step:  139000, Batch Loss:     3.557234, Lr: 0.000053, Tokens per sec:   2719
2023-03-14 09:15:41,470 - INFO - __main__ - Epoch  64, Step:  139100, Batch Loss:     2.653263, Lr: 0.000053, Tokens per sec:   2717
2023-03-14 09:16:01,475 - INFO - __main__ - Epoch  64, Step:  139200, Batch Loss:     4.502824, Lr: 0.000053, Tokens per sec:   2702
2023-03-14 09:16:21,436 - INFO - __main__ - Epoch  64, Step:  139300, Batch Loss:     3.721301, Lr: 0.000053, Tokens per sec:   2657
2023-03-14 09:16:41,404 - INFO - __main__ - Epoch  64, Step:  139400, Batch Loss:     5.069635, Lr: 0.000053, Tokens per sec:   2726
2023-03-14 09:16:52,652 - INFO - __main__ - Epoch  64: total training loss 8149.89
2023-03-14 09:20:39,132 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 09:20:39,133 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 44.95223501933941, rouge_l = 57.1521123106562, meteor = 0
2023-03-14 09:20:40,272 - INFO - __main__ - Delete test_dir3/135098.ckpt
2023-03-14 09:20:40,426 - INFO - __main__ - Delete /home/tongye2/ytnmt/src_integration/test_dir3/135098.ckpt
2023-03-14 09:20:40,426 - WARNING - __main__ - Want to delete old checkpoint /home/tongye2/ytnmt/src_integration/test_dir3/135098.ckptbut file does not exist. ([Errno 2] No such file or directory: '/home/tongye2/ytnmt/src_integration/test_dir3/135098.ckpt')
2023-03-14 09:20:40,427 - INFO - __main__ - Example #0
2023-03-14 09:20:40,427 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 09:20:40,427 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 09:20:40,427 - INFO - __main__ - Example #1
2023-03-14 09:20:40,427 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 09:20:40,427 - INFO - __main__ - 	Hypothesis: call when the connection be start .
2023-03-14 09:20:40,427 - INFO - __main__ - Example #2
2023-03-14 09:20:40,427 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 09:20:40,427 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 09:20:40,437 - INFO - __main__ - Validation time = 223.616361618042s.
2023-03-14 09:20:40,437 - INFO - __main__ - Epoch 65
2023-03-14 09:20:49,250 - INFO - __main__ - Epoch  65, Step:  139500, Batch Loss:     2.955589, Lr: 0.000053, Tokens per sec:   2701
2023-03-14 09:21:08,183 - INFO - __main__ - Epoch  65, Step:  139600, Batch Loss:     3.852504, Lr: 0.000053, Tokens per sec:   2822
2023-03-14 09:21:27,250 - INFO - __main__ - Epoch  65, Step:  139700, Batch Loss:     3.424220, Lr: 0.000053, Tokens per sec:   2804
2023-03-14 09:21:46,191 - INFO - __main__ - Epoch  65, Step:  139800, Batch Loss:     3.337341, Lr: 0.000053, Tokens per sec:   2847
2023-03-14 09:22:06,129 - INFO - __main__ - Epoch  65, Step:  139900, Batch Loss:     3.913231, Lr: 0.000053, Tokens per sec:   2684
2023-03-14 09:22:26,123 - INFO - __main__ - Epoch  65, Step:  140000, Batch Loss:     3.596872, Lr: 0.000053, Tokens per sec:   2675
2023-03-14 09:22:46,079 - INFO - __main__ - Epoch  65, Step:  140100, Batch Loss:     3.283614, Lr: 0.000053, Tokens per sec:   2721
2023-03-14 09:23:05,290 - INFO - __main__ - Epoch  65, Step:  140200, Batch Loss:     2.495205, Lr: 0.000053, Tokens per sec:   2774
2023-03-14 09:23:24,863 - INFO - __main__ - Epoch  65, Step:  140300, Batch Loss:     4.386438, Lr: 0.000053, Tokens per sec:   2735
2023-03-14 09:23:43,983 - INFO - __main__ - Epoch  65, Step:  140400, Batch Loss:     4.390162, Lr: 0.000053, Tokens per sec:   2798
2023-03-14 09:24:02,933 - INFO - __main__ - Epoch  65, Step:  140500, Batch Loss:     3.287164, Lr: 0.000053, Tokens per sec:   2835
2023-03-14 09:24:22,846 - INFO - __main__ - Epoch  65, Step:  140600, Batch Loss:     2.866457, Lr: 0.000053, Tokens per sec:   2712
2023-03-14 09:24:42,359 - INFO - __main__ - Epoch  65, Step:  140700, Batch Loss:     2.687048, Lr: 0.000053, Tokens per sec:   2799
2023-03-14 09:25:01,461 - INFO - __main__ - Epoch  65, Step:  140800, Batch Loss:     3.201797, Lr: 0.000053, Tokens per sec:   2752
2023-03-14 09:25:20,635 - INFO - __main__ - Epoch  65, Step:  140900, Batch Loss:     3.367722, Lr: 0.000053, Tokens per sec:   2831
2023-03-14 09:25:39,646 - INFO - __main__ - Epoch  65, Step:  141000, Batch Loss:     4.784226, Lr: 0.000053, Tokens per sec:   2876
2023-03-14 09:25:58,433 - INFO - __main__ - Epoch  65, Step:  141100, Batch Loss:     4.488344, Lr: 0.000053, Tokens per sec:   2879
2023-03-14 09:26:17,757 - INFO - __main__ - Epoch  65, Step:  141200, Batch Loss:     4.433818, Lr: 0.000053, Tokens per sec:   2766
2023-03-14 09:26:36,878 - INFO - __main__ - Epoch  65, Step:  141300, Batch Loss:     4.025444, Lr: 0.000053, Tokens per sec:   2870
2023-03-14 09:26:56,175 - INFO - __main__ - Epoch  65, Step:  141400, Batch Loss:     4.938429, Lr: 0.000053, Tokens per sec:   2786
2023-03-14 09:27:16,151 - INFO - __main__ - Epoch  65, Step:  141500, Batch Loss:     2.948988, Lr: 0.000053, Tokens per sec:   2702
2023-03-14 09:27:36,121 - INFO - __main__ - Epoch  65, Step:  141600, Batch Loss:     4.231516, Lr: 0.000053, Tokens per sec:   2701
2023-03-14 09:27:43,190 - INFO - __main__ - Epoch  65: total training loss 7946.88
2023-03-14 09:27:43,191 - INFO - __main__ - Epoch 66
2023-03-14 09:27:56,549 - INFO - __main__ - Epoch  66, Step:  141700, Batch Loss:     2.855143, Lr: 0.000052, Tokens per sec:   2606
2023-03-14 09:28:16,539 - INFO - __main__ - Epoch  66, Step:  141800, Batch Loss:     4.210937, Lr: 0.000052, Tokens per sec:   2700
2023-03-14 09:28:37,086 - INFO - __main__ - Epoch  66, Step:  141900, Batch Loss:     1.906449, Lr: 0.000052, Tokens per sec:   2657
2023-03-14 09:28:56,504 - INFO - __main__ - Epoch  66, Step:  142000, Batch Loss:     4.159341, Lr: 0.000052, Tokens per sec:   2820
2023-03-14 09:29:15,041 - INFO - __main__ - Epoch  66, Step:  142100, Batch Loss:     3.125283, Lr: 0.000052, Tokens per sec:   2904
2023-03-14 09:29:33,736 - INFO - __main__ - Epoch  66, Step:  142200, Batch Loss:     2.995057, Lr: 0.000052, Tokens per sec:   2906
2023-03-14 09:29:52,426 - INFO - __main__ - Epoch  66, Step:  142300, Batch Loss:     4.130595, Lr: 0.000052, Tokens per sec:   2903
2023-03-14 09:30:12,079 - INFO - __main__ - Epoch  66, Step:  142400, Batch Loss:     3.377201, Lr: 0.000052, Tokens per sec:   2707
2023-03-14 09:30:31,551 - INFO - __main__ - Epoch  66, Step:  142500, Batch Loss:     4.491242, Lr: 0.000052, Tokens per sec:   2801
2023-03-14 09:30:51,338 - INFO - __main__ - Epoch  66, Step:  142600, Batch Loss:     3.344961, Lr: 0.000052, Tokens per sec:   2727
2023-03-14 09:31:11,196 - INFO - __main__ - Epoch  66, Step:  142700, Batch Loss:     2.523128, Lr: 0.000052, Tokens per sec:   2700
2023-03-14 09:31:31,236 - INFO - __main__ - Epoch  66, Step:  142800, Batch Loss:     3.709807, Lr: 0.000052, Tokens per sec:   2707
2023-03-14 09:31:51,117 - INFO - __main__ - Epoch  66, Step:  142900, Batch Loss:     4.226657, Lr: 0.000052, Tokens per sec:   2691
2023-03-14 09:32:11,115 - INFO - __main__ - Epoch  66, Step:  143000, Batch Loss:     4.870125, Lr: 0.000052, Tokens per sec:   2675
2023-03-14 09:32:31,128 - INFO - __main__ - Epoch  66, Step:  143100, Batch Loss:     4.119699, Lr: 0.000052, Tokens per sec:   2665
2023-03-14 09:32:50,713 - INFO - __main__ - Epoch  66, Step:  143200, Batch Loss:     4.480963, Lr: 0.000052, Tokens per sec:   2799
2023-03-14 09:33:10,860 - INFO - __main__ - Epoch  66, Step:  143300, Batch Loss:     3.219914, Lr: 0.000052, Tokens per sec:   2620
2023-03-14 09:33:30,770 - INFO - __main__ - Epoch  66, Step:  143400, Batch Loss:     3.253140, Lr: 0.000052, Tokens per sec:   2721
2023-03-14 09:33:50,655 - INFO - __main__ - Epoch  66, Step:  143500, Batch Loss:     4.307514, Lr: 0.000052, Tokens per sec:   2671
2023-03-14 09:34:10,527 - INFO - __main__ - Epoch  66, Step:  143600, Batch Loss:     3.770965, Lr: 0.000052, Tokens per sec:   2646
2023-03-14 09:34:30,119 - INFO - __main__ - Epoch  66, Step:  143700, Batch Loss:     3.516579, Lr: 0.000052, Tokens per sec:   2786
2023-03-14 09:34:49,834 - INFO - __main__ - Epoch  66, Step:  143800, Batch Loss:     3.155523, Lr: 0.000052, Tokens per sec:   2694
2023-03-14 09:34:52,698 - INFO - __main__ - Epoch  66: total training loss 7788.77
2023-03-14 09:38:47,885 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 09:38:47,886 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.00577148833363, rouge_l = 57.09833125741201, meteor = 0
2023-03-14 09:38:49,000 - INFO - __main__ - Delete test_dir3/130740.ckpt
2023-03-14 09:38:49,140 - INFO - __main__ - Example #0
2023-03-14 09:38:49,140 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 09:38:49,140 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 09:38:49,140 - INFO - __main__ - Example #1
2023-03-14 09:38:49,140 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 09:38:49,140 - INFO - __main__ - 	Hypothesis: call when the connection be start . < p > by default , this be a block pattern .
2023-03-14 09:38:49,140 - INFO - __main__ - Example #2
2023-03-14 09:38:49,140 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 09:38:49,140 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 09:38:49,149 - INFO - __main__ - Validation time = 232.29531812667847s.
2023-03-14 09:38:49,149 - INFO - __main__ - Epoch 67
2023-03-14 09:39:06,299 - INFO - __main__ - Epoch  67, Step:  143900, Batch Loss:     2.995485, Lr: 0.000052, Tokens per sec:   2710
2023-03-14 09:39:25,828 - INFO - __main__ - Epoch  67, Step:  144000, Batch Loss:     1.882582, Lr: 0.000052, Tokens per sec:   2808
2023-03-14 09:39:45,436 - INFO - __main__ - Epoch  67, Step:  144100, Batch Loss:     3.241718, Lr: 0.000052, Tokens per sec:   2750
2023-03-14 09:40:04,607 - INFO - __main__ - Epoch  67, Step:  144200, Batch Loss:     3.076069, Lr: 0.000052, Tokens per sec:   2774
2023-03-14 09:40:23,053 - INFO - __main__ - Epoch  67, Step:  144300, Batch Loss:     4.310730, Lr: 0.000052, Tokens per sec:   2918
2023-03-14 09:40:42,494 - INFO - __main__ - Epoch  67, Step:  144400, Batch Loss:     2.808994, Lr: 0.000052, Tokens per sec:   2729
2023-03-14 09:41:02,092 - INFO - __main__ - Epoch  67, Step:  144500, Batch Loss:     3.717826, Lr: 0.000052, Tokens per sec:   2708
2023-03-14 09:41:20,138 - INFO - __main__ - Epoch  67, Step:  144600, Batch Loss:     3.129906, Lr: 0.000052, Tokens per sec:   2963
2023-03-14 09:41:38,925 - INFO - __main__ - Epoch  67, Step:  144700, Batch Loss:     3.041252, Lr: 0.000052, Tokens per sec:   2900
2023-03-14 09:41:57,909 - INFO - __main__ - Epoch  67, Step:  144800, Batch Loss:     2.807261, Lr: 0.000052, Tokens per sec:   2836
2023-03-14 09:42:16,560 - INFO - __main__ - Epoch  67, Step:  144900, Batch Loss:     3.803090, Lr: 0.000052, Tokens per sec:   2891
2023-03-14 09:42:35,981 - INFO - __main__ - Epoch  67, Step:  145000, Batch Loss:     4.045029, Lr: 0.000052, Tokens per sec:   2805
2023-03-14 09:42:54,555 - INFO - __main__ - Epoch  67, Step:  145100, Batch Loss:     3.864795, Lr: 0.000052, Tokens per sec:   2885
2023-03-14 09:43:13,411 - INFO - __main__ - Epoch  67, Step:  145200, Batch Loss:     2.604814, Lr: 0.000052, Tokens per sec:   2813
2023-03-14 09:43:33,014 - INFO - __main__ - Epoch  67, Step:  145300, Batch Loss:     3.057621, Lr: 0.000052, Tokens per sec:   2731
2023-03-14 09:43:52,726 - INFO - __main__ - Epoch  67, Step:  145400, Batch Loss:     3.368284, Lr: 0.000052, Tokens per sec:   2753
2023-03-14 09:44:12,077 - INFO - __main__ - Epoch  67, Step:  145500, Batch Loss:     3.416276, Lr: 0.000052, Tokens per sec:   2786
2023-03-14 09:44:30,914 - INFO - __main__ - Epoch  67, Step:  145600, Batch Loss:     3.001827, Lr: 0.000052, Tokens per sec:   2880
2023-03-14 09:44:50,588 - INFO - __main__ - Epoch  67, Step:  145700, Batch Loss:     4.242597, Lr: 0.000052, Tokens per sec:   2742
2023-03-14 09:45:09,494 - INFO - __main__ - Epoch  67, Step:  145800, Batch Loss:     3.428454, Lr: 0.000052, Tokens per sec:   2842
2023-03-14 09:45:27,771 - INFO - __main__ - Epoch  67, Step:  145900, Batch Loss:     4.568470, Lr: 0.000052, Tokens per sec:   2967
2023-03-14 09:45:45,859 - INFO - __main__ - Epoch  67: total training loss 7636.87
2023-03-14 09:45:45,860 - INFO - __main__ - Epoch 68
2023-03-14 09:45:47,560 - INFO - __main__ - Epoch  68, Step:  146000, Batch Loss:     3.215271, Lr: 0.000051, Tokens per sec:   2432
2023-03-14 09:46:07,206 - INFO - __main__ - Epoch  68, Step:  146100, Batch Loss:     3.862430, Lr: 0.000051, Tokens per sec:   2744
2023-03-14 09:46:26,400 - INFO - __main__ - Epoch  68, Step:  146200, Batch Loss:     3.815866, Lr: 0.000051, Tokens per sec:   2820
2023-03-14 09:46:45,103 - INFO - __main__ - Epoch  68, Step:  146300, Batch Loss:     3.226983, Lr: 0.000051, Tokens per sec:   2868
2023-03-14 09:47:04,524 - INFO - __main__ - Epoch  68, Step:  146400, Batch Loss:     1.774107, Lr: 0.000051, Tokens per sec:   2745
2023-03-14 09:47:23,035 - INFO - __main__ - Epoch  68, Step:  146500, Batch Loss:     3.975048, Lr: 0.000051, Tokens per sec:   2908
2023-03-14 09:47:42,221 - INFO - __main__ - Epoch  68, Step:  146600, Batch Loss:     3.671454, Lr: 0.000051, Tokens per sec:   2822
2023-03-14 09:48:00,888 - INFO - __main__ - Epoch  68, Step:  146700, Batch Loss:     3.202359, Lr: 0.000051, Tokens per sec:   2917
2023-03-14 09:48:19,190 - INFO - __main__ - Epoch  68, Step:  146800, Batch Loss:     3.565428, Lr: 0.000051, Tokens per sec:   2912
2023-03-14 09:48:38,667 - INFO - __main__ - Epoch  68, Step:  146900, Batch Loss:     2.827832, Lr: 0.000051, Tokens per sec:   2768
2023-03-14 09:48:57,254 - INFO - __main__ - Epoch  68, Step:  147000, Batch Loss:     3.069499, Lr: 0.000051, Tokens per sec:   2879
2023-03-14 09:49:16,515 - INFO - __main__ - Epoch  68, Step:  147100, Batch Loss:     2.458182, Lr: 0.000051, Tokens per sec:   2790
2023-03-14 09:49:35,053 - INFO - __main__ - Epoch  68, Step:  147200, Batch Loss:     4.056100, Lr: 0.000051, Tokens per sec:   2930
2023-03-14 09:49:53,596 - INFO - __main__ - Epoch  68, Step:  147300, Batch Loss:     3.323300, Lr: 0.000051, Tokens per sec:   2915
2023-03-14 09:50:13,271 - INFO - __main__ - Epoch  68, Step:  147400, Batch Loss:     3.965974, Lr: 0.000051, Tokens per sec:   2668
2023-03-14 09:50:32,191 - INFO - __main__ - Epoch  68, Step:  147500, Batch Loss:     4.147558, Lr: 0.000051, Tokens per sec:   2878
2023-03-14 09:50:51,892 - INFO - __main__ - Epoch  68, Step:  147600, Batch Loss:     3.220377, Lr: 0.000051, Tokens per sec:   2784
2023-03-14 09:51:10,532 - INFO - __main__ - Epoch  68, Step:  147700, Batch Loss:     3.452710, Lr: 0.000051, Tokens per sec:   2868
2023-03-14 09:51:29,336 - INFO - __main__ - Epoch  68, Step:  147800, Batch Loss:     4.293505, Lr: 0.000051, Tokens per sec:   2869
2023-03-14 09:51:48,990 - INFO - __main__ - Epoch  68, Step:  147900, Batch Loss:     3.210072, Lr: 0.000051, Tokens per sec:   2705
2023-03-14 09:52:08,210 - INFO - __main__ - Epoch  68, Step:  148000, Batch Loss:     2.953292, Lr: 0.000051, Tokens per sec:   2803
2023-03-14 09:52:27,222 - INFO - __main__ - Epoch  68, Step:  148100, Batch Loss:     3.518800, Lr: 0.000051, Tokens per sec:   2832
2023-03-14 09:52:39,984 - INFO - __main__ - Epoch  68: total training loss 7478.97
2023-03-14 09:56:26,028 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 09:56:26,029 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.17161335645923, rouge_l = 57.27534820551353, meteor = 0
2023-03-14 09:56:27,142 - INFO - __main__ - Delete test_dir3/126382.ckpt
2023-03-14 09:56:27,279 - INFO - __main__ - Example #0
2023-03-14 09:56:27,279 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 09:56:27,279 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 09:56:27,279 - INFO - __main__ - Example #1
2023-03-14 09:56:27,279 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 09:56:27,279 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 09:56:27,279 - INFO - __main__ - Example #2
2023-03-14 09:56:27,279 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 09:56:27,279 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 09:56:27,286 - INFO - __main__ - Validation time = 223.19330644607544s.
2023-03-14 09:56:27,286 - INFO - __main__ - Epoch 69
2023-03-14 09:56:33,149 - INFO - __main__ - Epoch  69, Step:  148200, Batch Loss:     2.839417, Lr: 0.000050, Tokens per sec:   2587
2023-03-14 09:56:52,998 - INFO - __main__ - Epoch  69, Step:  148300, Batch Loss:     3.289091, Lr: 0.000050, Tokens per sec:   2696
2023-03-14 09:57:12,822 - INFO - __main__ - Epoch  69, Step:  148400, Batch Loss:     3.961674, Lr: 0.000050, Tokens per sec:   2705
2023-03-14 09:57:32,620 - INFO - __main__ - Epoch  69, Step:  148500, Batch Loss:     3.119435, Lr: 0.000050, Tokens per sec:   2700
2023-03-14 09:57:52,396 - INFO - __main__ - Epoch  69, Step:  148600, Batch Loss:     3.219100, Lr: 0.000050, Tokens per sec:   2741
2023-03-14 09:58:12,167 - INFO - __main__ - Epoch  69, Step:  148700, Batch Loss:     2.372365, Lr: 0.000050, Tokens per sec:   2696
2023-03-14 09:58:31,958 - INFO - __main__ - Epoch  69, Step:  148800, Batch Loss:     2.432156, Lr: 0.000050, Tokens per sec:   2724
2023-03-14 09:58:51,775 - INFO - __main__ - Epoch  69, Step:  148900, Batch Loss:     2.411691, Lr: 0.000050, Tokens per sec:   2681
2023-03-14 09:59:11,470 - INFO - __main__ - Epoch  69, Step:  149000, Batch Loss:     3.046951, Lr: 0.000050, Tokens per sec:   2775
2023-03-14 09:59:31,257 - INFO - __main__ - Epoch  69, Step:  149100, Batch Loss:     3.431760, Lr: 0.000050, Tokens per sec:   2717
2023-03-14 09:59:51,040 - INFO - __main__ - Epoch  69, Step:  149200, Batch Loss:     2.416687, Lr: 0.000050, Tokens per sec:   2740
2023-03-14 10:00:10,868 - INFO - __main__ - Epoch  69, Step:  149300, Batch Loss:     3.636695, Lr: 0.000050, Tokens per sec:   2750
2023-03-14 10:00:30,701 - INFO - __main__ - Epoch  69, Step:  149400, Batch Loss:     1.839654, Lr: 0.000050, Tokens per sec:   2708
2023-03-14 10:00:50,451 - INFO - __main__ - Epoch  69, Step:  149500, Batch Loss:     2.871342, Lr: 0.000050, Tokens per sec:   2730
2023-03-14 10:01:10,157 - INFO - __main__ - Epoch  69, Step:  149600, Batch Loss:     2.198345, Lr: 0.000050, Tokens per sec:   2750
2023-03-14 10:01:29,877 - INFO - __main__ - Epoch  69, Step:  149700, Batch Loss:     3.475832, Lr: 0.000050, Tokens per sec:   2757
2023-03-14 10:01:49,611 - INFO - __main__ - Epoch  69, Step:  149800, Batch Loss:     2.684346, Lr: 0.000050, Tokens per sec:   2742
2023-03-14 10:02:09,356 - INFO - __main__ - Epoch  69, Step:  149900, Batch Loss:     2.617957, Lr: 0.000050, Tokens per sec:   2722
2023-03-14 10:02:29,032 - INFO - __main__ - Epoch  69, Step:  150000, Batch Loss:     4.501429, Lr: 0.000050, Tokens per sec:   2740
2023-03-14 10:02:48,756 - INFO - __main__ - Epoch  69, Step:  150100, Batch Loss:     3.699983, Lr: 0.000050, Tokens per sec:   2751
2023-03-14 10:03:08,532 - INFO - __main__ - Epoch  69, Step:  150200, Batch Loss:     3.072667, Lr: 0.000050, Tokens per sec:   2704
2023-03-14 10:03:28,243 - INFO - __main__ - Epoch  69, Step:  150300, Batch Loss:     3.112562, Lr: 0.000050, Tokens per sec:   2674
2023-03-14 10:03:38,368 - INFO - __main__ - Epoch  69: total training loss 7307.00
2023-03-14 10:03:38,369 - INFO - __main__ - Epoch 70
2023-03-14 10:03:47,830 - INFO - __main__ - Epoch  70, Step:  150400, Batch Loss:     2.313017, Lr: 0.000050, Tokens per sec:   2756
2023-03-14 10:04:07,524 - INFO - __main__ - Epoch  70, Step:  150500, Batch Loss:     2.671734, Lr: 0.000050, Tokens per sec:   2750
2023-03-14 10:04:27,288 - INFO - __main__ - Epoch  70, Step:  150600, Batch Loss:     3.208100, Lr: 0.000050, Tokens per sec:   2751
2023-03-14 10:04:47,094 - INFO - __main__ - Epoch  70, Step:  150700, Batch Loss:     2.661078, Lr: 0.000050, Tokens per sec:   2708
2023-03-14 10:05:06,911 - INFO - __main__ - Epoch  70, Step:  150800, Batch Loss:     3.514840, Lr: 0.000050, Tokens per sec:   2669
2023-03-14 10:05:26,729 - INFO - __main__ - Epoch  70, Step:  150900, Batch Loss:     4.327147, Lr: 0.000050, Tokens per sec:   2746
2023-03-14 10:05:46,501 - INFO - __main__ - Epoch  70, Step:  151000, Batch Loss:     3.440583, Lr: 0.000050, Tokens per sec:   2744
2023-03-14 10:06:06,257 - INFO - __main__ - Epoch  70, Step:  151100, Batch Loss:     3.542388, Lr: 0.000050, Tokens per sec:   2772
2023-03-14 10:06:25,089 - INFO - __main__ - Epoch  70, Step:  151200, Batch Loss:     3.217388, Lr: 0.000050, Tokens per sec:   2779
2023-03-14 10:06:44,823 - INFO - __main__ - Epoch  70, Step:  151300, Batch Loss:     3.460781, Lr: 0.000050, Tokens per sec:   2716
2023-03-14 10:07:04,575 - INFO - __main__ - Epoch  70, Step:  151400, Batch Loss:     3.549995, Lr: 0.000050, Tokens per sec:   2703
2023-03-14 10:07:24,327 - INFO - __main__ - Epoch  70, Step:  151500, Batch Loss:     2.832724, Lr: 0.000050, Tokens per sec:   2747
2023-03-14 10:07:44,143 - INFO - __main__ - Epoch  70, Step:  151600, Batch Loss:     3.101959, Lr: 0.000050, Tokens per sec:   2725
2023-03-14 10:08:03,877 - INFO - __main__ - Epoch  70, Step:  151700, Batch Loss:     3.363883, Lr: 0.000050, Tokens per sec:   2719
2023-03-14 10:08:23,604 - INFO - __main__ - Epoch  70, Step:  151800, Batch Loss:     3.158638, Lr: 0.000050, Tokens per sec:   2663
2023-03-14 10:08:43,413 - INFO - __main__ - Epoch  70, Step:  151900, Batch Loss:     2.918111, Lr: 0.000050, Tokens per sec:   2744
2023-03-14 10:09:03,161 - INFO - __main__ - Epoch  70, Step:  152000, Batch Loss:     3.200183, Lr: 0.000050, Tokens per sec:   2709
2023-03-14 10:09:22,255 - INFO - __main__ - Epoch  70, Step:  152100, Batch Loss:     2.922580, Lr: 0.000050, Tokens per sec:   2801
2023-03-14 10:09:39,974 - INFO - __main__ - Epoch  70, Step:  152200, Batch Loss:     2.550142, Lr: 0.000050, Tokens per sec:   3046
2023-03-14 10:09:57,679 - INFO - __main__ - Epoch  70, Step:  152300, Batch Loss:     3.164727, Lr: 0.000050, Tokens per sec:   3066
2023-03-14 10:10:15,398 - INFO - __main__ - Epoch  70, Step:  152400, Batch Loss:     4.302436, Lr: 0.000050, Tokens per sec:   3092
2023-03-14 10:10:33,105 - INFO - __main__ - Epoch  70, Step:  152500, Batch Loss:     2.741525, Lr: 0.000050, Tokens per sec:   3065
2023-03-14 10:10:38,461 - INFO - __main__ - Epoch  70: total training loss 7186.23
2023-03-14 10:14:24,900 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 44.90037648663965, rouge_l = 57.06188623399826, meteor = 0
2023-03-14 10:14:24,901 - INFO - __main__ - Example #0
2023-03-14 10:14:24,901 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 10:14:24,901 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 10:14:24,901 - INFO - __main__ - Example #1
2023-03-14 10:14:24,901 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 10:14:24,901 - INFO - __main__ - 	Hypothesis: call when the bean start . < p > this be fire by the implementation .
2023-03-14 10:14:24,901 - INFO - __main__ - Example #2
2023-03-14 10:14:24,901 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 10:14:24,901 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 10:14:24,907 - INFO - __main__ - Validation time = 223.12231636047363s.
2023-03-14 10:14:24,907 - INFO - __main__ - Epoch 71
2023-03-14 10:14:38,785 - INFO - __main__ - Epoch  71, Step:  152600, Batch Loss:     2.454638, Lr: 0.000049, Tokens per sec:   2652
2023-03-14 10:14:57,741 - INFO - __main__ - Epoch  71, Step:  152700, Batch Loss:     3.523140, Lr: 0.000049, Tokens per sec:   2866
2023-03-14 10:15:17,044 - INFO - __main__ - Epoch  71, Step:  152800, Batch Loss:     3.545571, Lr: 0.000049, Tokens per sec:   2749
2023-03-14 10:15:36,229 - INFO - __main__ - Epoch  71, Step:  152900, Batch Loss:     2.581647, Lr: 0.000049, Tokens per sec:   2801
2023-03-14 10:15:55,342 - INFO - __main__ - Epoch  71, Step:  153000, Batch Loss:     2.443360, Lr: 0.000049, Tokens per sec:   2791
2023-03-14 10:16:14,733 - INFO - __main__ - Epoch  71, Step:  153100, Batch Loss:     3.540739, Lr: 0.000049, Tokens per sec:   2764
2023-03-14 10:16:34,309 - INFO - __main__ - Epoch  71, Step:  153200, Batch Loss:     2.667723, Lr: 0.000049, Tokens per sec:   2745
2023-03-14 10:16:53,818 - INFO - __main__ - Epoch  71, Step:  153300, Batch Loss:     2.564371, Lr: 0.000049, Tokens per sec:   2746
2023-03-14 10:17:13,157 - INFO - __main__ - Epoch  71, Step:  153400, Batch Loss:     2.340225, Lr: 0.000049, Tokens per sec:   2805
2023-03-14 10:17:32,501 - INFO - __main__ - Epoch  71, Step:  153500, Batch Loss:     3.866185, Lr: 0.000049, Tokens per sec:   2813
2023-03-14 10:17:51,945 - INFO - __main__ - Epoch  71, Step:  153600, Batch Loss:     3.803002, Lr: 0.000049, Tokens per sec:   2796
2023-03-14 10:18:11,315 - INFO - __main__ - Epoch  71, Step:  153700, Batch Loss:     4.404896, Lr: 0.000049, Tokens per sec:   2790
2023-03-14 10:18:30,643 - INFO - __main__ - Epoch  71, Step:  153800, Batch Loss:     2.499177, Lr: 0.000049, Tokens per sec:   2769
2023-03-14 10:18:50,052 - INFO - __main__ - Epoch  71, Step:  153900, Batch Loss:     3.308516, Lr: 0.000049, Tokens per sec:   2761
2023-03-14 10:19:09,411 - INFO - __main__ - Epoch  71, Step:  154000, Batch Loss:     3.422657, Lr: 0.000049, Tokens per sec:   2757
2023-03-14 10:19:28,727 - INFO - __main__ - Epoch  71, Step:  154100, Batch Loss:     4.081617, Lr: 0.000049, Tokens per sec:   2783
2023-03-14 10:19:47,894 - INFO - __main__ - Epoch  71, Step:  154200, Batch Loss:     2.544678, Lr: 0.000049, Tokens per sec:   2780
2023-03-14 10:20:07,027 - INFO - __main__ - Epoch  71, Step:  154300, Batch Loss:     3.288355, Lr: 0.000049, Tokens per sec:   2830
2023-03-14 10:20:26,789 - INFO - __main__ - Epoch  71, Step:  154400, Batch Loss:     3.664473, Lr: 0.000049, Tokens per sec:   2742
2023-03-14 10:20:45,953 - INFO - __main__ - Epoch  71, Step:  154500, Batch Loss:     3.415914, Lr: 0.000049, Tokens per sec:   2837
2023-03-14 10:21:05,022 - INFO - __main__ - Epoch  71, Step:  154600, Batch Loss:     2.041991, Lr: 0.000049, Tokens per sec:   2847
2023-03-14 10:21:24,504 - INFO - __main__ - Epoch  71, Step:  154700, Batch Loss:     3.790263, Lr: 0.000049, Tokens per sec:   2785
2023-03-14 10:21:26,286 - INFO - __main__ - Epoch  71: total training loss 7032.93
2023-03-14 10:21:26,287 - INFO - __main__ - Epoch 72
2023-03-14 10:21:44,227 - INFO - __main__ - Epoch  72, Step:  154800, Batch Loss:     1.823878, Lr: 0.000049, Tokens per sec:   2727
2023-03-14 10:22:03,345 - INFO - __main__ - Epoch  72, Step:  154900, Batch Loss:     2.280502, Lr: 0.000049, Tokens per sec:   2826
2023-03-14 10:22:22,759 - INFO - __main__ - Epoch  72, Step:  155000, Batch Loss:     2.567295, Lr: 0.000049, Tokens per sec:   2756
2023-03-14 10:22:42,071 - INFO - __main__ - Epoch  72, Step:  155100, Batch Loss:     1.684161, Lr: 0.000049, Tokens per sec:   2779
2023-03-14 10:23:01,645 - INFO - __main__ - Epoch  72, Step:  155200, Batch Loss:     2.602736, Lr: 0.000049, Tokens per sec:   2775
2023-03-14 10:23:20,917 - INFO - __main__ - Epoch  72, Step:  155300, Batch Loss:     3.173281, Lr: 0.000049, Tokens per sec:   2836
2023-03-14 10:23:40,246 - INFO - __main__ - Epoch  72, Step:  155400, Batch Loss:     3.604347, Lr: 0.000049, Tokens per sec:   2786
2023-03-14 10:23:59,639 - INFO - __main__ - Epoch  72, Step:  155500, Batch Loss:     2.478952, Lr: 0.000049, Tokens per sec:   2775
2023-03-14 10:24:19,094 - INFO - __main__ - Epoch  72, Step:  155600, Batch Loss:     2.386420, Lr: 0.000049, Tokens per sec:   2813
2023-03-14 10:24:38,342 - INFO - __main__ - Epoch  72, Step:  155700, Batch Loss:     3.440613, Lr: 0.000049, Tokens per sec:   2796
2023-03-14 10:24:57,973 - INFO - __main__ - Epoch  72, Step:  155800, Batch Loss:     3.989774, Lr: 0.000049, Tokens per sec:   2746
2023-03-14 10:25:17,171 - INFO - __main__ - Epoch  72, Step:  155900, Batch Loss:     2.197626, Lr: 0.000049, Tokens per sec:   2815
2023-03-14 10:25:36,295 - INFO - __main__ - Epoch  72, Step:  156000, Batch Loss:     2.696555, Lr: 0.000049, Tokens per sec:   2794
2023-03-14 10:25:55,946 - INFO - __main__ - Epoch  72, Step:  156100, Batch Loss:     2.193313, Lr: 0.000049, Tokens per sec:   2705
2023-03-14 10:26:15,503 - INFO - __main__ - Epoch  72, Step:  156200, Batch Loss:     2.748981, Lr: 0.000049, Tokens per sec:   2809
2023-03-14 10:26:34,894 - INFO - __main__ - Epoch  72, Step:  156300, Batch Loss:     3.111137, Lr: 0.000049, Tokens per sec:   2771
2023-03-14 10:26:54,779 - INFO - __main__ - Epoch  72, Step:  156400, Batch Loss:     3.685124, Lr: 0.000049, Tokens per sec:   2707
2023-03-14 10:27:14,086 - INFO - __main__ - Epoch  72, Step:  156500, Batch Loss:     3.838645, Lr: 0.000049, Tokens per sec:   2775
2023-03-14 10:27:33,221 - INFO - __main__ - Epoch  72, Step:  156600, Batch Loss:     3.542885, Lr: 0.000049, Tokens per sec:   2784
2023-03-14 10:27:52,697 - INFO - __main__ - Epoch  72, Step:  156700, Batch Loss:     2.100139, Lr: 0.000049, Tokens per sec:   2735
2023-03-14 10:28:11,899 - INFO - __main__ - Epoch  72, Step:  156800, Batch Loss:     3.399373, Lr: 0.000049, Tokens per sec:   2817
2023-03-14 10:28:29,131 - INFO - __main__ - Epoch  72: total training loss 6831.53
2023-03-14 10:33:51,680 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 10:33:51,680 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.19242572091032, rouge_l = 57.23217612339, meteor = 0
2023-03-14 10:33:52,764 - INFO - __main__ - Delete test_dir3/139456.ckpt
2023-03-14 10:33:52,909 - INFO - __main__ - Example #0
2023-03-14 10:33:52,909 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 10:33:52,909 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 10:33:52,909 - INFO - __main__ - Example #1
2023-03-14 10:33:52,909 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 10:33:52,909 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 10:33:52,909 - INFO - __main__ - Example #2
2023-03-14 10:33:52,909 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 10:33:52,909 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 10:33:52,916 - INFO - __main__ - Validation time = 319.5039441585541s.
2023-03-14 10:33:52,916 - INFO - __main__ - Epoch 73
2023-03-14 10:33:55,696 - INFO - __main__ - Epoch  73, Step:  156900, Batch Loss:     3.002617, Lr: 0.000048, Tokens per sec:   2318
2023-03-14 10:34:15,680 - INFO - __main__ - Epoch  73, Step:  157000, Batch Loss:     2.578772, Lr: 0.000048, Tokens per sec:   2700
2023-03-14 10:34:35,906 - INFO - __main__ - Epoch  73, Step:  157100, Batch Loss:     3.689053, Lr: 0.000048, Tokens per sec:   2644
2023-03-14 10:34:56,130 - INFO - __main__ - Epoch  73, Step:  157200, Batch Loss:     2.766053, Lr: 0.000048, Tokens per sec:   2712
2023-03-14 10:35:16,275 - INFO - __main__ - Epoch  73, Step:  157300, Batch Loss:     2.887131, Lr: 0.000048, Tokens per sec:   2676
2023-03-14 10:35:36,323 - INFO - __main__ - Epoch  73, Step:  157400, Batch Loss:     2.115351, Lr: 0.000048, Tokens per sec:   2690
2023-03-14 10:35:56,520 - INFO - __main__ - Epoch  73, Step:  157500, Batch Loss:     2.833498, Lr: 0.000048, Tokens per sec:   2648
2023-03-14 10:36:16,120 - INFO - __main__ - Epoch  73, Step:  157600, Batch Loss:     3.264336, Lr: 0.000048, Tokens per sec:   2745
2023-03-14 10:36:35,834 - INFO - __main__ - Epoch  73, Step:  157700, Batch Loss:     2.586436, Lr: 0.000048, Tokens per sec:   2722
2023-03-14 10:36:56,139 - INFO - __main__ - Epoch  73, Step:  157800, Batch Loss:     3.170017, Lr: 0.000048, Tokens per sec:   2623
2023-03-14 10:37:16,734 - INFO - __main__ - Epoch  73, Step:  157900, Batch Loss:     2.185459, Lr: 0.000048, Tokens per sec:   2615
2023-03-14 10:37:36,953 - INFO - __main__ - Epoch  73, Step:  158000, Batch Loss:     2.733345, Lr: 0.000048, Tokens per sec:   2679
2023-03-14 10:37:57,350 - INFO - __main__ - Epoch  73, Step:  158100, Batch Loss:     3.975505, Lr: 0.000048, Tokens per sec:   2690
2023-03-14 10:38:18,272 - INFO - __main__ - Epoch  73, Step:  158200, Batch Loss:     2.636036, Lr: 0.000048, Tokens per sec:   2582
2023-03-14 10:38:38,673 - INFO - __main__ - Epoch  73, Step:  158300, Batch Loss:     3.209548, Lr: 0.000048, Tokens per sec:   2633
2023-03-14 10:38:57,955 - INFO - __main__ - Epoch  73, Step:  158400, Batch Loss:     2.763341, Lr: 0.000048, Tokens per sec:   2806
2023-03-14 10:39:18,503 - INFO - __main__ - Epoch  73, Step:  158500, Batch Loss:     2.324227, Lr: 0.000048, Tokens per sec:   2611
2023-03-14 10:39:38,886 - INFO - __main__ - Epoch  73, Step:  158600, Batch Loss:     3.980238, Lr: 0.000048, Tokens per sec:   2624
2023-03-14 10:39:59,533 - INFO - __main__ - Epoch  73, Step:  158700, Batch Loss:     1.688570, Lr: 0.000048, Tokens per sec:   2576
2023-03-14 10:40:19,897 - INFO - __main__ - Epoch  73, Step:  158800, Batch Loss:     3.987065, Lr: 0.000048, Tokens per sec:   2669
2023-03-14 10:40:39,959 - INFO - __main__ - Epoch  73, Step:  158900, Batch Loss:     3.611356, Lr: 0.000048, Tokens per sec:   2652
2023-03-14 10:41:00,519 - INFO - __main__ - Epoch  73, Step:  159000, Batch Loss:     3.614443, Lr: 0.000048, Tokens per sec:   2615
2023-03-14 10:41:14,267 - INFO - __main__ - Epoch  73: total training loss 6712.00
2023-03-14 10:41:14,268 - INFO - __main__ - Epoch 74
2023-03-14 10:41:21,172 - INFO - __main__ - Epoch  74, Step:  159100, Batch Loss:     2.909832, Lr: 0.000048, Tokens per sec:   2671
2023-03-14 10:41:41,558 - INFO - __main__ - Epoch  74, Step:  159200, Batch Loss:     3.062863, Lr: 0.000048, Tokens per sec:   2659
2023-03-14 10:42:02,272 - INFO - __main__ - Epoch  74, Step:  159300, Batch Loss:     2.896361, Lr: 0.000048, Tokens per sec:   2601
2023-03-14 10:42:22,846 - INFO - __main__ - Epoch  74, Step:  159400, Batch Loss:     2.732046, Lr: 0.000048, Tokens per sec:   2655
2023-03-14 10:42:43,132 - INFO - __main__ - Epoch  74, Step:  159500, Batch Loss:     2.783918, Lr: 0.000048, Tokens per sec:   2672
2023-03-14 10:43:03,145 - INFO - __main__ - Epoch  74, Step:  159600, Batch Loss:     2.840151, Lr: 0.000048, Tokens per sec:   2708
2023-03-14 10:43:23,663 - INFO - __main__ - Epoch  74, Step:  159700, Batch Loss:     3.104204, Lr: 0.000048, Tokens per sec:   2602
2023-03-14 10:43:43,830 - INFO - __main__ - Epoch  74, Step:  159800, Batch Loss:     2.854682, Lr: 0.000048, Tokens per sec:   2698
2023-03-14 10:44:04,389 - INFO - __main__ - Epoch  74, Step:  159900, Batch Loss:     3.755388, Lr: 0.000048, Tokens per sec:   2636
2023-03-14 10:44:24,520 - INFO - __main__ - Epoch  74, Step:  160000, Batch Loss:     3.807554, Lr: 0.000048, Tokens per sec:   2672
2023-03-14 10:44:44,475 - INFO - __main__ - Epoch  74, Step:  160100, Batch Loss:     3.610077, Lr: 0.000048, Tokens per sec:   2699
2023-03-14 10:45:04,998 - INFO - __main__ - Epoch  74, Step:  160200, Batch Loss:     2.485594, Lr: 0.000048, Tokens per sec:   2601
2023-03-14 10:45:24,847 - INFO - __main__ - Epoch  74, Step:  160300, Batch Loss:     2.530376, Lr: 0.000048, Tokens per sec:   2729
2023-03-14 10:45:44,615 - INFO - __main__ - Epoch  74, Step:  160400, Batch Loss:     3.991450, Lr: 0.000048, Tokens per sec:   2710
2023-03-14 10:46:04,499 - INFO - __main__ - Epoch  74, Step:  160500, Batch Loss:     1.832111, Lr: 0.000048, Tokens per sec:   2645
2023-03-14 10:46:24,186 - INFO - __main__ - Epoch  74, Step:  160600, Batch Loss:     3.370062, Lr: 0.000048, Tokens per sec:   2782
2023-03-14 10:46:44,738 - INFO - __main__ - Epoch  74, Step:  160700, Batch Loss:     1.821617, Lr: 0.000048, Tokens per sec:   2584
2023-03-14 10:47:05,941 - INFO - __main__ - Epoch  74, Step:  160800, Batch Loss:     2.562257, Lr: 0.000048, Tokens per sec:   2533
2023-03-14 10:47:26,706 - INFO - __main__ - Epoch  74, Step:  160900, Batch Loss:     3.554719, Lr: 0.000048, Tokens per sec:   2599
2023-03-14 10:47:46,610 - INFO - __main__ - Epoch  74, Step:  161000, Batch Loss:     2.675730, Lr: 0.000048, Tokens per sec:   2650
2023-03-14 10:48:06,418 - INFO - __main__ - Epoch  74, Step:  161100, Batch Loss:     3.212740, Lr: 0.000048, Tokens per sec:   2700
2023-03-14 10:48:27,089 - INFO - __main__ - Epoch  74, Step:  161200, Batch Loss:     1.466354, Lr: 0.000048, Tokens per sec:   2587
2023-03-14 10:48:36,775 - INFO - __main__ - Epoch  74: total training loss 6595.54
2023-03-14 10:53:50,701 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.17042544363333, rouge_l = 57.253900675966506, meteor = 0
2023-03-14 10:53:51,810 - INFO - __main__ - Delete test_dir3/143814.ckpt
2023-03-14 10:53:51,948 - INFO - __main__ - Example #0
2023-03-14 10:53:51,948 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 10:53:51,948 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 10:53:51,948 - INFO - __main__ - Example #1
2023-03-14 10:53:51,948 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 10:53:51,949 - INFO - __main__ - 	Hypothesis: call when the connection be start . < p > by default , not prove .
2023-03-14 10:53:51,949 - INFO - __main__ - Example #2
2023-03-14 10:53:51,949 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 10:53:51,949 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this jtext
2023-03-14 10:53:51,955 - INFO - __main__ - Validation time = 310.7612347602844s.
2023-03-14 10:53:51,955 - INFO - __main__ - Epoch 75
2023-03-14 10:54:03,143 - INFO - __main__ - Epoch  75, Step:  161300, Batch Loss:     4.128463, Lr: 0.000048, Tokens per sec:   2565
2023-03-14 10:54:23,051 - INFO - __main__ - Epoch  75, Step:  161400, Batch Loss:     2.221356, Lr: 0.000048, Tokens per sec:   2703
2023-03-14 10:54:42,613 - INFO - __main__ - Epoch  75, Step:  161500, Batch Loss:     2.933029, Lr: 0.000048, Tokens per sec:   2718
2023-03-14 10:55:02,759 - INFO - __main__ - Epoch  75, Step:  161600, Batch Loss:     3.442766, Lr: 0.000048, Tokens per sec:   2660
2023-03-14 10:55:23,195 - INFO - __main__ - Epoch  75, Step:  161700, Batch Loss:     2.774135, Lr: 0.000048, Tokens per sec:   2677
2023-03-14 10:55:43,039 - INFO - __main__ - Epoch  75, Step:  161800, Batch Loss:     4.119149, Lr: 0.000048, Tokens per sec:   2709
2023-03-14 10:56:03,173 - INFO - __main__ - Epoch  75, Step:  161900, Batch Loss:     2.329572, Lr: 0.000048, Tokens per sec:   2654
2023-03-14 10:56:23,187 - INFO - __main__ - Epoch  75, Step:  162000, Batch Loss:     3.404739, Lr: 0.000048, Tokens per sec:   2677
2023-03-14 10:56:43,699 - INFO - __main__ - Epoch  75, Step:  162100, Batch Loss:     5.026495, Lr: 0.000048, Tokens per sec:   2656
2023-03-14 10:57:04,388 - INFO - __main__ - Epoch  75, Step:  162200, Batch Loss:     4.143116, Lr: 0.000048, Tokens per sec:   2617
2023-03-14 10:57:24,502 - INFO - __main__ - Epoch  75, Step:  162300, Batch Loss:     3.730727, Lr: 0.000048, Tokens per sec:   2715
2023-03-14 10:57:44,580 - INFO - __main__ - Epoch  75, Step:  162400, Batch Loss:     3.362995, Lr: 0.000048, Tokens per sec:   2690
2023-03-14 10:58:04,884 - INFO - __main__ - Epoch  75, Step:  162500, Batch Loss:     3.630621, Lr: 0.000048, Tokens per sec:   2663
2023-03-14 10:58:24,790 - INFO - __main__ - Epoch  75, Step:  162600, Batch Loss:     3.820355, Lr: 0.000048, Tokens per sec:   2749
2023-03-14 10:58:45,139 - INFO - __main__ - Epoch  75, Step:  162700, Batch Loss:     2.841379, Lr: 0.000048, Tokens per sec:   2654
2023-03-14 10:59:05,610 - INFO - __main__ - Epoch  75, Step:  162800, Batch Loss:     3.704733, Lr: 0.000048, Tokens per sec:   2662
2023-03-14 10:59:25,997 - INFO - __main__ - Epoch  75, Step:  162900, Batch Loss:     2.946797, Lr: 0.000048, Tokens per sec:   2614
2023-03-14 10:59:46,393 - INFO - __main__ - Epoch  75, Step:  163000, Batch Loss:     3.054606, Lr: 0.000048, Tokens per sec:   2631
2023-03-14 11:00:06,923 - INFO - __main__ - Epoch  75, Step:  163100, Batch Loss:     4.227953, Lr: 0.000048, Tokens per sec:   2622
2023-03-14 11:00:26,992 - INFO - __main__ - Epoch  75, Step:  163200, Batch Loss:     3.167469, Lr: 0.000048, Tokens per sec:   2637
2023-03-14 11:00:46,512 - INFO - __main__ - Epoch  75, Step:  163300, Batch Loss:     3.227446, Lr: 0.000048, Tokens per sec:   2739
2023-03-14 11:01:06,611 - INFO - __main__ - Epoch  75, Step:  163400, Batch Loss:     3.155447, Lr: 0.000048, Tokens per sec:   2652
2023-03-14 11:01:11,469 - INFO - __main__ - Epoch  75: total training loss 6501.31
2023-03-14 11:01:11,470 - INFO - __main__ - Epoch 76
2023-03-14 11:01:26,757 - INFO - __main__ - Epoch  76, Step:  163500, Batch Loss:     3.424232, Lr: 0.000047, Tokens per sec:   2671
2023-03-14 11:01:46,499 - INFO - __main__ - Epoch  76, Step:  163600, Batch Loss:     2.787094, Lr: 0.000047, Tokens per sec:   2741
2023-03-14 11:02:06,253 - INFO - __main__ - Epoch  76, Step:  163700, Batch Loss:     3.065905, Lr: 0.000047, Tokens per sec:   2713
2023-03-14 11:02:26,477 - INFO - __main__ - Epoch  76, Step:  163800, Batch Loss:     2.444780, Lr: 0.000047, Tokens per sec:   2645
2023-03-14 11:02:46,935 - INFO - __main__ - Epoch  76, Step:  163900, Batch Loss:     2.963125, Lr: 0.000047, Tokens per sec:   2627
2023-03-14 11:03:07,553 - INFO - __main__ - Epoch  76, Step:  164000, Batch Loss:     2.443792, Lr: 0.000047, Tokens per sec:   2567
2023-03-14 11:03:27,523 - INFO - __main__ - Epoch  76, Step:  164100, Batch Loss:     2.293348, Lr: 0.000047, Tokens per sec:   2608
2023-03-14 11:03:48,600 - INFO - __main__ - Epoch  76, Step:  164200, Batch Loss:     2.971785, Lr: 0.000047, Tokens per sec:   2522
2023-03-14 11:04:08,105 - INFO - __main__ - Epoch  76, Step:  164300, Batch Loss:     2.447622, Lr: 0.000047, Tokens per sec:   2794
2023-03-14 11:04:28,179 - INFO - __main__ - Epoch  76, Step:  164400, Batch Loss:     2.790045, Lr: 0.000047, Tokens per sec:   2687
2023-03-14 11:04:48,582 - INFO - __main__ - Epoch  76, Step:  164500, Batch Loss:     2.741508, Lr: 0.000047, Tokens per sec:   2631
2023-03-14 11:05:08,456 - INFO - __main__ - Epoch  76, Step:  164600, Batch Loss:     3.312144, Lr: 0.000047, Tokens per sec:   2759
2023-03-14 11:05:28,673 - INFO - __main__ - Epoch  76, Step:  164700, Batch Loss:     3.235566, Lr: 0.000047, Tokens per sec:   2649
2023-03-14 11:05:48,770 - INFO - __main__ - Epoch  76, Step:  164800, Batch Loss:     3.485488, Lr: 0.000047, Tokens per sec:   2656
2023-03-14 11:06:08,973 - INFO - __main__ - Epoch  76, Step:  164900, Batch Loss:     3.452213, Lr: 0.000047, Tokens per sec:   2680
2023-03-14 11:06:29,199 - INFO - __main__ - Epoch  76, Step:  165000, Batch Loss:     4.079810, Lr: 0.000047, Tokens per sec:   2648
2023-03-14 11:06:50,080 - INFO - __main__ - Epoch  76, Step:  165100, Batch Loss:     2.616035, Lr: 0.000047, Tokens per sec:   2626
2023-03-14 11:07:10,489 - INFO - __main__ - Epoch  76, Step:  165200, Batch Loss:     2.395304, Lr: 0.000047, Tokens per sec:   2667
2023-03-14 11:07:30,508 - INFO - __main__ - Epoch  76, Step:  165300, Batch Loss:     2.578453, Lr: 0.000047, Tokens per sec:   2748
2023-03-14 11:07:51,099 - INFO - __main__ - Epoch  76, Step:  165400, Batch Loss:     3.267976, Lr: 0.000047, Tokens per sec:   2604
2023-03-14 11:08:11,766 - INFO - __main__ - Epoch  76, Step:  165500, Batch Loss:     3.569163, Lr: 0.000047, Tokens per sec:   2574
2023-03-14 11:08:31,470 - INFO - __main__ - Epoch  76, Step:  165600, Batch Loss:     2.311112, Lr: 0.000047, Tokens per sec:   2753
2023-03-14 11:08:32,243 - INFO - __main__ - Epoch  76: total training loss 6368.08
2023-03-14 11:13:52,682 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 11:13:52,683 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.43379104686597, rouge_l = 57.56914010466868, meteor = 0
2023-03-14 11:13:54,136 - INFO - __main__ - Delete test_dir3/161246.ckpt
2023-03-14 11:13:54,261 - INFO - __main__ - Delete /home/tongye2/ytnmt/src_integration/test_dir3/161246.ckpt
2023-03-14 11:13:54,261 - WARNING - __main__ - Want to delete old checkpoint /home/tongye2/ytnmt/src_integration/test_dir3/161246.ckptbut file does not exist. ([Errno 2] No such file or directory: '/home/tongye2/ytnmt/src_integration/test_dir3/161246.ckpt')
2023-03-14 11:13:54,261 - INFO - __main__ - Example #0
2023-03-14 11:13:54,262 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 11:13:54,262 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 11:13:54,262 - INFO - __main__ - Example #1
2023-03-14 11:13:54,262 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 11:13:54,262 - INFO - __main__ - 	Hypothesis: call when the connection be start . < p > by default , not prove - know , etc .
2023-03-14 11:13:54,262 - INFO - __main__ - Example #2
2023-03-14 11:13:54,262 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 11:13:54,262 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 11:13:54,269 - INFO - __main__ - Validation time = 317.48920702934265s.
2023-03-14 11:13:54,270 - INFO - __main__ - Epoch 77
2023-03-14 11:14:13,485 - INFO - __main__ - Epoch  77, Step:  165700, Batch Loss:     2.995310, Lr: 0.000047, Tokens per sec:   2726
2023-03-14 11:14:33,225 - INFO - __main__ - Epoch  77, Step:  165800, Batch Loss:     2.537497, Lr: 0.000047, Tokens per sec:   2775
2023-03-14 11:14:52,970 - INFO - __main__ - Epoch  77, Step:  165900, Batch Loss:     3.079347, Lr: 0.000047, Tokens per sec:   2700
2023-03-14 11:15:12,438 - INFO - __main__ - Epoch  77, Step:  166000, Batch Loss:     3.044221, Lr: 0.000047, Tokens per sec:   2755
2023-03-14 11:15:32,124 - INFO - __main__ - Epoch  77, Step:  166100, Batch Loss:     2.226066, Lr: 0.000047, Tokens per sec:   2717
2023-03-14 11:15:51,573 - INFO - __main__ - Epoch  77, Step:  166200, Batch Loss:     3.073060, Lr: 0.000047, Tokens per sec:   2762
2023-03-14 11:16:11,144 - INFO - __main__ - Epoch  77, Step:  166300, Batch Loss:     3.483526, Lr: 0.000047, Tokens per sec:   2781
2023-03-14 11:16:30,913 - INFO - __main__ - Epoch  77, Step:  166400, Batch Loss:     2.626514, Lr: 0.000047, Tokens per sec:   2764
2023-03-14 11:16:50,720 - INFO - __main__ - Epoch  77, Step:  166500, Batch Loss:     4.785433, Lr: 0.000047, Tokens per sec:   2723
2023-03-14 11:17:10,719 - INFO - __main__ - Epoch  77, Step:  166600, Batch Loss:     3.817863, Lr: 0.000047, Tokens per sec:   2661
2023-03-14 11:17:30,417 - INFO - __main__ - Epoch  77, Step:  166700, Batch Loss:     3.140385, Lr: 0.000047, Tokens per sec:   2630
2023-03-14 11:17:50,067 - INFO - __main__ - Epoch  77, Step:  166800, Batch Loss:     3.618734, Lr: 0.000047, Tokens per sec:   2714
2023-03-14 11:18:10,030 - INFO - __main__ - Epoch  77, Step:  166900, Batch Loss:     2.308134, Lr: 0.000047, Tokens per sec:   2713
2023-03-14 11:18:29,754 - INFO - __main__ - Epoch  77, Step:  167000, Batch Loss:     2.584041, Lr: 0.000047, Tokens per sec:   2721
2023-03-14 11:18:49,088 - INFO - __main__ - Epoch  77, Step:  167100, Batch Loss:     3.035190, Lr: 0.000047, Tokens per sec:   2795
2023-03-14 11:19:08,751 - INFO - __main__ - Epoch  77, Step:  167200, Batch Loss:     3.700044, Lr: 0.000047, Tokens per sec:   2777
2023-03-14 11:19:28,245 - INFO - __main__ - Epoch  77, Step:  167300, Batch Loss:     2.503719, Lr: 0.000047, Tokens per sec:   2710
2023-03-14 11:19:47,626 - INFO - __main__ - Epoch  77, Step:  167400, Batch Loss:     2.818053, Lr: 0.000047, Tokens per sec:   2775
2023-03-14 11:20:07,693 - INFO - __main__ - Epoch  77, Step:  167500, Batch Loss:     2.605073, Lr: 0.000047, Tokens per sec:   2669
2023-03-14 11:20:27,585 - INFO - __main__ - Epoch  77, Step:  167600, Batch Loss:     3.387557, Lr: 0.000047, Tokens per sec:   2736
2023-03-14 11:20:47,405 - INFO - __main__ - Epoch  77, Step:  167700, Batch Loss:     1.955696, Lr: 0.000047, Tokens per sec:   2752
2023-03-14 11:21:04,154 - INFO - __main__ - Epoch  77: total training loss 6243.21
2023-03-14 11:21:04,154 - INFO - __main__ - Epoch 78
2023-03-14 11:21:07,869 - INFO - __main__ - Epoch  78, Step:  167800, Batch Loss:     3.791477, Lr: 0.000046, Tokens per sec:   2517
2023-03-14 11:21:27,610 - INFO - __main__ - Epoch  78, Step:  167900, Batch Loss:     1.684027, Lr: 0.000046, Tokens per sec:   2739
2023-03-14 11:21:47,163 - INFO - __main__ - Epoch  78, Step:  168000, Batch Loss:     2.214777, Lr: 0.000046, Tokens per sec:   2737
2023-03-14 11:22:06,571 - INFO - __main__ - Epoch  78, Step:  168100, Batch Loss:     2.672533, Lr: 0.000046, Tokens per sec:   2774
2023-03-14 11:22:26,488 - INFO - __main__ - Epoch  78, Step:  168200, Batch Loss:     3.592237, Lr: 0.000046, Tokens per sec:   2708
2023-03-14 11:22:46,189 - INFO - __main__ - Epoch  78, Step:  168300, Batch Loss:     3.613399, Lr: 0.000046, Tokens per sec:   2722
2023-03-14 11:23:05,897 - INFO - __main__ - Epoch  78, Step:  168400, Batch Loss:     3.628372, Lr: 0.000046, Tokens per sec:   2758
2023-03-14 11:23:25,768 - INFO - __main__ - Epoch  78, Step:  168500, Batch Loss:     2.973026, Lr: 0.000046, Tokens per sec:   2735
2023-03-14 11:23:45,241 - INFO - __main__ - Epoch  78, Step:  168600, Batch Loss:     3.251533, Lr: 0.000046, Tokens per sec:   2702
2023-03-14 11:24:04,906 - INFO - __main__ - Epoch  78, Step:  168700, Batch Loss:     1.767028, Lr: 0.000046, Tokens per sec:   2786
2023-03-14 11:24:24,588 - INFO - __main__ - Epoch  78, Step:  168800, Batch Loss:     2.989378, Lr: 0.000046, Tokens per sec:   2756
2023-03-14 11:24:44,065 - INFO - __main__ - Epoch  78, Step:  168900, Batch Loss:     3.088518, Lr: 0.000046, Tokens per sec:   2789
2023-03-14 11:25:04,248 - INFO - __main__ - Epoch  78, Step:  169000, Batch Loss:     3.297169, Lr: 0.000046, Tokens per sec:   2697
2023-03-14 11:25:24,161 - INFO - __main__ - Epoch  78, Step:  169100, Batch Loss:     3.177839, Lr: 0.000046, Tokens per sec:   2669
2023-03-14 11:25:43,882 - INFO - __main__ - Epoch  78, Step:  169200, Batch Loss:     3.150541, Lr: 0.000046, Tokens per sec:   2742
2023-03-14 11:26:03,613 - INFO - __main__ - Epoch  78, Step:  169300, Batch Loss:     1.756974, Lr: 0.000046, Tokens per sec:   2652
2023-03-14 11:26:23,482 - INFO - __main__ - Epoch  78, Step:  169400, Batch Loss:     3.104334, Lr: 0.000046, Tokens per sec:   2730
2023-03-14 11:26:44,061 - INFO - __main__ - Epoch  78, Step:  169500, Batch Loss:     2.834972, Lr: 0.000046, Tokens per sec:   2616
2023-03-14 11:27:04,535 - INFO - __main__ - Epoch  78, Step:  169600, Batch Loss:     2.746634, Lr: 0.000046, Tokens per sec:   2615
2023-03-14 11:27:25,831 - INFO - __main__ - Epoch  78, Step:  169700, Batch Loss:     2.701256, Lr: 0.000046, Tokens per sec:   2537
2023-03-14 11:27:45,579 - INFO - __main__ - Epoch  78, Step:  169800, Batch Loss:     3.630478, Lr: 0.000046, Tokens per sec:   2722
2023-03-14 11:28:05,709 - INFO - __main__ - Epoch  78, Step:  169900, Batch Loss:     3.309060, Lr: 0.000046, Tokens per sec:   2675
2023-03-14 11:28:18,074 - INFO - __main__ - Epoch  78: total training loss 6134.04
2023-03-14 11:33:33,853 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.29913359045819, rouge_l = 57.4006868782597, meteor = 0
2023-03-14 11:33:35,170 - INFO - __main__ - Delete test_dir3/148172.ckpt
2023-03-14 11:33:35,311 - INFO - __main__ - Example #0
2023-03-14 11:33:35,312 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 11:33:35,312 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 11:33:35,312 - INFO - __main__ - Example #1
2023-03-14 11:33:35,312 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 11:33:35,312 - INFO - __main__ - 	Hypothesis: call when the connection be start . < p > by default , not prove .
2023-03-14 11:33:35,312 - INFO - __main__ - Example #2
2023-03-14 11:33:35,312 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 11:33:35,312 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 11:33:35,321 - INFO - __main__ - Validation time = 312.70812916755676s.
2023-03-14 11:33:35,321 - INFO - __main__ - Epoch 79
2023-03-14 11:33:43,446 - INFO - __main__ - Epoch  79, Step:  170000, Batch Loss:     2.318666, Lr: 0.000046, Tokens per sec:   2525
2023-03-14 11:34:03,581 - INFO - __main__ - Epoch  79, Step:  170100, Batch Loss:     3.055373, Lr: 0.000046, Tokens per sec:   2686
2023-03-14 11:34:23,430 - INFO - __main__ - Epoch  79, Step:  170200, Batch Loss:     2.729201, Lr: 0.000046, Tokens per sec:   2751
2023-03-14 11:34:43,359 - INFO - __main__ - Epoch  79, Step:  170300, Batch Loss:     2.386463, Lr: 0.000046, Tokens per sec:   2684
2023-03-14 11:35:03,324 - INFO - __main__ - Epoch  79, Step:  170400, Batch Loss:     2.460815, Lr: 0.000046, Tokens per sec:   2664
2023-03-14 11:35:23,376 - INFO - __main__ - Epoch  79, Step:  170500, Batch Loss:     2.032449, Lr: 0.000046, Tokens per sec:   2655
2023-03-14 11:35:42,995 - INFO - __main__ - Epoch  79, Step:  170600, Batch Loss:     2.178838, Lr: 0.000046, Tokens per sec:   2759
2023-03-14 11:36:02,531 - INFO - __main__ - Epoch  79, Step:  170700, Batch Loss:     2.752363, Lr: 0.000046, Tokens per sec:   2769
2023-03-14 11:36:22,091 - INFO - __main__ - Epoch  79, Step:  170800, Batch Loss:     2.706284, Lr: 0.000046, Tokens per sec:   2803
2023-03-14 11:36:41,762 - INFO - __main__ - Epoch  79, Step:  170900, Batch Loss:     3.366194, Lr: 0.000046, Tokens per sec:   2721
2023-03-14 11:37:00,944 - INFO - __main__ - Epoch  79, Step:  171000, Batch Loss:     2.909364, Lr: 0.000046, Tokens per sec:   2801
2023-03-14 11:37:20,462 - INFO - __main__ - Epoch  79, Step:  171100, Batch Loss:     3.440197, Lr: 0.000046, Tokens per sec:   2786
2023-03-14 11:37:39,991 - INFO - __main__ - Epoch  79, Step:  171200, Batch Loss:     3.177691, Lr: 0.000046, Tokens per sec:   2774
2023-03-14 11:37:59,429 - INFO - __main__ - Epoch  79, Step:  171300, Batch Loss:     3.206089, Lr: 0.000046, Tokens per sec:   2749
2023-03-14 11:38:18,760 - INFO - __main__ - Epoch  79, Step:  171400, Batch Loss:     2.405384, Lr: 0.000046, Tokens per sec:   2780
2023-03-14 11:38:38,152 - INFO - __main__ - Epoch  79, Step:  171500, Batch Loss:     3.006398, Lr: 0.000046, Tokens per sec:   2733
2023-03-14 11:38:57,509 - INFO - __main__ - Epoch  79, Step:  171600, Batch Loss:     2.021565, Lr: 0.000046, Tokens per sec:   2776
2023-03-14 11:39:16,830 - INFO - __main__ - Epoch  79, Step:  171700, Batch Loss:     2.552729, Lr: 0.000046, Tokens per sec:   2764
2023-03-14 11:39:36,686 - INFO - __main__ - Epoch  79, Step:  171800, Batch Loss:     4.130251, Lr: 0.000046, Tokens per sec:   2700
2023-03-14 11:39:56,719 - INFO - __main__ - Epoch  79, Step:  171900, Batch Loss:     2.829675, Lr: 0.000046, Tokens per sec:   2687
2023-03-14 11:40:16,467 - INFO - __main__ - Epoch  79, Step:  172000, Batch Loss:     2.932566, Lr: 0.000046, Tokens per sec:   2758
2023-03-14 11:40:36,081 - INFO - __main__ - Epoch  79, Step:  172100, Batch Loss:     2.099850, Lr: 0.000046, Tokens per sec:   2738
2023-03-14 11:40:44,122 - INFO - __main__ - Epoch  79: total training loss 5979.80
2023-03-14 11:40:44,123 - INFO - __main__ - Epoch 80
2023-03-14 11:40:56,186 - INFO - __main__ - Epoch  80, Step:  172200, Batch Loss:     2.769584, Lr: 0.000045, Tokens per sec:   2608
2023-03-14 11:41:15,375 - INFO - __main__ - Epoch  80, Step:  172300, Batch Loss:     1.495705, Lr: 0.000045, Tokens per sec:   2811
2023-03-14 11:41:34,962 - INFO - __main__ - Epoch  80, Step:  172400, Batch Loss:     3.749430, Lr: 0.000045, Tokens per sec:   2713
2023-03-14 11:41:54,606 - INFO - __main__ - Epoch  80, Step:  172500, Batch Loss:     3.532601, Lr: 0.000045, Tokens per sec:   2720
2023-03-14 11:42:14,326 - INFO - __main__ - Epoch  80, Step:  172600, Batch Loss:     1.826110, Lr: 0.000045, Tokens per sec:   2740
2023-03-14 11:42:34,106 - INFO - __main__ - Epoch  80, Step:  172700, Batch Loss:     2.652810, Lr: 0.000045, Tokens per sec:   2732
2023-03-14 11:42:53,525 - INFO - __main__ - Epoch  80, Step:  172800, Batch Loss:     1.549846, Lr: 0.000045, Tokens per sec:   2771
2023-03-14 11:43:12,736 - INFO - __main__ - Epoch  80, Step:  172900, Batch Loss:     3.388172, Lr: 0.000045, Tokens per sec:   2738
2023-03-14 11:43:32,042 - INFO - __main__ - Epoch  80, Step:  173000, Batch Loss:     3.052590, Lr: 0.000045, Tokens per sec:   2832
2023-03-14 11:43:51,527 - INFO - __main__ - Epoch  80, Step:  173100, Batch Loss:     3.070010, Lr: 0.000045, Tokens per sec:   2777
2023-03-14 11:44:11,132 - INFO - __main__ - Epoch  80, Step:  173200, Batch Loss:     2.454715, Lr: 0.000045, Tokens per sec:   2787
2023-03-14 11:44:30,562 - INFO - __main__ - Epoch  80, Step:  173300, Batch Loss:     3.186270, Lr: 0.000045, Tokens per sec:   2750
2023-03-14 11:44:50,170 - INFO - __main__ - Epoch  80, Step:  173400, Batch Loss:     2.787350, Lr: 0.000045, Tokens per sec:   2731
2023-03-14 11:45:09,660 - INFO - __main__ - Epoch  80, Step:  173500, Batch Loss:     2.971324, Lr: 0.000045, Tokens per sec:   2789
2023-03-14 11:45:29,295 - INFO - __main__ - Epoch  80, Step:  173600, Batch Loss:     2.819417, Lr: 0.000045, Tokens per sec:   2724
2023-03-14 11:45:49,058 - INFO - __main__ - Epoch  80, Step:  173700, Batch Loss:     3.566350, Lr: 0.000045, Tokens per sec:   2701
2023-03-14 11:46:08,651 - INFO - __main__ - Epoch  80, Step:  173800, Batch Loss:     2.637041, Lr: 0.000045, Tokens per sec:   2779
2023-03-14 11:46:28,080 - INFO - __main__ - Epoch  80, Step:  173900, Batch Loss:     2.685048, Lr: 0.000045, Tokens per sec:   2747
2023-03-14 11:46:47,943 - INFO - __main__ - Epoch  80, Step:  174000, Batch Loss:     2.411484, Lr: 0.000045, Tokens per sec:   2778
2023-03-14 11:47:07,462 - INFO - __main__ - Epoch  80, Step:  174100, Batch Loss:     2.420759, Lr: 0.000045, Tokens per sec:   2784
2023-03-14 11:47:27,197 - INFO - __main__ - Epoch  80, Step:  174200, Batch Loss:     3.871068, Lr: 0.000045, Tokens per sec:   2744
2023-03-14 11:47:46,151 - INFO - __main__ - Epoch  80, Step:  174300, Batch Loss:     2.118345, Lr: 0.000045, Tokens per sec:   2795
2023-03-14 11:47:50,194 - INFO - __main__ - Epoch  80: total training loss 5883.77
2023-03-14 11:51:41,087 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.25061021602183, rouge_l = 57.31062987497938, meteor = 0
2023-03-14 11:51:42,258 - INFO - __main__ - Delete test_dir3/156888.ckpt
2023-03-14 11:51:42,376 - INFO - __main__ - Example #0
2023-03-14 11:51:42,376 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 11:51:42,376 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 11:51:42,376 - INFO - __main__ - Example #1
2023-03-14 11:51:42,376 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 11:51:42,376 - INFO - __main__ - 	Hypothesis: call when the connection be start . < p > by default , not prove - know , because a user doesn ' t start .
2023-03-14 11:51:42,376 - INFO - __main__ - Example #2
2023-03-14 11:51:42,376 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 11:51:42,376 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this so
2023-03-14 11:51:42,382 - INFO - __main__ - Validation time = 227.47713899612427s.
2023-03-14 11:51:42,383 - INFO - __main__ - Epoch 81
2023-03-14 11:51:58,452 - INFO - __main__ - Epoch  81, Step:  174400, Batch Loss:     2.630425, Lr: 0.000045, Tokens per sec:   2684
2023-03-14 11:52:18,474 - INFO - __main__ - Epoch  81, Step:  174500, Batch Loss:     2.406055, Lr: 0.000045, Tokens per sec:   2709
2023-03-14 11:52:38,363 - INFO - __main__ - Epoch  81, Step:  174600, Batch Loss:     2.038281, Lr: 0.000045, Tokens per sec:   2692
2023-03-14 11:52:58,012 - INFO - __main__ - Epoch  81, Step:  174700, Batch Loss:     2.573178, Lr: 0.000045, Tokens per sec:   2745
2023-03-14 11:53:16,897 - INFO - __main__ - Epoch  81, Step:  174800, Batch Loss:     2.738209, Lr: 0.000045, Tokens per sec:   2794
2023-03-14 11:53:36,645 - INFO - __main__ - Epoch  81, Step:  174900, Batch Loss:     2.599841, Lr: 0.000045, Tokens per sec:   2715
2023-03-14 11:53:56,200 - INFO - __main__ - Epoch  81, Step:  175000, Batch Loss:     2.610966, Lr: 0.000045, Tokens per sec:   2758
2023-03-14 11:54:16,234 - INFO - __main__ - Epoch  81, Step:  175100, Batch Loss:     2.629034, Lr: 0.000045, Tokens per sec:   2673
2023-03-14 11:54:36,229 - INFO - __main__ - Epoch  81, Step:  175200, Batch Loss:     2.252961, Lr: 0.000045, Tokens per sec:   2629
2023-03-14 11:54:56,209 - INFO - __main__ - Epoch  81, Step:  175300, Batch Loss:     2.450815, Lr: 0.000045, Tokens per sec:   2718
2023-03-14 11:55:15,814 - INFO - __main__ - Epoch  81, Step:  175400, Batch Loss:     2.646152, Lr: 0.000045, Tokens per sec:   2768
2023-03-14 11:55:35,850 - INFO - __main__ - Epoch  81, Step:  175500, Batch Loss:     2.529561, Lr: 0.000045, Tokens per sec:   2708
2023-03-14 11:55:55,281 - INFO - __main__ - Epoch  81, Step:  175600, Batch Loss:     2.530980, Lr: 0.000045, Tokens per sec:   2837
2023-03-14 11:56:14,820 - INFO - __main__ - Epoch  81, Step:  175700, Batch Loss:     1.924145, Lr: 0.000045, Tokens per sec:   2743
2023-03-14 11:56:33,874 - INFO - __main__ - Epoch  81, Step:  175800, Batch Loss:     1.929923, Lr: 0.000045, Tokens per sec:   2826
2023-03-14 11:56:53,857 - INFO - __main__ - Epoch  81, Step:  175900, Batch Loss:     3.711926, Lr: 0.000045, Tokens per sec:   2712
2023-03-14 11:57:12,410 - INFO - __main__ - Epoch  81, Step:  176000, Batch Loss:     1.862219, Lr: 0.000045, Tokens per sec:   2916
2023-03-14 11:57:32,227 - INFO - __main__ - Epoch  81, Step:  176100, Batch Loss:     3.397005, Lr: 0.000045, Tokens per sec:   2695
2023-03-14 11:57:51,944 - INFO - __main__ - Epoch  81, Step:  176200, Batch Loss:     3.737045, Lr: 0.000045, Tokens per sec:   2733
2023-03-14 11:58:11,458 - INFO - __main__ - Epoch  81, Step:  176300, Batch Loss:     3.063264, Lr: 0.000045, Tokens per sec:   2753
2023-03-14 11:58:30,317 - INFO - __main__ - Epoch  81, Step:  176400, Batch Loss:     3.070842, Lr: 0.000045, Tokens per sec:   2893
2023-03-14 11:58:49,742 - INFO - __main__ - Epoch  81: total training loss 5798.09
2023-03-14 11:58:49,742 - INFO - __main__ - Epoch 82
2023-03-14 11:58:50,354 - INFO - __main__ - Epoch  82, Step:  176500, Batch Loss:     3.066308, Lr: 0.000044, Tokens per sec:    977
2023-03-14 11:59:09,138 - INFO - __main__ - Epoch  82, Step:  176600, Batch Loss:     1.449311, Lr: 0.000044, Tokens per sec:   2808
2023-03-14 11:59:29,193 - INFO - __main__ - Epoch  82, Step:  176700, Batch Loss:     2.447151, Lr: 0.000044, Tokens per sec:   2681
2023-03-14 11:59:48,653 - INFO - __main__ - Epoch  82, Step:  176800, Batch Loss:     2.946171, Lr: 0.000044, Tokens per sec:   2776
2023-03-14 12:00:07,841 - INFO - __main__ - Epoch  82, Step:  176900, Batch Loss:     2.056506, Lr: 0.000044, Tokens per sec:   2777
2023-03-14 12:00:26,519 - INFO - __main__ - Epoch  82, Step:  177000, Batch Loss:     3.190606, Lr: 0.000044, Tokens per sec:   2839
2023-03-14 12:00:46,417 - INFO - __main__ - Epoch  82, Step:  177100, Batch Loss:     2.653995, Lr: 0.000044, Tokens per sec:   2679
2023-03-14 12:01:05,638 - INFO - __main__ - Epoch  82, Step:  177200, Batch Loss:     2.688293, Lr: 0.000044, Tokens per sec:   2862
2023-03-14 12:01:25,005 - INFO - __main__ - Epoch  82, Step:  177300, Batch Loss:     3.450205, Lr: 0.000044, Tokens per sec:   2779
2023-03-14 12:01:44,512 - INFO - __main__ - Epoch  82, Step:  177400, Batch Loss:     4.077814, Lr: 0.000044, Tokens per sec:   2798
2023-03-14 12:02:04,105 - INFO - __main__ - Epoch  82, Step:  177500, Batch Loss:     4.018476, Lr: 0.000044, Tokens per sec:   2741
2023-03-14 12:02:23,323 - INFO - __main__ - Epoch  82, Step:  177600, Batch Loss:     2.424034, Lr: 0.000044, Tokens per sec:   2827
2023-03-14 12:02:42,913 - INFO - __main__ - Epoch  82, Step:  177700, Batch Loss:     2.275656, Lr: 0.000044, Tokens per sec:   2748
2023-03-14 12:03:01,992 - INFO - __main__ - Epoch  82, Step:  177800, Batch Loss:     1.827954, Lr: 0.000044, Tokens per sec:   2822
2023-03-14 12:03:21,140 - INFO - __main__ - Epoch  82, Step:  177900, Batch Loss:     2.682057, Lr: 0.000044, Tokens per sec:   2790
2023-03-14 12:03:39,444 - INFO - __main__ - Epoch  82, Step:  178000, Batch Loss:     3.292677, Lr: 0.000044, Tokens per sec:   2948
2023-03-14 12:03:58,272 - INFO - __main__ - Epoch  82, Step:  178100, Batch Loss:     2.342860, Lr: 0.000044, Tokens per sec:   2883
2023-03-14 12:04:17,760 - INFO - __main__ - Epoch  82, Step:  178200, Batch Loss:     2.821787, Lr: 0.000044, Tokens per sec:   2792
2023-03-14 12:04:37,324 - INFO - __main__ - Epoch  82, Step:  178300, Batch Loss:     2.277053, Lr: 0.000044, Tokens per sec:   2759
2023-03-14 12:04:56,714 - INFO - __main__ - Epoch  82, Step:  178400, Batch Loss:     2.182528, Lr: 0.000044, Tokens per sec:   2784
2023-03-14 12:05:16,647 - INFO - __main__ - Epoch  82, Step:  178500, Batch Loss:     1.881813, Lr: 0.000044, Tokens per sec:   2644
2023-03-14 12:05:36,576 - INFO - __main__ - Epoch  82, Step:  178600, Batch Loss:     1.666177, Lr: 0.000044, Tokens per sec:   2742
2023-03-14 12:05:52,299 - INFO - __main__ - Epoch  82: total training loss 5687.73
2023-03-14 12:09:41,927 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.39868579847308, rouge_l = 57.40531193932773, meteor = 0
2023-03-14 12:09:43,049 - INFO - __main__ - Delete test_dir3/174320.ckpt
2023-03-14 12:09:43,200 - INFO - __main__ - Delete /home/tongye2/ytnmt/src_integration/test_dir3/174320.ckpt
2023-03-14 12:09:43,200 - WARNING - __main__ - Want to delete old checkpoint /home/tongye2/ytnmt/src_integration/test_dir3/174320.ckptbut file does not exist. ([Errno 2] No such file or directory: '/home/tongye2/ytnmt/src_integration/test_dir3/174320.ckpt')
2023-03-14 12:09:43,201 - INFO - __main__ - Example #0
2023-03-14 12:09:43,201 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 12:09:43,201 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 12:09:43,201 - INFO - __main__ - Example #1
2023-03-14 12:09:43,201 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 12:09:43,201 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 12:09:43,201 - INFO - __main__ - Example #2
2023-03-14 12:09:43,201 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 12:09:43,201 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 12:09:43,208 - INFO - __main__ - Validation time = 226.72465777397156s.
2023-03-14 12:09:43,208 - INFO - __main__ - Epoch 83
2023-03-14 12:09:47,819 - INFO - __main__ - Epoch  83, Step:  178700, Batch Loss:     2.043603, Lr: 0.000044, Tokens per sec:   2657
2023-03-14 12:10:06,653 - INFO - __main__ - Epoch  83, Step:  178800, Batch Loss:     2.399751, Lr: 0.000044, Tokens per sec:   2843
2023-03-14 12:10:26,063 - INFO - __main__ - Epoch  83, Step:  178900, Batch Loss:     2.979563, Lr: 0.000044, Tokens per sec:   2805
2023-03-14 12:10:45,021 - INFO - __main__ - Epoch  83, Step:  179000, Batch Loss:     2.064805, Lr: 0.000044, Tokens per sec:   2816
2023-03-14 12:11:04,447 - INFO - __main__ - Epoch  83, Step:  179100, Batch Loss:     3.261079, Lr: 0.000044, Tokens per sec:   2799
2023-03-14 12:11:24,153 - INFO - __main__ - Epoch  83, Step:  179200, Batch Loss:     1.990248, Lr: 0.000044, Tokens per sec:   2736
2023-03-14 12:11:42,996 - INFO - __main__ - Epoch  83, Step:  179300, Batch Loss:     1.245144, Lr: 0.000044, Tokens per sec:   2836
2023-03-14 12:12:02,151 - INFO - __main__ - Epoch  83, Step:  179400, Batch Loss:     3.437962, Lr: 0.000044, Tokens per sec:   2787
2023-03-14 12:12:21,895 - INFO - __main__ - Epoch  83, Step:  179500, Batch Loss:     2.655100, Lr: 0.000044, Tokens per sec:   2665
2023-03-14 12:12:41,163 - INFO - __main__ - Epoch  83, Step:  179600, Batch Loss:     2.118105, Lr: 0.000044, Tokens per sec:   2778
2023-03-14 12:12:59,933 - INFO - __main__ - Epoch  83, Step:  179700, Batch Loss:     2.214476, Lr: 0.000044, Tokens per sec:   2868
2023-03-14 12:13:18,447 - INFO - __main__ - Epoch  83, Step:  179800, Batch Loss:     2.652489, Lr: 0.000044, Tokens per sec:   2938
2023-03-14 12:13:37,290 - INFO - __main__ - Epoch  83, Step:  179900, Batch Loss:     2.015579, Lr: 0.000044, Tokens per sec:   2856
2023-03-14 12:13:56,128 - INFO - __main__ - Epoch  83, Step:  180000, Batch Loss:     3.478353, Lr: 0.000044, Tokens per sec:   2816
2023-03-14 12:14:15,069 - INFO - __main__ - Epoch  83, Step:  180100, Batch Loss:     2.715828, Lr: 0.000044, Tokens per sec:   2866
2023-03-14 12:14:34,916 - INFO - __main__ - Epoch  83, Step:  180200, Batch Loss:     2.452224, Lr: 0.000044, Tokens per sec:   2715
2023-03-14 12:14:54,185 - INFO - __main__ - Epoch  83, Step:  180300, Batch Loss:     2.514705, Lr: 0.000044, Tokens per sec:   2805
2023-03-14 12:15:13,340 - INFO - __main__ - Epoch  83, Step:  180400, Batch Loss:     3.595392, Lr: 0.000044, Tokens per sec:   2792
2023-03-14 12:15:32,371 - INFO - __main__ - Epoch  83, Step:  180500, Batch Loss:     3.363350, Lr: 0.000044, Tokens per sec:   2885
2023-03-14 12:15:51,242 - INFO - __main__ - Epoch  83, Step:  180600, Batch Loss:     2.768991, Lr: 0.000044, Tokens per sec:   2868
2023-03-14 12:16:10,859 - INFO - __main__ - Epoch  83, Step:  180700, Batch Loss:     1.687047, Lr: 0.000044, Tokens per sec:   2730
2023-03-14 12:16:29,389 - INFO - __main__ - Epoch  83, Step:  180800, Batch Loss:     3.945717, Lr: 0.000044, Tokens per sec:   2909
2023-03-14 12:16:39,961 - INFO - __main__ - Epoch  83: total training loss 5593.16
2023-03-14 12:16:39,962 - INFO - __main__ - Epoch 84
2023-03-14 12:16:48,534 - INFO - __main__ - Epoch  84, Step:  180900, Batch Loss:     1.650091, Lr: 0.000043, Tokens per sec:   2715
2023-03-14 12:17:08,154 - INFO - __main__ - Epoch  84, Step:  181000, Batch Loss:     2.341920, Lr: 0.000043, Tokens per sec:   2742
2023-03-14 12:17:26,930 - INFO - __main__ - Epoch  84, Step:  181100, Batch Loss:     1.810164, Lr: 0.000043, Tokens per sec:   2861
2023-03-14 12:17:45,302 - INFO - __main__ - Epoch  84, Step:  181200, Batch Loss:     2.059085, Lr: 0.000043, Tokens per sec:   2920
2023-03-14 12:18:03,776 - INFO - __main__ - Epoch  84, Step:  181300, Batch Loss:     2.573081, Lr: 0.000043, Tokens per sec:   2938
2023-03-14 12:18:23,365 - INFO - __main__ - Epoch  84, Step:  181400, Batch Loss:     2.228372, Lr: 0.000043, Tokens per sec:   2718
2023-03-14 12:18:42,196 - INFO - __main__ - Epoch  84, Step:  181500, Batch Loss:     2.417417, Lr: 0.000043, Tokens per sec:   2886
2023-03-14 12:19:01,023 - INFO - __main__ - Epoch  84, Step:  181600, Batch Loss:     3.616174, Lr: 0.000043, Tokens per sec:   2848
2023-03-14 12:19:19,686 - INFO - __main__ - Epoch  84, Step:  181700, Batch Loss:     3.073246, Lr: 0.000043, Tokens per sec:   2912
2023-03-14 12:19:38,658 - INFO - __main__ - Epoch  84, Step:  181800, Batch Loss:     1.940036, Lr: 0.000043, Tokens per sec:   2818
2023-03-14 12:19:57,783 - INFO - __main__ - Epoch  84, Step:  181900, Batch Loss:     1.453654, Lr: 0.000043, Tokens per sec:   2809
2023-03-14 12:20:16,569 - INFO - __main__ - Epoch  84, Step:  182000, Batch Loss:     2.751567, Lr: 0.000043, Tokens per sec:   2898
2023-03-14 12:20:35,023 - INFO - __main__ - Epoch  84, Step:  182100, Batch Loss:     3.509076, Lr: 0.000043, Tokens per sec:   2932
2023-03-14 12:20:54,050 - INFO - __main__ - Epoch  84, Step:  182200, Batch Loss:     2.777904, Lr: 0.000043, Tokens per sec:   2831
2023-03-14 12:21:12,585 - INFO - __main__ - Epoch  84, Step:  182300, Batch Loss:     3.571580, Lr: 0.000043, Tokens per sec:   2919
2023-03-14 12:21:31,725 - INFO - __main__ - Epoch  84, Step:  182400, Batch Loss:     3.516261, Lr: 0.000043, Tokens per sec:   2832
2023-03-14 12:21:51,545 - INFO - __main__ - Epoch  84, Step:  182500, Batch Loss:     3.102724, Lr: 0.000043, Tokens per sec:   2724
2023-03-14 12:22:11,422 - INFO - __main__ - Epoch  84, Step:  182600, Batch Loss:     2.153012, Lr: 0.000043, Tokens per sec:   2682
2023-03-14 12:22:31,401 - INFO - __main__ - Epoch  84, Step:  182700, Batch Loss:     4.083514, Lr: 0.000043, Tokens per sec:   2649
2023-03-14 12:22:51,295 - INFO - __main__ - Epoch  84, Step:  182800, Batch Loss:     3.162314, Lr: 0.000043, Tokens per sec:   2645
2023-03-14 12:23:11,165 - INFO - __main__ - Epoch  84, Step:  182900, Batch Loss:     1.850889, Lr: 0.000043, Tokens per sec:   2683
2023-03-14 12:23:31,094 - INFO - __main__ - Epoch  84, Step:  183000, Batch Loss:     2.071391, Lr: 0.000043, Tokens per sec:   2736
2023-03-14 12:23:38,322 - INFO - __main__ - Epoch  84: total training loss 5474.15
2023-03-14 12:27:27,479 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.23539127000034, rouge_l = 57.30761501618707, meteor = 0
2023-03-14 12:27:27,481 - INFO - __main__ - Example #0
2023-03-14 12:27:27,481 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 12:27:27,481 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 12:27:27,481 - INFO - __main__ - Example #1
2023-03-14 12:27:27,481 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 12:27:27,481 - INFO - __main__ - 	Hypothesis: call when the activity start . < p > this callback will be call by user .
2023-03-14 12:27:27,481 - INFO - __main__ - Example #2
2023-03-14 12:27:27,481 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 12:27:27,481 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 12:27:27,491 - INFO - __main__ - Validation time = 226.16853213310242s.
2023-03-14 12:27:27,491 - INFO - __main__ - Epoch 85
2023-03-14 12:27:40,606 - INFO - __main__ - Epoch  85, Step:  183100, Batch Loss:     2.013783, Lr: 0.000043, Tokens per sec:   2602
2023-03-14 12:27:59,605 - INFO - __main__ - Epoch  85, Step:  183200, Batch Loss:     1.645164, Lr: 0.000043, Tokens per sec:   2826
2023-03-14 12:28:19,040 - INFO - __main__ - Epoch  85, Step:  183300, Batch Loss:     2.386893, Lr: 0.000043, Tokens per sec:   2792
2023-03-14 12:28:38,719 - INFO - __main__ - Epoch  85, Step:  183400, Batch Loss:     2.118089, Lr: 0.000043, Tokens per sec:   2711
2023-03-14 12:28:57,647 - INFO - __main__ - Epoch  85, Step:  183500, Batch Loss:     3.557946, Lr: 0.000043, Tokens per sec:   2854
2023-03-14 12:29:16,580 - INFO - __main__ - Epoch  85, Step:  183600, Batch Loss:     2.845402, Lr: 0.000043, Tokens per sec:   2873
2023-03-14 12:29:35,582 - INFO - __main__ - Epoch  85, Step:  183700, Batch Loss:     1.941367, Lr: 0.000043, Tokens per sec:   2833
2023-03-14 12:29:55,289 - INFO - __main__ - Epoch  85, Step:  183800, Batch Loss:     2.915152, Lr: 0.000043, Tokens per sec:   2726
2023-03-14 12:30:14,265 - INFO - __main__ - Epoch  85, Step:  183900, Batch Loss:     2.907678, Lr: 0.000043, Tokens per sec:   2815
2023-03-14 12:30:32,934 - INFO - __main__ - Epoch  85, Step:  184000, Batch Loss:     2.950309, Lr: 0.000043, Tokens per sec:   2937
2023-03-14 12:30:51,452 - INFO - __main__ - Epoch  85, Step:  184100, Batch Loss:     2.078889, Lr: 0.000043, Tokens per sec:   2953
2023-03-14 12:31:10,040 - INFO - __main__ - Epoch  85, Step:  184200, Batch Loss:     2.835521, Lr: 0.000043, Tokens per sec:   2899
2023-03-14 12:31:29,223 - INFO - __main__ - Epoch  85, Step:  184300, Batch Loss:     2.588005, Lr: 0.000043, Tokens per sec:   2770
2023-03-14 12:31:47,943 - INFO - __main__ - Epoch  85, Step:  184400, Batch Loss:     3.299582, Lr: 0.000043, Tokens per sec:   2836
2023-03-14 12:32:06,611 - INFO - __main__ - Epoch  85, Step:  184500, Batch Loss:     1.680755, Lr: 0.000043, Tokens per sec:   2876
2023-03-14 12:32:25,428 - INFO - __main__ - Epoch  85, Step:  184600, Batch Loss:     1.939834, Lr: 0.000043, Tokens per sec:   2832
2023-03-14 12:32:43,731 - INFO - __main__ - Epoch  85, Step:  184700, Batch Loss:     2.690460, Lr: 0.000043, Tokens per sec:   2907
2023-03-14 12:33:02,604 - INFO - __main__ - Epoch  85, Step:  184800, Batch Loss:     1.739965, Lr: 0.000043, Tokens per sec:   2871
2023-03-14 12:33:21,590 - INFO - __main__ - Epoch  85, Step:  184900, Batch Loss:     2.603949, Lr: 0.000043, Tokens per sec:   2789
2023-03-14 12:33:40,997 - INFO - __main__ - Epoch  85, Step:  185000, Batch Loss:     2.898982, Lr: 0.000043, Tokens per sec:   2820
2023-03-14 12:34:00,482 - INFO - __main__ - Epoch  85, Step:  185100, Batch Loss:     2.919158, Lr: 0.000043, Tokens per sec:   2729
2023-03-14 12:34:19,117 - INFO - __main__ - Epoch  85, Step:  185200, Batch Loss:     1.877382, Lr: 0.000043, Tokens per sec:   2946
2023-03-14 12:34:21,910 - INFO - __main__ - Epoch  85: total training loss 5426.02
2023-03-14 12:34:21,911 - INFO - __main__ - Epoch 86
2023-03-14 12:34:38,091 - INFO - __main__ - Epoch  86, Step:  185300, Batch Loss:     2.781319, Lr: 0.000043, Tokens per sec:   2851
2023-03-14 12:34:57,091 - INFO - __main__ - Epoch  86, Step:  185400, Batch Loss:     0.980222, Lr: 0.000043, Tokens per sec:   2848
2023-03-14 12:35:16,282 - INFO - __main__ - Epoch  86, Step:  185500, Batch Loss:     2.216753, Lr: 0.000043, Tokens per sec:   2777
2023-03-14 12:35:36,083 - INFO - __main__ - Epoch  86, Step:  185600, Batch Loss:     3.275838, Lr: 0.000043, Tokens per sec:   2711
2023-03-14 12:35:55,449 - INFO - __main__ - Epoch  86, Step:  185700, Batch Loss:     2.595944, Lr: 0.000043, Tokens per sec:   2751
2023-03-14 12:36:15,218 - INFO - __main__ - Epoch  86, Step:  185800, Batch Loss:     2.841212, Lr: 0.000043, Tokens per sec:   2742
2023-03-14 12:36:34,511 - INFO - __main__ - Epoch  86, Step:  185900, Batch Loss:     2.974655, Lr: 0.000043, Tokens per sec:   2796
2023-03-14 12:36:53,682 - INFO - __main__ - Epoch  86, Step:  186000, Batch Loss:     2.594327, Lr: 0.000043, Tokens per sec:   2836
2023-03-14 12:37:13,199 - INFO - __main__ - Epoch  86, Step:  186100, Batch Loss:     1.952976, Lr: 0.000043, Tokens per sec:   2720
2023-03-14 12:37:32,449 - INFO - __main__ - Epoch  86, Step:  186200, Batch Loss:     1.880850, Lr: 0.000043, Tokens per sec:   2801
2023-03-14 12:37:52,423 - INFO - __main__ - Epoch  86, Step:  186300, Batch Loss:     1.925602, Lr: 0.000043, Tokens per sec:   2643
2023-03-14 12:38:12,280 - INFO - __main__ - Epoch  86, Step:  186400, Batch Loss:     2.554420, Lr: 0.000043, Tokens per sec:   2740
2023-03-14 12:38:31,972 - INFO - __main__ - Epoch  86, Step:  186500, Batch Loss:     3.035110, Lr: 0.000043, Tokens per sec:   2773
2023-03-14 12:38:51,986 - INFO - __main__ - Epoch  86, Step:  186600, Batch Loss:     3.017428, Lr: 0.000043, Tokens per sec:   2701
2023-03-14 12:39:11,736 - INFO - __main__ - Epoch  86, Step:  186700, Batch Loss:     2.822607, Lr: 0.000043, Tokens per sec:   2760
2023-03-14 12:39:31,633 - INFO - __main__ - Epoch  86, Step:  186800, Batch Loss:     3.217816, Lr: 0.000043, Tokens per sec:   2686
2023-03-14 12:39:51,296 - INFO - __main__ - Epoch  86, Step:  186900, Batch Loss:     2.541416, Lr: 0.000043, Tokens per sec:   2749
2023-03-14 12:40:11,256 - INFO - __main__ - Epoch  86, Step:  187000, Batch Loss:     2.518067, Lr: 0.000043, Tokens per sec:   2706
2023-03-14 12:40:31,294 - INFO - __main__ - Epoch  86, Step:  187100, Batch Loss:     3.209404, Lr: 0.000043, Tokens per sec:   2681
2023-03-14 12:40:51,238 - INFO - __main__ - Epoch  86, Step:  187200, Batch Loss:     3.913280, Lr: 0.000043, Tokens per sec:   2701
2023-03-14 12:41:10,981 - INFO - __main__ - Epoch  86, Step:  187300, Batch Loss:     1.321731, Lr: 0.000043, Tokens per sec:   2716
2023-03-14 12:41:29,791 - INFO - __main__ - Epoch  86: total training loss 5301.20
2023-03-14 12:45:20,630 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.380645199978694, rouge_l = 57.432553122654774, meteor = 0
2023-03-14 12:45:21,819 - INFO - __main__ - Delete test_dir3/169962.ckpt
2023-03-14 12:45:21,950 - INFO - __main__ - Example #0
2023-03-14 12:45:21,950 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 12:45:21,950 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 12:45:21,950 - INFO - __main__ - Example #1
2023-03-14 12:45:21,950 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 12:45:21,950 - INFO - __main__ - 	Hypothesis: call when the activity be start . < p > by default , this callback be call to update the gui .
2023-03-14 12:45:21,950 - INFO - __main__ - Example #2
2023-03-14 12:45:21,950 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 12:45:21,950 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this jtext
2023-03-14 12:45:21,960 - INFO - __main__ - Validation time = 227.88962984085083s.
2023-03-14 12:45:21,960 - INFO - __main__ - Epoch 87
2023-03-14 12:45:23,498 - INFO - __main__ - Epoch  87, Step:  187400, Batch Loss:     1.328866, Lr: 0.000042, Tokens per sec:   1922
2023-03-14 12:45:43,021 - INFO - __main__ - Epoch  87, Step:  187500, Batch Loss:     1.817548, Lr: 0.000042, Tokens per sec:   2752
2023-03-14 12:46:02,469 - INFO - __main__ - Epoch  87, Step:  187600, Batch Loss:     2.780070, Lr: 0.000042, Tokens per sec:   2796
2023-03-14 12:46:21,831 - INFO - __main__ - Epoch  87, Step:  187700, Batch Loss:     1.581971, Lr: 0.000042, Tokens per sec:   2794
2023-03-14 12:46:40,881 - INFO - __main__ - Epoch  87, Step:  187800, Batch Loss:     2.752110, Lr: 0.000042, Tokens per sec:   2864
2023-03-14 12:46:59,948 - INFO - __main__ - Epoch  87, Step:  187900, Batch Loss:     2.575054, Lr: 0.000042, Tokens per sec:   2847
2023-03-14 12:47:20,035 - INFO - __main__ - Epoch  87, Step:  188000, Batch Loss:     2.416600, Lr: 0.000042, Tokens per sec:   2672
2023-03-14 12:47:39,811 - INFO - __main__ - Epoch  87, Step:  188100, Batch Loss:     2.765535, Lr: 0.000042, Tokens per sec:   2749
2023-03-14 12:47:59,337 - INFO - __main__ - Epoch  87, Step:  188200, Batch Loss:     2.431200, Lr: 0.000042, Tokens per sec:   2766
2023-03-14 12:48:19,104 - INFO - __main__ - Epoch  87, Step:  188300, Batch Loss:     2.218043, Lr: 0.000042, Tokens per sec:   2739
2023-03-14 12:48:38,191 - INFO - __main__ - Epoch  87, Step:  188400, Batch Loss:     2.557537, Lr: 0.000042, Tokens per sec:   2827
2023-03-14 12:48:57,344 - INFO - __main__ - Epoch  87, Step:  188500, Batch Loss:     2.625543, Lr: 0.000042, Tokens per sec:   2800
2023-03-14 12:49:17,001 - INFO - __main__ - Epoch  87, Step:  188600, Batch Loss:     1.427212, Lr: 0.000042, Tokens per sec:   2659
2023-03-14 12:49:36,910 - INFO - __main__ - Epoch  87, Step:  188700, Batch Loss:     2.404387, Lr: 0.000042, Tokens per sec:   2701
2023-03-14 12:49:56,671 - INFO - __main__ - Epoch  87, Step:  188800, Batch Loss:     2.762518, Lr: 0.000042, Tokens per sec:   2761
2023-03-14 12:50:16,138 - INFO - __main__ - Epoch  87, Step:  188900, Batch Loss:     2.669164, Lr: 0.000042, Tokens per sec:   2746
2023-03-14 12:50:35,164 - INFO - __main__ - Epoch  87, Step:  189000, Batch Loss:     2.141727, Lr: 0.000042, Tokens per sec:   2860
2023-03-14 12:50:54,752 - INFO - __main__ - Epoch  87, Step:  189100, Batch Loss:     2.333926, Lr: 0.000042, Tokens per sec:   2696
2023-03-14 12:51:13,901 - INFO - __main__ - Epoch  87, Step:  189200, Batch Loss:     3.314155, Lr: 0.000042, Tokens per sec:   2807
2023-03-14 12:51:32,988 - INFO - __main__ - Epoch  87, Step:  189300, Batch Loss:     2.949997, Lr: 0.000042, Tokens per sec:   2820
2023-03-14 12:51:52,389 - INFO - __main__ - Epoch  87, Step:  189400, Batch Loss:     3.049782, Lr: 0.000042, Tokens per sec:   2788
2023-03-14 12:52:11,625 - INFO - __main__ - Epoch  87, Step:  189500, Batch Loss:     3.351106, Lr: 0.000042, Tokens per sec:   2780
2023-03-14 12:52:25,293 - INFO - __main__ - Epoch  87: total training loss 5220.65
2023-03-14 12:52:25,294 - INFO - __main__ - Epoch 88
2023-03-14 12:52:31,078 - INFO - __main__ - Epoch  88, Step:  189600, Batch Loss:     2.245143, Lr: 0.000042, Tokens per sec:   2450
2023-03-14 12:52:51,074 - INFO - __main__ - Epoch  88, Step:  189700, Batch Loss:     1.841336, Lr: 0.000042, Tokens per sec:   2705
2023-03-14 12:53:10,423 - INFO - __main__ - Epoch  88, Step:  189800, Batch Loss:     2.538370, Lr: 0.000042, Tokens per sec:   2807
2023-03-14 12:53:30,113 - INFO - __main__ - Epoch  88, Step:  189900, Batch Loss:     2.889267, Lr: 0.000042, Tokens per sec:   2746
2023-03-14 12:53:49,288 - INFO - __main__ - Epoch  88, Step:  190000, Batch Loss:     3.153296, Lr: 0.000042, Tokens per sec:   2756
2023-03-14 12:54:08,421 - INFO - __main__ - Epoch  88, Step:  190100, Batch Loss:     2.126146, Lr: 0.000042, Tokens per sec:   2826
2023-03-14 12:54:28,072 - INFO - __main__ - Epoch  88, Step:  190200, Batch Loss:     1.669156, Lr: 0.000042, Tokens per sec:   2730
2023-03-14 12:54:47,328 - INFO - __main__ - Epoch  88, Step:  190300, Batch Loss:     3.100618, Lr: 0.000042, Tokens per sec:   2818
2023-03-14 12:55:07,247 - INFO - __main__ - Epoch  88, Step:  190400, Batch Loss:     1.709024, Lr: 0.000042, Tokens per sec:   2732
2023-03-14 12:55:26,499 - INFO - __main__ - Epoch  88, Step:  190500, Batch Loss:     2.050618, Lr: 0.000042, Tokens per sec:   2785
2023-03-14 12:55:45,920 - INFO - __main__ - Epoch  88, Step:  190600, Batch Loss:     2.848222, Lr: 0.000042, Tokens per sec:   2812
2023-03-14 12:56:05,966 - INFO - __main__ - Epoch  88, Step:  190700, Batch Loss:     2.765666, Lr: 0.000042, Tokens per sec:   2682
2023-03-14 12:56:25,280 - INFO - __main__ - Epoch  88, Step:  190800, Batch Loss:     3.694661, Lr: 0.000042, Tokens per sec:   2737
2023-03-14 12:56:44,641 - INFO - __main__ - Epoch  88, Step:  190900, Batch Loss:     2.813605, Lr: 0.000042, Tokens per sec:   2757
2023-03-14 12:57:04,201 - INFO - __main__ - Epoch  88, Step:  191000, Batch Loss:     2.098576, Lr: 0.000042, Tokens per sec:   2754
2023-03-14 12:57:23,571 - INFO - __main__ - Epoch  88, Step:  191100, Batch Loss:     2.525429, Lr: 0.000042, Tokens per sec:   2747
2023-03-14 12:57:43,277 - INFO - __main__ - Epoch  88, Step:  191200, Batch Loss:     3.015453, Lr: 0.000042, Tokens per sec:   2773
2023-03-14 12:58:03,263 - INFO - __main__ - Epoch  88, Step:  191300, Batch Loss:     2.709720, Lr: 0.000042, Tokens per sec:   2695
2023-03-14 12:58:22,819 - INFO - __main__ - Epoch  88, Step:  191400, Batch Loss:     1.522812, Lr: 0.000042, Tokens per sec:   2806
2023-03-14 12:58:42,800 - INFO - __main__ - Epoch  88, Step:  191500, Batch Loss:     1.838092, Lr: 0.000042, Tokens per sec:   2687
2023-03-14 12:59:02,414 - INFO - __main__ - Epoch  88, Step:  191600, Batch Loss:     2.951738, Lr: 0.000042, Tokens per sec:   2761
2023-03-14 12:59:21,420 - INFO - __main__ - Epoch  88, Step:  191700, Batch Loss:     2.631575, Lr: 0.000042, Tokens per sec:   2810
2023-03-14 12:59:31,311 - INFO - __main__ - Epoch  88: total training loss 5160.75
2023-03-14 13:03:19,450 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.33052147401222, rouge_l = 57.40289513388901, meteor = 0
2023-03-14 13:03:19,450 - INFO - __main__ - Example #0
2023-03-14 13:03:19,451 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 13:03:19,451 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 13:03:19,451 - INFO - __main__ - Example #1
2023-03-14 13:03:19,451 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 13:03:19,451 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 13:03:19,451 - INFO - __main__ - Example #2
2023-03-14 13:03:19,451 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 13:03:19,451 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this jtext
2023-03-14 13:03:19,457 - INFO - __main__ - Validation time = 225.26110363006592s.
2023-03-14 13:03:19,457 - INFO - __main__ - Epoch 89
2023-03-14 13:03:29,040 - INFO - __main__ - Epoch  89, Step:  191800, Batch Loss:     2.547916, Lr: 0.000041, Tokens per sec:   2756
2023-03-14 13:03:49,206 - INFO - __main__ - Epoch  89, Step:  191900, Batch Loss:     1.860999, Lr: 0.000041, Tokens per sec:   2656
2023-03-14 13:04:08,950 - INFO - __main__ - Epoch  89, Step:  192000, Batch Loss:     2.134129, Lr: 0.000041, Tokens per sec:   2736
2023-03-14 13:04:28,984 - INFO - __main__ - Epoch  89, Step:  192100, Batch Loss:     2.191700, Lr: 0.000041, Tokens per sec:   2729
2023-03-14 13:04:49,074 - INFO - __main__ - Epoch  89, Step:  192200, Batch Loss:     4.096336, Lr: 0.000041, Tokens per sec:   2702
2023-03-14 13:05:08,794 - INFO - __main__ - Epoch  89, Step:  192300, Batch Loss:     2.318167, Lr: 0.000041, Tokens per sec:   2671
2023-03-14 13:05:28,037 - INFO - __main__ - Epoch  89, Step:  192400, Batch Loss:     2.462445, Lr: 0.000041, Tokens per sec:   2839
2023-03-14 13:05:47,419 - INFO - __main__ - Epoch  89, Step:  192500, Batch Loss:     2.171770, Lr: 0.000041, Tokens per sec:   2723
2023-03-14 13:06:07,279 - INFO - __main__ - Epoch  89, Step:  192600, Batch Loss:     3.312714, Lr: 0.000041, Tokens per sec:   2744
2023-03-14 13:06:27,257 - INFO - __main__ - Epoch  89, Step:  192700, Batch Loss:     2.801403, Lr: 0.000041, Tokens per sec:   2669
2023-03-14 13:06:47,152 - INFO - __main__ - Epoch  89, Step:  192800, Batch Loss:     2.194284, Lr: 0.000041, Tokens per sec:   2715
2023-03-14 13:07:05,860 - INFO - __main__ - Epoch  89, Step:  192900, Batch Loss:     2.134943, Lr: 0.000041, Tokens per sec:   2856
2023-03-14 13:07:24,557 - INFO - __main__ - Epoch  89, Step:  193000, Batch Loss:     1.332868, Lr: 0.000041, Tokens per sec:   2849
2023-03-14 13:07:44,019 - INFO - __main__ - Epoch  89, Step:  193100, Batch Loss:     1.761787, Lr: 0.000041, Tokens per sec:   2774
2023-03-14 13:08:03,312 - INFO - __main__ - Epoch  89, Step:  193200, Batch Loss:     1.913066, Lr: 0.000041, Tokens per sec:   2772
2023-03-14 13:08:22,016 - INFO - __main__ - Epoch  89, Step:  193300, Batch Loss:     2.503428, Lr: 0.000041, Tokens per sec:   2918
2023-03-14 13:08:41,629 - INFO - __main__ - Epoch  89, Step:  193400, Batch Loss:     1.431108, Lr: 0.000041, Tokens per sec:   2719
2023-03-14 13:09:00,857 - INFO - __main__ - Epoch  89, Step:  193500, Batch Loss:     1.868978, Lr: 0.000041, Tokens per sec:   2821
2023-03-14 13:09:20,741 - INFO - __main__ - Epoch  89, Step:  193600, Batch Loss:     1.831689, Lr: 0.000041, Tokens per sec:   2664
2023-03-14 13:09:39,691 - INFO - __main__ - Epoch  89, Step:  193700, Batch Loss:     3.035633, Lr: 0.000041, Tokens per sec:   2839
2023-03-14 13:09:59,271 - INFO - __main__ - Epoch  89, Step:  193800, Batch Loss:     2.119219, Lr: 0.000041, Tokens per sec:   2743
2023-03-14 13:10:17,990 - INFO - __main__ - Epoch  89, Step:  193900, Batch Loss:     2.899525, Lr: 0.000041, Tokens per sec:   2948
2023-03-14 13:10:23,778 - INFO - __main__ - Epoch  89: total training loss 5065.84
2023-03-14 13:10:23,778 - INFO - __main__ - Epoch 90
2023-03-14 13:10:37,898 - INFO - __main__ - Epoch  90, Step:  194000, Batch Loss:     2.725839, Lr: 0.000041, Tokens per sec:   2562
2023-03-14 13:10:57,648 - INFO - __main__ - Epoch  90, Step:  194100, Batch Loss:     3.096912, Lr: 0.000041, Tokens per sec:   2732
2023-03-14 13:11:17,242 - INFO - __main__ - Epoch  90, Step:  194200, Batch Loss:     2.630838, Lr: 0.000041, Tokens per sec:   2762
2023-03-14 13:11:37,216 - INFO - __main__ - Epoch  90, Step:  194300, Batch Loss:     2.988064, Lr: 0.000041, Tokens per sec:   2680
2023-03-14 13:11:56,524 - INFO - __main__ - Epoch  90, Step:  194400, Batch Loss:     1.987992, Lr: 0.000041, Tokens per sec:   2785
2023-03-14 13:12:16,364 - INFO - __main__ - Epoch  90, Step:  194500, Batch Loss:     2.839940, Lr: 0.000041, Tokens per sec:   2702
2023-03-14 13:12:34,845 - INFO - __main__ - Epoch  90, Step:  194600, Batch Loss:     1.748997, Lr: 0.000041, Tokens per sec:   2866
2023-03-14 13:12:54,329 - INFO - __main__ - Epoch  90, Step:  194700, Batch Loss:     2.726254, Lr: 0.000041, Tokens per sec:   2740
2023-03-14 13:13:13,991 - INFO - __main__ - Epoch  90, Step:  194800, Batch Loss:     1.714466, Lr: 0.000041, Tokens per sec:   2741
2023-03-14 13:13:32,814 - INFO - __main__ - Epoch  90, Step:  194900, Batch Loss:     2.690944, Lr: 0.000041, Tokens per sec:   2866
2023-03-14 13:13:52,573 - INFO - __main__ - Epoch  90, Step:  195000, Batch Loss:     2.684429, Lr: 0.000041, Tokens per sec:   2731
2023-03-14 13:14:12,026 - INFO - __main__ - Epoch  90, Step:  195100, Batch Loss:     2.885644, Lr: 0.000041, Tokens per sec:   2766
2023-03-14 13:14:31,147 - INFO - __main__ - Epoch  90, Step:  195200, Batch Loss:     1.964193, Lr: 0.000041, Tokens per sec:   2835
2023-03-14 13:14:50,230 - INFO - __main__ - Epoch  90, Step:  195300, Batch Loss:     1.860319, Lr: 0.000041, Tokens per sec:   2842
2023-03-14 13:15:09,529 - INFO - __main__ - Epoch  90, Step:  195400, Batch Loss:     2.650662, Lr: 0.000041, Tokens per sec:   2804
2023-03-14 13:15:29,375 - INFO - __main__ - Epoch  90, Step:  195500, Batch Loss:     3.056598, Lr: 0.000041, Tokens per sec:   2737
2023-03-14 13:15:48,992 - INFO - __main__ - Epoch  90, Step:  195600, Batch Loss:     2.398145, Lr: 0.000041, Tokens per sec:   2767
2023-03-14 13:16:08,030 - INFO - __main__ - Epoch  90, Step:  195700, Batch Loss:     2.167289, Lr: 0.000041, Tokens per sec:   2840
2023-03-14 13:16:26,447 - INFO - __main__ - Epoch  90, Step:  195800, Batch Loss:     2.371264, Lr: 0.000041, Tokens per sec:   2914
2023-03-14 13:16:45,945 - INFO - __main__ - Epoch  90, Step:  195900, Batch Loss:     2.606306, Lr: 0.000041, Tokens per sec:   2812
2023-03-14 13:17:04,923 - INFO - __main__ - Epoch  90, Step:  196000, Batch Loss:     2.557057, Lr: 0.000041, Tokens per sec:   2810
2023-03-14 13:17:23,673 - INFO - __main__ - Epoch  90, Step:  196100, Batch Loss:     3.088516, Lr: 0.000041, Tokens per sec:   2858
2023-03-14 13:17:25,724 - INFO - __main__ - Epoch  90: total training loss 4954.27
2023-03-14 13:21:14,845 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.39912245688158, rouge_l = 57.34673980087792, meteor = 0
2023-03-14 13:21:15,958 - INFO - __main__ - Delete test_dir3/187394.ckpt
2023-03-14 13:21:16,126 - INFO - __main__ - Delete /home/tongye2/ytnmt/src_integration/test_dir3/187394.ckpt
2023-03-14 13:21:16,126 - WARNING - __main__ - Want to delete old checkpoint /home/tongye2/ytnmt/src_integration/test_dir3/187394.ckptbut file does not exist. ([Errno 2] No such file or directory: '/home/tongye2/ytnmt/src_integration/test_dir3/187394.ckpt')
2023-03-14 13:21:16,127 - INFO - __main__ - Example #0
2023-03-14 13:21:16,127 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 13:21:16,127 - INFO - __main__ - 	Hypothesis: return a hash code for this class constant object .
2023-03-14 13:21:16,127 - INFO - __main__ - Example #1
2023-03-14 13:21:16,127 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 13:21:16,127 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 13:21:16,127 - INFO - __main__ - Example #2
2023-03-14 13:21:16,127 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 13:21:16,127 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this jtext
2023-03-14 13:21:16,133 - INFO - __main__ - Validation time = 225.89235162734985s.
2023-03-14 13:21:16,133 - INFO - __main__ - Epoch 91
2023-03-14 13:21:33,720 - INFO - __main__ - Epoch  91, Step:  196200, Batch Loss:     2.002715, Lr: 0.000040, Tokens per sec:   2766
2023-03-14 13:21:52,430 - INFO - __main__ - Epoch  91, Step:  196300, Batch Loss:     2.694362, Lr: 0.000040, Tokens per sec:   2884
2023-03-14 13:22:12,273 - INFO - __main__ - Epoch  91, Step:  196400, Batch Loss:     2.560853, Lr: 0.000040, Tokens per sec:   2725
2023-03-14 13:22:32,278 - INFO - __main__ - Epoch  91, Step:  196500, Batch Loss:     1.844458, Lr: 0.000040, Tokens per sec:   2741
2023-03-14 13:22:52,140 - INFO - __main__ - Epoch  91, Step:  196600, Batch Loss:     1.828406, Lr: 0.000040, Tokens per sec:   2716
2023-03-14 13:23:12,085 - INFO - __main__ - Epoch  91, Step:  196700, Batch Loss:     2.397714, Lr: 0.000040, Tokens per sec:   2725
2023-03-14 13:23:32,160 - INFO - __main__ - Epoch  91, Step:  196800, Batch Loss:     1.805483, Lr: 0.000040, Tokens per sec:   2687
2023-03-14 13:23:52,084 - INFO - __main__ - Epoch  91, Step:  196900, Batch Loss:     2.428643, Lr: 0.000040, Tokens per sec:   2779
2023-03-14 13:24:12,046 - INFO - __main__ - Epoch  91, Step:  197000, Batch Loss:     1.664901, Lr: 0.000040, Tokens per sec:   2678
2023-03-14 13:24:31,825 - INFO - __main__ - Epoch  91, Step:  197100, Batch Loss:     1.810403, Lr: 0.000040, Tokens per sec:   2692
2023-03-14 13:24:51,296 - INFO - __main__ - Epoch  91, Step:  197200, Batch Loss:     2.006462, Lr: 0.000040, Tokens per sec:   2751
2023-03-14 13:25:11,132 - INFO - __main__ - Epoch  91, Step:  197300, Batch Loss:     2.047908, Lr: 0.000040, Tokens per sec:   2719
2023-03-14 13:25:30,592 - INFO - __main__ - Epoch  91, Step:  197400, Batch Loss:     1.292944, Lr: 0.000040, Tokens per sec:   2761
2023-03-14 13:25:50,079 - INFO - __main__ - Epoch  91, Step:  197500, Batch Loss:     2.460124, Lr: 0.000040, Tokens per sec:   2697
2023-03-14 13:26:09,641 - INFO - __main__ - Epoch  91, Step:  197600, Batch Loss:     1.386442, Lr: 0.000040, Tokens per sec:   2782
2023-03-14 13:26:28,240 - INFO - __main__ - Epoch  91, Step:  197700, Batch Loss:     2.970425, Lr: 0.000040, Tokens per sec:   2938
2023-03-14 13:26:47,879 - INFO - __main__ - Epoch  91, Step:  197800, Batch Loss:     1.338806, Lr: 0.000040, Tokens per sec:   2737
2023-03-14 13:27:07,955 - INFO - __main__ - Epoch  91, Step:  197900, Batch Loss:     2.295594, Lr: 0.000040, Tokens per sec:   2677
2023-03-14 13:27:27,945 - INFO - __main__ - Epoch  91, Step:  198000, Batch Loss:     1.998941, Lr: 0.000040, Tokens per sec:   2673
2023-03-14 13:27:47,436 - INFO - __main__ - Epoch  91, Step:  198100, Batch Loss:     2.069146, Lr: 0.000040, Tokens per sec:   2760
2023-03-14 13:28:07,244 - INFO - __main__ - Epoch  91, Step:  198200, Batch Loss:     1.263111, Lr: 0.000040, Tokens per sec:   2680
2023-03-14 13:28:25,156 - INFO - __main__ - Epoch  91: total training loss 4891.17
2023-03-14 13:28:25,157 - INFO - __main__ - Epoch 92
2023-03-14 13:28:27,555 - INFO - __main__ - Epoch  92, Step:  198300, Batch Loss:     2.188844, Lr: 0.000040, Tokens per sec:   2647
2023-03-14 13:28:47,126 - INFO - __main__ - Epoch  92, Step:  198400, Batch Loss:     1.295006, Lr: 0.000040, Tokens per sec:   2734
2023-03-14 13:29:07,179 - INFO - __main__ - Epoch  92, Step:  198500, Batch Loss:     1.848061, Lr: 0.000040, Tokens per sec:   2683
2023-03-14 13:29:27,164 - INFO - __main__ - Epoch  92, Step:  198600, Batch Loss:     1.856866, Lr: 0.000040, Tokens per sec:   2713
2023-03-14 13:29:46,787 - INFO - __main__ - Epoch  92, Step:  198700, Batch Loss:     1.816362, Lr: 0.000040, Tokens per sec:   2789
2023-03-14 13:30:06,720 - INFO - __main__ - Epoch  92, Step:  198800, Batch Loss:     2.223021, Lr: 0.000040, Tokens per sec:   2657
2023-03-14 13:30:26,791 - INFO - __main__ - Epoch  92, Step:  198900, Batch Loss:     1.440732, Lr: 0.000040, Tokens per sec:   2687
2023-03-14 13:30:46,508 - INFO - __main__ - Epoch  92, Step:  199000, Batch Loss:     2.693784, Lr: 0.000040, Tokens per sec:   2708
2023-03-14 13:31:05,552 - INFO - __main__ - Epoch  92, Step:  199100, Batch Loss:     2.588842, Lr: 0.000040, Tokens per sec:   2825
2023-03-14 13:31:24,869 - INFO - __main__ - Epoch  92, Step:  199200, Batch Loss:     3.043537, Lr: 0.000040, Tokens per sec:   2753
2023-03-14 13:31:44,252 - INFO - __main__ - Epoch  92, Step:  199300, Batch Loss:     1.482949, Lr: 0.000040, Tokens per sec:   2807
2023-03-14 13:32:03,648 - INFO - __main__ - Epoch  92, Step:  199400, Batch Loss:     2.057981, Lr: 0.000040, Tokens per sec:   2814
2023-03-14 13:32:23,042 - INFO - __main__ - Epoch  92, Step:  199500, Batch Loss:     2.853891, Lr: 0.000040, Tokens per sec:   2779
2023-03-14 13:32:43,092 - INFO - __main__ - Epoch  92, Step:  199600, Batch Loss:     1.760201, Lr: 0.000040, Tokens per sec:   2664
2023-03-14 13:33:02,696 - INFO - __main__ - Epoch  92, Step:  199700, Batch Loss:     2.918791, Lr: 0.000040, Tokens per sec:   2725
2023-03-14 13:33:22,665 - INFO - __main__ - Epoch  92, Step:  199800, Batch Loss:     2.960205, Lr: 0.000040, Tokens per sec:   2690
2023-03-14 13:33:42,367 - INFO - __main__ - Epoch  92, Step:  199900, Batch Loss:     2.778827, Lr: 0.000040, Tokens per sec:   2785
2023-03-14 13:34:01,840 - INFO - __main__ - Epoch  92, Step:  200000, Batch Loss:     2.740750, Lr: 0.000040, Tokens per sec:   2765
2023-03-14 13:34:21,843 - INFO - __main__ - Epoch  92, Step:  200100, Batch Loss:     1.477509, Lr: 0.000040, Tokens per sec:   2721
2023-03-14 13:34:41,850 - INFO - __main__ - Epoch  92, Step:  200200, Batch Loss:     2.209341, Lr: 0.000040, Tokens per sec:   2649
2023-03-14 13:35:01,953 - INFO - __main__ - Epoch  92, Step:  200300, Batch Loss:     2.928566, Lr: 0.000040, Tokens per sec:   2658
2023-03-14 13:35:21,700 - INFO - __main__ - Epoch  92, Step:  200400, Batch Loss:     1.653290, Lr: 0.000040, Tokens per sec:   2674
2023-03-14 13:35:35,459 - INFO - __main__ - Epoch  92: total training loss 4831.00
2023-03-14 13:39:26,403 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 13:39:26,403 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.64053833465947, rouge_l = 57.542708635648154, meteor = 0
2023-03-14 13:39:27,532 - INFO - __main__ - Delete test_dir3/178678.ckpt
2023-03-14 13:39:27,660 - INFO - __main__ - Example #0
2023-03-14 13:39:27,660 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 13:39:27,660 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 13:39:27,660 - INFO - __main__ - Example #1
2023-03-14 13:39:27,660 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 13:39:27,660 - INFO - __main__ - 	Hypothesis: call when the activity be start . < p > override this method be call by the implementation .
2023-03-14 13:39:27,660 - INFO - __main__ - Example #2
2023-03-14 13:39:27,660 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 13:39:27,660 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this method
2023-03-14 13:39:27,669 - INFO - __main__ - Validation time = 228.01807975769043s.
2023-03-14 13:39:27,669 - INFO - __main__ - Epoch 93
2023-03-14 13:39:34,446 - INFO - __main__ - Epoch  93, Step:  200500, Batch Loss:     2.284656, Lr: 0.000040, Tokens per sec:   2539
2023-03-14 13:39:53,683 - INFO - __main__ - Epoch  93, Step:  200600, Batch Loss:     3.123194, Lr: 0.000040, Tokens per sec:   2776
2023-03-14 13:40:13,020 - INFO - __main__ - Epoch  93, Step:  200700, Batch Loss:     1.686383, Lr: 0.000040, Tokens per sec:   2765
2023-03-14 13:40:31,937 - INFO - __main__ - Epoch  93, Step:  200800, Batch Loss:     1.758306, Lr: 0.000040, Tokens per sec:   2873
2023-03-14 13:40:51,187 - INFO - __main__ - Epoch  93, Step:  200900, Batch Loss:     2.303954, Lr: 0.000040, Tokens per sec:   2786
2023-03-14 13:41:10,409 - INFO - __main__ - Epoch  93, Step:  201000, Batch Loss:     2.957392, Lr: 0.000040, Tokens per sec:   2838
2023-03-14 13:41:30,571 - INFO - __main__ - Epoch  93, Step:  201100, Batch Loss:     1.750615, Lr: 0.000040, Tokens per sec:   2654
2023-03-14 13:41:50,401 - INFO - __main__ - Epoch  93, Step:  201200, Batch Loss:     3.124281, Lr: 0.000040, Tokens per sec:   2743
2023-03-14 13:42:10,231 - INFO - __main__ - Epoch  93, Step:  201300, Batch Loss:     2.318065, Lr: 0.000040, Tokens per sec:   2721
2023-03-14 13:42:29,129 - INFO - __main__ - Epoch  93, Step:  201400, Batch Loss:     2.303622, Lr: 0.000040, Tokens per sec:   2838
2023-03-14 13:42:48,782 - INFO - __main__ - Epoch  93, Step:  201500, Batch Loss:     1.450114, Lr: 0.000040, Tokens per sec:   2732
2023-03-14 13:43:07,257 - INFO - __main__ - Epoch  93, Step:  201600, Batch Loss:     1.896923, Lr: 0.000040, Tokens per sec:   2901
2023-03-14 13:43:26,455 - INFO - __main__ - Epoch  93, Step:  201700, Batch Loss:     1.654230, Lr: 0.000040, Tokens per sec:   2759
2023-03-14 13:43:45,547 - INFO - __main__ - Epoch  93, Step:  201800, Batch Loss:     1.737557, Lr: 0.000040, Tokens per sec:   2832
2023-03-14 13:44:04,501 - INFO - __main__ - Epoch  93, Step:  201900, Batch Loss:     2.196117, Lr: 0.000040, Tokens per sec:   2848
2023-03-14 13:44:24,213 - INFO - __main__ - Epoch  93, Step:  202000, Batch Loss:     2.298395, Lr: 0.000040, Tokens per sec:   2728
2023-03-14 13:44:43,941 - INFO - __main__ - Epoch  93, Step:  202100, Batch Loss:     2.478033, Lr: 0.000040, Tokens per sec:   2782
2023-03-14 13:45:02,958 - INFO - __main__ - Epoch  93, Step:  202200, Batch Loss:     1.952167, Lr: 0.000040, Tokens per sec:   2763
2023-03-14 13:45:22,483 - INFO - __main__ - Epoch  93, Step:  202300, Batch Loss:     2.720415, Lr: 0.000040, Tokens per sec:   2787
2023-03-14 13:45:41,902 - INFO - __main__ - Epoch  93, Step:  202400, Batch Loss:     2.417517, Lr: 0.000040, Tokens per sec:   2825
2023-03-14 13:46:01,818 - INFO - __main__ - Epoch  93, Step:  202500, Batch Loss:     2.544974, Lr: 0.000040, Tokens per sec:   2663
2023-03-14 13:46:21,471 - INFO - __main__ - Epoch  93, Step:  202600, Batch Loss:     2.124823, Lr: 0.000040, Tokens per sec:   2750
2023-03-14 13:46:30,841 - INFO - __main__ - Epoch  93: total training loss 4739.78
2023-03-14 13:46:30,841 - INFO - __main__ - Epoch 94
2023-03-14 13:46:41,805 - INFO - __main__ - Epoch  94, Step:  202700, Batch Loss:     2.205727, Lr: 0.000039, Tokens per sec:   2649
2023-03-14 13:47:01,842 - INFO - __main__ - Epoch  94, Step:  202800, Batch Loss:     1.913356, Lr: 0.000039, Tokens per sec:   2737
2023-03-14 13:47:21,061 - INFO - __main__ - Epoch  94, Step:  202900, Batch Loss:     2.647943, Lr: 0.000039, Tokens per sec:   2816
2023-03-14 13:47:39,431 - INFO - __main__ - Epoch  94, Step:  203000, Batch Loss:     1.867448, Lr: 0.000039, Tokens per sec:   2928
2023-03-14 13:47:59,485 - INFO - __main__ - Epoch  94, Step:  203100, Batch Loss:     1.354517, Lr: 0.000039, Tokens per sec:   2689
2023-03-14 13:48:19,051 - INFO - __main__ - Epoch  94, Step:  203200, Batch Loss:     2.397033, Lr: 0.000039, Tokens per sec:   2810
2023-03-14 13:48:39,110 - INFO - __main__ - Epoch  94, Step:  203300, Batch Loss:     2.458797, Lr: 0.000039, Tokens per sec:   2712
2023-03-14 13:48:58,979 - INFO - __main__ - Epoch  94, Step:  203400, Batch Loss:     1.684106, Lr: 0.000039, Tokens per sec:   2678
2023-03-14 13:49:18,471 - INFO - __main__ - Epoch  94, Step:  203500, Batch Loss:     1.845923, Lr: 0.000039, Tokens per sec:   2741
2023-03-14 13:49:38,132 - INFO - __main__ - Epoch  94, Step:  203600, Batch Loss:     2.340431, Lr: 0.000039, Tokens per sec:   2776
2023-03-14 13:49:57,616 - INFO - __main__ - Epoch  94, Step:  203700, Batch Loss:     2.238640, Lr: 0.000039, Tokens per sec:   2723
2023-03-14 13:50:17,331 - INFO - __main__ - Epoch  94, Step:  203800, Batch Loss:     3.245834, Lr: 0.000039, Tokens per sec:   2717
2023-03-14 13:50:36,756 - INFO - __main__ - Epoch  94, Step:  203900, Batch Loss:     2.343472, Lr: 0.000039, Tokens per sec:   2760
2023-03-14 13:50:56,655 - INFO - __main__ - Epoch  94, Step:  204000, Batch Loss:     1.966847, Lr: 0.000039, Tokens per sec:   2767
2023-03-14 13:51:16,263 - INFO - __main__ - Epoch  94, Step:  204100, Batch Loss:     1.802621, Lr: 0.000039, Tokens per sec:   2727
2023-03-14 13:51:35,840 - INFO - __main__ - Epoch  94, Step:  204200, Batch Loss:     1.145489, Lr: 0.000039, Tokens per sec:   2778
2023-03-14 13:51:55,906 - INFO - __main__ - Epoch  94, Step:  204300, Batch Loss:     2.140677, Lr: 0.000039, Tokens per sec:   2639
2023-03-14 13:52:15,577 - INFO - __main__ - Epoch  94, Step:  204400, Batch Loss:     1.876053, Lr: 0.000039, Tokens per sec:   2754
2023-03-14 13:52:35,129 - INFO - __main__ - Epoch  94, Step:  204500, Batch Loss:     1.682341, Lr: 0.000039, Tokens per sec:   2713
2023-03-14 13:52:55,303 - INFO - __main__ - Epoch  94, Step:  204600, Batch Loss:     2.132698, Lr: 0.000039, Tokens per sec:   2640
2023-03-14 13:53:15,186 - INFO - __main__ - Epoch  94, Step:  204700, Batch Loss:     2.359838, Lr: 0.000039, Tokens per sec:   2697
2023-03-14 13:53:35,244 - INFO - __main__ - Epoch  94, Step:  204800, Batch Loss:     2.023887, Lr: 0.000039, Tokens per sec:   2619
2023-03-14 13:53:40,561 - INFO - __main__ - Epoch  94: total training loss 4637.00
2023-03-14 13:57:30,979 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.55320865308423, rouge_l = 57.50373610301265, meteor = 0
2023-03-14 13:57:32,118 - INFO - __main__ - Delete test_dir3/196110.ckpt
2023-03-14 13:57:32,213 - INFO - __main__ - Example #0
2023-03-14 13:57:32,214 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 13:57:32,214 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 13:57:32,214 - INFO - __main__ - Example #1
2023-03-14 13:57:32,214 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 13:57:32,214 - INFO - __main__ - 	Hypothesis: call when the renderer be start .
2023-03-14 13:57:32,214 - INFO - __main__ - Example #2
2023-03-14 13:57:32,214 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 13:57:32,214 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this jtext
2023-03-14 13:57:32,220 - INFO - __main__ - Validation time = 227.09983944892883s.
2023-03-14 13:57:32,220 - INFO - __main__ - Epoch 95
2023-03-14 13:57:47,226 - INFO - __main__ - Epoch  95, Step:  204900, Batch Loss:     2.082041, Lr: 0.000039, Tokens per sec:   2698
2023-03-14 13:58:07,094 - INFO - __main__ - Epoch  95, Step:  205000, Batch Loss:     2.555653, Lr: 0.000039, Tokens per sec:   2714
2023-03-14 13:58:26,626 - INFO - __main__ - Epoch  95, Step:  205100, Batch Loss:     1.166983, Lr: 0.000039, Tokens per sec:   2807
2023-03-14 13:58:45,963 - INFO - __main__ - Epoch  95, Step:  205200, Batch Loss:     1.510298, Lr: 0.000039, Tokens per sec:   2786
2023-03-14 13:59:05,749 - INFO - __main__ - Epoch  95, Step:  205300, Batch Loss:     3.330566, Lr: 0.000039, Tokens per sec:   2752
2023-03-14 13:59:25,524 - INFO - __main__ - Epoch  95, Step:  205400, Batch Loss:     2.224498, Lr: 0.000039, Tokens per sec:   2749
2023-03-14 13:59:45,376 - INFO - __main__ - Epoch  95, Step:  205500, Batch Loss:     2.359875, Lr: 0.000039, Tokens per sec:   2751
2023-03-14 14:00:05,134 - INFO - __main__ - Epoch  95, Step:  205600, Batch Loss:     1.861364, Lr: 0.000039, Tokens per sec:   2690
2023-03-14 14:00:25,187 - INFO - __main__ - Epoch  95, Step:  205700, Batch Loss:     3.010789, Lr: 0.000039, Tokens per sec:   2664
2023-03-14 14:00:44,840 - INFO - __main__ - Epoch  95, Step:  205800, Batch Loss:     2.405139, Lr: 0.000039, Tokens per sec:   2692
2023-03-14 14:01:04,841 - INFO - __main__ - Epoch  95, Step:  205900, Batch Loss:     1.562731, Lr: 0.000039, Tokens per sec:   2694
2023-03-14 14:01:24,591 - INFO - __main__ - Epoch  95, Step:  206000, Batch Loss:     1.837513, Lr: 0.000039, Tokens per sec:   2702
2023-03-14 14:01:44,173 - INFO - __main__ - Epoch  95, Step:  206100, Batch Loss:     1.995722, Lr: 0.000039, Tokens per sec:   2746
2023-03-14 14:02:04,240 - INFO - __main__ - Epoch  95, Step:  206200, Batch Loss:     2.142550, Lr: 0.000039, Tokens per sec:   2686
2023-03-14 14:02:24,333 - INFO - __main__ - Epoch  95, Step:  206300, Batch Loss:     2.212094, Lr: 0.000039, Tokens per sec:   2676
2023-03-14 14:02:44,263 - INFO - __main__ - Epoch  95, Step:  206400, Batch Loss:     3.092948, Lr: 0.000039, Tokens per sec:   2707
2023-03-14 14:03:04,200 - INFO - __main__ - Epoch  95, Step:  206500, Batch Loss:     1.865109, Lr: 0.000039, Tokens per sec:   2648
2023-03-14 14:03:24,312 - INFO - __main__ - Epoch  95, Step:  206600, Batch Loss:     2.092157, Lr: 0.000039, Tokens per sec:   2708
2023-03-14 14:03:44,115 - INFO - __main__ - Epoch  95, Step:  206700, Batch Loss:     2.633205, Lr: 0.000039, Tokens per sec:   2653
2023-03-14 14:04:03,936 - INFO - __main__ - Epoch  95, Step:  206800, Batch Loss:     2.336652, Lr: 0.000039, Tokens per sec:   2727
2023-03-14 14:04:23,343 - INFO - __main__ - Epoch  95, Step:  206900, Batch Loss:     2.448608, Lr: 0.000039, Tokens per sec:   2750
2023-03-14 14:04:43,493 - INFO - __main__ - Epoch  95, Step:  207000, Batch Loss:     2.132904, Lr: 0.000039, Tokens per sec:   2706
2023-03-14 14:04:44,579 - INFO - __main__ - Epoch  95: total training loss 4622.65
2023-03-14 14:04:44,580 - INFO - __main__ - Epoch 96
2023-03-14 14:05:04,010 - INFO - __main__ - Epoch  96, Step:  207100, Batch Loss:     1.755445, Lr: 0.000038, Tokens per sec:   2642
2023-03-14 14:05:24,069 - INFO - __main__ - Epoch  96, Step:  207200, Batch Loss:     1.606742, Lr: 0.000038, Tokens per sec:   2704
2023-03-14 14:05:43,877 - INFO - __main__ - Epoch  96, Step:  207300, Batch Loss:     1.408229, Lr: 0.000038, Tokens per sec:   2662
2023-03-14 14:06:03,901 - INFO - __main__ - Epoch  96, Step:  207400, Batch Loss:     3.061768, Lr: 0.000038, Tokens per sec:   2634
2023-03-14 14:06:24,243 - INFO - __main__ - Epoch  96, Step:  207500, Batch Loss:     2.440463, Lr: 0.000038, Tokens per sec:   2640
2023-03-14 14:06:43,878 - INFO - __main__ - Epoch  96, Step:  207600, Batch Loss:     1.483740, Lr: 0.000038, Tokens per sec:   2707
2023-03-14 14:07:03,870 - INFO - __main__ - Epoch  96, Step:  207700, Batch Loss:     0.926688, Lr: 0.000038, Tokens per sec:   2677
2023-03-14 14:07:23,621 - INFO - __main__ - Epoch  96, Step:  207800, Batch Loss:     2.180356, Lr: 0.000038, Tokens per sec:   2764
2023-03-14 14:07:43,568 - INFO - __main__ - Epoch  96, Step:  207900, Batch Loss:     1.604032, Lr: 0.000038, Tokens per sec:   2683
2023-03-14 14:08:03,700 - INFO - __main__ - Epoch  96, Step:  208000, Batch Loss:     2.396991, Lr: 0.000038, Tokens per sec:   2696
2023-03-14 14:08:23,491 - INFO - __main__ - Epoch  96, Step:  208100, Batch Loss:     1.383513, Lr: 0.000038, Tokens per sec:   2715
2023-03-14 14:08:43,534 - INFO - __main__ - Epoch  96, Step:  208200, Batch Loss:     1.784510, Lr: 0.000038, Tokens per sec:   2676
2023-03-14 14:09:03,353 - INFO - __main__ - Epoch  96, Step:  208300, Batch Loss:     1.334083, Lr: 0.000038, Tokens per sec:   2731
2023-03-14 14:09:23,177 - INFO - __main__ - Epoch  96, Step:  208400, Batch Loss:     2.332707, Lr: 0.000038, Tokens per sec:   2751
2023-03-14 14:09:43,054 - INFO - __main__ - Epoch  96, Step:  208500, Batch Loss:     1.589653, Lr: 0.000038, Tokens per sec:   2722
2023-03-14 14:10:02,699 - INFO - __main__ - Epoch  96, Step:  208600, Batch Loss:     1.704673, Lr: 0.000038, Tokens per sec:   2677
2023-03-14 14:10:22,603 - INFO - __main__ - Epoch  96, Step:  208700, Batch Loss:     2.568199, Lr: 0.000038, Tokens per sec:   2693
2023-03-14 14:10:41,996 - INFO - __main__ - Epoch  96, Step:  208800, Batch Loss:     1.742852, Lr: 0.000038, Tokens per sec:   2797
2023-03-14 14:11:02,245 - INFO - __main__ - Epoch  96, Step:  208900, Batch Loss:     2.834788, Lr: 0.000038, Tokens per sec:   2748
2023-03-14 14:11:22,231 - INFO - __main__ - Epoch  96, Step:  209000, Batch Loss:     2.004996, Lr: 0.000038, Tokens per sec:   2678
2023-03-14 14:11:42,305 - INFO - __main__ - Epoch  96, Step:  209100, Batch Loss:     1.872773, Lr: 0.000038, Tokens per sec:   2684
2023-03-14 14:11:59,171 - INFO - __main__ - Epoch  96: total training loss 4507.81
2023-03-14 14:15:50,398 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.496530021268214, rouge_l = 57.48228055301963, meteor = 0
2023-03-14 14:15:51,534 - INFO - __main__ - Delete test_dir3/165604.ckpt
2023-03-14 14:15:51,548 - INFO - __main__ - Example #0
2023-03-14 14:15:51,548 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 14:15:51,548 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 14:15:51,548 - INFO - __main__ - Example #1
2023-03-14 14:15:51,548 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 14:15:51,548 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 14:15:51,548 - INFO - __main__ - Example #2
2023-03-14 14:15:51,548 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 14:15:51,548 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this jtext
2023-03-14 14:15:51,555 - INFO - __main__ - Validation time = 228.30200004577637s.
2023-03-14 14:15:51,556 - INFO - __main__ - Epoch 97
2023-03-14 14:15:55,160 - INFO - __main__ - Epoch  97, Step:  209200, Batch Loss:     1.751164, Lr: 0.000038, Tokens per sec:   2341
2023-03-14 14:16:15,147 - INFO - __main__ - Epoch  97, Step:  209300, Batch Loss:     1.309409, Lr: 0.000038, Tokens per sec:   2715
2023-03-14 14:16:34,687 - INFO - __main__ - Epoch  97, Step:  209400, Batch Loss:     2.469558, Lr: 0.000038, Tokens per sec:   2735
2023-03-14 14:16:54,716 - INFO - __main__ - Epoch  97, Step:  209500, Batch Loss:     2.585342, Lr: 0.000038, Tokens per sec:   2710
2023-03-14 14:17:14,740 - INFO - __main__ - Epoch  97, Step:  209600, Batch Loss:     2.567978, Lr: 0.000038, Tokens per sec:   2691
2023-03-14 14:17:34,664 - INFO - __main__ - Epoch  97, Step:  209700, Batch Loss:     1.846473, Lr: 0.000038, Tokens per sec:   2674
2023-03-14 14:17:54,606 - INFO - __main__ - Epoch  97, Step:  209800, Batch Loss:     2.261893, Lr: 0.000038, Tokens per sec:   2652
2023-03-14 14:18:14,918 - INFO - __main__ - Epoch  97, Step:  209900, Batch Loss:     2.760563, Lr: 0.000038, Tokens per sec:   2689
2023-03-14 14:18:34,206 - INFO - __main__ - Epoch  97, Step:  210000, Batch Loss:     2.700998, Lr: 0.000038, Tokens per sec:   2772
2023-03-14 14:18:54,207 - INFO - __main__ - Epoch  97, Step:  210100, Batch Loss:     1.199040, Lr: 0.000038, Tokens per sec:   2642
2023-03-14 14:19:13,934 - INFO - __main__ - Epoch  97, Step:  210200, Batch Loss:     1.823029, Lr: 0.000038, Tokens per sec:   2736
2023-03-14 14:19:34,289 - INFO - __main__ - Epoch  97, Step:  210300, Batch Loss:     1.839413, Lr: 0.000038, Tokens per sec:   2611
2023-03-14 14:19:54,281 - INFO - __main__ - Epoch  97, Step:  210400, Batch Loss:     1.539894, Lr: 0.000038, Tokens per sec:   2715
2023-03-14 14:20:13,066 - INFO - __main__ - Epoch  97, Step:  210500, Batch Loss:     2.707311, Lr: 0.000038, Tokens per sec:   2885
2023-03-14 14:20:33,143 - INFO - __main__ - Epoch  97, Step:  210600, Batch Loss:     1.888764, Lr: 0.000038, Tokens per sec:   2705
2023-03-14 14:20:53,107 - INFO - __main__ - Epoch  97, Step:  210700, Batch Loss:     1.743385, Lr: 0.000038, Tokens per sec:   2667
2023-03-14 14:21:12,715 - INFO - __main__ - Epoch  97, Step:  210800, Batch Loss:     1.897353, Lr: 0.000038, Tokens per sec:   2788
2023-03-14 14:21:32,434 - INFO - __main__ - Epoch  97, Step:  210900, Batch Loss:     1.649409, Lr: 0.000038, Tokens per sec:   2732
2023-03-14 14:21:52,412 - INFO - __main__ - Epoch  97, Step:  211000, Batch Loss:     1.937894, Lr: 0.000038, Tokens per sec:   2663
2023-03-14 14:22:11,598 - INFO - __main__ - Epoch  97, Step:  211100, Batch Loss:     2.206388, Lr: 0.000038, Tokens per sec:   2787
2023-03-14 14:22:31,093 - INFO - __main__ - Epoch  97, Step:  211200, Batch Loss:     2.229189, Lr: 0.000038, Tokens per sec:   2781
2023-03-14 14:22:51,182 - INFO - __main__ - Epoch  97, Step:  211300, Batch Loss:     2.946937, Lr: 0.000038, Tokens per sec:   2751
2023-03-14 14:23:03,887 - INFO - __main__ - Epoch  97: total training loss 4473.06
2023-03-14 14:23:03,888 - INFO - __main__ - Epoch 98
2023-03-14 14:23:11,636 - INFO - __main__ - Epoch  98, Step:  211400, Batch Loss:     2.865803, Lr: 0.000038, Tokens per sec:   2633
2023-03-14 14:23:31,682 - INFO - __main__ - Epoch  98, Step:  211500, Batch Loss:     1.962717, Lr: 0.000038, Tokens per sec:   2696
2023-03-14 14:23:51,667 - INFO - __main__ - Epoch  98, Step:  211600, Batch Loss:     1.471570, Lr: 0.000038, Tokens per sec:   2686
2023-03-14 14:24:11,669 - INFO - __main__ - Epoch  98, Step:  211700, Batch Loss:     2.946220, Lr: 0.000038, Tokens per sec:   2704
2023-03-14 14:24:31,675 - INFO - __main__ - Epoch  98, Step:  211800, Batch Loss:     2.248019, Lr: 0.000038, Tokens per sec:   2696
2023-03-14 14:24:51,552 - INFO - __main__ - Epoch  98, Step:  211900, Batch Loss:     1.964736, Lr: 0.000038, Tokens per sec:   2724
2023-03-14 14:25:10,821 - INFO - __main__ - Epoch  98, Step:  212000, Batch Loss:     1.684687, Lr: 0.000038, Tokens per sec:   2820
2023-03-14 14:25:30,098 - INFO - __main__ - Epoch  98, Step:  212100, Batch Loss:     2.449161, Lr: 0.000038, Tokens per sec:   2739
2023-03-14 14:25:49,656 - INFO - __main__ - Epoch  98, Step:  212200, Batch Loss:     2.099204, Lr: 0.000038, Tokens per sec:   2753
2023-03-14 14:26:08,946 - INFO - __main__ - Epoch  98, Step:  212300, Batch Loss:     1.942863, Lr: 0.000038, Tokens per sec:   2799
2023-03-14 14:26:28,924 - INFO - __main__ - Epoch  98, Step:  212400, Batch Loss:     1.478454, Lr: 0.000038, Tokens per sec:   2694
2023-03-14 14:26:48,932 - INFO - __main__ - Epoch  98, Step:  212500, Batch Loss:     2.486269, Lr: 0.000038, Tokens per sec:   2674
2023-03-14 14:27:08,803 - INFO - __main__ - Epoch  98, Step:  212600, Batch Loss:     2.256656, Lr: 0.000038, Tokens per sec:   2744
2023-03-14 14:27:28,756 - INFO - __main__ - Epoch  98, Step:  212700, Batch Loss:     2.088929, Lr: 0.000038, Tokens per sec:   2678
2023-03-14 14:27:48,396 - INFO - __main__ - Epoch  98, Step:  212800, Batch Loss:     2.118768, Lr: 0.000038, Tokens per sec:   2771
2023-03-14 14:28:08,365 - INFO - __main__ - Epoch  98, Step:  212900, Batch Loss:     2.570850, Lr: 0.000038, Tokens per sec:   2694
2023-03-14 14:28:28,402 - INFO - __main__ - Epoch  98, Step:  213000, Batch Loss:     1.623969, Lr: 0.000038, Tokens per sec:   2669
2023-03-14 14:28:47,715 - INFO - __main__ - Epoch  98, Step:  213100, Batch Loss:     0.913306, Lr: 0.000038, Tokens per sec:   2731
2023-03-14 14:29:07,631 - INFO - __main__ - Epoch  98, Step:  213200, Batch Loss:     1.523825, Lr: 0.000038, Tokens per sec:   2688
2023-03-14 14:29:27,563 - INFO - __main__ - Epoch  98, Step:  213300, Batch Loss:     1.982221, Lr: 0.000038, Tokens per sec:   2708
2023-03-14 14:29:47,558 - INFO - __main__ - Epoch  98, Step:  213400, Batch Loss:     2.379385, Lr: 0.000038, Tokens per sec:   2729
2023-03-14 14:30:07,680 - INFO - __main__ - Epoch  98, Step:  213500, Batch Loss:     2.126263, Lr: 0.000038, Tokens per sec:   2641
2023-03-14 14:30:16,296 - INFO - __main__ - Epoch  98: total training loss 4352.07
2023-03-14 14:34:13,597 - INFO - __main__ - Horray! New best validation score [bleu]!
2023-03-14 14:34:13,597 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.752861937784, rouge_l = 57.69668356664497, meteor = 0
2023-03-14 14:34:14,749 - INFO - __main__ - Delete test_dir3/209184.ckpt
2023-03-14 14:34:14,868 - INFO - __main__ - Delete /home/tongye2/ytnmt/src_integration/test_dir3/209184.ckpt
2023-03-14 14:34:14,868 - WARNING - __main__ - Want to delete old checkpoint /home/tongye2/ytnmt/src_integration/test_dir3/209184.ckptbut file does not exist. ([Errno 2] No such file or directory: '/home/tongye2/ytnmt/src_integration/test_dir3/209184.ckpt')
2023-03-14 14:34:14,869 - INFO - __main__ - Example #0
2023-03-14 14:34:14,869 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 14:34:14,869 - INFO - __main__ - 	Hypothesis: return a hash code value for this object .
2023-03-14 14:34:14,869 - INFO - __main__ - Example #1
2023-03-14 14:34:14,869 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 14:34:14,869 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 14:34:14,869 - INFO - __main__ - Example #2
2023-03-14 14:34:14,869 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 14:34:14,869 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 14:34:14,878 - INFO - __main__ - Validation time = 234.40760397911072s.
2023-03-14 14:34:14,879 - INFO - __main__ - Epoch 99
2023-03-14 14:34:26,975 - INFO - __main__ - Epoch  99, Step:  213600, Batch Loss:     1.718894, Lr: 0.000037, Tokens per sec:   2555
2023-03-14 14:34:46,858 - INFO - __main__ - Epoch  99, Step:  213700, Batch Loss:     2.919299, Lr: 0.000037, Tokens per sec:   2677
2023-03-14 14:35:06,811 - INFO - __main__ - Epoch  99, Step:  213800, Batch Loss:     2.503129, Lr: 0.000037, Tokens per sec:   2700
2023-03-14 14:35:26,760 - INFO - __main__ - Epoch  99, Step:  213900, Batch Loss:     1.503001, Lr: 0.000037, Tokens per sec:   2732
2023-03-14 14:35:46,863 - INFO - __main__ - Epoch  99, Step:  214000, Batch Loss:     2.433102, Lr: 0.000037, Tokens per sec:   2669
2023-03-14 14:36:07,060 - INFO - __main__ - Epoch  99, Step:  214100, Batch Loss:     1.375751, Lr: 0.000037, Tokens per sec:   2640
2023-03-14 14:36:27,017 - INFO - __main__ - Epoch  99, Step:  214200, Batch Loss:     1.835592, Lr: 0.000037, Tokens per sec:   2708
2023-03-14 14:36:46,592 - INFO - __main__ - Epoch  99, Step:  214300, Batch Loss:     1.194528, Lr: 0.000037, Tokens per sec:   2767
2023-03-14 14:37:05,500 - INFO - __main__ - Epoch  99, Step:  214400, Batch Loss:     0.958215, Lr: 0.000037, Tokens per sec:   2880
2023-03-14 14:37:24,702 - INFO - __main__ - Epoch  99, Step:  214500, Batch Loss:     2.574801, Lr: 0.000037, Tokens per sec:   2819
2023-03-14 14:37:44,461 - INFO - __main__ - Epoch  99, Step:  214600, Batch Loss:     1.952383, Lr: 0.000037, Tokens per sec:   2706
2023-03-14 14:38:04,439 - INFO - __main__ - Epoch  99, Step:  214700, Batch Loss:     1.265869, Lr: 0.000037, Tokens per sec:   2706
2023-03-14 14:38:24,457 - INFO - __main__ - Epoch  99, Step:  214800, Batch Loss:     2.212735, Lr: 0.000037, Tokens per sec:   2710
2023-03-14 14:38:44,409 - INFO - __main__ - Epoch  99, Step:  214900, Batch Loss:     2.418456, Lr: 0.000037, Tokens per sec:   2705
2023-03-14 14:39:04,101 - INFO - __main__ - Epoch  99, Step:  215000, Batch Loss:     1.632691, Lr: 0.000037, Tokens per sec:   2713
2023-03-14 14:39:24,192 - INFO - __main__ - Epoch  99, Step:  215100, Batch Loss:     1.701517, Lr: 0.000037, Tokens per sec:   2704
2023-03-14 14:39:43,727 - INFO - __main__ - Epoch  99, Step:  215200, Batch Loss:     2.852111, Lr: 0.000037, Tokens per sec:   2727
2023-03-14 14:40:03,667 - INFO - __main__ - Epoch  99, Step:  215300, Batch Loss:     2.248159, Lr: 0.000037, Tokens per sec:   2737
2023-03-14 14:40:23,336 - INFO - __main__ - Epoch  99, Step:  215400, Batch Loss:     1.910295, Lr: 0.000037, Tokens per sec:   2733
2023-03-14 14:40:43,026 - INFO - __main__ - Epoch  99, Step:  215500, Batch Loss:     2.676919, Lr: 0.000037, Tokens per sec:   2731
2023-03-14 14:41:02,523 - INFO - __main__ - Epoch  99, Step:  215600, Batch Loss:     2.439924, Lr: 0.000037, Tokens per sec:   2752
2023-03-14 14:41:22,152 - INFO - __main__ - Epoch  99, Step:  215700, Batch Loss:     1.859825, Lr: 0.000037, Tokens per sec:   2697
2023-03-14 14:41:26,413 - INFO - __main__ - Epoch  99: total training loss 4273.73
2023-03-14 14:41:26,415 - INFO - __main__ - Epoch 100
2023-03-14 14:41:41,678 - INFO - __main__ - Epoch 100, Step:  215800, Batch Loss:     2.504781, Lr: 0.000037, Tokens per sec:   2743
2023-03-14 14:42:01,600 - INFO - __main__ - Epoch 100, Step:  215900, Batch Loss:     1.574634, Lr: 0.000037, Tokens per sec:   2700
2023-03-14 14:42:21,664 - INFO - __main__ - Epoch 100, Step:  216000, Batch Loss:     2.024750, Lr: 0.000037, Tokens per sec:   2651
2023-03-14 14:42:41,529 - INFO - __main__ - Epoch 100, Step:  216100, Batch Loss:     1.635106, Lr: 0.000037, Tokens per sec:   2686
2023-03-14 14:43:01,401 - INFO - __main__ - Epoch 100, Step:  216200, Batch Loss:     2.484749, Lr: 0.000037, Tokens per sec:   2737
2023-03-14 14:43:21,420 - INFO - __main__ - Epoch 100, Step:  216300, Batch Loss:     1.812724, Lr: 0.000037, Tokens per sec:   2724
2023-03-14 14:43:41,377 - INFO - __main__ - Epoch 100, Step:  216400, Batch Loss:     2.036705, Lr: 0.000037, Tokens per sec:   2683
2023-03-14 14:44:00,685 - INFO - __main__ - Epoch 100, Step:  216500, Batch Loss:     1.161739, Lr: 0.000037, Tokens per sec:   2797
2023-03-14 14:44:19,715 - INFO - __main__ - Epoch 100, Step:  216600, Batch Loss:     2.521143, Lr: 0.000037, Tokens per sec:   2859
2023-03-14 14:44:39,102 - INFO - __main__ - Epoch 100, Step:  216700, Batch Loss:     1.931936, Lr: 0.000037, Tokens per sec:   2714
2023-03-14 14:44:58,963 - INFO - __main__ - Epoch 100, Step:  216800, Batch Loss:     1.945992, Lr: 0.000037, Tokens per sec:   2740
2023-03-14 14:45:18,304 - INFO - __main__ - Epoch 100, Step:  216900, Batch Loss:     3.516063, Lr: 0.000037, Tokens per sec:   2726
2023-03-14 14:45:37,862 - INFO - __main__ - Epoch 100, Step:  217000, Batch Loss:     1.611841, Lr: 0.000037, Tokens per sec:   2740
2023-03-14 14:45:57,049 - INFO - __main__ - Epoch 100, Step:  217100, Batch Loss:     1.787493, Lr: 0.000037, Tokens per sec:   2774
2023-03-14 14:46:16,983 - INFO - __main__ - Epoch 100, Step:  217200, Batch Loss:     1.507603, Lr: 0.000037, Tokens per sec:   2709
2023-03-14 14:46:36,962 - INFO - __main__ - Epoch 100, Step:  217300, Batch Loss:     1.871394, Lr: 0.000037, Tokens per sec:   2735
2023-03-14 14:46:57,018 - INFO - __main__ - Epoch 100, Step:  217400, Batch Loss:     2.335791, Lr: 0.000037, Tokens per sec:   2707
2023-03-14 14:47:16,472 - INFO - __main__ - Epoch 100, Step:  217500, Batch Loss:     2.771658, Lr: 0.000037, Tokens per sec:   2791
2023-03-14 14:47:35,938 - INFO - __main__ - Epoch 100, Step:  217600, Batch Loss:     1.691506, Lr: 0.000037, Tokens per sec:   2775
2023-03-14 14:47:55,907 - INFO - __main__ - Epoch 100, Step:  217700, Batch Loss:     3.182504, Lr: 0.000037, Tokens per sec:   2722
2023-03-14 14:48:15,904 - INFO - __main__ - Epoch 100, Step:  217800, Batch Loss:     2.092767, Lr: 0.000037, Tokens per sec:   2705
2023-03-14 14:48:34,552 - INFO - __main__ - Epoch 100, Step:  217900, Batch Loss:     0.788303, Lr: 0.000037, Tokens per sec:   2867
2023-03-14 14:48:34,685 - INFO - __main__ - Epoch 100: total training loss 4249.73
2023-03-14 14:52:29,622 - INFO - __main__ - Evaluation result(Greedy Search) Bleu = 45.477967095238334, rouge_l = 57.52003946936877, meteor = 0
2023-03-14 14:52:29,623 - INFO - __main__ - Example #0
2023-03-14 14:52:29,624 - INFO - __main__ - 	Reference: this return the hash code for the key . the hash code be generate from the internal string the key represent .
2023-03-14 14:52:29,624 - INFO - __main__ - 	Hypothesis: return a hash code for this string constant object .
2023-03-14 14:52:29,624 - INFO - __main__ - Example #1
2023-03-14 14:52:29,624 - INFO - __main__ - 	Reference: fire when the request be start , override to handle in your own code
2023-03-14 14:52:29,624 - INFO - __main__ - 	Hypothesis: call when the activity be start .
2023-03-14 14:52:29,624 - INFO - __main__ - Example #2
2023-03-14 14:52:29,624 - INFO - __main__ - 	Reference: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 14:52:29,624 - INFO - __main__ - 	Hypothesis: this method get call when a bind property be change on the associate jtext component . this be a hook which ui implementation may change to reflect how the ui display bind property of jtext component subclass . this be
2023-03-14 14:52:29,634 - INFO - __main__ - Validation time = 231.70599818229675s.
2023-03-14 14:52:29,634 - INFO - __main__ - Training ended after 100 epoches!
2023-03-14 14:52:29,634 - INFO - __main__ - Best Validation result (greedy) at step   213542:  45.75 bleu.
