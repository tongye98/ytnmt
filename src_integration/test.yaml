# relative
data:
    train_data_path: "../datas/codescribe_java/train_data.json"
    valid_data_path: "../datas/codescribe_java/valid_data.json"
    test_data_path: "../datas/codescribe_java/test_data.json"
    dataset_type: "plain"
    code:
        language: "code"
        level: "word" # word bpe
        tokenizer_type: "sentencepiece" # subword-nmt sentencepiece
        vocab_min_freq: 1
        vocab_max_size: 50000
        lowercase: True
        normalize: True # tokernizer pre-process sentence.strip()
        code_token_max_length: 200
        min_length: 1
        filter_or_truncate: "truncate"

    position:
        vocab_min_freq: 1
        vocab_max_size: 50000
        
    text: 
        language: "nl"
        level: "word"
        tokenizer_type: "sentencepiece"
        vocab_min_freq: 1
        vocab_max_size: 30000
        lowercase: True
        normalize: True 
        text_token_max_length: 40
        min_length: 1
        filter_or_truncate: "truncate"

training:
    model_dir: "test_dir_gnn_no_residual"
    overwrite: True

    logging_frequence: 100
    validation_frequence: 200 # after how many epochs
    log_valid_samples: [0,1,2]

    use_cuda: True
    num_workers: 4

    epochs: 200
    shuffle: True 
    max_updates: 10000000
    batch_size: 32
    random_seed: 980820

    load_model: "test_dir_gnn_no_residual/217905.ckpt"
    reset_best_ckpt: False
    reset_scheduler: False
    reset_optimzer: False
    reset_iter_state: False

    learning_rate: 0.0001
    learning_rate_min: 1.0e-8
    # clip_grad_val: 1 
    clip_grad_norm: 5.0
    optimizer: "adam"
    weight_decay: 0
    adam_betas: [0.9, 0.999]
    eps: 1.e-8
    early_stop_metric: "bleu"
    scheduling: "StepLR"   # "ReduceLROnPlateau", "StepLR", "ExponentialLR"
    mode: "max"
    factor: 0.5
    patience: 5
    step_size: 1
    gamma: 0.99

    num_ckpts_keep: 3


testing:
    batch_size: 64
    batch_type: "sentence"
    max_output_length: 40
    min_output_length: 1
    eval_metrics: ['bleu','rouge-l']
    n_best: 1
    beam_size: 4
    beam_alpha: -1
    return_attention: True
    return_probability: True
    generate_unk: False
    repetition_penalty: -1       # >1, -1 means no repetition penalty. # no implemented
    # no_repeat_ngram_size: -1    # no implemented

retrieval:
    hidden_representation_path: "hidden_representation"
    token_map_path: "token_map_path"
    use_code_representation: True
    index_path: "index"
    index_type: "INNER" # INNER OR L2


model:
    initializer: "xavier_uniform"     # xavier_uniform xavier_normal uniform normal
    embed_initializer: "xavier_uniform"
    tied_softmax: False
    tied_embeddings: False

    embeddings:
        embedding_dim: 512
        scale: False
        freeze: False
        dropout: 0.2
        # load_pretrained

    transformer_encoder:
        model_dim: 512
        ff_dim: 2048
        num_layers: 6
        head_count: 8
        layer_norm_position: "pre"
        freeze: False
        dropout: 0.2
        src_pos_emb: "learnable"         # encoder "absolute", "learnable", "relative"
        max_src_len: 400                  # for learnable. Keep same with data segment
        max_relative_position: 0       # only for relative position, else must be set to 0
        use_negative_distance: False     # for relative position

    gnn_encoder:
        gnn_type: "SAGEConv"            # SAGEConv, GCNConv, GATConv
        aggr: "mean"                    # mean, max, lstm
        model_dim: 512
        num_layers: 6
        dropout: 0.2
        
    transformer_decoder:
        model_dim: 512
        ff_dim: 2048
        num_layers: 6
        head_count: 8
        layer_norm_position: "pre"
        freeze: False
        dropout: 0.2
        trg_pos_emb: "learnable"        # encoder "absolute", "learnable","relative"
        max_trg_len: 50                 # for learnable. keep same with data segment
        max_relative_position: 0        # only for relative position, else must be set to 0
        use_negative_distance: False    # for relative position