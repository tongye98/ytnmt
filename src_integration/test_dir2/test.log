2023-03-09 11:53:26,494 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-09 11:54:29,817 - INFO - data - average code tokens = 109.28999515442095
2023-03-09 11:54:29,817 - INFO - data - average ast tokens = 188.85505342888476
2023-03-09 11:54:29,817 - INFO - data - average text tokens = 15.993139680191783
2023-03-09 11:54:29,817 - INFO - data - average position tokens = 188.85505342888476
2023-03-09 11:54:29,817 - INFO - data - average ast edges = 375.7101068577695
2023-03-09 11:54:42,393 - INFO - data - code vocab length = 26684
2023-03-09 11:54:42,393 - INFO - data - text vocab length = 13207
2023-03-09 11:54:42,393 - INFO - data - position vocab length = 20587
2023-03-09 11:54:51,985 - INFO - model - Build Model...
2023-03-09 11:54:52,545 - DEBUG - model - Model(
	Transformer_encoder=TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (2): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (3): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (4): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (5): TransformerEncoderLayer(
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (src_src_attenion): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	GNN_encoder=GNNEncoder(
  (layers): ModuleList(
    (0): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNEncoderLayer(
      (gnn): SAGEConv(512, 512, aggr=mean)
      (relu): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	Transformer_decoder=TransformerDecoder(
  (layers): ModuleList(
    (0): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerDecoderLayer(
      (trg_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (src_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (gnn_trg_attention): MultiHeadedAttention(
        (key_project): Linear(in_features=512, out_features=512, bias=True)
        (query_project): Linear(in_features=512, out_features=512, bias=True)
        (value_project): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (softmax): Softmax(dim=-1)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (feed_forward): PositionwiseFeedForward(
        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pwff): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.2, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.2, inplace=False)
        )
      )
      (dropout): Dropout(p=0.2, inplace=False)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
),
	src_embed=Embeddings(
  (lut): Embedding(26684, 512, padding_idx=1)
),
	code_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(400, 512)
),
	position_embed=Embeddings(
  (lut): Embedding(20587, 512, padding_idx=1)
),
	trg_embed=Embeddings(
  (lut): Embedding(13207, 512, padding_idx=1)
),
	text_learnable_embed=LearnablePositionalEncoding(
  (learn_lut): Embedding(50, 512)
),
	loss_function=XentLoss(
  (criterion): NLLLoss()
))
2023-03-09 11:54:52,550 - INFO - model - Total parameters number: 91563520
2023-03-09 11:54:52,550 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-09 11:54:52,550 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-09 11:54:52,550 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-09 11:54:52,550 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-09 11:54:52,550 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.weight               [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.bias                 [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.weight               [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.bias                 [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.weight               [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.bias                 [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.weight [512, 512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.bias [512]
2023-03-09 11:54:52,551 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.bias [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.bias [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.weight               [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.bias                 [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.bias [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.bias [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.bias [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.bias [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.weight                                 [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): gnn_encoder.layernorm.bias                                   [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,552 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,553 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,554 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-09 11:54:52,555 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-09 11:54:52,556 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-09 11:54:52,556 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-09 11:54:52,556 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-09 11:54:52,556 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-09 11:54:52,556 - INFO - model - The model is built.
2023-03-09 11:54:52,557 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-09 11:54:52,557 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-09 11:54:53,604 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-09 11:54:53,604 - INFO - __main__ - Scheduler = StepLR
2023-03-09 11:54:53,605 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-09 11:54:53,605 - INFO - __main__ - Epoch 1
2023-03-09 11:55:14,389 - INFO - __main__ - Epoch   1, Step:     100, Batch Loss:    90.265327, Lr: 0.000100, Tokens per sec:   2548
2023-03-09 11:55:33,854 - INFO - __main__ - Epoch   1, Step:     200, Batch Loss:    85.495705, Lr: 0.000100, Tokens per sec:   2729
2023-03-09 11:55:53,974 - INFO - __main__ - Epoch   1, Step:     300, Batch Loss:    75.007904, Lr: 0.000100, Tokens per sec:   2731
2023-03-09 11:56:13,924 - INFO - __main__ - Epoch   1, Step:     400, Batch Loss:    72.288429, Lr: 0.000100, Tokens per sec:   2722
2023-03-09 11:56:33,551 - INFO - __main__ - Epoch   1, Step:     500, Batch Loss:    67.453041, Lr: 0.000100, Tokens per sec:   2768
2023-03-09 11:56:53,319 - INFO - __main__ - Epoch   1, Step:     600, Batch Loss:    90.127663, Lr: 0.000100, Tokens per sec:   2726
2023-03-09 11:57:13,183 - INFO - __main__ - Epoch   1, Step:     700, Batch Loss:    78.876282, Lr: 0.000100, Tokens per sec:   2700
2023-03-09 11:57:33,034 - INFO - __main__ - Epoch   1, Step:     800, Batch Loss:    70.684464, Lr: 0.000100, Tokens per sec:   2715
2023-03-09 11:57:53,027 - INFO - __main__ - Epoch   1, Step:     900, Batch Loss:    73.519157, Lr: 0.000100, Tokens per sec:   2733
2023-03-09 11:58:12,384 - INFO - __main__ - Epoch   1, Step:    1000, Batch Loss:    82.011620, Lr: 0.000100, Tokens per sec:   2771
2023-03-09 11:58:32,570 - INFO - __main__ - Epoch   1, Step:    1100, Batch Loss:    80.751694, Lr: 0.000100, Tokens per sec:   2618
2023-03-09 11:58:52,741 - INFO - __main__ - Epoch   1, Step:    1200, Batch Loss:    68.841400, Lr: 0.000100, Tokens per sec:   2663
2023-03-09 11:59:13,038 - INFO - __main__ - Epoch   1, Step:    1300, Batch Loss:    74.228989, Lr: 0.000100, Tokens per sec:   2683
2023-03-09 11:59:33,480 - INFO - __main__ - Epoch   1, Step:    1400, Batch Loss:    76.784431, Lr: 0.000100, Tokens per sec:   2593
2023-03-09 11:59:53,698 - INFO - __main__ - Epoch   1, Step:    1500, Batch Loss:    87.912956, Lr: 0.000100, Tokens per sec:   2642
2023-03-09 12:00:13,776 - INFO - __main__ - Epoch   1, Step:    1600, Batch Loss:    96.816101, Lr: 0.000100, Tokens per sec:   2696
2023-03-09 12:00:33,946 - INFO - __main__ - Epoch   1, Step:    1700, Batch Loss:    87.475441, Lr: 0.000100, Tokens per sec:   2713
2023-03-09 12:00:54,043 - INFO - __main__ - Epoch   1, Step:    1800, Batch Loss:    86.696678, Lr: 0.000100, Tokens per sec:   2697
2023-03-09 12:01:14,387 - INFO - __main__ - Epoch   1, Step:    1900, Batch Loss:    90.260445, Lr: 0.000100, Tokens per sec:   2646
2023-03-09 12:01:34,667 - INFO - __main__ - Epoch   1, Step:    2000, Batch Loss:    70.672516, Lr: 0.000100, Tokens per sec:   2665
2023-03-09 12:01:54,818 - INFO - __main__ - Epoch   1, Step:    2100, Batch Loss:    71.961357, Lr: 0.000100, Tokens per sec:   2648
2023-03-09 12:02:10,777 - INFO - __main__ - Epoch   1: total training loss 171065.69
2023-03-09 12:02:10,778 - INFO - __main__ - Epoch 2
2023-03-09 12:02:15,232 - INFO - __main__ - Epoch   2, Step:    2200, Batch Loss:    71.177505, Lr: 0.000099, Tokens per sec:   2516
2023-03-09 12:02:35,612 - INFO - __main__ - Epoch   2, Step:    2300, Batch Loss:    63.154514, Lr: 0.000099, Tokens per sec:   2627
2023-03-09 12:02:55,770 - INFO - __main__ - Epoch   2, Step:    2400, Batch Loss:    61.430855, Lr: 0.000099, Tokens per sec:   2624
2023-03-09 12:03:16,012 - INFO - __main__ - Epoch   2, Step:    2500, Batch Loss:    49.790493, Lr: 0.000099, Tokens per sec:   2697
2023-03-09 12:03:36,386 - INFO - __main__ - Epoch   2, Step:    2600, Batch Loss:    62.452011, Lr: 0.000099, Tokens per sec:   2653
2023-03-09 12:03:56,638 - INFO - __main__ - Epoch   2, Step:    2700, Batch Loss:    58.629597, Lr: 0.000099, Tokens per sec:   2653
2023-03-09 12:04:17,002 - INFO - __main__ - Epoch   2, Step:    2800, Batch Loss:    72.944550, Lr: 0.000099, Tokens per sec:   2664
2023-03-09 12:04:37,198 - INFO - __main__ - Epoch   2, Step:    2900, Batch Loss:    71.507881, Lr: 0.000099, Tokens per sec:   2668
2023-03-09 12:04:57,458 - INFO - __main__ - Epoch   2, Step:    3000, Batch Loss:    67.422340, Lr: 0.000099, Tokens per sec:   2658
2023-03-09 12:05:17,835 - INFO - __main__ - Epoch   2, Step:    3100, Batch Loss:    57.393925, Lr: 0.000099, Tokens per sec:   2646
2023-03-09 12:05:37,891 - INFO - __main__ - Epoch   2, Step:    3200, Batch Loss:    52.809723, Lr: 0.000099, Tokens per sec:   2702
2023-03-09 12:05:58,267 - INFO - __main__ - Epoch   2, Step:    3300, Batch Loss:    59.538177, Lr: 0.000099, Tokens per sec:   2613
2023-03-09 12:06:18,617 - INFO - __main__ - Epoch   2, Step:    3400, Batch Loss:    64.317802, Lr: 0.000099, Tokens per sec:   2642
2023-03-09 12:06:38,923 - INFO - __main__ - Epoch   2, Step:    3500, Batch Loss:    61.950855, Lr: 0.000099, Tokens per sec:   2646
2023-03-09 12:06:59,169 - INFO - __main__ - Epoch   2, Step:    3600, Batch Loss:    49.753941, Lr: 0.000099, Tokens per sec:   2657
2023-03-09 12:07:19,520 - INFO - __main__ - Epoch   2, Step:    3700, Batch Loss:    58.615730, Lr: 0.000099, Tokens per sec:   2708
2023-03-09 12:07:39,882 - INFO - __main__ - Epoch   2, Step:    3800, Batch Loss:    77.980743, Lr: 0.000099, Tokens per sec:   2634
2023-03-09 12:07:59,930 - INFO - __main__ - Epoch   2, Step:    3900, Batch Loss:    56.004578, Lr: 0.000099, Tokens per sec:   2713
2023-03-09 12:08:19,983 - INFO - __main__ - Epoch   2, Step:    4000, Batch Loss:    53.935108, Lr: 0.000099, Tokens per sec:   2683
2023-03-09 12:08:40,232 - INFO - __main__ - Epoch   2, Step:    4100, Batch Loss:    50.446152, Lr: 0.000099, Tokens per sec:   2587
2023-03-09 12:09:00,528 - INFO - __main__ - Epoch   2, Step:    4200, Batch Loss:    67.542809, Lr: 0.000099, Tokens per sec:   2625
2023-03-09 12:09:20,569 - INFO - __main__ - Epoch   2, Step:    4300, Batch Loss:    79.532516, Lr: 0.000099, Tokens per sec:   2716
2023-03-09 12:09:32,207 - INFO - __main__ - Epoch   2: total training loss 139105.77
2023-03-09 12:09:32,208 - INFO - __main__ - Epoch 3
2023-03-09 12:09:41,042 - INFO - __main__ - Epoch   3, Step:    4400, Batch Loss:    69.982956, Lr: 0.000098, Tokens per sec:   2586
2023-03-09 12:10:01,184 - INFO - __main__ - Epoch   3, Step:    4500, Batch Loss:    63.256783, Lr: 0.000098, Tokens per sec:   2671
2023-03-09 12:10:21,482 - INFO - __main__ - Epoch   3, Step:    4600, Batch Loss:    46.064308, Lr: 0.000098, Tokens per sec:   2635
2023-03-09 12:10:41,702 - INFO - __main__ - Epoch   3, Step:    4700, Batch Loss:    53.339970, Lr: 0.000098, Tokens per sec:   2699
2023-03-09 12:11:01,875 - INFO - __main__ - Epoch   3, Step:    4800, Batch Loss:    69.702019, Lr: 0.000098, Tokens per sec:   2704
2023-03-09 12:11:22,096 - INFO - __main__ - Epoch   3, Step:    4900, Batch Loss:    54.401276, Lr: 0.000098, Tokens per sec:   2645
2023-03-09 12:11:42,505 - INFO - __main__ - Epoch   3, Step:    5000, Batch Loss:    44.030220, Lr: 0.000098, Tokens per sec:   2644
2023-03-09 12:12:02,852 - INFO - __main__ - Epoch   3, Step:    5100, Batch Loss:    60.398739, Lr: 0.000098, Tokens per sec:   2648
2023-03-09 12:12:23,069 - INFO - __main__ - Epoch   3, Step:    5200, Batch Loss:    58.343300, Lr: 0.000098, Tokens per sec:   2686
2023-03-09 12:12:43,199 - INFO - __main__ - Epoch   3, Step:    5300, Batch Loss:    54.340801, Lr: 0.000098, Tokens per sec:   2638
2023-03-09 12:13:03,419 - INFO - __main__ - Epoch   3, Step:    5400, Batch Loss:    55.005295, Lr: 0.000098, Tokens per sec:   2662
2023-03-09 12:13:23,738 - INFO - __main__ - Epoch   3, Step:    5500, Batch Loss:    68.556084, Lr: 0.000098, Tokens per sec:   2605
2023-03-09 12:13:43,849 - INFO - __main__ - Epoch   3, Step:    5600, Batch Loss:    52.215012, Lr: 0.000098, Tokens per sec:   2679
2023-03-09 12:14:03,968 - INFO - __main__ - Epoch   3, Step:    5700, Batch Loss:    43.031490, Lr: 0.000098, Tokens per sec:   2693
2023-03-09 12:14:24,190 - INFO - __main__ - Epoch   3, Step:    5800, Batch Loss:    72.437004, Lr: 0.000098, Tokens per sec:   2640
2023-03-09 12:14:44,364 - INFO - __main__ - Epoch   3, Step:    5900, Batch Loss:    58.813465, Lr: 0.000098, Tokens per sec:   2665
2023-03-09 12:15:04,471 - INFO - __main__ - Epoch   3, Step:    6000, Batch Loss:    60.517689, Lr: 0.000098, Tokens per sec:   2704
2023-03-09 12:15:24,614 - INFO - __main__ - Epoch   3, Step:    6100, Batch Loss:    43.434196, Lr: 0.000098, Tokens per sec:   2652
2023-03-09 12:15:44,844 - INFO - __main__ - Epoch   3, Step:    6200, Batch Loss:    55.229134, Lr: 0.000098, Tokens per sec:   2680
2023-03-09 12:16:05,016 - INFO - __main__ - Epoch   3, Step:    6300, Batch Loss:    62.363922, Lr: 0.000098, Tokens per sec:   2663
2023-03-09 12:16:25,394 - INFO - __main__ - Epoch   3, Step:    6400, Batch Loss:    48.142445, Lr: 0.000098, Tokens per sec:   2670
2023-03-09 12:16:45,513 - INFO - __main__ - Epoch   3, Step:    6500, Batch Loss:    60.954342, Lr: 0.000098, Tokens per sec:   2662
2023-03-09 12:16:53,156 - INFO - __main__ - Epoch   3: total training loss 124335.97
2023-03-09 12:16:53,158 - INFO - __main__ - Epoch 4
2023-03-09 12:17:06,234 - INFO - __main__ - Epoch   4, Step:    6600, Batch Loss:    44.244972, Lr: 0.000097, Tokens per sec:   2635
2023-03-09 12:17:25,984 - INFO - __main__ - Epoch   4, Step:    6700, Batch Loss:    58.113949, Lr: 0.000097, Tokens per sec:   2714
2023-03-09 12:17:46,126 - INFO - __main__ - Epoch   4, Step:    6800, Batch Loss:    54.844589, Lr: 0.000097, Tokens per sec:   2695
2023-03-09 12:18:06,420 - INFO - __main__ - Epoch   4, Step:    6900, Batch Loss:    54.507000, Lr: 0.000097, Tokens per sec:   2665
2023-03-09 12:18:26,604 - INFO - __main__ - Epoch   4, Step:    7000, Batch Loss:    53.821522, Lr: 0.000097, Tokens per sec:   2632
2023-03-09 12:18:46,753 - INFO - __main__ - Epoch   4, Step:    7100, Batch Loss:    63.457207, Lr: 0.000097, Tokens per sec:   2724
2023-03-09 12:19:07,007 - INFO - __main__ - Epoch   4, Step:    7200, Batch Loss:    47.245388, Lr: 0.000097, Tokens per sec:   2615
2023-03-09 12:19:27,151 - INFO - __main__ - Epoch   4, Step:    7300, Batch Loss:    58.104198, Lr: 0.000097, Tokens per sec:   2706
2023-03-09 12:19:47,294 - INFO - __main__ - Epoch   4, Step:    7400, Batch Loss:    53.942154, Lr: 0.000097, Tokens per sec:   2660
2023-03-09 12:20:07,587 - INFO - __main__ - Epoch   4, Step:    7500, Batch Loss:    47.289165, Lr: 0.000097, Tokens per sec:   2704
2023-03-09 12:20:27,792 - INFO - __main__ - Epoch   4, Step:    7600, Batch Loss:    57.745201, Lr: 0.000097, Tokens per sec:   2688
2023-03-09 12:20:48,168 - INFO - __main__ - Epoch   4, Step:    7700, Batch Loss:    49.645401, Lr: 0.000097, Tokens per sec:   2593
2023-03-09 12:21:08,507 - INFO - __main__ - Epoch   4, Step:    7800, Batch Loss:    51.185162, Lr: 0.000097, Tokens per sec:   2628
2023-03-09 12:21:28,613 - INFO - __main__ - Epoch   4, Step:    7900, Batch Loss:    62.887726, Lr: 0.000097, Tokens per sec:   2654
2023-03-09 12:21:48,788 - INFO - __main__ - Epoch   4, Step:    8000, Batch Loss:    57.506973, Lr: 0.000097, Tokens per sec:   2667
2023-03-09 12:22:08,604 - INFO - __main__ - Epoch   4, Step:    8100, Batch Loss:    56.202610, Lr: 0.000097, Tokens per sec:   2732
2023-03-09 12:22:28,610 - INFO - __main__ - Epoch   4, Step:    8200, Batch Loss:    43.152794, Lr: 0.000097, Tokens per sec:   2667
2023-03-09 12:22:48,121 - INFO - __main__ - Epoch   4, Step:    8300, Batch Loss:    41.024284, Lr: 0.000097, Tokens per sec:   2746
2023-03-09 12:23:08,178 - INFO - __main__ - Epoch   4, Step:    8400, Batch Loss:    46.504093, Lr: 0.000097, Tokens per sec:   2674
2023-03-09 12:23:28,169 - INFO - __main__ - Epoch   4, Step:    8500, Batch Loss:    52.509281, Lr: 0.000097, Tokens per sec:   2736
2023-03-09 12:23:48,261 - INFO - __main__ - Epoch   4, Step:    8600, Batch Loss:    40.016987, Lr: 0.000097, Tokens per sec:   2640
2023-03-09 12:24:08,091 - INFO - __main__ - Epoch   4, Step:    8700, Batch Loss:    42.554546, Lr: 0.000097, Tokens per sec:   2723
2023-03-09 12:24:11,293 - INFO - __main__ - Epoch   4: total training loss 113532.89
2023-03-09 12:24:11,294 - INFO - __main__ - Epoch 5
2023-03-09 12:24:27,975 - INFO - __main__ - Epoch   5, Step:    8800, Batch Loss:    48.568825, Lr: 0.000096, Tokens per sec:   2690
2023-03-09 12:24:47,891 - INFO - __main__ - Epoch   5, Step:    8900, Batch Loss:    55.432476, Lr: 0.000096, Tokens per sec:   2726
2023-03-09 12:25:07,815 - INFO - __main__ - Epoch   5, Step:    9000, Batch Loss:    49.523529, Lr: 0.000096, Tokens per sec:   2709
2023-03-09 12:25:27,676 - INFO - __main__ - Epoch   5, Step:    9100, Batch Loss:    42.005375, Lr: 0.000096, Tokens per sec:   2662
2023-03-09 12:25:47,656 - INFO - __main__ - Epoch   5, Step:    9200, Batch Loss:    57.059071, Lr: 0.000096, Tokens per sec:   2726
2023-03-09 12:26:07,633 - INFO - __main__ - Epoch   5, Step:    9300, Batch Loss:    43.001503, Lr: 0.000096, Tokens per sec:   2659
2023-03-09 12:26:27,515 - INFO - __main__ - Epoch   5, Step:    9400, Batch Loss:    40.734192, Lr: 0.000096, Tokens per sec:   2702
2023-03-09 12:26:47,394 - INFO - __main__ - Epoch   5, Step:    9500, Batch Loss:    41.961296, Lr: 0.000096, Tokens per sec:   2680
2023-03-09 12:27:06,584 - INFO - __main__ - Epoch   5, Step:    9600, Batch Loss:    64.217049, Lr: 0.000096, Tokens per sec:   2778
2023-03-09 12:27:26,424 - INFO - __main__ - Epoch   5, Step:    9700, Batch Loss:    50.431381, Lr: 0.000096, Tokens per sec:   2763
2023-03-09 12:27:46,411 - INFO - __main__ - Epoch   5, Step:    9800, Batch Loss:    36.447685, Lr: 0.000096, Tokens per sec:   2692
2023-03-09 12:28:05,363 - INFO - __main__ - Epoch   5, Step:    9900, Batch Loss:    50.563198, Lr: 0.000096, Tokens per sec:   2792
2023-03-09 12:28:25,288 - INFO - __main__ - Epoch   5, Step:   10000, Batch Loss:    48.368374, Lr: 0.000096, Tokens per sec:   2716
2023-03-09 12:28:45,086 - INFO - __main__ - Epoch   5, Step:   10100, Batch Loss:    54.363731, Lr: 0.000096, Tokens per sec:   2756
2023-03-09 12:29:04,910 - INFO - __main__ - Epoch   5, Step:   10200, Batch Loss:    50.158901, Lr: 0.000096, Tokens per sec:   2735
2023-03-09 12:29:24,877 - INFO - __main__ - Epoch   5, Step:   10300, Batch Loss:    47.715942, Lr: 0.000096, Tokens per sec:   2716
2023-03-09 12:29:44,559 - INFO - __main__ - Epoch   5, Step:   10400, Batch Loss:    55.131790, Lr: 0.000096, Tokens per sec:   2740
2023-03-09 12:30:04,363 - INFO - __main__ - Epoch   5, Step:   10500, Batch Loss:    49.695328, Lr: 0.000096, Tokens per sec:   2746
2023-03-09 12:30:23,978 - INFO - __main__ - Epoch   5, Step:   10600, Batch Loss:    36.112469, Lr: 0.000096, Tokens per sec:   2760
2023-03-09 12:30:43,036 - INFO - __main__ - Epoch   5, Step:   10700, Batch Loss:    46.310383, Lr: 0.000096, Tokens per sec:   2805
2023-03-09 12:31:02,607 - INFO - __main__ - Epoch   5, Step:   10800, Batch Loss:    38.777332, Lr: 0.000096, Tokens per sec:   2760
2023-03-09 12:31:21,560 - INFO - __main__ - Epoch   5: total training loss 104642.70
2023-03-09 12:31:21,561 - INFO - __main__ - Epoch 6
2023-03-09 12:31:22,820 - INFO - __main__ - Epoch   6, Step:   10900, Batch Loss:    51.393410, Lr: 0.000095, Tokens per sec:   2396
2023-03-09 12:31:42,387 - INFO - __main__ - Epoch   6, Step:   11000, Batch Loss:    31.618137, Lr: 0.000095, Tokens per sec:   2799
2023-03-09 12:32:01,403 - INFO - __main__ - Epoch   6, Step:   11100, Batch Loss:    50.529209, Lr: 0.000095, Tokens per sec:   2858
2023-03-09 12:32:20,720 - INFO - __main__ - Epoch   6, Step:   11200, Batch Loss:    41.768024, Lr: 0.000095, Tokens per sec:   2788
2023-03-09 12:32:40,485 - INFO - __main__ - Epoch   6, Step:   11300, Batch Loss:    62.359184, Lr: 0.000095, Tokens per sec:   2719
2023-03-09 12:32:59,579 - INFO - __main__ - Epoch   6, Step:   11400, Batch Loss:    34.650444, Lr: 0.000095, Tokens per sec:   2829
2023-03-09 12:33:18,727 - INFO - __main__ - Epoch   6, Step:   11500, Batch Loss:    32.526093, Lr: 0.000095, Tokens per sec:   2800
2023-03-09 12:33:38,568 - INFO - __main__ - Epoch   6, Step:   11600, Batch Loss:    37.025112, Lr: 0.000095, Tokens per sec:   2678
2023-03-09 12:33:58,544 - INFO - __main__ - Epoch   6, Step:   11700, Batch Loss:    47.982170, Lr: 0.000095, Tokens per sec:   2685
2023-03-09 12:34:18,718 - INFO - __main__ - Epoch   6, Step:   11800, Batch Loss:    36.199402, Lr: 0.000095, Tokens per sec:   2649
2023-03-09 12:34:38,275 - INFO - __main__ - Epoch   6, Step:   11900, Batch Loss:    47.039291, Lr: 0.000095, Tokens per sec:   2735
2023-03-09 12:34:58,089 - INFO - __main__ - Epoch   6, Step:   12000, Batch Loss:    48.566898, Lr: 0.000095, Tokens per sec:   2713
2023-03-09 12:35:17,258 - INFO - __main__ - Epoch   6, Step:   12100, Batch Loss:    40.288486, Lr: 0.000095, Tokens per sec:   2795
2023-03-09 12:35:36,732 - INFO - __main__ - Epoch   6, Step:   12200, Batch Loss:    34.983887, Lr: 0.000095, Tokens per sec:   2782
2023-03-09 12:35:55,593 - INFO - __main__ - Epoch   6, Step:   12300, Batch Loss:    55.383003, Lr: 0.000095, Tokens per sec:   2907
2023-03-09 12:36:15,370 - INFO - __main__ - Epoch   6, Step:   12400, Batch Loss:    36.776299, Lr: 0.000095, Tokens per sec:   2719
2023-03-09 12:36:35,301 - INFO - __main__ - Epoch   6, Step:   12500, Batch Loss:    40.662212, Lr: 0.000095, Tokens per sec:   2673
2023-03-09 12:36:55,018 - INFO - __main__ - Epoch   6, Step:   12600, Batch Loss:    38.347900, Lr: 0.000095, Tokens per sec:   2745
2023-03-09 12:37:15,097 - INFO - __main__ - Epoch   6, Step:   12700, Batch Loss:    53.432812, Lr: 0.000095, Tokens per sec:   2678
2023-03-09 12:37:34,563 - INFO - __main__ - Epoch   6, Step:   12800, Batch Loss:    48.185043, Lr: 0.000095, Tokens per sec:   2746
2023-03-09 12:37:54,638 - INFO - __main__ - Epoch   6, Step:   12900, Batch Loss:    38.996655, Lr: 0.000095, Tokens per sec:   2659
2023-03-09 12:38:13,958 - INFO - __main__ - Epoch   6, Step:   13000, Batch Loss:    44.410904, Lr: 0.000095, Tokens per sec:   2803
2023-03-09 12:38:28,799 - INFO - __main__ - Epoch   6: total training loss 96985.48
2023-03-09 12:38:28,800 - INFO - __main__ - Epoch 7
2023-03-09 12:38:34,005 - INFO - __main__ - Epoch   7, Step:   13100, Batch Loss:    34.805389, Lr: 0.000094, Tokens per sec:   2776
2023-03-09 12:38:53,829 - INFO - __main__ - Epoch   7, Step:   13200, Batch Loss:    42.072800, Lr: 0.000094, Tokens per sec:   2709
2023-03-09 12:39:13,079 - INFO - __main__ - Epoch   7, Step:   13300, Batch Loss:    47.697773, Lr: 0.000094, Tokens per sec:   2829
2023-03-09 12:39:31,768 - INFO - __main__ - Epoch   7, Step:   13400, Batch Loss:    46.912563, Lr: 0.000094, Tokens per sec:   2859
2023-03-09 12:39:51,563 - INFO - __main__ - Epoch   7, Step:   13500, Batch Loss:    39.874847, Lr: 0.000094, Tokens per sec:   2725
2023-03-09 12:40:10,540 - INFO - __main__ - Epoch   7, Step:   13600, Batch Loss:    32.997356, Lr: 0.000094, Tokens per sec:   2814
2023-03-09 12:40:30,531 - INFO - __main__ - Epoch   7, Step:   13700, Batch Loss:    36.933128, Lr: 0.000094, Tokens per sec:   2662
2023-03-09 12:40:50,332 - INFO - __main__ - Epoch   7, Step:   13800, Batch Loss:    53.751377, Lr: 0.000094, Tokens per sec:   2675
2023-03-09 12:41:10,339 - INFO - __main__ - Epoch   7, Step:   13900, Batch Loss:    37.239025, Lr: 0.000094, Tokens per sec:   2722
2023-03-09 12:41:30,137 - INFO - __main__ - Epoch   7, Step:   14000, Batch Loss:    47.500011, Lr: 0.000094, Tokens per sec:   2743
2023-03-09 12:41:50,114 - INFO - __main__ - Epoch   7, Step:   14100, Batch Loss:    40.197273, Lr: 0.000094, Tokens per sec:   2690
2023-03-09 12:42:09,818 - INFO - __main__ - Epoch   7, Step:   14200, Batch Loss:    30.520409, Lr: 0.000094, Tokens per sec:   2762
2023-03-09 12:42:29,334 - INFO - __main__ - Epoch   7, Step:   14300, Batch Loss:    39.173553, Lr: 0.000094, Tokens per sec:   2755
2023-03-09 12:42:49,238 - INFO - __main__ - Epoch   7, Step:   14400, Batch Loss:    46.623768, Lr: 0.000094, Tokens per sec:   2731
2023-03-09 12:43:09,244 - INFO - __main__ - Epoch   7, Step:   14500, Batch Loss:    47.104019, Lr: 0.000094, Tokens per sec:   2734
2023-03-09 12:43:29,176 - INFO - __main__ - Epoch   7, Step:   14600, Batch Loss:    46.246735, Lr: 0.000094, Tokens per sec:   2702
2023-03-09 12:43:48,889 - INFO - __main__ - Epoch   7, Step:   14700, Batch Loss:    40.195049, Lr: 0.000094, Tokens per sec:   2716
2023-03-09 12:44:08,742 - INFO - __main__ - Epoch   7, Step:   14800, Batch Loss:    43.458092, Lr: 0.000094, Tokens per sec:   2702
2023-03-09 12:44:28,593 - INFO - __main__ - Epoch   7, Step:   14900, Batch Loss:    42.304836, Lr: 0.000094, Tokens per sec:   2701
2023-03-09 12:44:47,408 - INFO - __main__ - Epoch   7, Step:   15000, Batch Loss:    36.973164, Lr: 0.000094, Tokens per sec:   2838
2023-03-09 12:45:07,121 - INFO - __main__ - Epoch   7, Step:   15100, Batch Loss:    30.550709, Lr: 0.000094, Tokens per sec:   2706
2023-03-09 12:45:26,941 - INFO - __main__ - Epoch   7, Step:   15200, Batch Loss:    40.916885, Lr: 0.000094, Tokens per sec:   2689
2023-03-09 12:45:37,107 - INFO - __main__ - Epoch   7: total training loss 90222.29
2023-03-09 12:45:37,108 - INFO - __main__ - Epoch 8
2023-03-09 12:45:46,730 - INFO - __main__ - Epoch   8, Step:   15300, Batch Loss:    43.441303, Lr: 0.000093, Tokens per sec:   2577
2023-03-09 12:46:06,396 - INFO - __main__ - Epoch   8, Step:   15400, Batch Loss:    41.920509, Lr: 0.000093, Tokens per sec:   2781
2023-03-09 12:46:26,283 - INFO - __main__ - Epoch   8, Step:   15500, Batch Loss:    54.194546, Lr: 0.000093, Tokens per sec:   2707
2023-03-09 12:46:45,879 - INFO - __main__ - Epoch   8, Step:   15600, Batch Loss:    43.878273, Lr: 0.000093, Tokens per sec:   2777
2023-03-09 12:47:05,720 - INFO - __main__ - Epoch   8, Step:   15700, Batch Loss:    39.786026, Lr: 0.000093, Tokens per sec:   2743
2023-03-09 12:47:25,600 - INFO - __main__ - Epoch   8, Step:   15800, Batch Loss:    31.234713, Lr: 0.000093, Tokens per sec:   2699
2023-03-09 12:47:44,332 - INFO - __main__ - Epoch   8, Step:   15900, Batch Loss:    53.791492, Lr: 0.000093, Tokens per sec:   2871
2023-03-09 12:48:03,820 - INFO - __main__ - Epoch   8, Step:   16000, Batch Loss:    37.635044, Lr: 0.000093, Tokens per sec:   2730
2023-03-09 12:48:23,507 - INFO - __main__ - Epoch   8, Step:   16100, Batch Loss:    41.079723, Lr: 0.000093, Tokens per sec:   2748
2023-03-09 12:48:43,152 - INFO - __main__ - Epoch   8, Step:   16200, Batch Loss:    39.872681, Lr: 0.000093, Tokens per sec:   2762
2023-03-09 12:49:03,135 - INFO - __main__ - Epoch   8, Step:   16300, Batch Loss:    36.563911, Lr: 0.000093, Tokens per sec:   2645
2023-03-09 12:49:23,017 - INFO - __main__ - Epoch   8, Step:   16400, Batch Loss:    30.753794, Lr: 0.000093, Tokens per sec:   2731
2023-03-09 12:49:42,899 - INFO - __main__ - Epoch   8, Step:   16500, Batch Loss:    45.131615, Lr: 0.000093, Tokens per sec:   2718
2023-03-09 12:50:03,002 - INFO - __main__ - Epoch   8, Step:   16600, Batch Loss:    44.452274, Lr: 0.000093, Tokens per sec:   2666
2023-03-09 12:50:22,862 - INFO - __main__ - Epoch   8, Step:   16700, Batch Loss:    46.244827, Lr: 0.000093, Tokens per sec:   2668
2023-03-09 12:50:42,683 - INFO - __main__ - Epoch   8, Step:   16800, Batch Loss:    40.980473, Lr: 0.000093, Tokens per sec:   2724
2023-03-09 12:51:02,530 - INFO - __main__ - Epoch   8, Step:   16900, Batch Loss:    31.499716, Lr: 0.000093, Tokens per sec:   2717
2023-03-09 12:51:22,434 - INFO - __main__ - Epoch   8, Step:   17000, Batch Loss:    45.832012, Lr: 0.000093, Tokens per sec:   2728
2023-03-09 12:51:42,330 - INFO - __main__ - Epoch   8, Step:   17100, Batch Loss:    51.298012, Lr: 0.000093, Tokens per sec:   2686
2023-03-09 12:52:02,321 - INFO - __main__ - Epoch   8, Step:   17200, Batch Loss:    34.721558, Lr: 0.000093, Tokens per sec:   2701
2023-03-09 12:52:21,443 - INFO - __main__ - Epoch   8, Step:   17300, Batch Loss:    42.393311, Lr: 0.000093, Tokens per sec:   2788
2023-03-09 12:52:40,914 - INFO - __main__ - Epoch   8, Step:   17400, Batch Loss:    46.232117, Lr: 0.000093, Tokens per sec:   2787
2023-03-09 12:52:47,202 - INFO - __main__ - Epoch   8: total training loss 84164.83
2023-03-09 12:52:47,203 - INFO - __main__ - Epoch 9
2023-03-09 12:53:00,949 - INFO - __main__ - Epoch   9, Step:   17500, Batch Loss:    36.641121, Lr: 0.000092, Tokens per sec:   2647
2023-03-09 12:53:20,580 - INFO - __main__ - Epoch   9, Step:   17600, Batch Loss:    30.065390, Lr: 0.000092, Tokens per sec:   2739
2023-03-09 12:53:40,480 - INFO - __main__ - Epoch   9, Step:   17700, Batch Loss:    33.333416, Lr: 0.000092, Tokens per sec:   2742
2023-03-09 12:53:59,860 - INFO - __main__ - Epoch   9, Step:   17800, Batch Loss:    25.526243, Lr: 0.000092, Tokens per sec:   2800
2023-03-09 12:54:19,802 - INFO - __main__ - Epoch   9, Step:   17900, Batch Loss:    33.167847, Lr: 0.000092, Tokens per sec:   2678
2023-03-09 12:54:39,609 - INFO - __main__ - Epoch   9, Step:   18000, Batch Loss:    34.115692, Lr: 0.000092, Tokens per sec:   2712
2023-03-09 12:54:59,605 - INFO - __main__ - Epoch   9, Step:   18100, Batch Loss:    42.505833, Lr: 0.000092, Tokens per sec:   2706
2023-03-09 12:55:19,028 - INFO - __main__ - Epoch   9, Step:   18200, Batch Loss:    25.062122, Lr: 0.000092, Tokens per sec:   2843
2023-03-09 12:55:38,982 - INFO - __main__ - Epoch   9, Step:   18300, Batch Loss:    21.271990, Lr: 0.000092, Tokens per sec:   2697
2023-03-09 12:55:58,672 - INFO - __main__ - Epoch   9, Step:   18400, Batch Loss:    41.896832, Lr: 0.000092, Tokens per sec:   2713
2023-03-09 12:56:18,827 - INFO - __main__ - Epoch   9, Step:   18500, Batch Loss:    41.781807, Lr: 0.000092, Tokens per sec:   2687
2023-03-09 12:56:38,881 - INFO - __main__ - Epoch   9, Step:   18600, Batch Loss:    52.537182, Lr: 0.000092, Tokens per sec:   2680
2023-03-09 12:56:58,829 - INFO - __main__ - Epoch   9, Step:   18700, Batch Loss:    21.818638, Lr: 0.000092, Tokens per sec:   2714
2023-03-09 12:57:18,251 - INFO - __main__ - Epoch   9, Step:   18800, Batch Loss:    42.320473, Lr: 0.000092, Tokens per sec:   2758
2023-03-09 12:57:37,738 - INFO - __main__ - Epoch   9, Step:   18900, Batch Loss:    37.443916, Lr: 0.000092, Tokens per sec:   2751
2023-03-09 12:57:57,463 - INFO - __main__ - Epoch   9, Step:   19000, Batch Loss:    41.403854, Lr: 0.000092, Tokens per sec:   2722
2023-03-09 12:58:17,387 - INFO - __main__ - Epoch   9, Step:   19100, Batch Loss:    39.865879, Lr: 0.000092, Tokens per sec:   2727
2023-03-09 12:58:37,122 - INFO - __main__ - Epoch   9, Step:   19200, Batch Loss:    40.557255, Lr: 0.000092, Tokens per sec:   2708
2023-03-09 12:58:56,935 - INFO - __main__ - Epoch   9, Step:   19300, Batch Loss:    27.738003, Lr: 0.000092, Tokens per sec:   2681
2023-03-09 12:59:16,852 - INFO - __main__ - Epoch   9, Step:   19400, Batch Loss:    34.016197, Lr: 0.000092, Tokens per sec:   2718
2023-03-09 12:59:36,782 - INFO - __main__ - Epoch   9, Step:   19500, Batch Loss:    31.935387, Lr: 0.000092, Tokens per sec:   2669
2023-03-09 12:59:56,853 - INFO - __main__ - Epoch   9, Step:   19600, Batch Loss:    32.316902, Lr: 0.000092, Tokens per sec:   2664
2023-03-09 12:59:59,089 - INFO - __main__ - Epoch   9: total training loss 78611.18
2023-03-09 12:59:59,089 - INFO - __main__ - Epoch 10
2023-03-09 13:00:16,273 - INFO - __main__ - Epoch  10, Step:   19700, Batch Loss:    33.412193, Lr: 0.000091, Tokens per sec:   2783
2023-03-09 13:00:35,732 - INFO - __main__ - Epoch  10, Step:   19800, Batch Loss:    37.577385, Lr: 0.000091, Tokens per sec:   2789
2023-03-09 13:00:55,660 - INFO - __main__ - Epoch  10, Step:   19900, Batch Loss:    29.898655, Lr: 0.000091, Tokens per sec:   2751
2023-03-09 13:01:15,352 - INFO - __main__ - Epoch  10, Step:   20000, Batch Loss:    29.894400, Lr: 0.000091, Tokens per sec:   2700
2023-03-09 13:01:35,310 - INFO - __main__ - Epoch  10, Step:   20100, Batch Loss:    35.666435, Lr: 0.000091, Tokens per sec:   2731
2023-03-09 13:01:54,939 - INFO - __main__ - Epoch  10, Step:   20200, Batch Loss:    36.321980, Lr: 0.000091, Tokens per sec:   2762
2023-03-09 13:02:14,697 - INFO - __main__ - Epoch  10, Step:   20300, Batch Loss:    38.674805, Lr: 0.000091, Tokens per sec:   2747
2023-03-09 13:02:34,738 - INFO - __main__ - Epoch  10, Step:   20400, Batch Loss:    33.663326, Lr: 0.000091, Tokens per sec:   2634
2023-03-09 13:02:54,631 - INFO - __main__ - Epoch  10, Step:   20500, Batch Loss:    22.936871, Lr: 0.000091, Tokens per sec:   2729
2023-03-09 13:03:14,663 - INFO - __main__ - Epoch  10, Step:   20600, Batch Loss:    41.738407, Lr: 0.000091, Tokens per sec:   2716
2023-03-09 13:03:34,631 - INFO - __main__ - Epoch  10, Step:   20700, Batch Loss:    43.121822, Lr: 0.000091, Tokens per sec:   2677
2023-03-09 13:03:54,462 - INFO - __main__ - Epoch  10, Step:   20800, Batch Loss:    42.684315, Lr: 0.000091, Tokens per sec:   2692
2023-03-09 13:04:14,398 - INFO - __main__ - Epoch  10, Step:   20900, Batch Loss:    31.044127, Lr: 0.000091, Tokens per sec:   2686
2023-03-09 13:04:33,520 - INFO - __main__ - Epoch  10, Step:   21000, Batch Loss:    30.198135, Lr: 0.000091, Tokens per sec:   2839
2023-03-09 13:04:53,465 - INFO - __main__ - Epoch  10, Step:   21100, Batch Loss:    39.255291, Lr: 0.000091, Tokens per sec:   2659
2023-03-09 13:05:13,405 - INFO - __main__ - Epoch  10, Step:   21200, Batch Loss:    23.241627, Lr: 0.000091, Tokens per sec:   2680
2023-03-09 13:05:33,201 - INFO - __main__ - Epoch  10, Step:   21300, Batch Loss:    33.642620, Lr: 0.000091, Tokens per sec:   2699
2023-03-09 13:05:53,581 - INFO - __main__ - Epoch  10, Step:   21400, Batch Loss:    40.070305, Lr: 0.000091, Tokens per sec:   2655
2023-03-09 13:06:13,849 - INFO - __main__ - Epoch  10, Step:   21500, Batch Loss:    36.047535, Lr: 0.000091, Tokens per sec:   2650
2023-03-09 13:06:34,152 - INFO - __main__ - Epoch  10, Step:   21600, Batch Loss:    15.877576, Lr: 0.000091, Tokens per sec:   2623
2023-03-09 13:06:54,416 - INFO - __main__ - Epoch  10, Step:   21700, Batch Loss:    38.033714, Lr: 0.000091, Tokens per sec:   2712
2023-03-09 13:07:12,781 - INFO - __main__ - Epoch  10: total training loss 73594.29
2023-03-09 13:07:12,782 - INFO - __main__ - Epoch 11
2023-03-09 13:07:15,154 - INFO - __main__ - Epoch  11, Step:   21800, Batch Loss:    26.178421, Lr: 0.000090, Tokens per sec:   2184
2023-03-09 13:07:35,598 - INFO - __main__ - Epoch  11, Step:   21900, Batch Loss:    28.719021, Lr: 0.000090, Tokens per sec:   2658
2023-03-09 13:07:55,793 - INFO - __main__ - Epoch  11, Step:   22000, Batch Loss:    32.654209, Lr: 0.000090, Tokens per sec:   2604
2023-03-09 13:08:16,037 - INFO - __main__ - Epoch  11, Step:   22100, Batch Loss:    36.416405, Lr: 0.000090, Tokens per sec:   2662
2023-03-09 13:08:36,275 - INFO - __main__ - Epoch  11, Step:   22200, Batch Loss:    24.805109, Lr: 0.000090, Tokens per sec:   2652
2023-03-09 13:08:56,558 - INFO - __main__ - Epoch  11, Step:   22300, Batch Loss:    27.264492, Lr: 0.000090, Tokens per sec:   2658
2023-03-09 13:09:16,944 - INFO - __main__ - Epoch  11, Step:   22400, Batch Loss:    28.118452, Lr: 0.000090, Tokens per sec:   2639
2023-03-09 13:09:37,218 - INFO - __main__ - Epoch  11, Step:   22500, Batch Loss:    27.576256, Lr: 0.000090, Tokens per sec:   2682
2023-03-09 13:09:58,131 - INFO - __main__ - Epoch  11, Step:   22600, Batch Loss:    27.479349, Lr: 0.000090, Tokens per sec:   2558
2023-03-09 13:10:18,190 - INFO - __main__ - Epoch  11, Step:   22700, Batch Loss:    35.225437, Lr: 0.000090, Tokens per sec:   2724
2023-03-09 13:10:38,786 - INFO - __main__ - Epoch  11, Step:   22800, Batch Loss:    29.739607, Lr: 0.000090, Tokens per sec:   2663
2023-03-09 13:10:58,884 - INFO - __main__ - Epoch  11, Step:   22900, Batch Loss:    37.751732, Lr: 0.000090, Tokens per sec:   2639
2023-03-09 13:11:19,060 - INFO - __main__ - Epoch  11, Step:   23000, Batch Loss:    23.424120, Lr: 0.000090, Tokens per sec:   2689
2023-03-09 13:11:39,372 - INFO - __main__ - Epoch  11, Step:   23100, Batch Loss:    24.459253, Lr: 0.000090, Tokens per sec:   2644
2023-03-09 13:11:59,554 - INFO - __main__ - Epoch  11, Step:   23200, Batch Loss:    36.926003, Lr: 0.000090, Tokens per sec:   2683
2023-03-09 13:12:19,127 - INFO - __main__ - Epoch  11, Step:   23300, Batch Loss:    31.044794, Lr: 0.000090, Tokens per sec:   2719
2023-03-09 13:12:39,094 - INFO - __main__ - Epoch  11, Step:   23400, Batch Loss:    32.739223, Lr: 0.000090, Tokens per sec:   2678
2023-03-09 13:12:59,081 - INFO - __main__ - Epoch  11, Step:   23500, Batch Loss:    34.481213, Lr: 0.000090, Tokens per sec:   2705
2023-03-09 13:13:19,288 - INFO - __main__ - Epoch  11, Step:   23600, Batch Loss:    29.659323, Lr: 0.000090, Tokens per sec:   2691
2023-03-09 13:13:39,407 - INFO - __main__ - Epoch  11, Step:   23700, Batch Loss:    25.996542, Lr: 0.000090, Tokens per sec:   2617
2023-03-09 13:13:59,616 - INFO - __main__ - Epoch  11, Step:   23800, Batch Loss:    34.132191, Lr: 0.000090, Tokens per sec:   2651
2023-03-09 13:14:19,580 - INFO - __main__ - Epoch  11, Step:   23900, Batch Loss:    33.716618, Lr: 0.000090, Tokens per sec:   2735
2023-03-09 13:14:33,510 - INFO - __main__ - Epoch  11: total training loss 68984.51
2023-03-09 13:14:33,511 - INFO - __main__ - Epoch 12
2023-03-09 13:14:40,088 - INFO - __main__ - Epoch  12, Step:   24000, Batch Loss:    23.938698, Lr: 0.000090, Tokens per sec:   2521
2023-03-09 13:14:59,770 - INFO - __main__ - Epoch  12, Step:   24100, Batch Loss:    31.464874, Lr: 0.000090, Tokens per sec:   2718
2023-03-09 13:15:19,836 - INFO - __main__ - Epoch  12, Step:   24200, Batch Loss:    29.722538, Lr: 0.000090, Tokens per sec:   2644
2023-03-09 13:15:39,596 - INFO - __main__ - Epoch  12, Step:   24300, Batch Loss:    31.733749, Lr: 0.000090, Tokens per sec:   2719
2023-03-09 13:15:59,714 - INFO - __main__ - Epoch  12, Step:   24400, Batch Loss:    30.520790, Lr: 0.000090, Tokens per sec:   2671
2023-03-09 13:16:20,031 - INFO - __main__ - Epoch  12, Step:   24500, Batch Loss:    27.594063, Lr: 0.000090, Tokens per sec:   2667
2023-03-09 13:16:40,285 - INFO - __main__ - Epoch  12, Step:   24600, Batch Loss:    21.386976, Lr: 0.000090, Tokens per sec:   2630
2023-03-09 13:17:00,435 - INFO - __main__ - Epoch  12, Step:   24700, Batch Loss:    25.002613, Lr: 0.000090, Tokens per sec:   2678
2023-03-09 13:17:20,555 - INFO - __main__ - Epoch  12, Step:   24800, Batch Loss:    28.844322, Lr: 0.000090, Tokens per sec:   2685
2023-03-09 13:17:40,645 - INFO - __main__ - Epoch  12, Step:   24900, Batch Loss:    33.339546, Lr: 0.000090, Tokens per sec:   2703
2023-03-09 13:18:00,217 - INFO - __main__ - Epoch  12, Step:   25000, Batch Loss:    15.314161, Lr: 0.000090, Tokens per sec:   2729
2023-03-09 13:18:20,301 - INFO - __main__ - Epoch  12, Step:   25100, Batch Loss:    33.396683, Lr: 0.000090, Tokens per sec:   2675
2023-03-09 13:18:40,456 - INFO - __main__ - Epoch  12, Step:   25200, Batch Loss:    31.572926, Lr: 0.000090, Tokens per sec:   2705
2023-03-09 13:19:00,690 - INFO - __main__ - Epoch  12, Step:   25300, Batch Loss:    23.941246, Lr: 0.000090, Tokens per sec:   2625
2023-03-09 13:19:20,998 - INFO - __main__ - Epoch  12, Step:   25400, Batch Loss:    24.358063, Lr: 0.000090, Tokens per sec:   2688
2023-03-09 13:19:41,237 - INFO - __main__ - Epoch  12, Step:   25500, Batch Loss:    23.173340, Lr: 0.000090, Tokens per sec:   2637
2023-03-09 13:20:01,384 - INFO - __main__ - Epoch  12, Step:   25600, Batch Loss:    29.057985, Lr: 0.000090, Tokens per sec:   2647
2023-03-09 13:20:20,832 - INFO - __main__ - Epoch  12, Step:   25700, Batch Loss:    42.234406, Lr: 0.000090, Tokens per sec:   2785
2023-03-09 13:20:41,011 - INFO - __main__ - Epoch  12, Step:   25800, Batch Loss:    31.623194, Lr: 0.000090, Tokens per sec:   2686
2023-03-09 13:21:01,465 - INFO - __main__ - Epoch  12, Step:   25900, Batch Loss:    36.710899, Lr: 0.000090, Tokens per sec:   2639
2023-03-09 13:21:21,595 - INFO - __main__ - Epoch  12, Step:   26000, Batch Loss:    25.577148, Lr: 0.000090, Tokens per sec:   2661
2023-03-09 13:21:42,792 - INFO - __main__ - Epoch  12, Step:   26100, Batch Loss:    23.145622, Lr: 0.000090, Tokens per sec:   2590
2023-03-09 13:21:52,740 - INFO - __main__ - Epoch  12: total training loss 64719.12
2023-03-09 13:21:52,742 - INFO - __main__ - Epoch 13
2023-03-09 13:22:03,943 - INFO - __main__ - Epoch  13, Step:   26200, Batch Loss:    20.077515, Lr: 0.000089, Tokens per sec:   2508
2023-03-09 13:22:24,547 - INFO - __main__ - Epoch  13, Step:   26300, Batch Loss:    22.357664, Lr: 0.000089, Tokens per sec:   2602
2023-03-09 13:22:44,814 - INFO - __main__ - Epoch  13, Step:   26400, Batch Loss:    28.574913, Lr: 0.000089, Tokens per sec:   2655
2023-03-09 13:23:04,918 - INFO - __main__ - Epoch  13, Step:   26500, Batch Loss:    21.173140, Lr: 0.000089, Tokens per sec:   2664
2023-03-09 13:23:25,181 - INFO - __main__ - Epoch  13, Step:   26600, Batch Loss:    30.984358, Lr: 0.000089, Tokens per sec:   2660
2023-03-09 13:23:45,478 - INFO - __main__ - Epoch  13, Step:   26700, Batch Loss:    21.628229, Lr: 0.000089, Tokens per sec:   2675
2023-03-09 13:24:05,630 - INFO - __main__ - Epoch  13, Step:   26800, Batch Loss:    31.652946, Lr: 0.000089, Tokens per sec:   2699
2023-03-09 13:24:25,646 - INFO - __main__ - Epoch  13, Step:   26900, Batch Loss:    28.143494, Lr: 0.000089, Tokens per sec:   2682
2023-03-09 13:24:45,909 - INFO - __main__ - Epoch  13, Step:   27000, Batch Loss:    28.819630, Lr: 0.000089, Tokens per sec:   2675
2023-03-09 13:25:06,149 - INFO - __main__ - Epoch  13, Step:   27100, Batch Loss:    21.844496, Lr: 0.000089, Tokens per sec:   2684
2023-03-09 13:25:27,220 - INFO - __main__ - Epoch  13, Step:   27200, Batch Loss:    27.741022, Lr: 0.000089, Tokens per sec:   2575
2023-03-09 13:25:48,365 - INFO - __main__ - Epoch  13, Step:   27300, Batch Loss:    36.338268, Lr: 0.000089, Tokens per sec:   2541
2023-03-09 13:26:08,700 - INFO - __main__ - Epoch  13, Step:   27400, Batch Loss:    32.303043, Lr: 0.000089, Tokens per sec:   2621
2023-03-09 13:26:29,400 - INFO - __main__ - Epoch  13, Step:   27500, Batch Loss:    23.188425, Lr: 0.000089, Tokens per sec:   2608
2023-03-09 13:26:49,779 - INFO - __main__ - Epoch  13, Step:   27600, Batch Loss:    30.340216, Lr: 0.000089, Tokens per sec:   2643
2023-03-09 13:27:11,121 - INFO - __main__ - Epoch  13, Step:   27700, Batch Loss:    25.991280, Lr: 0.000089, Tokens per sec:   2539
2023-03-09 13:27:32,505 - INFO - __main__ - Epoch  13, Step:   27800, Batch Loss:    29.927076, Lr: 0.000089, Tokens per sec:   2539
2023-03-09 13:27:52,876 - INFO - __main__ - Epoch  13, Step:   27900, Batch Loss:    21.985424, Lr: 0.000089, Tokens per sec:   2646
2023-03-09 13:28:13,149 - INFO - __main__ - Epoch  13, Step:   28000, Batch Loss:    23.488579, Lr: 0.000089, Tokens per sec:   2605
2023-03-09 13:28:34,310 - INFO - __main__ - Epoch  13, Step:   28100, Batch Loss:    25.592615, Lr: 0.000089, Tokens per sec:   2483
2023-03-09 13:28:54,660 - INFO - __main__ - Epoch  13, Step:   28200, Batch Loss:    32.465546, Lr: 0.000089, Tokens per sec:   2634
2023-03-09 13:29:14,865 - INFO - __main__ - Epoch  13, Step:   28300, Batch Loss:    23.870550, Lr: 0.000089, Tokens per sec:   2667
2023-03-09 13:29:20,513 - INFO - __main__ - Epoch  13: total training loss 60770.29
2023-03-09 13:29:20,514 - INFO - __main__ - Epoch 14
2023-03-09 13:29:36,707 - INFO - __main__ - Epoch  14, Step:   28400, Batch Loss:    22.143326, Lr: 0.000088, Tokens per sec:   2446
2023-03-09 13:29:57,771 - INFO - __main__ - Epoch  14, Step:   28500, Batch Loss:    30.491167, Lr: 0.000088, Tokens per sec:   2572
2023-03-09 13:30:18,311 - INFO - __main__ - Epoch  14, Step:   28600, Batch Loss:    27.736748, Lr: 0.000088, Tokens per sec:   2617
2023-03-09 13:30:38,250 - INFO - __main__ - Epoch  14, Step:   28700, Batch Loss:    24.974112, Lr: 0.000088, Tokens per sec:   2688
2023-03-09 13:30:58,136 - INFO - __main__ - Epoch  14, Step:   28800, Batch Loss:    27.649611, Lr: 0.000088, Tokens per sec:   2700
2023-03-09 13:31:18,255 - INFO - __main__ - Epoch  14, Step:   28900, Batch Loss:    31.979313, Lr: 0.000088, Tokens per sec:   2650
2023-03-09 13:31:38,560 - INFO - __main__ - Epoch  14, Step:   29000, Batch Loss:    28.535921, Lr: 0.000088, Tokens per sec:   2677
2023-03-09 13:31:58,904 - INFO - __main__ - Epoch  14, Step:   29100, Batch Loss:    20.414803, Lr: 0.000088, Tokens per sec:   2640
2023-03-09 13:32:19,143 - INFO - __main__ - Epoch  14, Step:   29200, Batch Loss:    24.646034, Lr: 0.000088, Tokens per sec:   2672
2023-03-09 13:32:39,315 - INFO - __main__ - Epoch  14, Step:   29300, Batch Loss:    19.770611, Lr: 0.000088, Tokens per sec:   2644
2023-03-09 13:32:59,364 - INFO - __main__ - Epoch  14, Step:   29400, Batch Loss:    33.896858, Lr: 0.000088, Tokens per sec:   2681
2023-03-09 13:33:19,444 - INFO - __main__ - Epoch  14, Step:   29500, Batch Loss:    28.183214, Lr: 0.000088, Tokens per sec:   2692
2023-03-09 13:33:39,552 - INFO - __main__ - Epoch  14, Step:   29600, Batch Loss:    25.091686, Lr: 0.000088, Tokens per sec:   2686
2023-03-09 13:33:59,739 - INFO - __main__ - Epoch  14, Step:   29700, Batch Loss:    28.704041, Lr: 0.000088, Tokens per sec:   2643
2023-03-09 13:34:19,742 - INFO - __main__ - Epoch  14, Step:   29800, Batch Loss:    28.474190, Lr: 0.000088, Tokens per sec:   2671
2023-03-09 13:34:39,753 - INFO - __main__ - Epoch  14, Step:   29900, Batch Loss:    30.276632, Lr: 0.000088, Tokens per sec:   2733
2023-03-09 13:34:59,605 - INFO - __main__ - Epoch  14, Step:   30000, Batch Loss:    22.805933, Lr: 0.000088, Tokens per sec:   2673
2023-03-09 13:35:19,229 - INFO - __main__ - Epoch  14, Step:   30100, Batch Loss:    28.980816, Lr: 0.000088, Tokens per sec:   2779
2023-03-09 13:35:39,535 - INFO - __main__ - Epoch  14, Step:   30200, Batch Loss:    25.704473, Lr: 0.000088, Tokens per sec:   2683
2023-03-09 13:35:59,734 - INFO - __main__ - Epoch  14, Step:   30300, Batch Loss:    23.661728, Lr: 0.000088, Tokens per sec:   2665
2023-03-09 13:36:20,003 - INFO - __main__ - Epoch  14, Step:   30400, Batch Loss:    28.513617, Lr: 0.000088, Tokens per sec:   2641
2023-03-09 13:36:40,186 - INFO - __main__ - Epoch  14, Step:   30500, Batch Loss:    34.379601, Lr: 0.000088, Tokens per sec:   2651
2023-03-09 13:36:41,439 - INFO - __main__ - Epoch  14: total training loss 57082.46
2023-03-09 13:36:41,439 - INFO - __main__ - Epoch 15
2023-03-09 13:37:00,755 - INFO - __main__ - Epoch  15, Step:   30600, Batch Loss:    21.516348, Lr: 0.000087, Tokens per sec:   2603
2023-03-09 13:37:20,751 - INFO - __main__ - Epoch  15, Step:   30700, Batch Loss:    23.847548, Lr: 0.000087, Tokens per sec:   2695
2023-03-09 13:37:40,876 - INFO - __main__ - Epoch  15, Step:   30800, Batch Loss:    12.981828, Lr: 0.000087, Tokens per sec:   2622
2023-03-09 13:38:00,944 - INFO - __main__ - Epoch  15, Step:   30900, Batch Loss:    23.313313, Lr: 0.000087, Tokens per sec:   2632
2023-03-09 13:38:21,099 - INFO - __main__ - Epoch  15, Step:   31000, Batch Loss:    26.192757, Lr: 0.000087, Tokens per sec:   2696
2023-03-09 13:38:40,998 - INFO - __main__ - Epoch  15, Step:   31100, Batch Loss:    23.694185, Lr: 0.000087, Tokens per sec:   2728
2023-03-09 13:39:00,986 - INFO - __main__ - Epoch  15, Step:   31200, Batch Loss:    23.603579, Lr: 0.000087, Tokens per sec:   2716
2023-03-09 13:39:20,995 - INFO - __main__ - Epoch  15, Step:   31300, Batch Loss:    34.721653, Lr: 0.000087, Tokens per sec:   2689
2023-03-09 13:39:41,395 - INFO - __main__ - Epoch  15, Step:   31400, Batch Loss:    26.169701, Lr: 0.000087, Tokens per sec:   2650
2023-03-09 13:40:01,788 - INFO - __main__ - Epoch  15, Step:   31500, Batch Loss:    24.685900, Lr: 0.000087, Tokens per sec:   2649
2023-03-09 13:40:21,988 - INFO - __main__ - Epoch  15, Step:   31600, Batch Loss:    31.975159, Lr: 0.000087, Tokens per sec:   2639
2023-03-09 13:40:42,053 - INFO - __main__ - Epoch  15, Step:   31700, Batch Loss:    36.914051, Lr: 0.000087, Tokens per sec:   2688
2023-03-09 13:41:02,213 - INFO - __main__ - Epoch  15, Step:   31800, Batch Loss:    29.103134, Lr: 0.000087, Tokens per sec:   2659
2023-03-09 13:41:22,345 - INFO - __main__ - Epoch  15, Step:   31900, Batch Loss:    26.726671, Lr: 0.000087, Tokens per sec:   2680
2023-03-09 13:41:42,718 - INFO - __main__ - Epoch  15, Step:   32000, Batch Loss:    17.777496, Lr: 0.000087, Tokens per sec:   2663
2023-03-09 13:42:02,919 - INFO - __main__ - Epoch  15, Step:   32100, Batch Loss:    19.250439, Lr: 0.000087, Tokens per sec:   2656
2023-03-09 13:42:23,290 - INFO - __main__ - Epoch  15, Step:   32200, Batch Loss:    28.733904, Lr: 0.000087, Tokens per sec:   2664
2023-03-09 13:42:43,359 - INFO - __main__ - Epoch  15, Step:   32300, Batch Loss:    18.467743, Lr: 0.000087, Tokens per sec:   2677
2023-03-09 13:43:03,294 - INFO - __main__ - Epoch  15, Step:   32400, Batch Loss:    24.277716, Lr: 0.000087, Tokens per sec:   2724
2023-03-09 13:43:23,232 - INFO - __main__ - Epoch  15, Step:   32500, Batch Loss:    23.932751, Lr: 0.000087, Tokens per sec:   2721
2023-03-09 13:43:43,062 - INFO - __main__ - Epoch  15, Step:   32600, Batch Loss:    30.019773, Lr: 0.000087, Tokens per sec:   2726
2023-03-09 13:44:00,172 - INFO - __main__ - Epoch  15: total training loss 53793.52
2023-03-09 13:44:00,173 - INFO - __main__ - Epoch 16
2023-03-09 13:44:03,615 - INFO - __main__ - Epoch  16, Step:   32700, Batch Loss:    23.550095, Lr: 0.000086, Tokens per sec:   2408
2023-03-09 13:44:23,458 - INFO - __main__ - Epoch  16, Step:   32800, Batch Loss:    23.603603, Lr: 0.000086, Tokens per sec:   2706
2023-03-09 13:44:43,483 - INFO - __main__ - Epoch  16, Step:   32900, Batch Loss:    21.424109, Lr: 0.000086, Tokens per sec:   2692
2023-03-09 13:45:03,618 - INFO - __main__ - Epoch  16, Step:   33000, Batch Loss:    32.290073, Lr: 0.000086, Tokens per sec:   2689
2023-03-09 13:45:23,881 - INFO - __main__ - Epoch  16, Step:   33100, Batch Loss:    24.054752, Lr: 0.000086, Tokens per sec:   2643
2023-03-09 13:45:44,063 - INFO - __main__ - Epoch  16, Step:   33200, Batch Loss:    19.828243, Lr: 0.000086, Tokens per sec:   2693
2023-03-09 13:46:04,261 - INFO - __main__ - Epoch  16, Step:   33300, Batch Loss:    23.355801, Lr: 0.000086, Tokens per sec:   2621
2023-03-09 13:46:24,452 - INFO - __main__ - Epoch  16, Step:   33400, Batch Loss:    21.700363, Lr: 0.000086, Tokens per sec:   2665
2023-03-09 13:46:44,833 - INFO - __main__ - Epoch  16, Step:   33500, Batch Loss:    19.755301, Lr: 0.000086, Tokens per sec:   2640
2023-03-09 13:47:05,204 - INFO - __main__ - Epoch  16, Step:   33600, Batch Loss:    23.813269, Lr: 0.000086, Tokens per sec:   2634
2023-03-09 13:47:25,392 - INFO - __main__ - Epoch  16, Step:   33700, Batch Loss:    21.002838, Lr: 0.000086, Tokens per sec:   2654
2023-03-09 13:47:45,526 - INFO - __main__ - Epoch  16, Step:   33800, Batch Loss:    23.365753, Lr: 0.000086, Tokens per sec:   2679
2023-03-09 13:48:05,700 - INFO - __main__ - Epoch  16, Step:   33900, Batch Loss:    31.359962, Lr: 0.000086, Tokens per sec:   2704
2023-03-09 13:48:25,867 - INFO - __main__ - Epoch  16, Step:   34000, Batch Loss:    16.278461, Lr: 0.000086, Tokens per sec:   2599
2023-03-09 13:48:45,857 - INFO - __main__ - Epoch  16, Step:   34100, Batch Loss:    24.028063, Lr: 0.000086, Tokens per sec:   2712
2023-03-09 13:49:06,244 - INFO - __main__ - Epoch  16, Step:   34200, Batch Loss:    25.468983, Lr: 0.000086, Tokens per sec:   2663
2023-03-09 13:49:26,555 - INFO - __main__ - Epoch  16, Step:   34300, Batch Loss:    31.747629, Lr: 0.000086, Tokens per sec:   2615
2023-03-09 13:49:46,860 - INFO - __main__ - Epoch  16, Step:   34400, Batch Loss:    22.220888, Lr: 0.000086, Tokens per sec:   2647
2023-03-09 13:50:07,099 - INFO - __main__ - Epoch  16, Step:   34500, Batch Loss:    20.165621, Lr: 0.000086, Tokens per sec:   2716
2023-03-09 13:50:27,189 - INFO - __main__ - Epoch  16, Step:   34600, Batch Loss:    23.373089, Lr: 0.000086, Tokens per sec:   2694
2023-03-09 13:50:47,294 - INFO - __main__ - Epoch  16, Step:   34700, Batch Loss:    20.597589, Lr: 0.000086, Tokens per sec:   2690
2023-03-09 13:51:07,732 - INFO - __main__ - Epoch  16, Step:   34800, Batch Loss:    16.662630, Lr: 0.000086, Tokens per sec:   2631
2023-03-09 13:51:20,780 - INFO - __main__ - Epoch  16: total training loss 50726.43
2023-03-09 13:51:20,782 - INFO - __main__ - Epoch 17
2023-03-09 13:51:28,352 - INFO - __main__ - Epoch  17, Step:   34900, Batch Loss:    16.508968, Lr: 0.000085, Tokens per sec:   2560
2023-03-09 13:51:48,368 - INFO - __main__ - Epoch  17, Step:   35000, Batch Loss:    15.504078, Lr: 0.000085, Tokens per sec:   2661
2023-03-09 13:52:08,354 - INFO - __main__ - Epoch  17, Step:   35100, Batch Loss:    12.395989, Lr: 0.000085, Tokens per sec:   2651
2023-03-09 13:52:28,812 - INFO - __main__ - Epoch  17, Step:   35200, Batch Loss:    17.257078, Lr: 0.000085, Tokens per sec:   2643
2023-03-09 13:52:48,591 - INFO - __main__ - Epoch  17, Step:   35300, Batch Loss:    23.874043, Lr: 0.000085, Tokens per sec:   2722
2023-03-09 13:53:08,679 - INFO - __main__ - Epoch  17, Step:   35400, Batch Loss:    22.892313, Lr: 0.000085, Tokens per sec:   2653
2023-03-09 13:53:28,756 - INFO - __main__ - Epoch  17, Step:   35500, Batch Loss:    14.512531, Lr: 0.000085, Tokens per sec:   2675
2023-03-09 13:53:48,901 - INFO - __main__ - Epoch  17, Step:   35600, Batch Loss:    23.570623, Lr: 0.000085, Tokens per sec:   2717
2023-03-09 13:54:09,156 - INFO - __main__ - Epoch  17, Step:   35700, Batch Loss:    21.108994, Lr: 0.000085, Tokens per sec:   2738
2023-03-09 13:54:29,412 - INFO - __main__ - Epoch  17, Step:   35800, Batch Loss:    32.800327, Lr: 0.000085, Tokens per sec:   2692
2023-03-09 13:54:49,403 - INFO - __main__ - Epoch  17, Step:   35900, Batch Loss:    21.071110, Lr: 0.000085, Tokens per sec:   2700
2023-03-09 13:55:09,408 - INFO - __main__ - Epoch  17, Step:   36000, Batch Loss:    14.325325, Lr: 0.000085, Tokens per sec:   2664
2023-03-09 13:55:29,251 - INFO - __main__ - Epoch  17, Step:   36100, Batch Loss:    21.751291, Lr: 0.000085, Tokens per sec:   2714
2023-03-09 13:55:49,453 - INFO - __main__ - Epoch  17, Step:   36200, Batch Loss:    26.185394, Lr: 0.000085, Tokens per sec:   2689
2023-03-09 13:56:09,773 - INFO - __main__ - Epoch  17, Step:   36300, Batch Loss:    19.635942, Lr: 0.000085, Tokens per sec:   2677
2023-03-09 13:56:30,085 - INFO - __main__ - Epoch  17, Step:   36400, Batch Loss:    21.683172, Lr: 0.000085, Tokens per sec:   2615
2023-03-09 13:56:50,635 - INFO - __main__ - Epoch  17, Step:   36500, Batch Loss:    33.126751, Lr: 0.000085, Tokens per sec:   2598
2023-03-09 13:57:10,616 - INFO - __main__ - Epoch  17, Step:   36600, Batch Loss:    26.075703, Lr: 0.000085, Tokens per sec:   2713
2023-03-09 13:57:30,936 - INFO - __main__ - Epoch  17, Step:   36700, Batch Loss:    22.266258, Lr: 0.000085, Tokens per sec:   2624
2023-03-09 13:57:50,841 - INFO - __main__ - Epoch  17, Step:   36800, Batch Loss:    16.443632, Lr: 0.000085, Tokens per sec:   2666
2023-03-09 13:58:10,923 - INFO - __main__ - Epoch  17, Step:   36900, Batch Loss:    25.291155, Lr: 0.000085, Tokens per sec:   2715
2023-03-09 13:58:31,234 - INFO - __main__ - Epoch  17, Step:   37000, Batch Loss:    24.119032, Lr: 0.000085, Tokens per sec:   2638
2023-03-09 13:58:40,006 - INFO - __main__ - Epoch  17: total training loss 47826.82
2023-03-09 13:58:40,007 - INFO - __main__ - Epoch 18
2023-03-09 13:58:51,896 - INFO - __main__ - Epoch  18, Step:   37100, Batch Loss:    15.287522, Lr: 0.000084, Tokens per sec:   2543
2023-03-09 13:59:12,372 - INFO - __main__ - Epoch  18, Step:   37200, Batch Loss:    15.286018, Lr: 0.000084, Tokens per sec:   2644
2023-03-09 13:59:32,662 - INFO - __main__ - Epoch  18, Step:   37300, Batch Loss:    27.571753, Lr: 0.000084, Tokens per sec:   2645
2023-03-09 13:59:52,908 - INFO - __main__ - Epoch  18, Step:   37400, Batch Loss:    26.469997, Lr: 0.000084, Tokens per sec:   2684
2023-03-09 14:00:12,927 - INFO - __main__ - Epoch  18, Step:   37500, Batch Loss:    24.487610, Lr: 0.000084, Tokens per sec:   2689
2023-03-09 14:00:33,067 - INFO - __main__ - Epoch  18, Step:   37600, Batch Loss:    19.265812, Lr: 0.000084, Tokens per sec:   2643
2023-03-09 14:00:53,359 - INFO - __main__ - Epoch  18, Step:   37700, Batch Loss:    27.495502, Lr: 0.000084, Tokens per sec:   2672
2023-03-09 14:01:13,520 - INFO - __main__ - Epoch  18, Step:   37800, Batch Loss:    18.214361, Lr: 0.000084, Tokens per sec:   2647
2023-03-09 14:01:33,681 - INFO - __main__ - Epoch  18, Step:   37900, Batch Loss:    27.855160, Lr: 0.000084, Tokens per sec:   2691
2023-03-09 14:01:53,869 - INFO - __main__ - Epoch  18, Step:   38000, Batch Loss:    16.075762, Lr: 0.000084, Tokens per sec:   2659
2023-03-09 14:02:14,154 - INFO - __main__ - Epoch  18, Step:   38100, Batch Loss:    11.693925, Lr: 0.000084, Tokens per sec:   2640
2023-03-09 14:02:34,256 - INFO - __main__ - Epoch  18, Step:   38200, Batch Loss:    20.581558, Lr: 0.000084, Tokens per sec:   2675
2023-03-09 14:02:54,440 - INFO - __main__ - Epoch  18, Step:   38300, Batch Loss:    22.921896, Lr: 0.000084, Tokens per sec:   2649
2023-03-09 14:03:14,705 - INFO - __main__ - Epoch  18, Step:   38400, Batch Loss:    20.189268, Lr: 0.000084, Tokens per sec:   2655
2023-03-09 14:03:34,692 - INFO - __main__ - Epoch  18, Step:   38500, Batch Loss:    20.703424, Lr: 0.000084, Tokens per sec:   2698
2023-03-09 14:03:54,865 - INFO - __main__ - Epoch  18, Step:   38600, Batch Loss:    27.429457, Lr: 0.000084, Tokens per sec:   2681
2023-03-09 14:04:14,672 - INFO - __main__ - Epoch  18, Step:   38700, Batch Loss:    24.205658, Lr: 0.000084, Tokens per sec:   2709
2023-03-09 14:04:34,936 - INFO - __main__ - Epoch  18, Step:   38800, Batch Loss:    20.401676, Lr: 0.000084, Tokens per sec:   2653
2023-03-09 14:04:55,215 - INFO - __main__ - Epoch  18, Step:   38900, Batch Loss:    17.873436, Lr: 0.000084, Tokens per sec:   2624
2023-03-09 14:05:15,243 - INFO - __main__ - Epoch  18, Step:   39000, Batch Loss:    15.739921, Lr: 0.000084, Tokens per sec:   2707
2023-03-09 14:05:35,249 - INFO - __main__ - Epoch  18, Step:   39100, Batch Loss:    18.464939, Lr: 0.000084, Tokens per sec:   2755
2023-03-09 14:05:55,383 - INFO - __main__ - Epoch  18, Step:   39200, Batch Loss:    25.336277, Lr: 0.000084, Tokens per sec:   2654
2023-03-09 14:05:59,739 - INFO - __main__ - Epoch  18: total training loss 45184.60
2023-03-09 14:05:59,740 - INFO - __main__ - Epoch 19
2023-03-09 14:06:15,803 - INFO - __main__ - Epoch  19, Step:   39300, Batch Loss:    18.952599, Lr: 0.000083, Tokens per sec:   2614
2023-03-09 14:06:35,571 - INFO - __main__ - Epoch  19, Step:   39400, Batch Loss:    22.710777, Lr: 0.000083, Tokens per sec:   2704
2023-03-09 14:06:55,729 - INFO - __main__ - Epoch  19, Step:   39500, Batch Loss:    13.303163, Lr: 0.000083, Tokens per sec:   2670
2023-03-09 14:07:15,786 - INFO - __main__ - Epoch  19, Step:   39600, Batch Loss:    23.611124, Lr: 0.000083, Tokens per sec:   2720
2023-03-09 14:07:35,892 - INFO - __main__ - Epoch  19, Step:   39700, Batch Loss:    17.932932, Lr: 0.000083, Tokens per sec:   2657
2023-03-09 14:07:56,136 - INFO - __main__ - Epoch  19, Step:   39800, Batch Loss:    19.792072, Lr: 0.000083, Tokens per sec:   2675
2023-03-09 14:08:16,266 - INFO - __main__ - Epoch  19, Step:   39900, Batch Loss:    15.713045, Lr: 0.000083, Tokens per sec:   2658
2023-03-09 14:08:36,598 - INFO - __main__ - Epoch  19, Step:   40000, Batch Loss:    19.040556, Lr: 0.000083, Tokens per sec:   2639
2023-03-09 14:08:56,624 - INFO - __main__ - Epoch  19, Step:   40100, Batch Loss:    22.307421, Lr: 0.000083, Tokens per sec:   2740
2023-03-09 14:09:16,900 - INFO - __main__ - Epoch  19, Step:   40200, Batch Loss:    16.878872, Lr: 0.000083, Tokens per sec:   2683
2023-03-09 14:09:37,466 - INFO - __main__ - Epoch  19, Step:   40300, Batch Loss:    19.769156, Lr: 0.000083, Tokens per sec:   2589
2023-03-09 14:09:57,482 - INFO - __main__ - Epoch  19, Step:   40400, Batch Loss:    24.302656, Lr: 0.000083, Tokens per sec:   2667
2023-03-09 14:10:18,116 - INFO - __main__ - Epoch  19, Step:   40500, Batch Loss:    20.094780, Lr: 0.000083, Tokens per sec:   2608
2023-03-09 14:10:39,428 - INFO - __main__ - Epoch  19, Step:   40600, Batch Loss:    22.048809, Lr: 0.000083, Tokens per sec:   2517
2023-03-09 14:11:00,004 - INFO - __main__ - Epoch  19, Step:   40700, Batch Loss:    26.742271, Lr: 0.000083, Tokens per sec:   2629
2023-03-09 14:11:20,322 - INFO - __main__ - Epoch  19, Step:   40800, Batch Loss:    23.165007, Lr: 0.000083, Tokens per sec:   2639
2023-03-09 14:11:40,626 - INFO - __main__ - Epoch  19, Step:   40900, Batch Loss:    22.656807, Lr: 0.000083, Tokens per sec:   2641
2023-03-09 14:12:00,725 - INFO - __main__ - Epoch  19, Step:   41000, Batch Loss:    19.791281, Lr: 0.000083, Tokens per sec:   2632
2023-03-09 14:12:20,892 - INFO - __main__ - Epoch  19, Step:   41100, Batch Loss:    20.166906, Lr: 0.000083, Tokens per sec:   2639
2023-03-09 14:12:41,179 - INFO - __main__ - Epoch  19, Step:   41200, Batch Loss:    15.594366, Lr: 0.000083, Tokens per sec:   2707
2023-03-09 14:13:01,127 - INFO - __main__ - Epoch  19, Step:   41300, Batch Loss:    18.352364, Lr: 0.000083, Tokens per sec:   2732
2023-03-09 14:13:21,201 - INFO - __main__ - Epoch  19, Step:   41400, Batch Loss:    24.095140, Lr: 0.000083, Tokens per sec:   2682
2023-03-09 14:13:21,470 - INFO - __main__ - Epoch  19: total training loss 42727.59
2023-03-09 14:13:21,471 - INFO - __main__ - Epoch 20
2023-03-09 14:13:41,672 - INFO - __main__ - Epoch  20, Step:   41500, Batch Loss:    15.891776, Lr: 0.000083, Tokens per sec:   2621
2023-03-09 14:14:01,824 - INFO - __main__ - Epoch  20, Step:   41600, Batch Loss:    14.659222, Lr: 0.000083, Tokens per sec:   2705
2023-03-09 14:14:21,897 - INFO - __main__ - Epoch  20, Step:   41700, Batch Loss:    19.448500, Lr: 0.000083, Tokens per sec:   2673
2023-03-09 14:14:41,882 - INFO - __main__ - Epoch  20, Step:   41800, Batch Loss:    21.228857, Lr: 0.000083, Tokens per sec:   2717
2023-03-09 14:15:02,053 - INFO - __main__ - Epoch  20, Step:   41900, Batch Loss:    15.073963, Lr: 0.000083, Tokens per sec:   2659
2023-03-09 14:15:22,368 - INFO - __main__ - Epoch  20, Step:   42000, Batch Loss:    19.461576, Lr: 0.000083, Tokens per sec:   2666
2023-03-09 14:15:42,127 - INFO - __main__ - Epoch  20, Step:   42100, Batch Loss:    22.984236, Lr: 0.000083, Tokens per sec:   2764
2023-03-09 14:16:02,202 - INFO - __main__ - Epoch  20, Step:   42200, Batch Loss:    15.302918, Lr: 0.000083, Tokens per sec:   2666
2023-03-09 14:16:22,222 - INFO - __main__ - Epoch  20, Step:   42300, Batch Loss:    21.443899, Lr: 0.000083, Tokens per sec:   2664
2023-03-09 14:16:42,231 - INFO - __main__ - Epoch  20, Step:   42400, Batch Loss:    14.514975, Lr: 0.000083, Tokens per sec:   2695
2023-03-09 14:17:01,953 - INFO - __main__ - Epoch  20, Step:   42500, Batch Loss:    22.892559, Lr: 0.000083, Tokens per sec:   2705
2023-03-09 14:17:22,435 - INFO - __main__ - Epoch  20, Step:   42600, Batch Loss:    16.031919, Lr: 0.000083, Tokens per sec:   2612
2023-03-09 14:17:42,378 - INFO - __main__ - Epoch  20, Step:   42700, Batch Loss:    17.031610, Lr: 0.000083, Tokens per sec:   2699
2023-03-09 14:18:02,119 - INFO - __main__ - Epoch  20, Step:   42800, Batch Loss:    15.994611, Lr: 0.000083, Tokens per sec:   2780
2023-03-09 14:18:20,635 - INFO - __main__ - Epoch  20, Step:   42900, Batch Loss:    18.552629, Lr: 0.000083, Tokens per sec:   2942
2023-03-09 14:18:40,949 - INFO - __main__ - Epoch  20, Step:   43000, Batch Loss:    16.335037, Lr: 0.000083, Tokens per sec:   2617
2023-03-09 14:19:00,972 - INFO - __main__ - Epoch  20, Step:   43100, Batch Loss:    19.912495, Lr: 0.000083, Tokens per sec:   2708
2023-03-09 14:19:21,107 - INFO - __main__ - Epoch  20, Step:   43200, Batch Loss:    20.950239, Lr: 0.000083, Tokens per sec:   2655
2023-03-09 14:19:41,192 - INFO - __main__ - Epoch  20, Step:   43300, Batch Loss:    19.159966, Lr: 0.000083, Tokens per sec:   2669
2023-03-09 14:20:01,366 - INFO - __main__ - Epoch  20, Step:   43400, Batch Loss:    17.280981, Lr: 0.000083, Tokens per sec:   2638
2023-03-09 14:20:21,441 - INFO - __main__ - Epoch  20, Step:   43500, Batch Loss:    26.408253, Lr: 0.000083, Tokens per sec:   2664
2023-03-09 14:20:37,557 - INFO - __main__ - Epoch  20: total training loss 40399.21
2023-03-09 14:20:37,558 - INFO - __main__ - Epoch 21
2023-03-09 14:20:41,755 - INFO - __main__ - Epoch  21, Step:   43600, Batch Loss:    13.403597, Lr: 0.000082, Tokens per sec:   2509
2023-03-09 14:21:00,947 - INFO - __main__ - Epoch  21, Step:   43700, Batch Loss:    15.908335, Lr: 0.000082, Tokens per sec:   2849
2023-03-09 14:21:20,561 - INFO - __main__ - Epoch  21, Step:   43800, Batch Loss:    12.922179, Lr: 0.000082, Tokens per sec:   2744
2023-03-09 14:21:40,601 - INFO - __main__ - Epoch  21, Step:   43900, Batch Loss:    13.381630, Lr: 0.000082, Tokens per sec:   2663
2023-03-09 14:22:00,657 - INFO - __main__ - Epoch  21, Step:   44000, Batch Loss:    18.285400, Lr: 0.000082, Tokens per sec:   2691
2023-03-09 14:22:20,663 - INFO - __main__ - Epoch  21, Step:   44100, Batch Loss:    14.859137, Lr: 0.000082, Tokens per sec:   2695
2023-03-09 14:22:40,591 - INFO - __main__ - Epoch  21, Step:   44200, Batch Loss:    12.488053, Lr: 0.000082, Tokens per sec:   2656
2023-03-09 14:23:00,689 - INFO - __main__ - Epoch  21, Step:   44300, Batch Loss:    15.448838, Lr: 0.000082, Tokens per sec:   2638
2023-03-09 14:23:20,707 - INFO - __main__ - Epoch  21, Step:   44400, Batch Loss:    15.225768, Lr: 0.000082, Tokens per sec:   2706
2023-03-09 14:23:40,757 - INFO - __main__ - Epoch  21, Step:   44500, Batch Loss:    16.168762, Lr: 0.000082, Tokens per sec:   2692
2023-03-09 14:24:00,842 - INFO - __main__ - Epoch  21, Step:   44600, Batch Loss:    23.416550, Lr: 0.000082, Tokens per sec:   2687
2023-03-09 14:24:20,859 - INFO - __main__ - Epoch  21, Step:   44700, Batch Loss:    16.954622, Lr: 0.000082, Tokens per sec:   2717
2023-03-09 14:24:40,893 - INFO - __main__ - Epoch  21, Step:   44800, Batch Loss:    24.716805, Lr: 0.000082, Tokens per sec:   2652
2023-03-09 14:25:00,887 - INFO - __main__ - Epoch  21, Step:   44900, Batch Loss:    21.845312, Lr: 0.000082, Tokens per sec:   2726
2023-03-09 14:25:20,925 - INFO - __main__ - Epoch  21, Step:   45000, Batch Loss:    20.675539, Lr: 0.000082, Tokens per sec:   2704
2023-03-09 14:25:40,974 - INFO - __main__ - Epoch  21, Step:   45100, Batch Loss:    16.256485, Lr: 0.000082, Tokens per sec:   2688
2023-03-09 14:26:01,566 - INFO - __main__ - Epoch  21, Step:   45200, Batch Loss:    20.605492, Lr: 0.000082, Tokens per sec:   2651
2023-03-09 14:26:21,990 - INFO - __main__ - Epoch  21, Step:   45300, Batch Loss:    16.094973, Lr: 0.000082, Tokens per sec:   2643
2023-03-09 14:26:42,050 - INFO - __main__ - Epoch  21, Step:   45400, Batch Loss:    20.114458, Lr: 0.000082, Tokens per sec:   2640
2023-03-09 14:27:02,052 - INFO - __main__ - Epoch  21, Step:   45500, Batch Loss:    20.741007, Lr: 0.000082, Tokens per sec:   2689
2023-03-09 14:27:21,957 - INFO - __main__ - Epoch  21, Step:   45600, Batch Loss:    16.435581, Lr: 0.000082, Tokens per sec:   2692
2023-03-09 14:27:40,280 - INFO - __main__ - Epoch  21, Step:   45700, Batch Loss:    20.395023, Lr: 0.000082, Tokens per sec:   2943
2023-03-09 14:27:52,159 - INFO - __main__ - Epoch  21: total training loss 38237.82
2023-03-09 14:27:52,161 - INFO - __main__ - Epoch 22
2023-03-09 14:28:00,728 - INFO - __main__ - Epoch  22, Step:   45800, Batch Loss:    15.138195, Lr: 0.000081, Tokens per sec:   2606
2023-03-09 14:28:20,799 - INFO - __main__ - Epoch  22, Step:   45900, Batch Loss:    14.569027, Lr: 0.000081, Tokens per sec:   2665
2023-03-09 14:28:40,876 - INFO - __main__ - Epoch  22, Step:   46000, Batch Loss:    18.843092, Lr: 0.000081, Tokens per sec:   2685
2023-03-09 14:28:59,608 - INFO - __main__ - Epoch  22, Step:   46100, Batch Loss:    14.451270, Lr: 0.000081, Tokens per sec:   2874
2023-03-09 14:29:19,443 - INFO - __main__ - Epoch  22, Step:   46200, Batch Loss:    15.376424, Lr: 0.000081, Tokens per sec:   2768
2023-03-09 14:29:37,936 - INFO - __main__ - Epoch  22, Step:   46300, Batch Loss:    20.372156, Lr: 0.000081, Tokens per sec:   2955
2023-03-09 14:29:56,683 - INFO - __main__ - Epoch  22, Step:   46400, Batch Loss:    17.287508, Lr: 0.000081, Tokens per sec:   2875
2023-03-09 14:30:16,713 - INFO - __main__ - Epoch  22, Step:   46500, Batch Loss:    19.150782, Lr: 0.000081, Tokens per sec:   2678
2023-03-09 14:30:35,384 - INFO - __main__ - Epoch  22, Step:   46600, Batch Loss:    19.709509, Lr: 0.000081, Tokens per sec:   2838
2023-03-09 14:30:53,692 - INFO - __main__ - Epoch  22, Step:   46700, Batch Loss:    19.392376, Lr: 0.000081, Tokens per sec:   2936
2023-03-09 14:31:13,454 - INFO - __main__ - Epoch  22, Step:   46800, Batch Loss:    15.068835, Lr: 0.000081, Tokens per sec:   2680
2023-03-09 14:31:32,183 - INFO - __main__ - Epoch  22, Step:   46900, Batch Loss:    15.356582, Lr: 0.000081, Tokens per sec:   2870
2023-03-09 14:31:52,178 - INFO - __main__ - Epoch  22, Step:   47000, Batch Loss:    23.883631, Lr: 0.000081, Tokens per sec:   2709
2023-03-09 14:32:12,105 - INFO - __main__ - Epoch  22, Step:   47100, Batch Loss:    16.555906, Lr: 0.000081, Tokens per sec:   2679
2023-03-09 14:32:32,229 - INFO - __main__ - Epoch  22, Step:   47200, Batch Loss:    12.271432, Lr: 0.000081, Tokens per sec:   2674
2023-03-09 14:32:52,218 - INFO - __main__ - Epoch  22, Step:   47300, Batch Loss:    12.593233, Lr: 0.000081, Tokens per sec:   2673
2023-03-09 14:33:12,269 - INFO - __main__ - Epoch  22, Step:   47400, Batch Loss:    15.747985, Lr: 0.000081, Tokens per sec:   2659
2023-03-09 14:33:32,336 - INFO - __main__ - Epoch  22, Step:   47500, Batch Loss:    22.669718, Lr: 0.000081, Tokens per sec:   2707
2023-03-09 14:33:51,036 - INFO - __main__ - Epoch  22, Step:   47600, Batch Loss:    17.470470, Lr: 0.000081, Tokens per sec:   2894
2023-03-09 14:34:09,905 - INFO - __main__ - Epoch  22, Step:   47700, Batch Loss:    17.935497, Lr: 0.000081, Tokens per sec:   2880
2023-03-09 14:34:29,903 - INFO - __main__ - Epoch  22, Step:   47800, Batch Loss:    19.056854, Lr: 0.000081, Tokens per sec:   2691
2023-03-09 14:34:49,875 - INFO - __main__ - Epoch  22, Step:   47900, Batch Loss:    14.849980, Lr: 0.000081, Tokens per sec:   2688
2023-03-09 14:34:57,555 - INFO - __main__ - Epoch  22: total training loss 36259.96
2023-03-09 14:34:57,556 - INFO - __main__ - Epoch 23
2023-03-09 14:35:09,769 - INFO - __main__ - Epoch  23, Step:   48000, Batch Loss:    14.748306, Lr: 0.000080, Tokens per sec:   2733
2023-03-09 14:35:28,506 - INFO - __main__ - Epoch  23, Step:   48100, Batch Loss:    12.745913, Lr: 0.000080, Tokens per sec:   2878
2023-03-09 14:35:48,530 - INFO - __main__ - Epoch  23, Step:   48200, Batch Loss:    14.299067, Lr: 0.000080, Tokens per sec:   2730
2023-03-09 14:36:06,991 - INFO - __main__ - Epoch  23, Step:   48300, Batch Loss:    12.810109, Lr: 0.000080, Tokens per sec:   2893
2023-03-09 14:36:25,506 - INFO - __main__ - Epoch  23, Step:   48400, Batch Loss:    11.883601, Lr: 0.000080, Tokens per sec:   2949
2023-03-09 14:36:45,580 - INFO - __main__ - Epoch  23, Step:   48500, Batch Loss:    16.714544, Lr: 0.000080, Tokens per sec:   2695
2023-03-09 14:37:05,610 - INFO - __main__ - Epoch  23, Step:   48600, Batch Loss:    16.977295, Lr: 0.000080, Tokens per sec:   2701
2023-03-09 14:37:25,631 - INFO - __main__ - Epoch  23, Step:   48700, Batch Loss:    15.979450, Lr: 0.000080, Tokens per sec:   2695
2023-03-09 14:37:45,616 - INFO - __main__ - Epoch  23, Step:   48800, Batch Loss:    11.097683, Lr: 0.000080, Tokens per sec:   2679
2023-03-09 14:38:05,402 - INFO - __main__ - Epoch  23, Step:   48900, Batch Loss:    13.844467, Lr: 0.000080, Tokens per sec:   2727
2023-03-09 14:38:23,659 - INFO - __main__ - Epoch  23, Step:   49000, Batch Loss:    16.538839, Lr: 0.000080, Tokens per sec:   2954
2023-03-09 14:38:41,918 - INFO - __main__ - Epoch  23, Step:   49100, Batch Loss:    17.798904, Lr: 0.000080, Tokens per sec:   2934
2023-03-09 14:39:00,157 - INFO - __main__ - Epoch  23, Step:   49200, Batch Loss:     8.512962, Lr: 0.000080, Tokens per sec:   2957
2023-03-09 14:39:18,530 - INFO - __main__ - Epoch  23, Step:   49300, Batch Loss:    18.125092, Lr: 0.000080, Tokens per sec:   2932
2023-03-09 14:39:38,206 - INFO - __main__ - Epoch  23, Step:   49400, Batch Loss:    18.521339, Lr: 0.000080, Tokens per sec:   2775
2023-03-09 14:39:57,874 - INFO - __main__ - Epoch  23, Step:   49500, Batch Loss:    17.603916, Lr: 0.000080, Tokens per sec:   2736
2023-03-09 14:40:17,891 - INFO - __main__ - Epoch  23, Step:   49600, Batch Loss:    11.611130, Lr: 0.000080, Tokens per sec:   2709
2023-03-09 14:40:37,200 - INFO - __main__ - Epoch  23, Step:   49700, Batch Loss:    17.199289, Lr: 0.000080, Tokens per sec:   2760
2023-03-09 14:40:57,050 - INFO - __main__ - Epoch  23, Step:   49800, Batch Loss:    16.214741, Lr: 0.000080, Tokens per sec:   2727
2023-03-09 14:41:16,362 - INFO - __main__ - Epoch  23, Step:   49900, Batch Loss:    14.945734, Lr: 0.000080, Tokens per sec:   2754
2023-03-09 14:41:36,424 - INFO - __main__ - Epoch  23, Step:   50000, Batch Loss:    14.024424, Lr: 0.000080, Tokens per sec:   2636
2023-03-09 14:41:56,231 - INFO - __main__ - Epoch  23, Step:   50100, Batch Loss:    15.417067, Lr: 0.000080, Tokens per sec:   2671
2023-03-09 14:41:59,746 - INFO - __main__ - Epoch  23: total training loss 34296.72
2023-03-09 14:41:59,747 - INFO - __main__ - Epoch 24
2023-03-09 14:42:16,794 - INFO - __main__ - Epoch  24, Step:   50200, Batch Loss:    12.450390, Lr: 0.000079, Tokens per sec:   2647
2023-03-09 14:42:36,855 - INFO - __main__ - Epoch  24, Step:   50300, Batch Loss:    18.235327, Lr: 0.000079, Tokens per sec:   2674
2023-03-09 14:42:57,157 - INFO - __main__ - Epoch  24, Step:   50400, Batch Loss:    17.315331, Lr: 0.000079, Tokens per sec:   2683
2023-03-09 14:43:17,128 - INFO - __main__ - Epoch  24, Step:   50500, Batch Loss:     9.945506, Lr: 0.000079, Tokens per sec:   2673
2023-03-09 14:43:37,213 - INFO - __main__ - Epoch  24, Step:   50600, Batch Loss:    13.912728, Lr: 0.000079, Tokens per sec:   2695
2023-03-09 14:43:57,359 - INFO - __main__ - Epoch  24, Step:   50700, Batch Loss:    12.875400, Lr: 0.000079, Tokens per sec:   2661
2023-03-09 14:44:17,457 - INFO - __main__ - Epoch  24, Step:   50800, Batch Loss:    12.863442, Lr: 0.000079, Tokens per sec:   2702
2023-03-09 14:44:37,557 - INFO - __main__ - Epoch  24, Step:   50900, Batch Loss:    13.459231, Lr: 0.000079, Tokens per sec:   2628
2023-03-09 14:44:57,600 - INFO - __main__ - Epoch  24, Step:   51000, Batch Loss:    13.533209, Lr: 0.000079, Tokens per sec:   2653
2023-03-09 14:45:17,785 - INFO - __main__ - Epoch  24, Step:   51100, Batch Loss:    15.641774, Lr: 0.000079, Tokens per sec:   2674
2023-03-09 14:45:37,886 - INFO - __main__ - Epoch  24, Step:   51200, Batch Loss:    14.042361, Lr: 0.000079, Tokens per sec:   2716
2023-03-09 14:45:58,059 - INFO - __main__ - Epoch  24, Step:   51300, Batch Loss:    10.593657, Lr: 0.000079, Tokens per sec:   2684
2023-03-09 14:46:17,790 - INFO - __main__ - Epoch  24, Step:   51400, Batch Loss:     9.736680, Lr: 0.000079, Tokens per sec:   2744
2023-03-09 14:46:37,881 - INFO - __main__ - Epoch  24, Step:   51500, Batch Loss:    12.702278, Lr: 0.000079, Tokens per sec:   2691
2023-03-09 14:46:57,846 - INFO - __main__ - Epoch  24, Step:   51600, Batch Loss:    19.651768, Lr: 0.000079, Tokens per sec:   2697
2023-03-09 14:47:17,797 - INFO - __main__ - Epoch  24, Step:   51700, Batch Loss:    16.137152, Lr: 0.000079, Tokens per sec:   2663
2023-03-09 14:47:37,578 - INFO - __main__ - Epoch  24, Step:   51800, Batch Loss:    14.860332, Lr: 0.000079, Tokens per sec:   2686
2023-03-09 14:47:57,824 - INFO - __main__ - Epoch  24, Step:   51900, Batch Loss:    14.278593, Lr: 0.000079, Tokens per sec:   2703
2023-03-09 14:48:17,879 - INFO - __main__ - Epoch  24, Step:   52000, Batch Loss:    16.941189, Lr: 0.000079, Tokens per sec:   2673
2023-03-09 14:48:37,887 - INFO - __main__ - Epoch  24, Step:   52100, Batch Loss:    10.414787, Lr: 0.000079, Tokens per sec:   2692
2023-03-09 14:48:57,734 - INFO - __main__ - Epoch  24, Step:   52200, Batch Loss:    12.302238, Lr: 0.000079, Tokens per sec:   2709
2023-03-09 14:49:17,046 - INFO - __main__ - Epoch  24: total training loss 32493.93
2023-03-09 14:49:17,047 - INFO - __main__ - Epoch 25
2023-03-09 14:49:18,227 - INFO - __main__ - Epoch  25, Step:   52300, Batch Loss:    11.705810, Lr: 0.000079, Tokens per sec:   1847
2023-03-09 14:49:38,286 - INFO - __main__ - Epoch  25, Step:   52400, Batch Loss:    15.670464, Lr: 0.000079, Tokens per sec:   2685
2023-03-09 14:49:57,990 - INFO - __main__ - Epoch  25, Step:   52500, Batch Loss:    10.657852, Lr: 0.000079, Tokens per sec:   2732
2023-03-09 14:50:18,095 - INFO - __main__ - Epoch  25, Step:   52600, Batch Loss:    15.218183, Lr: 0.000079, Tokens per sec:   2629
2023-03-09 14:50:38,030 - INFO - __main__ - Epoch  25, Step:   52700, Batch Loss:    16.424088, Lr: 0.000079, Tokens per sec:   2698
2023-03-09 14:50:58,137 - INFO - __main__ - Epoch  25, Step:   52800, Batch Loss:    16.708483, Lr: 0.000079, Tokens per sec:   2670
2023-03-09 14:51:18,029 - INFO - __main__ - Epoch  25, Step:   52900, Batch Loss:    19.596563, Lr: 0.000079, Tokens per sec:   2691
2023-03-09 14:51:37,612 - INFO - __main__ - Epoch  25, Step:   53000, Batch Loss:    13.982205, Lr: 0.000079, Tokens per sec:   2703
2023-03-09 14:51:57,288 - INFO - __main__ - Epoch  25, Step:   53100, Batch Loss:    23.792858, Lr: 0.000079, Tokens per sec:   2793
2023-03-09 14:52:17,514 - INFO - __main__ - Epoch  25, Step:   53200, Batch Loss:    13.089485, Lr: 0.000079, Tokens per sec:   2652
2023-03-09 14:52:37,594 - INFO - __main__ - Epoch  25, Step:   53300, Batch Loss:    12.157701, Lr: 0.000079, Tokens per sec:   2657
2023-03-09 14:52:57,844 - INFO - __main__ - Epoch  25, Step:   53400, Batch Loss:    20.437603, Lr: 0.000079, Tokens per sec:   2701
2023-03-09 14:53:17,764 - INFO - __main__ - Epoch  25, Step:   53500, Batch Loss:    17.336721, Lr: 0.000079, Tokens per sec:   2628
2023-03-09 14:53:37,866 - INFO - __main__ - Epoch  25, Step:   53600, Batch Loss:     8.412433, Lr: 0.000079, Tokens per sec:   2623
2023-03-09 14:53:57,937 - INFO - __main__ - Epoch  25, Step:   53700, Batch Loss:    13.824927, Lr: 0.000079, Tokens per sec:   2713
2023-03-09 14:54:18,243 - INFO - __main__ - Epoch  25, Step:   53800, Batch Loss:    19.240400, Lr: 0.000079, Tokens per sec:   2634
2023-03-09 14:54:38,046 - INFO - __main__ - Epoch  25, Step:   53900, Batch Loss:    14.162237, Lr: 0.000079, Tokens per sec:   2757
2023-03-09 14:54:58,128 - INFO - __main__ - Epoch  25, Step:   54000, Batch Loss:    17.509619, Lr: 0.000079, Tokens per sec:   2746
2023-03-09 14:55:18,237 - INFO - __main__ - Epoch  25, Step:   54100, Batch Loss:    13.976441, Lr: 0.000079, Tokens per sec:   2685
2023-03-09 14:55:38,166 - INFO - __main__ - Epoch  25, Step:   54200, Batch Loss:    14.642896, Lr: 0.000079, Tokens per sec:   2735
2023-03-09 14:55:58,109 - INFO - __main__ - Epoch  25, Step:   54300, Batch Loss:    23.373663, Lr: 0.000079, Tokens per sec:   2706
2023-03-09 14:56:18,064 - INFO - __main__ - Epoch  25, Step:   54400, Batch Loss:    15.473613, Lr: 0.000079, Tokens per sec:   2711
2023-03-09 14:56:33,084 - INFO - __main__ - Epoch  25: total training loss 30932.89
2023-03-09 14:56:33,086 - INFO - __main__ - Epoch 26
2023-03-09 14:56:38,458 - INFO - __main__ - Epoch  26, Step:   54500, Batch Loss:    11.173857, Lr: 0.000078, Tokens per sec:   2558
2023-03-09 14:56:58,605 - INFO - __main__ - Epoch  26, Step:   54600, Batch Loss:     7.577427, Lr: 0.000078, Tokens per sec:   2624
2023-03-09 14:57:18,704 - INFO - __main__ - Epoch  26, Step:   54700, Batch Loss:    10.690679, Lr: 0.000078, Tokens per sec:   2683
2023-03-09 14:57:38,817 - INFO - __main__ - Epoch  26, Step:   54800, Batch Loss:    12.440648, Lr: 0.000078, Tokens per sec:   2717
2023-03-09 14:57:59,048 - INFO - __main__ - Epoch  26, Step:   54900, Batch Loss:    16.002016, Lr: 0.000078, Tokens per sec:   2656
2023-03-09 14:58:18,716 - INFO - __main__ - Epoch  26, Step:   55000, Batch Loss:    12.541297, Lr: 0.000078, Tokens per sec:   2714
2023-03-09 14:58:38,803 - INFO - __main__ - Epoch  26, Step:   55100, Batch Loss:     8.563724, Lr: 0.000078, Tokens per sec:   2655
2023-03-09 14:58:58,987 - INFO - __main__ - Epoch  26, Step:   55200, Batch Loss:    14.160521, Lr: 0.000078, Tokens per sec:   2658
2023-03-09 14:59:19,164 - INFO - __main__ - Epoch  26, Step:   55300, Batch Loss:    14.408814, Lr: 0.000078, Tokens per sec:   2670
2023-03-09 14:59:39,352 - INFO - __main__ - Epoch  26, Step:   55400, Batch Loss:    10.948577, Lr: 0.000078, Tokens per sec:   2645
2023-03-09 14:59:59,503 - INFO - __main__ - Epoch  26, Step:   55500, Batch Loss:    10.868287, Lr: 0.000078, Tokens per sec:   2701
2023-03-09 15:00:19,632 - INFO - __main__ - Epoch  26, Step:   55600, Batch Loss:    11.814512, Lr: 0.000078, Tokens per sec:   2669
2023-03-09 15:00:39,739 - INFO - __main__ - Epoch  26, Step:   55700, Batch Loss:     9.071351, Lr: 0.000078, Tokens per sec:   2686
2023-03-09 15:00:59,992 - INFO - __main__ - Epoch  26, Step:   55800, Batch Loss:    13.780227, Lr: 0.000078, Tokens per sec:   2651
2023-03-09 15:01:20,221 - INFO - __main__ - Epoch  26, Step:   55900, Batch Loss:    16.885954, Lr: 0.000078, Tokens per sec:   2652
2023-03-09 15:01:40,162 - INFO - __main__ - Epoch  26, Step:   56000, Batch Loss:    17.004637, Lr: 0.000078, Tokens per sec:   2675
2023-03-09 15:01:59,817 - INFO - __main__ - Epoch  26, Step:   56100, Batch Loss:    12.225371, Lr: 0.000078, Tokens per sec:   2772
2023-03-09 15:02:19,954 - INFO - __main__ - Epoch  26, Step:   56200, Batch Loss:    14.473740, Lr: 0.000078, Tokens per sec:   2705
2023-03-09 15:02:40,026 - INFO - __main__ - Epoch  26, Step:   56300, Batch Loss:    13.963854, Lr: 0.000078, Tokens per sec:   2695
2023-03-09 15:03:00,212 - INFO - __main__ - Epoch  26, Step:   56400, Batch Loss:    18.201828, Lr: 0.000078, Tokens per sec:   2677
2023-03-09 15:03:20,240 - INFO - __main__ - Epoch  26, Step:   56500, Batch Loss:    12.526724, Lr: 0.000078, Tokens per sec:   2678
2023-03-09 15:03:40,291 - INFO - __main__ - Epoch  26, Step:   56600, Batch Loss:    13.749981, Lr: 0.000078, Tokens per sec:   2693
2023-03-09 15:03:51,196 - INFO - __main__ - Epoch  26: total training loss 29425.79
2023-03-09 15:03:51,197 - INFO - __main__ - Epoch 27
2023-03-09 15:04:00,780 - INFO - __main__ - Epoch  27, Step:   56700, Batch Loss:    14.536002, Lr: 0.000077, Tokens per sec:   2618
2023-03-09 15:04:20,849 - INFO - __main__ - Epoch  27, Step:   56800, Batch Loss:    13.704287, Lr: 0.000077, Tokens per sec:   2693
2023-03-09 15:04:40,867 - INFO - __main__ - Epoch  27, Step:   56900, Batch Loss:    10.609884, Lr: 0.000077, Tokens per sec:   2661
2023-03-09 15:05:01,457 - INFO - __main__ - Epoch  27, Step:   57000, Batch Loss:    15.348766, Lr: 0.000077, Tokens per sec:   2609
2023-03-09 15:05:21,556 - INFO - __main__ - Epoch  27, Step:   57100, Batch Loss:     8.391812, Lr: 0.000077, Tokens per sec:   2699
2023-03-09 15:05:41,725 - INFO - __main__ - Epoch  27, Step:   57200, Batch Loss:    16.545139, Lr: 0.000077, Tokens per sec:   2645
2023-03-09 15:06:01,774 - INFO - __main__ - Epoch  27, Step:   57300, Batch Loss:    17.444525, Lr: 0.000077, Tokens per sec:   2706
2023-03-09 15:06:21,484 - INFO - __main__ - Epoch  27, Step:   57400, Batch Loss:    17.162313, Lr: 0.000077, Tokens per sec:   2803
2023-03-09 15:06:41,536 - INFO - __main__ - Epoch  27, Step:   57500, Batch Loss:    12.447360, Lr: 0.000077, Tokens per sec:   2653
2023-03-09 15:07:01,570 - INFO - __main__ - Epoch  27, Step:   57600, Batch Loss:     9.440921, Lr: 0.000077, Tokens per sec:   2709
2023-03-09 15:07:21,680 - INFO - __main__ - Epoch  27, Step:   57700, Batch Loss:    13.337765, Lr: 0.000077, Tokens per sec:   2744
2023-03-09 15:07:41,712 - INFO - __main__ - Epoch  27, Step:   57800, Batch Loss:    10.575139, Lr: 0.000077, Tokens per sec:   2672
2023-03-09 15:08:01,906 - INFO - __main__ - Epoch  27, Step:   57900, Batch Loss:     8.452323, Lr: 0.000077, Tokens per sec:   2663
2023-03-09 15:08:22,187 - INFO - __main__ - Epoch  27, Step:   58000, Batch Loss:    11.239988, Lr: 0.000077, Tokens per sec:   2653
2023-03-09 15:08:42,288 - INFO - __main__ - Epoch  27, Step:   58100, Batch Loss:    14.512750, Lr: 0.000077, Tokens per sec:   2677
2023-03-09 15:09:02,441 - INFO - __main__ - Epoch  27, Step:   58200, Batch Loss:    13.694983, Lr: 0.000077, Tokens per sec:   2658
2023-03-09 15:09:22,506 - INFO - __main__ - Epoch  27, Step:   58300, Batch Loss:    15.856424, Lr: 0.000077, Tokens per sec:   2713
2023-03-09 15:09:42,542 - INFO - __main__ - Epoch  27, Step:   58400, Batch Loss:    12.110660, Lr: 0.000077, Tokens per sec:   2672
2023-03-09 15:10:02,567 - INFO - __main__ - Epoch  27, Step:   58500, Batch Loss:    10.583012, Lr: 0.000077, Tokens per sec:   2633
2023-03-09 15:10:22,684 - INFO - __main__ - Epoch  27, Step:   58600, Batch Loss:    11.452557, Lr: 0.000077, Tokens per sec:   2647
2023-03-09 15:10:42,760 - INFO - __main__ - Epoch  27, Step:   58700, Batch Loss:    14.769393, Lr: 0.000077, Tokens per sec:   2652
2023-03-09 15:11:02,875 - INFO - __main__ - Epoch  27, Step:   58800, Batch Loss:    12.192387, Lr: 0.000077, Tokens per sec:   2666
2023-03-09 15:11:09,575 - INFO - __main__ - Epoch  27: total training loss 27947.27
2023-03-09 15:11:09,576 - INFO - __main__ - Epoch 28
2023-03-09 15:11:23,074 - INFO - __main__ - Epoch  28, Step:   58900, Batch Loss:    14.931432, Lr: 0.000076, Tokens per sec:   2650
2023-03-09 15:11:43,213 - INFO - __main__ - Epoch  28, Step:   59000, Batch Loss:     9.780233, Lr: 0.000076, Tokens per sec:   2693
2023-03-09 15:12:03,454 - INFO - __main__ - Epoch  28, Step:   59100, Batch Loss:    15.656425, Lr: 0.000076, Tokens per sec:   2660
2023-03-09 15:12:23,670 - INFO - __main__ - Epoch  28, Step:   59200, Batch Loss:    12.180914, Lr: 0.000076, Tokens per sec:   2696
2023-03-09 15:12:43,018 - INFO - __main__ - Epoch  28, Step:   59300, Batch Loss:     9.384494, Lr: 0.000076, Tokens per sec:   2780
2023-03-09 15:13:02,901 - INFO - __main__ - Epoch  28, Step:   59400, Batch Loss:    11.474671, Lr: 0.000076, Tokens per sec:   2705
2023-03-09 15:13:22,538 - INFO - __main__ - Epoch  28, Step:   59500, Batch Loss:     9.425353, Lr: 0.000076, Tokens per sec:   2751
2023-03-09 15:13:42,586 - INFO - __main__ - Epoch  28, Step:   59600, Batch Loss:    11.558464, Lr: 0.000076, Tokens per sec:   2720
2023-03-09 15:14:02,252 - INFO - __main__ - Epoch  28, Step:   59700, Batch Loss:    11.699888, Lr: 0.000076, Tokens per sec:   2728
2023-03-09 15:14:22,269 - INFO - __main__ - Epoch  28, Step:   59800, Batch Loss:    10.157976, Lr: 0.000076, Tokens per sec:   2690
2023-03-09 15:14:41,999 - INFO - __main__ - Epoch  28, Step:   59900, Batch Loss:    15.490527, Lr: 0.000076, Tokens per sec:   2662
2023-03-09 15:15:02,241 - INFO - __main__ - Epoch  28, Step:   60000, Batch Loss:    11.987632, Lr: 0.000076, Tokens per sec:   2673
2023-03-09 15:15:22,294 - INFO - __main__ - Epoch  28, Step:   60100, Batch Loss:    15.285949, Lr: 0.000076, Tokens per sec:   2731
2023-03-09 15:15:42,332 - INFO - __main__ - Epoch  28, Step:   60200, Batch Loss:    16.747519, Lr: 0.000076, Tokens per sec:   2698
2023-03-09 15:16:02,544 - INFO - __main__ - Epoch  28, Step:   60300, Batch Loss:    11.787518, Lr: 0.000076, Tokens per sec:   2588
2023-03-09 15:16:22,654 - INFO - __main__ - Epoch  28, Step:   60400, Batch Loss:    10.960517, Lr: 0.000076, Tokens per sec:   2691
2023-03-09 15:16:42,973 - INFO - __main__ - Epoch  28, Step:   60500, Batch Loss:    14.145363, Lr: 0.000076, Tokens per sec:   2635
2023-03-09 15:17:03,911 - INFO - __main__ - Epoch  28, Step:   60600, Batch Loss:    12.871285, Lr: 0.000076, Tokens per sec:   2586
2023-03-09 15:17:25,369 - INFO - __main__ - Epoch  28, Step:   60700, Batch Loss:    13.959376, Lr: 0.000076, Tokens per sec:   2516
2023-03-09 15:17:45,591 - INFO - __main__ - Epoch  28, Step:   60800, Batch Loss:    15.335027, Lr: 0.000076, Tokens per sec:   2698
2023-03-09 15:18:05,836 - INFO - __main__ - Epoch  28, Step:   60900, Batch Loss:    10.860065, Lr: 0.000076, Tokens per sec:   2640
2023-03-09 15:18:26,013 - INFO - __main__ - Epoch  28, Step:   61000, Batch Loss:    12.927682, Lr: 0.000076, Tokens per sec:   2647
2023-03-09 15:18:28,377 - INFO - __main__ - Epoch  28: total training loss 26624.65
2023-03-09 15:18:28,378 - INFO - __main__ - Epoch 29
2023-03-09 15:18:47,403 - INFO - __main__ - Epoch  29, Step:   61100, Batch Loss:     7.656009, Lr: 0.000075, Tokens per sec:   2485
2023-03-09 15:19:08,317 - INFO - __main__ - Epoch  29, Step:   61200, Batch Loss:    11.222259, Lr: 0.000075, Tokens per sec:   2595
2023-03-09 15:19:28,605 - INFO - __main__ - Epoch  29, Step:   61300, Batch Loss:    10.885973, Lr: 0.000075, Tokens per sec:   2586
2023-03-09 15:19:48,984 - INFO - __main__ - Epoch  29, Step:   61400, Batch Loss:    13.721403, Lr: 0.000075, Tokens per sec:   2634
2023-03-09 15:20:09,304 - INFO - __main__ - Epoch  29, Step:   61500, Batch Loss:     8.172064, Lr: 0.000075, Tokens per sec:   2693
2023-03-09 15:20:29,862 - INFO - __main__ - Epoch  29, Step:   61600, Batch Loss:    14.899713, Lr: 0.000075, Tokens per sec:   2658
2023-03-09 15:20:49,938 - INFO - __main__ - Epoch  29, Step:   61700, Batch Loss:    13.218458, Lr: 0.000075, Tokens per sec:   2706
2023-03-09 15:21:10,348 - INFO - __main__ - Epoch  29, Step:   61800, Batch Loss:    11.240449, Lr: 0.000075, Tokens per sec:   2669
2023-03-09 15:21:30,658 - INFO - __main__ - Epoch  29, Step:   61900, Batch Loss:    15.187891, Lr: 0.000075, Tokens per sec:   2692
2023-03-09 15:21:50,750 - INFO - __main__ - Epoch  29, Step:   62000, Batch Loss:    12.477920, Lr: 0.000075, Tokens per sec:   2625
2023-03-09 15:22:10,853 - INFO - __main__ - Epoch  29, Step:   62100, Batch Loss:    11.653374, Lr: 0.000075, Tokens per sec:   2641
2023-03-09 15:22:31,263 - INFO - __main__ - Epoch  29, Step:   62200, Batch Loss:    11.460289, Lr: 0.000075, Tokens per sec:   2662
2023-03-09 15:22:51,673 - INFO - __main__ - Epoch  29, Step:   62300, Batch Loss:     8.045869, Lr: 0.000075, Tokens per sec:   2623
2023-03-09 15:23:11,872 - INFO - __main__ - Epoch  29, Step:   62400, Batch Loss:    12.287814, Lr: 0.000075, Tokens per sec:   2683
2023-03-09 15:23:32,150 - INFO - __main__ - Epoch  29, Step:   62500, Batch Loss:    12.537625, Lr: 0.000075, Tokens per sec:   2649
2023-03-09 15:23:52,307 - INFO - __main__ - Epoch  29, Step:   62600, Batch Loss:    13.179110, Lr: 0.000075, Tokens per sec:   2654
2023-03-09 15:24:12,428 - INFO - __main__ - Epoch  29, Step:   62700, Batch Loss:    12.659698, Lr: 0.000075, Tokens per sec:   2685
2023-03-09 15:24:32,709 - INFO - __main__ - Epoch  29, Step:   62800, Batch Loss:    14.986485, Lr: 0.000075, Tokens per sec:   2618
2023-03-09 15:24:53,040 - INFO - __main__ - Epoch  29, Step:   62900, Batch Loss:    13.913212, Lr: 0.000075, Tokens per sec:   2634
2023-03-09 15:25:13,267 - INFO - __main__ - Epoch  29, Step:   63000, Batch Loss:    13.771322, Lr: 0.000075, Tokens per sec:   2638
2023-03-09 15:25:33,282 - INFO - __main__ - Epoch  29, Step:   63100, Batch Loss:    14.970778, Lr: 0.000075, Tokens per sec:   2663
2023-03-09 15:25:51,697 - INFO - __main__ - Epoch  29: total training loss 25342.15
2023-03-09 15:25:51,698 - INFO - __main__ - Epoch 30
2023-03-09 15:25:53,936 - INFO - __main__ - Epoch  30, Step:   63200, Batch Loss:     8.068099, Lr: 0.000075, Tokens per sec:   2000
2023-03-09 15:26:13,892 - INFO - __main__ - Epoch  30, Step:   63300, Batch Loss:     8.290514, Lr: 0.000075, Tokens per sec:   2746
2023-03-09 15:26:33,989 - INFO - __main__ - Epoch  30, Step:   63400, Batch Loss:     8.912500, Lr: 0.000075, Tokens per sec:   2664
2023-03-09 15:26:54,178 - INFO - __main__ - Epoch  30, Step:   63500, Batch Loss:    13.115215, Lr: 0.000075, Tokens per sec:   2663
2023-03-09 15:27:14,070 - INFO - __main__ - Epoch  30, Step:   63600, Batch Loss:    11.093123, Lr: 0.000075, Tokens per sec:   2706
2023-03-09 15:27:34,188 - INFO - __main__ - Epoch  30, Step:   63700, Batch Loss:    12.700580, Lr: 0.000075, Tokens per sec:   2708
2023-03-09 15:27:54,377 - INFO - __main__ - Epoch  30, Step:   63800, Batch Loss:    13.319830, Lr: 0.000075, Tokens per sec:   2703
2023-03-09 15:28:14,216 - INFO - __main__ - Epoch  30, Step:   63900, Batch Loss:    10.390484, Lr: 0.000075, Tokens per sec:   2715
2023-03-09 15:28:34,157 - INFO - __main__ - Epoch  30, Step:   64000, Batch Loss:     8.864187, Lr: 0.000075, Tokens per sec:   2699
2023-03-09 15:28:54,171 - INFO - __main__ - Epoch  30, Step:   64100, Batch Loss:     9.632209, Lr: 0.000075, Tokens per sec:   2711
2023-03-09 15:29:14,347 - INFO - __main__ - Epoch  30, Step:   64200, Batch Loss:    12.080706, Lr: 0.000075, Tokens per sec:   2659
2023-03-09 15:29:34,364 - INFO - __main__ - Epoch  30, Step:   64300, Batch Loss:    13.859266, Lr: 0.000075, Tokens per sec:   2647
2023-03-09 15:29:54,533 - INFO - __main__ - Epoch  30, Step:   64400, Batch Loss:    11.654552, Lr: 0.000075, Tokens per sec:   2655
2023-03-09 15:30:14,203 - INFO - __main__ - Epoch  30, Step:   64500, Batch Loss:    15.580604, Lr: 0.000075, Tokens per sec:   2749
2023-03-09 15:30:34,282 - INFO - __main__ - Epoch  30, Step:   64600, Batch Loss:    11.784030, Lr: 0.000075, Tokens per sec:   2676
2023-03-09 15:30:54,140 - INFO - __main__ - Epoch  30, Step:   64700, Batch Loss:    11.191113, Lr: 0.000075, Tokens per sec:   2643
2023-03-09 15:31:14,234 - INFO - __main__ - Epoch  30, Step:   64800, Batch Loss:     7.962827, Lr: 0.000075, Tokens per sec:   2673
2023-03-09 15:31:34,472 - INFO - __main__ - Epoch  30, Step:   64900, Batch Loss:    15.489581, Lr: 0.000075, Tokens per sec:   2718
2023-03-09 15:31:54,424 - INFO - __main__ - Epoch  30, Step:   65000, Batch Loss:    12.812715, Lr: 0.000075, Tokens per sec:   2674
2023-03-09 15:32:14,435 - INFO - __main__ - Epoch  30, Step:   65100, Batch Loss:    10.033951, Lr: 0.000075, Tokens per sec:   2698
2023-03-09 15:32:34,705 - INFO - __main__ - Epoch  30, Step:   65200, Batch Loss:    10.891017, Lr: 0.000075, Tokens per sec:   2619
2023-03-09 15:32:54,847 - INFO - __main__ - Epoch  30, Step:   65300, Batch Loss:    13.895430, Lr: 0.000075, Tokens per sec:   2708
2023-03-09 15:33:09,205 - INFO - __main__ - Epoch  30: total training loss 24242.13
2023-03-09 15:33:09,206 - INFO - __main__ - Epoch 31
2023-03-09 15:33:15,620 - INFO - __main__ - Epoch  31, Step:   65400, Batch Loss:    12.173216, Lr: 0.000074, Tokens per sec:   2502
2023-03-09 15:33:35,894 - INFO - __main__ - Epoch  31, Step:   65500, Batch Loss:     7.663841, Lr: 0.000074, Tokens per sec:   2661
2023-03-09 15:33:56,125 - INFO - __main__ - Epoch  31, Step:   65600, Batch Loss:    11.840326, Lr: 0.000074, Tokens per sec:   2669
2023-03-09 15:34:16,341 - INFO - __main__ - Epoch  31, Step:   65700, Batch Loss:     8.977342, Lr: 0.000074, Tokens per sec:   2668
2023-03-09 15:34:36,427 - INFO - __main__ - Epoch  31, Step:   65800, Batch Loss:     9.194989, Lr: 0.000074, Tokens per sec:   2636
2023-03-09 15:34:56,231 - INFO - __main__ - Epoch  31, Step:   65900, Batch Loss:     8.164618, Lr: 0.000074, Tokens per sec:   2690
2023-03-09 15:35:16,166 - INFO - __main__ - Epoch  31, Step:   66000, Batch Loss:     9.763447, Lr: 0.000074, Tokens per sec:   2679
2023-03-09 15:35:36,370 - INFO - __main__ - Epoch  31, Step:   66100, Batch Loss:    11.474219, Lr: 0.000074, Tokens per sec:   2672
2023-03-09 15:35:56,129 - INFO - __main__ - Epoch  31, Step:   66200, Batch Loss:     8.824294, Lr: 0.000074, Tokens per sec:   2734
2023-03-09 15:36:16,077 - INFO - __main__ - Epoch  31, Step:   66300, Batch Loss:    11.959942, Lr: 0.000074, Tokens per sec:   2649
2023-03-09 15:36:36,445 - INFO - __main__ - Epoch  31, Step:   66400, Batch Loss:    11.416368, Lr: 0.000074, Tokens per sec:   2697
2023-03-09 15:36:56,381 - INFO - __main__ - Epoch  31, Step:   66500, Batch Loss:     8.734812, Lr: 0.000074, Tokens per sec:   2714
2023-03-09 15:37:16,300 - INFO - __main__ - Epoch  31, Step:   66600, Batch Loss:    11.908423, Lr: 0.000074, Tokens per sec:   2680
2023-03-09 15:37:36,541 - INFO - __main__ - Epoch  31, Step:   66700, Batch Loss:     8.595611, Lr: 0.000074, Tokens per sec:   2652
2023-03-09 15:37:56,705 - INFO - __main__ - Epoch  31, Step:   66800, Batch Loss:     8.613436, Lr: 0.000074, Tokens per sec:   2677
2023-03-09 15:38:16,913 - INFO - __main__ - Epoch  31, Step:   66900, Batch Loss:    11.134612, Lr: 0.000074, Tokens per sec:   2657
2023-03-09 15:38:36,970 - INFO - __main__ - Epoch  31, Step:   67000, Batch Loss:    11.331301, Lr: 0.000074, Tokens per sec:   2751
2023-03-09 15:38:56,946 - INFO - __main__ - Epoch  31, Step:   67100, Batch Loss:    10.700910, Lr: 0.000074, Tokens per sec:   2697
2023-03-09 15:39:17,100 - INFO - __main__ - Epoch  31, Step:   67200, Batch Loss:    13.212153, Lr: 0.000074, Tokens per sec:   2658
2023-03-09 15:39:37,412 - INFO - __main__ - Epoch  31, Step:   67300, Batch Loss:    15.476473, Lr: 0.000074, Tokens per sec:   2670
2023-03-09 15:39:57,216 - INFO - __main__ - Epoch  31, Step:   67400, Batch Loss:     9.178515, Lr: 0.000074, Tokens per sec:   2705
2023-03-09 15:40:17,158 - INFO - __main__ - Epoch  31, Step:   67500, Batch Loss:    11.458870, Lr: 0.000074, Tokens per sec:   2677
2023-03-09 15:40:26,645 - INFO - __main__ - Epoch  31: total training loss 23102.08
2023-03-09 15:40:26,646 - INFO - __main__ - Epoch 32
2023-03-09 15:40:37,324 - INFO - __main__ - Epoch  32, Step:   67600, Batch Loss:     7.357838, Lr: 0.000073, Tokens per sec:   2554
2023-03-09 15:40:57,089 - INFO - __main__ - Epoch  32, Step:   67700, Batch Loss:     9.985069, Lr: 0.000073, Tokens per sec:   2748
2023-03-09 15:41:16,955 - INFO - __main__ - Epoch  32, Step:   67800, Batch Loss:     8.811549, Lr: 0.000073, Tokens per sec:   2741
2023-03-09 15:41:36,809 - INFO - __main__ - Epoch  32, Step:   67900, Batch Loss:     9.694857, Lr: 0.000073, Tokens per sec:   2715
2023-03-09 15:41:56,794 - INFO - __main__ - Epoch  32, Step:   68000, Batch Loss:     8.226709, Lr: 0.000073, Tokens per sec:   2614
2023-03-09 15:42:17,074 - INFO - __main__ - Epoch  32, Step:   68100, Batch Loss:     9.657013, Lr: 0.000073, Tokens per sec:   2676
2023-03-09 15:42:37,122 - INFO - __main__ - Epoch  32, Step:   68200, Batch Loss:     9.481168, Lr: 0.000073, Tokens per sec:   2684
2023-03-09 15:42:57,264 - INFO - __main__ - Epoch  32, Step:   68300, Batch Loss:    15.198880, Lr: 0.000073, Tokens per sec:   2647
2023-03-09 15:43:17,167 - INFO - __main__ - Epoch  32, Step:   68400, Batch Loss:    10.090137, Lr: 0.000073, Tokens per sec:   2685
2023-03-09 15:43:37,142 - INFO - __main__ - Epoch  32, Step:   68500, Batch Loss:    10.214233, Lr: 0.000073, Tokens per sec:   2718
2023-03-09 15:43:57,154 - INFO - __main__ - Epoch  32, Step:   68600, Batch Loss:    11.487297, Lr: 0.000073, Tokens per sec:   2705
2023-03-09 15:44:16,982 - INFO - __main__ - Epoch  32, Step:   68700, Batch Loss:    10.790927, Lr: 0.000073, Tokens per sec:   2648
2023-03-09 15:44:36,854 - INFO - __main__ - Epoch  32, Step:   68800, Batch Loss:    12.040005, Lr: 0.000073, Tokens per sec:   2741
2023-03-09 15:44:56,950 - INFO - __main__ - Epoch  32, Step:   68900, Batch Loss:     8.708150, Lr: 0.000073, Tokens per sec:   2692
2023-03-09 15:45:16,893 - INFO - __main__ - Epoch  32, Step:   69000, Batch Loss:    12.105860, Lr: 0.000073, Tokens per sec:   2703
2023-03-09 15:45:37,152 - INFO - __main__ - Epoch  32, Step:   69100, Batch Loss:    10.745013, Lr: 0.000073, Tokens per sec:   2657
2023-03-09 15:45:57,198 - INFO - __main__ - Epoch  32, Step:   69200, Batch Loss:     6.887180, Lr: 0.000073, Tokens per sec:   2750
2023-03-09 15:46:17,333 - INFO - __main__ - Epoch  32, Step:   69300, Batch Loss:    11.065075, Lr: 0.000073, Tokens per sec:   2682
2023-03-09 15:46:37,281 - INFO - __main__ - Epoch  32, Step:   69400, Batch Loss:    10.377630, Lr: 0.000073, Tokens per sec:   2705
2023-03-09 15:46:56,983 - INFO - __main__ - Epoch  32, Step:   69500, Batch Loss:     7.692923, Lr: 0.000073, Tokens per sec:   2733
2023-03-09 15:47:17,161 - INFO - __main__ - Epoch  32, Step:   69600, Batch Loss:    10.678446, Lr: 0.000073, Tokens per sec:   2692
2023-03-09 15:47:37,179 - INFO - __main__ - Epoch  32, Step:   69700, Batch Loss:     9.934685, Lr: 0.000073, Tokens per sec:   2613
2023-03-09 15:47:42,859 - INFO - __main__ - Epoch  32: total training loss 22063.90
2023-03-09 15:47:42,860 - INFO - __main__ - Epoch 33
2023-03-09 15:47:57,775 - INFO - __main__ - Epoch  33, Step:   69800, Batch Loss:     8.155203, Lr: 0.000072, Tokens per sec:   2614
2023-03-09 15:48:17,760 - INFO - __main__ - Epoch  33, Step:   69900, Batch Loss:     8.792963, Lr: 0.000072, Tokens per sec:   2678
2023-03-09 15:48:37,642 - INFO - __main__ - Epoch  33, Step:   70000, Batch Loss:     6.802521, Lr: 0.000072, Tokens per sec:   2723
2023-03-09 15:48:57,651 - INFO - __main__ - Epoch  33, Step:   70100, Batch Loss:     8.447649, Lr: 0.000072, Tokens per sec:   2628
2023-03-09 15:49:17,572 - INFO - __main__ - Epoch  33, Step:   70200, Batch Loss:    12.003201, Lr: 0.000072, Tokens per sec:   2732
2023-03-09 15:49:37,318 - INFO - __main__ - Epoch  33, Step:   70300, Batch Loss:     9.087940, Lr: 0.000072, Tokens per sec:   2720
2023-03-09 15:49:56,843 - INFO - __main__ - Epoch  33, Step:   70400, Batch Loss:     6.645362, Lr: 0.000072, Tokens per sec:   2746
2023-03-09 15:50:16,876 - INFO - __main__ - Epoch  33, Step:   70500, Batch Loss:     8.691287, Lr: 0.000072, Tokens per sec:   2678
2023-03-09 15:50:36,963 - INFO - __main__ - Epoch  33, Step:   70600, Batch Loss:     6.194620, Lr: 0.000072, Tokens per sec:   2722
2023-03-09 15:50:56,374 - INFO - __main__ - Epoch  33, Step:   70700, Batch Loss:    11.625364, Lr: 0.000072, Tokens per sec:   2765
2023-03-09 15:51:16,490 - INFO - __main__ - Epoch  33, Step:   70800, Batch Loss:    13.099461, Lr: 0.000072, Tokens per sec:   2661
2023-03-09 15:51:36,601 - INFO - __main__ - Epoch  33, Step:   70900, Batch Loss:    10.788934, Lr: 0.000072, Tokens per sec:   2657
2023-03-09 15:51:56,690 - INFO - __main__ - Epoch  33, Step:   71000, Batch Loss:     6.661767, Lr: 0.000072, Tokens per sec:   2654
2023-03-09 15:52:16,389 - INFO - __main__ - Epoch  33, Step:   71100, Batch Loss:     8.467245, Lr: 0.000072, Tokens per sec:   2713
2023-03-09 15:52:36,390 - INFO - __main__ - Epoch  33, Step:   71200, Batch Loss:     8.851371, Lr: 0.000072, Tokens per sec:   2680
2023-03-09 15:52:55,374 - INFO - __main__ - Epoch  33, Step:   71300, Batch Loss:     8.583138, Lr: 0.000072, Tokens per sec:   2854
2023-03-09 15:53:15,577 - INFO - __main__ - Epoch  33, Step:   71400, Batch Loss:     8.621024, Lr: 0.000072, Tokens per sec:   2648
2023-03-09 15:53:35,507 - INFO - __main__ - Epoch  33, Step:   71500, Batch Loss:    11.615982, Lr: 0.000072, Tokens per sec:   2712
2023-03-09 15:53:55,756 - INFO - __main__ - Epoch  33, Step:   71600, Batch Loss:     9.518766, Lr: 0.000072, Tokens per sec:   2690
2023-03-09 15:54:15,863 - INFO - __main__ - Epoch  33, Step:   71700, Batch Loss:     9.963964, Lr: 0.000072, Tokens per sec:   2676
2023-03-09 15:54:35,779 - INFO - __main__ - Epoch  33, Step:   71800, Batch Loss:    11.965676, Lr: 0.000072, Tokens per sec:   2799
2023-03-09 15:54:55,834 - INFO - __main__ - Epoch  33, Step:   71900, Batch Loss:     9.863326, Lr: 0.000072, Tokens per sec:   2669
2023-03-09 15:54:57,184 - INFO - __main__ - Epoch  33: total training loss 21172.07
2023-03-09 15:54:57,185 - INFO - __main__ - Epoch 34
2023-03-09 15:55:16,290 - INFO - __main__ - Epoch  34, Step:   72000, Batch Loss:    12.532933, Lr: 0.000072, Tokens per sec:   2612
2023-03-09 15:55:36,284 - INFO - __main__ - Epoch  34, Step:   72100, Batch Loss:     8.269990, Lr: 0.000072, Tokens per sec:   2699
2023-03-09 15:55:56,316 - INFO - __main__ - Epoch  34, Step:   72200, Batch Loss:     7.580904, Lr: 0.000072, Tokens per sec:   2691
2023-03-09 15:56:16,389 - INFO - __main__ - Epoch  34, Step:   72300, Batch Loss:     7.076245, Lr: 0.000072, Tokens per sec:   2688
2023-03-09 15:56:36,371 - INFO - __main__ - Epoch  34, Step:   72400, Batch Loss:    11.119642, Lr: 0.000072, Tokens per sec:   2721
2023-03-09 15:56:56,377 - INFO - __main__ - Epoch  34, Step:   72500, Batch Loss:     8.766842, Lr: 0.000072, Tokens per sec:   2623
2023-03-09 15:57:16,493 - INFO - __main__ - Epoch  34, Step:   72600, Batch Loss:    11.140173, Lr: 0.000072, Tokens per sec:   2755
2023-03-09 15:57:36,326 - INFO - __main__ - Epoch  34, Step:   72700, Batch Loss:     7.884701, Lr: 0.000072, Tokens per sec:   2657
2023-03-09 15:57:56,125 - INFO - __main__ - Epoch  34, Step:   72800, Batch Loss:     9.449965, Lr: 0.000072, Tokens per sec:   2718
2023-03-09 15:58:16,400 - INFO - __main__ - Epoch  34, Step:   72900, Batch Loss:     8.100749, Lr: 0.000072, Tokens per sec:   2629
2023-03-09 15:58:36,376 - INFO - __main__ - Epoch  34, Step:   73000, Batch Loss:     8.327281, Lr: 0.000072, Tokens per sec:   2679
2023-03-09 15:58:56,641 - INFO - __main__ - Epoch  34, Step:   73100, Batch Loss:     9.253798, Lr: 0.000072, Tokens per sec:   2638
2023-03-09 15:59:17,007 - INFO - __main__ - Epoch  34, Step:   73200, Batch Loss:     8.112303, Lr: 0.000072, Tokens per sec:   2668
2023-03-09 15:59:37,088 - INFO - __main__ - Epoch  34, Step:   73300, Batch Loss:     6.386172, Lr: 0.000072, Tokens per sec:   2723
2023-03-09 15:59:57,058 - INFO - __main__ - Epoch  34, Step:   73400, Batch Loss:     8.567152, Lr: 0.000072, Tokens per sec:   2687
2023-03-09 16:00:17,281 - INFO - __main__ - Epoch  34, Step:   73500, Batch Loss:    11.046967, Lr: 0.000072, Tokens per sec:   2713
2023-03-09 16:00:36,870 - INFO - __main__ - Epoch  34, Step:   73600, Batch Loss:     6.822231, Lr: 0.000072, Tokens per sec:   2703
2023-03-09 16:00:57,143 - INFO - __main__ - Epoch  34, Step:   73700, Batch Loss:     8.874466, Lr: 0.000072, Tokens per sec:   2661
2023-03-09 16:01:17,233 - INFO - __main__ - Epoch  34, Step:   73800, Batch Loss:     7.794935, Lr: 0.000072, Tokens per sec:   2679
2023-03-09 16:01:37,320 - INFO - __main__ - Epoch  34, Step:   73900, Batch Loss:    13.806736, Lr: 0.000072, Tokens per sec:   2682
2023-03-09 16:01:57,186 - INFO - __main__ - Epoch  34, Step:   74000, Batch Loss:    12.087020, Lr: 0.000072, Tokens per sec:   2724
2023-03-09 16:02:14,631 - INFO - __main__ - Epoch  34: total training loss 20217.17
2023-03-09 16:02:14,632 - INFO - __main__ - Epoch 35
2023-03-09 16:02:17,801 - INFO - __main__ - Epoch  35, Step:   74100, Batch Loss:     9.355610, Lr: 0.000071, Tokens per sec:   2428
2023-03-09 16:02:37,392 - INFO - __main__ - Epoch  35, Step:   74200, Batch Loss:     8.399776, Lr: 0.000071, Tokens per sec:   2796
2023-03-09 16:02:57,476 - INFO - __main__ - Epoch  35, Step:   74300, Batch Loss:     6.940818, Lr: 0.000071, Tokens per sec:   2695
2023-03-09 16:03:17,630 - INFO - __main__ - Epoch  35, Step:   74400, Batch Loss:     8.287439, Lr: 0.000071, Tokens per sec:   2624
2023-03-09 16:03:37,825 - INFO - __main__ - Epoch  35, Step:   74500, Batch Loss:     9.626133, Lr: 0.000071, Tokens per sec:   2686
2023-03-09 16:03:57,820 - INFO - __main__ - Epoch  35, Step:   74600, Batch Loss:     7.965212, Lr: 0.000071, Tokens per sec:   2707
2023-03-09 16:04:17,757 - INFO - __main__ - Epoch  35, Step:   74700, Batch Loss:     9.129254, Lr: 0.000071, Tokens per sec:   2678
2023-03-09 16:04:37,617 - INFO - __main__ - Epoch  35, Step:   74800, Batch Loss:     7.382298, Lr: 0.000071, Tokens per sec:   2686
2023-03-09 16:04:57,795 - INFO - __main__ - Epoch  35, Step:   74900, Batch Loss:     7.125699, Lr: 0.000071, Tokens per sec:   2710
2023-03-09 16:05:17,838 - INFO - __main__ - Epoch  35, Step:   75000, Batch Loss:     8.563035, Lr: 0.000071, Tokens per sec:   2676
2023-03-09 16:05:37,278 - INFO - __main__ - Epoch  35, Step:   75100, Batch Loss:     9.892712, Lr: 0.000071, Tokens per sec:   2800
2023-03-09 16:05:56,988 - INFO - __main__ - Epoch  35, Step:   75200, Batch Loss:     7.609920, Lr: 0.000071, Tokens per sec:   2717
2023-03-09 16:06:16,530 - INFO - __main__ - Epoch  35, Step:   75300, Batch Loss:     9.557328, Lr: 0.000071, Tokens per sec:   2764
2023-03-09 16:06:36,130 - INFO - __main__ - Epoch  35, Step:   75400, Batch Loss:    11.190808, Lr: 0.000071, Tokens per sec:   2758
2023-03-09 16:06:56,118 - INFO - __main__ - Epoch  35, Step:   75500, Batch Loss:     9.737417, Lr: 0.000071, Tokens per sec:   2687
2023-03-09 16:07:16,255 - INFO - __main__ - Epoch  35, Step:   75600, Batch Loss:    11.021093, Lr: 0.000071, Tokens per sec:   2686
2023-03-09 16:07:36,249 - INFO - __main__ - Epoch  35, Step:   75700, Batch Loss:     9.752215, Lr: 0.000071, Tokens per sec:   2698
2023-03-09 16:07:56,246 - INFO - __main__ - Epoch  35, Step:   75800, Batch Loss:    12.983916, Lr: 0.000071, Tokens per sec:   2673
2023-03-09 16:08:15,914 - INFO - __main__ - Epoch  35, Step:   75900, Batch Loss:    15.902750, Lr: 0.000071, Tokens per sec:   2698
2023-03-09 16:08:35,998 - INFO - __main__ - Epoch  35, Step:   76000, Batch Loss:    10.262265, Lr: 0.000071, Tokens per sec:   2676
2023-03-09 16:08:56,229 - INFO - __main__ - Epoch  35, Step:   76100, Batch Loss:     9.433352, Lr: 0.000071, Tokens per sec:   2642
2023-03-09 16:09:16,146 - INFO - __main__ - Epoch  35, Step:   76200, Batch Loss:    10.640815, Lr: 0.000071, Tokens per sec:   2680
2023-03-09 16:09:29,117 - INFO - __main__ - Epoch  35: total training loss 19368.09
2023-03-09 16:09:29,118 - INFO - __main__ - Epoch 36
2023-03-09 16:09:36,094 - INFO - __main__ - Epoch  36, Step:   76300, Batch Loss:     7.593246, Lr: 0.000070, Tokens per sec:   2811
2023-03-09 16:09:56,184 - INFO - __main__ - Epoch  36, Step:   76400, Batch Loss:     9.499419, Lr: 0.000070, Tokens per sec:   2665
2023-03-09 16:10:16,114 - INFO - __main__ - Epoch  36, Step:   76500, Batch Loss:    11.316054, Lr: 0.000070, Tokens per sec:   2742
2023-03-09 16:10:36,314 - INFO - __main__ - Epoch  36, Step:   76600, Batch Loss:     7.604299, Lr: 0.000070, Tokens per sec:   2692
2023-03-09 16:10:56,370 - INFO - __main__ - Epoch  36, Step:   76700, Batch Loss:     7.473969, Lr: 0.000070, Tokens per sec:   2665
2023-03-09 16:11:16,550 - INFO - __main__ - Epoch  36, Step:   76800, Batch Loss:    11.019837, Lr: 0.000070, Tokens per sec:   2703
2023-03-09 16:11:36,647 - INFO - __main__ - Epoch  36, Step:   76900, Batch Loss:     7.763829, Lr: 0.000070, Tokens per sec:   2651
2023-03-09 16:11:56,623 - INFO - __main__ - Epoch  36, Step:   77000, Batch Loss:     6.363577, Lr: 0.000070, Tokens per sec:   2732
2023-03-09 16:12:16,717 - INFO - __main__ - Epoch  36, Step:   77100, Batch Loss:     6.241793, Lr: 0.000070, Tokens per sec:   2695
2023-03-09 16:12:36,840 - INFO - __main__ - Epoch  36, Step:   77200, Batch Loss:     9.676994, Lr: 0.000070, Tokens per sec:   2638
2023-03-09 16:12:56,932 - INFO - __main__ - Epoch  36, Step:   77300, Batch Loss:     7.621350, Lr: 0.000070, Tokens per sec:   2680
2023-03-09 16:13:16,964 - INFO - __main__ - Epoch  36, Step:   77400, Batch Loss:    11.956275, Lr: 0.000070, Tokens per sec:   2682
2023-03-09 16:13:36,904 - INFO - __main__ - Epoch  36, Step:   77500, Batch Loss:     9.657134, Lr: 0.000070, Tokens per sec:   2649
2023-03-09 16:13:57,047 - INFO - __main__ - Epoch  36, Step:   77600, Batch Loss:     7.708726, Lr: 0.000070, Tokens per sec:   2673
2023-03-09 16:14:17,146 - INFO - __main__ - Epoch  36, Step:   77700, Batch Loss:    12.486737, Lr: 0.000070, Tokens per sec:   2657
2023-03-09 16:14:37,170 - INFO - __main__ - Epoch  36, Step:   77800, Batch Loss:     6.227829, Lr: 0.000070, Tokens per sec:   2683
2023-03-09 16:14:56,674 - INFO - __main__ - Epoch  36, Step:   77900, Batch Loss:     7.684586, Lr: 0.000070, Tokens per sec:   2755
2023-03-09 16:15:16,730 - INFO - __main__ - Epoch  36, Step:   78000, Batch Loss:     8.174659, Lr: 0.000070, Tokens per sec:   2686
2023-03-09 16:15:36,781 - INFO - __main__ - Epoch  36, Step:   78100, Batch Loss:     8.988594, Lr: 0.000070, Tokens per sec:   2696
2023-03-09 16:15:56,605 - INFO - __main__ - Epoch  36, Step:   78200, Batch Loss:     7.248765, Lr: 0.000070, Tokens per sec:   2685
2023-03-09 16:16:16,591 - INFO - __main__ - Epoch  36, Step:   78300, Batch Loss:    10.156476, Lr: 0.000070, Tokens per sec:   2705
2023-03-09 16:16:36,183 - INFO - __main__ - Epoch  36, Step:   78400, Batch Loss:    10.608710, Lr: 0.000070, Tokens per sec:   2751
2023-03-09 16:16:45,050 - INFO - __main__ - Epoch  36: total training loss 18634.75
2023-03-09 16:16:45,051 - INFO - __main__ - Epoch 37
2023-03-09 16:16:56,713 - INFO - __main__ - Epoch  37, Step:   78500, Batch Loss:     9.672893, Lr: 0.000070, Tokens per sec:   2626
2023-03-09 16:17:16,339 - INFO - __main__ - Epoch  37, Step:   78600, Batch Loss:     6.906090, Lr: 0.000070, Tokens per sec:   2698
2023-03-09 16:17:35,998 - INFO - __main__ - Epoch  37, Step:   78700, Batch Loss:     9.004461, Lr: 0.000070, Tokens per sec:   2721
2023-03-09 16:17:56,065 - INFO - __main__ - Epoch  37, Step:   78800, Batch Loss:     7.293353, Lr: 0.000070, Tokens per sec:   2683
2023-03-09 16:18:16,316 - INFO - __main__ - Epoch  37, Step:   78900, Batch Loss:     8.913599, Lr: 0.000070, Tokens per sec:   2641
2023-03-09 16:18:36,197 - INFO - __main__ - Epoch  37, Step:   79000, Batch Loss:     8.097200, Lr: 0.000070, Tokens per sec:   2735
2023-03-09 16:18:56,513 - INFO - __main__ - Epoch  37, Step:   79100, Batch Loss:     7.336842, Lr: 0.000070, Tokens per sec:   2606
2023-03-09 16:19:15,991 - INFO - __main__ - Epoch  37, Step:   79200, Batch Loss:     5.692915, Lr: 0.000070, Tokens per sec:   2763
2023-03-09 16:19:35,969 - INFO - __main__ - Epoch  37, Step:   79300, Batch Loss:     8.185350, Lr: 0.000070, Tokens per sec:   2695
2023-03-09 16:19:55,880 - INFO - __main__ - Epoch  37, Step:   79400, Batch Loss:     7.175948, Lr: 0.000070, Tokens per sec:   2722
2023-03-09 16:20:16,044 - INFO - __main__ - Epoch  37, Step:   79500, Batch Loss:    10.100412, Lr: 0.000070, Tokens per sec:   2632
2023-03-09 16:20:36,270 - INFO - __main__ - Epoch  37, Step:   79600, Batch Loss:    11.634203, Lr: 0.000070, Tokens per sec:   2654
2023-03-09 16:20:56,365 - INFO - __main__ - Epoch  37, Step:   79700, Batch Loss:     7.846889, Lr: 0.000070, Tokens per sec:   2694
2023-03-09 16:21:16,520 - INFO - __main__ - Epoch  37, Step:   79800, Batch Loss:     8.674935, Lr: 0.000070, Tokens per sec:   2670
2023-03-09 16:21:36,831 - INFO - __main__ - Epoch  37, Step:   79900, Batch Loss:     8.968426, Lr: 0.000070, Tokens per sec:   2627
2023-03-09 16:21:56,993 - INFO - __main__ - Epoch  37, Step:   80000, Batch Loss:     5.363560, Lr: 0.000070, Tokens per sec:   2683
2023-03-09 16:22:17,092 - INFO - __main__ - Epoch  37, Step:   80100, Batch Loss:     7.344953, Lr: 0.000070, Tokens per sec:   2734
2023-03-09 16:22:36,867 - INFO - __main__ - Epoch  37, Step:   80200, Batch Loss:     7.007113, Lr: 0.000070, Tokens per sec:   2750
2023-03-09 16:22:56,954 - INFO - __main__ - Epoch  37, Step:   80300, Batch Loss:     8.536991, Lr: 0.000070, Tokens per sec:   2722
2023-03-09 16:23:17,043 - INFO - __main__ - Epoch  37, Step:   80400, Batch Loss:     7.086676, Lr: 0.000070, Tokens per sec:   2654
2023-03-09 16:23:37,072 - INFO - __main__ - Epoch  37, Step:   80500, Batch Loss:     9.912125, Lr: 0.000070, Tokens per sec:   2712
2023-03-09 16:23:57,226 - INFO - __main__ - Epoch  37, Step:   80600, Batch Loss:     7.340637, Lr: 0.000070, Tokens per sec:   2682
2023-03-09 16:24:01,886 - INFO - __main__ - Epoch  37: total training loss 17958.29
2023-03-09 16:24:01,887 - INFO - __main__ - Epoch 38
2023-03-09 16:24:17,856 - INFO - __main__ - Epoch  38, Step:   80700, Batch Loss:     7.024854, Lr: 0.000069, Tokens per sec:   2592
2023-03-09 16:24:37,931 - INFO - __main__ - Epoch  38, Step:   80800, Batch Loss:     5.578868, Lr: 0.000069, Tokens per sec:   2668
2023-03-09 16:24:58,050 - INFO - __main__ - Epoch  38, Step:   80900, Batch Loss:     6.480774, Lr: 0.000069, Tokens per sec:   2680
2023-03-09 16:25:18,000 - INFO - __main__ - Epoch  38, Step:   81000, Batch Loss:     8.047536, Lr: 0.000069, Tokens per sec:   2683
2023-03-09 16:25:37,995 - INFO - __main__ - Epoch  38, Step:   81100, Batch Loss:     8.797368, Lr: 0.000069, Tokens per sec:   2683
2023-03-09 16:25:58,193 - INFO - __main__ - Epoch  38, Step:   81200, Batch Loss:     6.262501, Lr: 0.000069, Tokens per sec:   2688
2023-03-09 16:26:18,011 - INFO - __main__ - Epoch  38, Step:   81300, Batch Loss:     9.326624, Lr: 0.000069, Tokens per sec:   2713
2023-03-09 16:26:37,920 - INFO - __main__ - Epoch  38, Step:   81400, Batch Loss:     8.697731, Lr: 0.000069, Tokens per sec:   2738
2023-03-09 16:26:58,080 - INFO - __main__ - Epoch  38, Step:   81500, Batch Loss:     5.828474, Lr: 0.000069, Tokens per sec:   2682
2023-03-09 16:27:18,009 - INFO - __main__ - Epoch  38, Step:   81600, Batch Loss:     8.158038, Lr: 0.000069, Tokens per sec:   2712
2023-03-09 16:27:38,099 - INFO - __main__ - Epoch  38, Step:   81700, Batch Loss:     7.004121, Lr: 0.000069, Tokens per sec:   2635
2023-03-09 16:27:57,937 - INFO - __main__ - Epoch  38, Step:   81800, Batch Loss:     7.253752, Lr: 0.000069, Tokens per sec:   2719
2023-03-09 16:28:16,484 - INFO - __main__ - Epoch  38, Step:   81900, Batch Loss:     6.855243, Lr: 0.000069, Tokens per sec:   2866
2023-03-09 16:28:35,963 - INFO - __main__ - Epoch  38, Step:   82000, Batch Loss:     6.887321, Lr: 0.000069, Tokens per sec:   2725
2023-03-09 16:28:56,058 - INFO - __main__ - Epoch  38, Step:   82100, Batch Loss:     7.548158, Lr: 0.000069, Tokens per sec:   2685
2023-03-09 16:29:16,299 - INFO - __main__ - Epoch  38, Step:   82200, Batch Loss:     8.386810, Lr: 0.000069, Tokens per sec:   2668
2023-03-09 16:29:36,231 - INFO - __main__ - Epoch  38, Step:   82300, Batch Loss:     5.901652, Lr: 0.000069, Tokens per sec:   2736
2023-03-09 16:29:56,547 - INFO - __main__ - Epoch  38, Step:   82400, Batch Loss:     9.679086, Lr: 0.000069, Tokens per sec:   2662
2023-03-09 16:30:16,725 - INFO - __main__ - Epoch  38, Step:   82500, Batch Loss:     6.593622, Lr: 0.000069, Tokens per sec:   2685
2023-03-09 16:30:36,865 - INFO - __main__ - Epoch  38, Step:   82600, Batch Loss:    10.763145, Lr: 0.000069, Tokens per sec:   2634
2023-03-09 16:30:57,041 - INFO - __main__ - Epoch  38, Step:   82700, Batch Loss:     8.173360, Lr: 0.000069, Tokens per sec:   2671
2023-03-09 16:31:17,204 - INFO - __main__ - Epoch  38, Step:   82800, Batch Loss:     8.805792, Lr: 0.000069, Tokens per sec:   2698
2023-03-09 16:31:17,701 - INFO - __main__ - Epoch  38: total training loss 17256.21
2023-03-09 16:31:17,702 - INFO - __main__ - Epoch 39
2023-03-09 16:31:37,688 - INFO - __main__ - Epoch  39, Step:   82900, Batch Loss:     6.467649, Lr: 0.000068, Tokens per sec:   2606
2023-03-09 16:31:57,733 - INFO - __main__ - Epoch  39, Step:   83000, Batch Loss:     6.791891, Lr: 0.000068, Tokens per sec:   2675
2023-03-09 16:32:17,814 - INFO - __main__ - Epoch  39, Step:   83100, Batch Loss:     5.890665, Lr: 0.000068, Tokens per sec:   2719
2023-03-09 16:32:37,603 - INFO - __main__ - Epoch  39, Step:   83200, Batch Loss:     5.960899, Lr: 0.000068, Tokens per sec:   2716
2023-03-09 16:32:56,751 - INFO - __main__ - Epoch  39, Step:   83300, Batch Loss:     6.712815, Lr: 0.000068, Tokens per sec:   2849
2023-03-09 16:33:16,095 - INFO - __main__ - Epoch  39, Step:   83400, Batch Loss:     8.976864, Lr: 0.000068, Tokens per sec:   2820
2023-03-09 16:33:35,766 - INFO - __main__ - Epoch  39, Step:   83500, Batch Loss:     7.871197, Lr: 0.000068, Tokens per sec:   2704
2023-03-09 16:33:56,047 - INFO - __main__ - Epoch  39, Step:   83600, Batch Loss:     7.575181, Lr: 0.000068, Tokens per sec:   2663
2023-03-09 16:34:16,069 - INFO - __main__ - Epoch  39, Step:   83700, Batch Loss:    10.301481, Lr: 0.000068, Tokens per sec:   2688
2023-03-09 16:34:36,274 - INFO - __main__ - Epoch  39, Step:   83800, Batch Loss:     7.206306, Lr: 0.000068, Tokens per sec:   2666
2023-03-09 16:34:56,384 - INFO - __main__ - Epoch  39, Step:   83900, Batch Loss:    10.720502, Lr: 0.000068, Tokens per sec:   2706
2023-03-09 16:35:15,574 - INFO - __main__ - Epoch  39, Step:   84000, Batch Loss:     7.827612, Lr: 0.000068, Tokens per sec:   2831
2023-03-09 16:35:34,954 - INFO - __main__ - Epoch  39, Step:   84100, Batch Loss:     7.876177, Lr: 0.000068, Tokens per sec:   2778
2023-03-09 16:35:54,692 - INFO - __main__ - Epoch  39, Step:   84200, Batch Loss:     4.755898, Lr: 0.000068, Tokens per sec:   2714
2023-03-09 16:36:14,416 - INFO - __main__ - Epoch  39, Step:   84300, Batch Loss:     9.997172, Lr: 0.000068, Tokens per sec:   2683
2023-03-09 16:36:34,485 - INFO - __main__ - Epoch  39, Step:   84400, Batch Loss:    11.796202, Lr: 0.000068, Tokens per sec:   2694
2023-03-09 16:36:54,568 - INFO - __main__ - Epoch  39, Step:   84500, Batch Loss:     7.362581, Lr: 0.000068, Tokens per sec:   2666
2023-03-09 16:37:14,558 - INFO - __main__ - Epoch  39, Step:   84600, Batch Loss:     7.545868, Lr: 0.000068, Tokens per sec:   2665
2023-03-09 16:37:34,217 - INFO - __main__ - Epoch  39, Step:   84700, Batch Loss:     6.937320, Lr: 0.000068, Tokens per sec:   2757
2023-03-09 16:37:54,127 - INFO - __main__ - Epoch  39, Step:   84800, Batch Loss:     9.074819, Lr: 0.000068, Tokens per sec:   2735
2023-03-09 16:38:13,046 - INFO - __main__ - Epoch  39, Step:   84900, Batch Loss:     8.539121, Lr: 0.000068, Tokens per sec:   2805
2023-03-09 16:38:28,615 - INFO - __main__ - Epoch  39: total training loss 16615.69
2023-03-09 16:38:28,615 - INFO - __main__ - Epoch 40
2023-03-09 16:38:32,777 - INFO - __main__ - Epoch  40, Step:   85000, Batch Loss:     5.165988, Lr: 0.000068, Tokens per sec:   2413
2023-03-09 16:38:52,182 - INFO - __main__ - Epoch  40, Step:   85100, Batch Loss:     5.822607, Lr: 0.000068, Tokens per sec:   2743
2023-03-09 16:39:12,400 - INFO - __main__ - Epoch  40, Step:   85200, Batch Loss:     7.247889, Lr: 0.000068, Tokens per sec:   2647
2023-03-09 16:39:32,031 - INFO - __main__ - Epoch  40, Step:   85300, Batch Loss:     6.136424, Lr: 0.000068, Tokens per sec:   2725
2023-03-09 16:39:52,121 - INFO - __main__ - Epoch  40, Step:   85400, Batch Loss:     5.907736, Lr: 0.000068, Tokens per sec:   2675
2023-03-09 16:40:11,861 - INFO - __main__ - Epoch  40, Step:   85500, Batch Loss:     6.255084, Lr: 0.000068, Tokens per sec:   2715
2023-03-09 16:40:31,636 - INFO - __main__ - Epoch  40, Step:   85600, Batch Loss:     7.214839, Lr: 0.000068, Tokens per sec:   2707
2023-03-09 16:40:51,589 - INFO - __main__ - Epoch  40, Step:   85700, Batch Loss:     6.118246, Lr: 0.000068, Tokens per sec:   2716
2023-03-09 16:41:11,543 - INFO - __main__ - Epoch  40, Step:   85800, Batch Loss:    11.871614, Lr: 0.000068, Tokens per sec:   2734
2023-03-09 16:41:31,740 - INFO - __main__ - Epoch  40, Step:   85900, Batch Loss:     7.290446, Lr: 0.000068, Tokens per sec:   2649
2023-03-09 16:41:51,630 - INFO - __main__ - Epoch  40, Step:   86000, Batch Loss:     7.734195, Lr: 0.000068, Tokens per sec:   2715
2023-03-09 16:42:10,921 - INFO - __main__ - Epoch  40, Step:   86100, Batch Loss:     7.243614, Lr: 0.000068, Tokens per sec:   2838
2023-03-09 16:42:30,938 - INFO - __main__ - Epoch  40, Step:   86200, Batch Loss:     8.806503, Lr: 0.000068, Tokens per sec:   2691
2023-03-09 16:42:51,177 - INFO - __main__ - Epoch  40, Step:   86300, Batch Loss:     5.308057, Lr: 0.000068, Tokens per sec:   2675
2023-03-09 16:43:11,350 - INFO - __main__ - Epoch  40, Step:   86400, Batch Loss:     7.947747, Lr: 0.000068, Tokens per sec:   2642
2023-03-09 16:43:30,731 - INFO - __main__ - Epoch  40, Step:   86500, Batch Loss:     8.204776, Lr: 0.000068, Tokens per sec:   2773
2023-03-09 16:43:50,657 - INFO - __main__ - Epoch  40, Step:   86600, Batch Loss:     8.081159, Lr: 0.000068, Tokens per sec:   2680
2023-03-09 16:44:10,281 - INFO - __main__ - Epoch  40, Step:   86700, Batch Loss:     8.177997, Lr: 0.000068, Tokens per sec:   2732
2023-03-09 16:44:30,340 - INFO - __main__ - Epoch  40, Step:   86800, Batch Loss:     8.141275, Lr: 0.000068, Tokens per sec:   2751
2023-03-09 16:44:50,431 - INFO - __main__ - Epoch  40, Step:   86900, Batch Loss:     7.551187, Lr: 0.000068, Tokens per sec:   2697
2023-03-09 16:45:10,395 - INFO - __main__ - Epoch  40, Step:   87000, Batch Loss:     8.098807, Lr: 0.000068, Tokens per sec:   2675
2023-03-09 16:45:30,585 - INFO - __main__ - Epoch  40, Step:   87100, Batch Loss:     5.587193, Lr: 0.000068, Tokens per sec:   2636
2023-03-09 16:45:42,419 - INFO - __main__ - Epoch  40: total training loss 16060.98
2023-03-09 16:45:42,419 - INFO - __main__ - Epoch 41
2023-03-09 16:45:50,813 - INFO - __main__ - Epoch  41, Step:   87200, Batch Loss:     9.054728, Lr: 0.000067, Tokens per sec:   2544
2023-03-09 16:46:10,888 - INFO - __main__ - Epoch  41, Step:   87300, Batch Loss:     7.594242, Lr: 0.000067, Tokens per sec:   2693
2023-03-09 16:46:30,843 - INFO - __main__ - Epoch  41, Step:   87400, Batch Loss:     6.406819, Lr: 0.000067, Tokens per sec:   2721
2023-03-09 16:46:50,797 - INFO - __main__ - Epoch  41, Step:   87500, Batch Loss:     5.497914, Lr: 0.000067, Tokens per sec:   2675
2023-03-09 16:47:10,590 - INFO - __main__ - Epoch  41, Step:   87600, Batch Loss:     8.911366, Lr: 0.000067, Tokens per sec:   2765
2023-03-09 16:47:30,359 - INFO - __main__ - Epoch  41, Step:   87700, Batch Loss:     5.794294, Lr: 0.000067, Tokens per sec:   2678
2023-03-09 16:47:50,398 - INFO - __main__ - Epoch  41, Step:   87800, Batch Loss:     4.403013, Lr: 0.000067, Tokens per sec:   2737
2023-03-09 16:48:10,292 - INFO - __main__ - Epoch  41, Step:   87900, Batch Loss:     5.278255, Lr: 0.000067, Tokens per sec:   2705
2023-03-09 16:48:30,286 - INFO - __main__ - Epoch  41, Step:   88000, Batch Loss:     6.442537, Lr: 0.000067, Tokens per sec:   2686
2023-03-09 16:48:50,399 - INFO - __main__ - Epoch  41, Step:   88100, Batch Loss:     6.673330, Lr: 0.000067, Tokens per sec:   2636
2023-03-09 16:49:10,426 - INFO - __main__ - Epoch  41, Step:   88200, Batch Loss:     7.325255, Lr: 0.000067, Tokens per sec:   2683
2023-03-09 16:49:29,909 - INFO - __main__ - Epoch  41, Step:   88300, Batch Loss:     5.598887, Lr: 0.000067, Tokens per sec:   2706
2023-03-09 16:49:49,618 - INFO - __main__ - Epoch  41, Step:   88400, Batch Loss:     7.939880, Lr: 0.000067, Tokens per sec:   2749
2023-03-09 16:50:09,618 - INFO - __main__ - Epoch  41, Step:   88500, Batch Loss:     9.142743, Lr: 0.000067, Tokens per sec:   2670
2023-03-09 16:50:29,091 - INFO - __main__ - Epoch  41, Step:   88600, Batch Loss:     7.855572, Lr: 0.000067, Tokens per sec:   2761
2023-03-09 16:50:49,245 - INFO - __main__ - Epoch  41, Step:   88700, Batch Loss:     8.131188, Lr: 0.000067, Tokens per sec:   2693
2023-03-09 16:51:09,451 - INFO - __main__ - Epoch  41, Step:   88800, Batch Loss:     7.728137, Lr: 0.000067, Tokens per sec:   2640
2023-03-09 16:51:29,457 - INFO - __main__ - Epoch  41, Step:   88900, Batch Loss:     5.978284, Lr: 0.000067, Tokens per sec:   2745
2023-03-09 16:51:49,299 - INFO - __main__ - Epoch  41, Step:   89000, Batch Loss:     4.039628, Lr: 0.000067, Tokens per sec:   2712
2023-03-09 16:52:09,233 - INFO - __main__ - Epoch  41, Step:   89100, Batch Loss:     4.535229, Lr: 0.000067, Tokens per sec:   2736
2023-03-09 16:52:29,174 - INFO - __main__ - Epoch  41, Step:   89200, Batch Loss:     6.703797, Lr: 0.000067, Tokens per sec:   2691
2023-03-09 16:52:48,938 - INFO - __main__ - Epoch  41, Step:   89300, Batch Loss:     8.767201, Lr: 0.000067, Tokens per sec:   2717
2023-03-09 16:52:56,874 - INFO - __main__ - Epoch  41: total training loss 15522.02
2023-03-09 16:52:56,876 - INFO - __main__ - Epoch 42
2023-03-09 16:53:09,502 - INFO - __main__ - Epoch  42, Step:   89400, Batch Loss:     5.151503, Lr: 0.000066, Tokens per sec:   2558
2023-03-09 16:53:29,293 - INFO - __main__ - Epoch  42, Step:   89500, Batch Loss:     3.045641, Lr: 0.000066, Tokens per sec:   2694
2023-03-09 16:53:49,325 - INFO - __main__ - Epoch  42, Step:   89600, Batch Loss:     7.379960, Lr: 0.000066, Tokens per sec:   2656
2023-03-09 16:54:08,654 - INFO - __main__ - Epoch  42, Step:   89700, Batch Loss:     6.370709, Lr: 0.000066, Tokens per sec:   2788
2023-03-09 16:54:28,763 - INFO - __main__ - Epoch  42, Step:   89800, Batch Loss:     8.497231, Lr: 0.000066, Tokens per sec:   2667
2023-03-09 16:54:48,889 - INFO - __main__ - Epoch  42, Step:   89900, Batch Loss:     6.251298, Lr: 0.000066, Tokens per sec:   2721
2023-03-09 16:55:09,121 - INFO - __main__ - Epoch  42, Step:   90000, Batch Loss:     4.638674, Lr: 0.000066, Tokens per sec:   2689
2023-03-09 16:55:29,064 - INFO - __main__ - Epoch  42, Step:   90100, Batch Loss:     6.299505, Lr: 0.000066, Tokens per sec:   2694
2023-03-09 16:55:48,565 - INFO - __main__ - Epoch  42, Step:   90200, Batch Loss:     6.449401, Lr: 0.000066, Tokens per sec:   2717
2023-03-09 16:56:08,255 - INFO - __main__ - Epoch  42, Step:   90300, Batch Loss:     4.317417, Lr: 0.000066, Tokens per sec:   2796
2023-03-09 16:56:28,154 - INFO - __main__ - Epoch  42, Step:   90400, Batch Loss:     4.913932, Lr: 0.000066, Tokens per sec:   2669
2023-03-09 16:56:47,933 - INFO - __main__ - Epoch  42, Step:   90500, Batch Loss:     6.636113, Lr: 0.000066, Tokens per sec:   2718
2023-03-09 16:57:07,124 - INFO - __main__ - Epoch  42, Step:   90600, Batch Loss:     7.123348, Lr: 0.000066, Tokens per sec:   2779
2023-03-09 16:57:27,055 - INFO - __main__ - Epoch  42, Step:   90700, Batch Loss:     5.545772, Lr: 0.000066, Tokens per sec:   2703
2023-03-09 16:57:47,129 - INFO - __main__ - Epoch  42, Step:   90800, Batch Loss:     6.839943, Lr: 0.000066, Tokens per sec:   2690
2023-03-09 16:58:07,182 - INFO - __main__ - Epoch  42, Step:   90900, Batch Loss:     7.388795, Lr: 0.000066, Tokens per sec:   2674
2023-03-09 16:58:27,172 - INFO - __main__ - Epoch  42, Step:   91000, Batch Loss:     6.211530, Lr: 0.000066, Tokens per sec:   2684
2023-03-09 16:58:46,400 - INFO - __main__ - Epoch  42, Step:   91100, Batch Loss:     7.161801, Lr: 0.000066, Tokens per sec:   2844
2023-03-09 16:59:04,664 - INFO - __main__ - Epoch  42, Step:   91200, Batch Loss:     6.547285, Lr: 0.000066, Tokens per sec:   2933
2023-03-09 16:59:24,058 - INFO - __main__ - Epoch  42, Step:   91300, Batch Loss:     5.542498, Lr: 0.000066, Tokens per sec:   2751
2023-03-09 16:59:44,074 - INFO - __main__ - Epoch  42, Step:   91400, Batch Loss:     9.283673, Lr: 0.000066, Tokens per sec:   2699
2023-03-09 17:00:04,092 - INFO - __main__ - Epoch  42, Step:   91500, Batch Loss:     5.647508, Lr: 0.000066, Tokens per sec:   2743
2023-03-09 17:00:07,748 - INFO - __main__ - Epoch  42: total training loss 14896.72
2023-03-09 17:00:07,749 - INFO - __main__ - Epoch 43
2023-03-09 17:00:24,511 - INFO - __main__ - Epoch  43, Step:   91600, Batch Loss:     6.961339, Lr: 0.000066, Tokens per sec:   2643
2023-03-09 17:00:44,592 - INFO - __main__ - Epoch  43, Step:   91700, Batch Loss:     5.189567, Lr: 0.000066, Tokens per sec:   2674
2023-03-09 17:01:04,709 - INFO - __main__ - Epoch  43, Step:   91800, Batch Loss:     4.377423, Lr: 0.000066, Tokens per sec:   2645
2023-03-09 17:01:23,903 - INFO - __main__ - Epoch  43, Step:   91900, Batch Loss:     4.562047, Lr: 0.000066, Tokens per sec:   2817
2023-03-09 17:01:43,954 - INFO - __main__ - Epoch  43, Step:   92000, Batch Loss:     7.386372, Lr: 0.000066, Tokens per sec:   2735
2023-03-09 17:02:03,983 - INFO - __main__ - Epoch  43, Step:   92100, Batch Loss:     2.865328, Lr: 0.000066, Tokens per sec:   2707
2023-03-09 17:02:23,980 - INFO - __main__ - Epoch  43, Step:   92200, Batch Loss:     7.705847, Lr: 0.000066, Tokens per sec:   2703
2023-03-09 17:02:43,995 - INFO - __main__ - Epoch  43, Step:   92300, Batch Loss:     5.881258, Lr: 0.000066, Tokens per sec:   2674
2023-03-09 17:03:03,988 - INFO - __main__ - Epoch  43, Step:   92400, Batch Loss:     6.288453, Lr: 0.000066, Tokens per sec:   2675
2023-03-09 17:03:24,023 - INFO - __main__ - Epoch  43, Step:   92500, Batch Loss:     8.597667, Lr: 0.000066, Tokens per sec:   2718
2023-03-09 17:03:44,036 - INFO - __main__ - Epoch  43, Step:   92600, Batch Loss:     6.996079, Lr: 0.000066, Tokens per sec:   2675
2023-03-09 17:04:03,730 - INFO - __main__ - Epoch  43, Step:   92700, Batch Loss:     6.698108, Lr: 0.000066, Tokens per sec:   2725
2023-03-09 17:04:23,028 - INFO - __main__ - Epoch  43, Step:   92800, Batch Loss:     6.694771, Lr: 0.000066, Tokens per sec:   2790
2023-03-09 17:04:43,140 - INFO - __main__ - Epoch  43, Step:   92900, Batch Loss:     5.745864, Lr: 0.000066, Tokens per sec:   2725
2023-03-09 17:05:03,189 - INFO - __main__ - Epoch  43, Step:   93000, Batch Loss:     5.491223, Lr: 0.000066, Tokens per sec:   2679
2023-03-09 17:05:22,441 - INFO - __main__ - Epoch  43, Step:   93100, Batch Loss:     8.156296, Lr: 0.000066, Tokens per sec:   2798
2023-03-09 17:05:41,782 - INFO - __main__ - Epoch  43, Step:   93200, Batch Loss:     7.775728, Lr: 0.000066, Tokens per sec:   2759
2023-03-09 17:06:01,769 - INFO - __main__ - Epoch  43, Step:   93300, Batch Loss:     6.605222, Lr: 0.000066, Tokens per sec:   2666
2023-03-09 17:06:21,876 - INFO - __main__ - Epoch  43, Step:   93400, Batch Loss:     6.508111, Lr: 0.000066, Tokens per sec:   2640
2023-03-09 17:06:41,842 - INFO - __main__ - Epoch  43, Step:   93500, Batch Loss:     9.579373, Lr: 0.000066, Tokens per sec:   2715
2023-03-09 17:07:01,888 - INFO - __main__ - Epoch  43, Step:   93600, Batch Loss:     5.112302, Lr: 0.000066, Tokens per sec:   2616
2023-03-09 17:07:21,524 - INFO - __main__ - Epoch  43: total training loss 14431.29
2023-03-09 17:07:21,525 - INFO - __main__ - Epoch 44
2023-03-09 17:07:22,489 - INFO - __main__ - Epoch  44, Step:   93700, Batch Loss:     5.568491, Lr: 0.000065, Tokens per sec:   1528
2023-03-09 17:07:42,728 - INFO - __main__ - Epoch  44, Step:   93800, Batch Loss:     6.064734, Lr: 0.000065, Tokens per sec:   2683
2023-03-09 17:08:02,896 - INFO - __main__ - Epoch  44, Step:   93900, Batch Loss:     4.736369, Lr: 0.000065, Tokens per sec:   2692
2023-03-09 17:08:22,966 - INFO - __main__ - Epoch  44, Step:   94000, Batch Loss:     5.102668, Lr: 0.000065, Tokens per sec:   2628
2023-03-09 17:08:43,007 - INFO - __main__ - Epoch  44, Step:   94100, Batch Loss:     5.949461, Lr: 0.000065, Tokens per sec:   2655
2023-03-09 17:09:02,757 - INFO - __main__ - Epoch  44, Step:   94200, Batch Loss:     4.662050, Lr: 0.000065, Tokens per sec:   2681
2023-03-09 17:09:22,927 - INFO - __main__ - Epoch  44, Step:   94300, Batch Loss:     7.460546, Lr: 0.000065, Tokens per sec:   2682
2023-03-09 17:09:42,912 - INFO - __main__ - Epoch  44, Step:   94400, Batch Loss:     7.052824, Lr: 0.000065, Tokens per sec:   2671
2023-03-09 17:10:03,005 - INFO - __main__ - Epoch  44, Step:   94500, Batch Loss:    10.287967, Lr: 0.000065, Tokens per sec:   2677
2023-03-09 17:10:23,579 - INFO - __main__ - Epoch  44, Step:   94600, Batch Loss:     6.414262, Lr: 0.000065, Tokens per sec:   2612
2023-03-09 17:10:44,251 - INFO - __main__ - Epoch  44, Step:   94700, Batch Loss:     4.491217, Lr: 0.000065, Tokens per sec:   2611
2023-03-09 17:11:04,728 - INFO - __main__ - Epoch  44, Step:   94800, Batch Loss:     8.719235, Lr: 0.000065, Tokens per sec:   2651
2023-03-09 17:11:24,833 - INFO - __main__ - Epoch  44, Step:   94900, Batch Loss:     8.931674, Lr: 0.000065, Tokens per sec:   2732
2023-03-09 17:11:45,119 - INFO - __main__ - Epoch  44, Step:   95000, Batch Loss:     7.173648, Lr: 0.000065, Tokens per sec:   2682
2023-03-09 17:12:06,468 - INFO - __main__ - Epoch  44, Step:   95100, Batch Loss:     6.508216, Lr: 0.000065, Tokens per sec:   2559
2023-03-09 17:12:29,308 - INFO - __main__ - Epoch  44, Step:   95200, Batch Loss:     6.208871, Lr: 0.000065, Tokens per sec:   2358
2023-03-09 17:12:50,435 - INFO - __main__ - Epoch  44, Step:   95300, Batch Loss:     7.832858, Lr: 0.000065, Tokens per sec:   2560
2023-03-09 17:13:10,726 - INFO - __main__ - Epoch  44, Step:   95400, Batch Loss:     6.998274, Lr: 0.000065, Tokens per sec:   2657
2023-03-09 17:13:30,743 - INFO - __main__ - Epoch  44, Step:   95500, Batch Loss:     9.578063, Lr: 0.000065, Tokens per sec:   2654
2023-03-09 17:13:51,145 - INFO - __main__ - Epoch  44, Step:   95600, Batch Loss:     7.714206, Lr: 0.000065, Tokens per sec:   2609
2023-03-09 17:14:11,367 - INFO - __main__ - Epoch  44, Step:   95700, Batch Loss:     7.132783, Lr: 0.000065, Tokens per sec:   2671
2023-03-09 17:14:31,524 - INFO - __main__ - Epoch  44, Step:   95800, Batch Loss:     7.211660, Lr: 0.000065, Tokens per sec:   2630
2023-03-09 17:14:47,113 - INFO - __main__ - Epoch  44: total training loss 13960.70
2023-03-09 17:14:47,114 - INFO - __main__ - Epoch 45
2023-03-09 17:14:52,373 - INFO - __main__ - Epoch  45, Step:   95900, Batch Loss:     6.118487, Lr: 0.000064, Tokens per sec:   2387
2023-03-09 17:15:12,630 - INFO - __main__ - Epoch  45, Step:   96000, Batch Loss:     5.196409, Lr: 0.000064, Tokens per sec:   2642
2023-03-09 17:15:32,795 - INFO - __main__ - Epoch  45, Step:   96100, Batch Loss:     5.664002, Lr: 0.000064, Tokens per sec:   2654
2023-03-09 17:15:53,061 - INFO - __main__ - Epoch  45, Step:   96200, Batch Loss:     7.049950, Lr: 0.000064, Tokens per sec:   2673
2023-03-09 17:16:13,328 - INFO - __main__ - Epoch  45, Step:   96300, Batch Loss:     5.011499, Lr: 0.000064, Tokens per sec:   2642
2023-03-09 17:16:33,380 - INFO - __main__ - Epoch  45, Step:   96400, Batch Loss:     6.018133, Lr: 0.000064, Tokens per sec:   2694
2023-03-09 17:16:53,374 - INFO - __main__ - Epoch  45, Step:   96500, Batch Loss:     4.506853, Lr: 0.000064, Tokens per sec:   2654
2023-03-09 17:17:13,534 - INFO - __main__ - Epoch  45, Step:   96600, Batch Loss:     5.377374, Lr: 0.000064, Tokens per sec:   2663
2023-03-09 17:17:33,569 - INFO - __main__ - Epoch  45, Step:   96700, Batch Loss:     6.848693, Lr: 0.000064, Tokens per sec:   2689
2023-03-09 17:17:53,690 - INFO - __main__ - Epoch  45, Step:   96800, Batch Loss:     5.479513, Lr: 0.000064, Tokens per sec:   2708
2023-03-09 17:18:13,805 - INFO - __main__ - Epoch  45, Step:   96900, Batch Loss:     4.976579, Lr: 0.000064, Tokens per sec:   2705
2023-03-09 17:18:33,890 - INFO - __main__ - Epoch  45, Step:   97000, Batch Loss:     5.337986, Lr: 0.000064, Tokens per sec:   2637
2023-03-09 17:18:53,706 - INFO - __main__ - Epoch  45, Step:   97100, Batch Loss:     6.495903, Lr: 0.000064, Tokens per sec:   2710
2023-03-09 17:19:13,445 - INFO - __main__ - Epoch  45, Step:   97200, Batch Loss:     6.584233, Lr: 0.000064, Tokens per sec:   2795
2023-03-09 17:19:33,591 - INFO - __main__ - Epoch  45, Step:   97300, Batch Loss:     4.898364, Lr: 0.000064, Tokens per sec:   2669
2023-03-09 17:19:53,447 - INFO - __main__ - Epoch  45, Step:   97400, Batch Loss:     6.423547, Lr: 0.000064, Tokens per sec:   2709
2023-03-09 17:20:13,367 - INFO - __main__ - Epoch  45, Step:   97500, Batch Loss:     4.328180, Lr: 0.000064, Tokens per sec:   2681
2023-03-09 17:20:33,304 - INFO - __main__ - Epoch  45, Step:   97600, Batch Loss:     5.963345, Lr: 0.000064, Tokens per sec:   2663
2023-03-09 17:20:53,491 - INFO - __main__ - Epoch  45, Step:   97700, Batch Loss:     5.192083, Lr: 0.000064, Tokens per sec:   2664
2023-03-09 17:21:13,587 - INFO - __main__ - Epoch  45, Step:   97800, Batch Loss:     8.944026, Lr: 0.000064, Tokens per sec:   2665
2023-03-09 17:21:33,713 - INFO - __main__ - Epoch  45, Step:   97900, Batch Loss:    10.267596, Lr: 0.000064, Tokens per sec:   2741
2023-03-09 17:21:53,743 - INFO - __main__ - Epoch  45, Step:   98000, Batch Loss:     6.854315, Lr: 0.000064, Tokens per sec:   2721
2023-03-09 17:22:04,900 - INFO - __main__ - Epoch  45: total training loss 13509.75
2023-03-09 17:22:04,901 - INFO - __main__ - Epoch 46
2023-03-09 17:22:14,343 - INFO - __main__ - Epoch  46, Step:   98100, Batch Loss:     5.523873, Lr: 0.000064, Tokens per sec:   2556
2023-03-09 17:22:34,504 - INFO - __main__ - Epoch  46, Step:   98200, Batch Loss:     4.853600, Lr: 0.000064, Tokens per sec:   2677
2023-03-09 17:22:54,501 - INFO - __main__ - Epoch  46, Step:   98300, Batch Loss:     4.965687, Lr: 0.000064, Tokens per sec:   2709
2023-03-09 17:23:14,538 - INFO - __main__ - Epoch  46, Step:   98400, Batch Loss:     7.943409, Lr: 0.000064, Tokens per sec:   2685
2023-03-09 17:23:34,785 - INFO - __main__ - Epoch  46, Step:   98500, Batch Loss:     4.799303, Lr: 0.000064, Tokens per sec:   2686
2023-03-09 17:23:54,658 - INFO - __main__ - Epoch  46, Step:   98600, Batch Loss:     5.094900, Lr: 0.000064, Tokens per sec:   2701
2023-03-09 17:24:14,594 - INFO - __main__ - Epoch  46, Step:   98700, Batch Loss:     7.084958, Lr: 0.000064, Tokens per sec:   2705
2023-03-09 17:24:34,493 - INFO - __main__ - Epoch  46, Step:   98800, Batch Loss:     4.691700, Lr: 0.000064, Tokens per sec:   2750
2023-03-09 17:24:54,492 - INFO - __main__ - Epoch  46, Step:   98900, Batch Loss:     6.433283, Lr: 0.000064, Tokens per sec:   2639
2023-03-09 17:25:14,577 - INFO - __main__ - Epoch  46, Step:   99000, Batch Loss:     7.120694, Lr: 0.000064, Tokens per sec:   2649
2023-03-09 17:25:34,670 - INFO - __main__ - Epoch  46, Step:   99100, Batch Loss:     4.664323, Lr: 0.000064, Tokens per sec:   2678
2023-03-09 17:25:54,790 - INFO - __main__ - Epoch  46, Step:   99200, Batch Loss:     5.493185, Lr: 0.000064, Tokens per sec:   2737
2023-03-09 17:26:14,892 - INFO - __main__ - Epoch  46, Step:   99300, Batch Loss:     4.809107, Lr: 0.000064, Tokens per sec:   2647
2023-03-09 17:26:34,911 - INFO - __main__ - Epoch  46, Step:   99400, Batch Loss:     7.665402, Lr: 0.000064, Tokens per sec:   2718
2023-03-09 17:26:54,855 - INFO - __main__ - Epoch  46, Step:   99500, Batch Loss:     5.474188, Lr: 0.000064, Tokens per sec:   2682
2023-03-09 17:27:14,806 - INFO - __main__ - Epoch  46, Step:   99600, Batch Loss:     6.336420, Lr: 0.000064, Tokens per sec:   2704
2023-03-09 17:27:34,766 - INFO - __main__ - Epoch  46, Step:   99700, Batch Loss:     6.612503, Lr: 0.000064, Tokens per sec:   2717
2023-03-09 17:27:54,689 - INFO - __main__ - Epoch  46, Step:   99800, Batch Loss:     7.072402, Lr: 0.000064, Tokens per sec:   2691
2023-03-09 17:28:14,544 - INFO - __main__ - Epoch  46, Step:   99900, Batch Loss:     6.575866, Lr: 0.000064, Tokens per sec:   2705
2023-03-09 17:28:34,543 - INFO - __main__ - Epoch  46, Step:  100000, Batch Loss:     7.748260, Lr: 0.000064, Tokens per sec:   2682
2023-03-09 17:28:54,529 - INFO - __main__ - Epoch  46, Step:  100100, Batch Loss:     5.627998, Lr: 0.000064, Tokens per sec:   2641
2023-03-09 17:29:14,221 - INFO - __main__ - Epoch  46, Step:  100200, Batch Loss:     5.897375, Lr: 0.000064, Tokens per sec:   2762
2023-03-09 17:29:21,155 - INFO - __main__ - Epoch  46: total training loss 13056.50
2023-03-09 17:29:21,156 - INFO - __main__ - Epoch 47
2023-03-09 17:29:34,658 - INFO - __main__ - Epoch  47, Step:  100300, Batch Loss:     8.461917, Lr: 0.000063, Tokens per sec:   2610
2023-03-09 17:29:54,628 - INFO - __main__ - Epoch  47, Step:  100400, Batch Loss:     5.747851, Lr: 0.000063, Tokens per sec:   2685
2023-03-09 17:30:14,442 - INFO - __main__ - Epoch  47, Step:  100500, Batch Loss:     3.648188, Lr: 0.000063, Tokens per sec:   2747
2023-03-09 17:30:34,049 - INFO - __main__ - Epoch  47, Step:  100600, Batch Loss:     4.629104, Lr: 0.000063, Tokens per sec:   2754
2023-03-09 17:30:54,061 - INFO - __main__ - Epoch  47, Step:  100700, Batch Loss:     8.044977, Lr: 0.000063, Tokens per sec:   2697
2023-03-09 17:31:13,890 - INFO - __main__ - Epoch  47, Step:  100800, Batch Loss:     5.120661, Lr: 0.000063, Tokens per sec:   2728
2023-03-09 17:31:33,875 - INFO - __main__ - Epoch  47, Step:  100900, Batch Loss:     6.500759, Lr: 0.000063, Tokens per sec:   2701
2023-03-09 17:31:53,577 - INFO - __main__ - Epoch  47, Step:  101000, Batch Loss:     5.019532, Lr: 0.000063, Tokens per sec:   2691
2023-03-09 17:32:13,827 - INFO - __main__ - Epoch  47, Step:  101100, Batch Loss:     4.933366, Lr: 0.000063, Tokens per sec:   2671
2023-03-09 17:32:33,559 - INFO - __main__ - Epoch  47, Step:  101200, Batch Loss:     4.745113, Lr: 0.000063, Tokens per sec:   2769
2023-03-09 17:32:53,421 - INFO - __main__ - Epoch  47, Step:  101300, Batch Loss:     6.793051, Lr: 0.000063, Tokens per sec:   2674
2023-03-09 17:33:13,260 - INFO - __main__ - Epoch  47, Step:  101400, Batch Loss:     8.637365, Lr: 0.000063, Tokens per sec:   2721
2023-03-09 17:33:32,926 - INFO - __main__ - Epoch  47, Step:  101500, Batch Loss:     4.763790, Lr: 0.000063, Tokens per sec:   2751
2023-03-09 17:33:53,196 - INFO - __main__ - Epoch  47, Step:  101600, Batch Loss:     4.365149, Lr: 0.000063, Tokens per sec:   2645
2023-03-09 17:34:13,084 - INFO - __main__ - Epoch  47, Step:  101700, Batch Loss:     8.032589, Lr: 0.000063, Tokens per sec:   2664
2023-03-09 17:34:32,574 - INFO - __main__ - Epoch  47, Step:  101800, Batch Loss:     4.630507, Lr: 0.000063, Tokens per sec:   2786
2023-03-09 17:34:52,623 - INFO - __main__ - Epoch  47, Step:  101900, Batch Loss:     4.894944, Lr: 0.000063, Tokens per sec:   2647
2023-03-09 17:35:12,491 - INFO - __main__ - Epoch  47, Step:  102000, Batch Loss:     8.359680, Lr: 0.000063, Tokens per sec:   2689
2023-03-09 17:35:32,659 - INFO - __main__ - Epoch  47, Step:  102100, Batch Loss:     7.008496, Lr: 0.000063, Tokens per sec:   2703
2023-03-09 17:35:51,822 - INFO - __main__ - Epoch  47, Step:  102200, Batch Loss:     7.505273, Lr: 0.000063, Tokens per sec:   2771
2023-03-09 17:36:11,002 - INFO - __main__ - Epoch  47, Step:  102300, Batch Loss:     4.022692, Lr: 0.000063, Tokens per sec:   2817
2023-03-09 17:36:31,033 - INFO - __main__ - Epoch  47, Step:  102400, Batch Loss:     6.929127, Lr: 0.000063, Tokens per sec:   2725
2023-03-09 17:36:33,660 - INFO - __main__ - Epoch  47: total training loss 12702.68
2023-03-09 17:36:33,662 - INFO - __main__ - Epoch 48
2023-03-09 17:36:51,342 - INFO - __main__ - Epoch  48, Step:  102500, Batch Loss:     6.108900, Lr: 0.000062, Tokens per sec:   2635
2023-03-09 17:37:11,024 - INFO - __main__ - Epoch  48, Step:  102600, Batch Loss:     5.829319, Lr: 0.000062, Tokens per sec:   2739
2023-03-09 17:37:30,501 - INFO - __main__ - Epoch  48, Step:  102700, Batch Loss:     4.462261, Lr: 0.000062, Tokens per sec:   2763
2023-03-09 17:37:50,576 - INFO - __main__ - Epoch  48, Step:  102800, Batch Loss:     6.553402, Lr: 0.000062, Tokens per sec:   2676
2023-03-09 17:38:10,755 - INFO - __main__ - Epoch  48, Step:  102900, Batch Loss:     4.915284, Lr: 0.000062, Tokens per sec:   2674
2023-03-09 17:38:30,970 - INFO - __main__ - Epoch  48, Step:  103000, Batch Loss:     5.311572, Lr: 0.000062, Tokens per sec:   2679
2023-03-09 17:38:51,109 - INFO - __main__ - Epoch  48, Step:  103100, Batch Loss:     4.183364, Lr: 0.000062, Tokens per sec:   2646
2023-03-09 17:39:11,374 - INFO - __main__ - Epoch  48, Step:  103200, Batch Loss:     3.366827, Lr: 0.000062, Tokens per sec:   2655
2023-03-09 17:39:31,143 - INFO - __main__ - Epoch  48, Step:  103300, Batch Loss:     6.309577, Lr: 0.000062, Tokens per sec:   2711
2023-03-09 17:39:51,445 - INFO - __main__ - Epoch  48, Step:  103400, Batch Loss:     6.668717, Lr: 0.000062, Tokens per sec:   2654
2023-03-09 17:40:11,735 - INFO - __main__ - Epoch  48, Step:  103500, Batch Loss:     5.902437, Lr: 0.000062, Tokens per sec:   2643
2023-03-09 17:40:32,082 - INFO - __main__ - Epoch  48, Step:  103600, Batch Loss:     6.347918, Lr: 0.000062, Tokens per sec:   2620
2023-03-09 17:40:52,297 - INFO - __main__ - Epoch  48, Step:  103700, Batch Loss:     5.438948, Lr: 0.000062, Tokens per sec:   2710
2023-03-09 17:41:12,076 - INFO - __main__ - Epoch  48, Step:  103800, Batch Loss:     6.730779, Lr: 0.000062, Tokens per sec:   2728
2023-03-09 17:41:32,196 - INFO - __main__ - Epoch  48, Step:  103900, Batch Loss:     7.142774, Lr: 0.000062, Tokens per sec:   2644
2023-03-09 17:41:52,226 - INFO - __main__ - Epoch  48, Step:  104000, Batch Loss:     5.890821, Lr: 0.000062, Tokens per sec:   2669
2023-03-09 17:42:12,093 - INFO - __main__ - Epoch  48, Step:  104100, Batch Loss:     6.329898, Lr: 0.000062, Tokens per sec:   2714
2023-03-09 17:42:32,063 - INFO - __main__ - Epoch  48, Step:  104200, Batch Loss:     4.799309, Lr: 0.000062, Tokens per sec:   2720
2023-03-09 17:42:51,459 - INFO - __main__ - Epoch  48, Step:  104300, Batch Loss:     5.300970, Lr: 0.000062, Tokens per sec:   2743
2023-03-09 17:43:11,700 - INFO - __main__ - Epoch  48, Step:  104400, Batch Loss:     7.641138, Lr: 0.000062, Tokens per sec:   2679
2023-03-09 17:43:32,035 - INFO - __main__ - Epoch  48, Step:  104500, Batch Loss:     6.837744, Lr: 0.000062, Tokens per sec:   2694
2023-03-09 17:43:50,317 - INFO - __main__ - Epoch  48: total training loss 12331.05
2023-03-09 17:43:50,319 - INFO - __main__ - Epoch 49
2023-03-09 17:43:52,267 - INFO - __main__ - Epoch  49, Step:  104600, Batch Loss:     5.813599, Lr: 0.000062, Tokens per sec:   2182
2023-03-09 17:44:12,351 - INFO - __main__ - Epoch  49, Step:  104700, Batch Loss:     4.719084, Lr: 0.000062, Tokens per sec:   2664
2023-03-09 17:44:32,508 - INFO - __main__ - Epoch  49, Step:  104800, Batch Loss:     5.434040, Lr: 0.000062, Tokens per sec:   2681
2023-03-09 17:44:52,692 - INFO - __main__ - Epoch  49, Step:  104900, Batch Loss:     6.344336, Lr: 0.000062, Tokens per sec:   2684
2023-03-09 17:45:12,345 - INFO - __main__ - Epoch  49, Step:  105000, Batch Loss:     5.407782, Lr: 0.000062, Tokens per sec:   2762
2023-03-09 17:45:32,184 - INFO - __main__ - Epoch  49, Step:  105100, Batch Loss:     4.001126, Lr: 0.000062, Tokens per sec:   2696
2023-03-09 17:45:51,458 - INFO - __main__ - Epoch  49, Step:  105200, Batch Loss:     6.745874, Lr: 0.000062, Tokens per sec:   2759
2023-03-09 17:46:11,661 - INFO - __main__ - Epoch  49, Step:  105300, Batch Loss:     4.368859, Lr: 0.000062, Tokens per sec:   2678
2023-03-09 17:46:31,864 - INFO - __main__ - Epoch  49, Step:  105400, Batch Loss:     5.810808, Lr: 0.000062, Tokens per sec:   2694
2023-03-09 17:46:51,799 - INFO - __main__ - Epoch  49, Step:  105500, Batch Loss:     4.499794, Lr: 0.000062, Tokens per sec:   2692
2023-03-09 17:47:12,033 - INFO - __main__ - Epoch  49, Step:  105600, Batch Loss:     3.063953, Lr: 0.000062, Tokens per sec:   2662
2023-03-09 17:47:32,320 - INFO - __main__ - Epoch  49, Step:  105700, Batch Loss:     5.487935, Lr: 0.000062, Tokens per sec:   2674
2023-03-09 17:47:52,355 - INFO - __main__ - Epoch  49, Step:  105800, Batch Loss:     6.636866, Lr: 0.000062, Tokens per sec:   2707
2023-03-09 17:48:12,436 - INFO - __main__ - Epoch  49, Step:  105900, Batch Loss:     5.545764, Lr: 0.000062, Tokens per sec:   2712
2023-03-09 17:48:32,380 - INFO - __main__ - Epoch  49, Step:  106000, Batch Loss:     5.466866, Lr: 0.000062, Tokens per sec:   2666
2023-03-09 17:48:51,647 - INFO - __main__ - Epoch  49, Step:  106100, Batch Loss:     5.774629, Lr: 0.000062, Tokens per sec:   2797
2023-03-09 17:49:11,546 - INFO - __main__ - Epoch  49, Step:  106200, Batch Loss:     4.547015, Lr: 0.000062, Tokens per sec:   2714
2023-03-09 17:49:31,651 - INFO - __main__ - Epoch  49, Step:  106300, Batch Loss:     4.413958, Lr: 0.000062, Tokens per sec:   2681
2023-03-09 17:49:51,743 - INFO - __main__ - Epoch  49, Step:  106400, Batch Loss:     5.559659, Lr: 0.000062, Tokens per sec:   2674
2023-03-09 17:50:11,745 - INFO - __main__ - Epoch  49, Step:  106500, Batch Loss:     7.536259, Lr: 0.000062, Tokens per sec:   2669
2023-03-09 17:50:31,800 - INFO - __main__ - Epoch  49, Step:  106600, Batch Loss:     6.394664, Lr: 0.000062, Tokens per sec:   2725
2023-03-09 17:50:51,926 - INFO - __main__ - Epoch  49, Step:  106700, Batch Loss:     6.310726, Lr: 0.000062, Tokens per sec:   2592
2023-03-09 17:51:06,242 - INFO - __main__ - Epoch  49: total training loss 11924.23
2023-03-09 17:51:06,243 - INFO - __main__ - Epoch 50
2023-03-09 17:51:12,295 - INFO - __main__ - Epoch  50, Step:  106800, Batch Loss:     4.243457, Lr: 0.000061, Tokens per sec:   2617
2023-03-09 17:51:32,393 - INFO - __main__ - Epoch  50, Step:  106900, Batch Loss:     5.661078, Lr: 0.000061, Tokens per sec:   2682
2023-03-09 17:51:52,467 - INFO - __main__ - Epoch  50, Step:  107000, Batch Loss:     5.248296, Lr: 0.000061, Tokens per sec:   2751
2023-03-09 17:52:12,582 - INFO - __main__ - Epoch  50, Step:  107100, Batch Loss:     4.763916, Lr: 0.000061, Tokens per sec:   2682
2023-03-09 17:52:32,630 - INFO - __main__ - Epoch  50, Step:  107200, Batch Loss:     4.590064, Lr: 0.000061, Tokens per sec:   2665
2023-03-09 17:52:52,607 - INFO - __main__ - Epoch  50, Step:  107300, Batch Loss:     4.395592, Lr: 0.000061, Tokens per sec:   2678
2023-03-09 17:53:13,196 - INFO - __main__ - Epoch  50, Step:  107400, Batch Loss:     4.112857, Lr: 0.000061, Tokens per sec:   2595
2023-03-09 17:53:33,340 - INFO - __main__ - Epoch  50, Step:  107500, Batch Loss:     6.235147, Lr: 0.000061, Tokens per sec:   2698
2023-03-09 17:53:53,462 - INFO - __main__ - Epoch  50, Step:  107600, Batch Loss:     5.070525, Lr: 0.000061, Tokens per sec:   2683
2023-03-09 17:54:13,672 - INFO - __main__ - Epoch  50, Step:  107700, Batch Loss:     5.602472, Lr: 0.000061, Tokens per sec:   2681
2023-03-09 17:54:33,557 - INFO - __main__ - Epoch  50, Step:  107800, Batch Loss:     3.643807, Lr: 0.000061, Tokens per sec:   2660
2023-03-09 17:54:53,751 - INFO - __main__ - Epoch  50, Step:  107900, Batch Loss:     5.583286, Lr: 0.000061, Tokens per sec:   2651
2023-03-09 17:55:13,339 - INFO - __main__ - Epoch  50, Step:  108000, Batch Loss:     5.522200, Lr: 0.000061, Tokens per sec:   2783
2023-03-09 17:55:33,018 - INFO - __main__ - Epoch  50, Step:  108100, Batch Loss:     4.678624, Lr: 0.000061, Tokens per sec:   2751
2023-03-09 17:55:52,602 - INFO - __main__ - Epoch  50, Step:  108200, Batch Loss:     3.974273, Lr: 0.000061, Tokens per sec:   2721
2023-03-09 17:56:12,425 - INFO - __main__ - Epoch  50, Step:  108300, Batch Loss:     3.894872, Lr: 0.000061, Tokens per sec:   2738
2023-03-09 17:56:32,256 - INFO - __main__ - Epoch  50, Step:  108400, Batch Loss:     5.966887, Lr: 0.000061, Tokens per sec:   2696
2023-03-09 17:56:51,833 - INFO - __main__ - Epoch  50, Step:  108500, Batch Loss:     5.985306, Lr: 0.000061, Tokens per sec:   2762
2023-03-09 17:57:11,819 - INFO - __main__ - Epoch  50, Step:  108600, Batch Loss:     4.975485, Lr: 0.000061, Tokens per sec:   2677
2023-03-09 17:57:31,299 - INFO - __main__ - Epoch  50, Step:  108700, Batch Loss:     4.451957, Lr: 0.000061, Tokens per sec:   2752
2023-03-09 17:57:51,473 - INFO - __main__ - Epoch  50, Step:  108800, Batch Loss:     8.062322, Lr: 0.000061, Tokens per sec:   2665
2023-03-09 17:58:11,370 - INFO - __main__ - Epoch  50, Step:  108900, Batch Loss:     3.021656, Lr: 0.000061, Tokens per sec:   2713
2023-03-09 17:58:21,588 - INFO - __main__ - Epoch  50: total training loss 11596.66
2023-03-09 17:58:21,589 - INFO - __main__ - Epoch 51
2023-03-09 17:58:32,036 - INFO - __main__ - Epoch  51, Step:  109000, Batch Loss:     5.995235, Lr: 0.000061, Tokens per sec:   2566
2023-03-09 17:58:51,355 - INFO - __main__ - Epoch  51, Step:  109100, Batch Loss:     3.269653, Lr: 0.000061, Tokens per sec:   2743
2023-03-09 17:59:11,020 - INFO - __main__ - Epoch  51, Step:  109200, Batch Loss:     4.966214, Lr: 0.000061, Tokens per sec:   2745
2023-03-09 17:59:30,411 - INFO - __main__ - Epoch  51, Step:  109300, Batch Loss:     4.599778, Lr: 0.000061, Tokens per sec:   2808
2023-03-09 17:59:50,421 - INFO - __main__ - Epoch  51, Step:  109400, Batch Loss:     6.842382, Lr: 0.000061, Tokens per sec:   2703
2023-03-09 18:00:09,930 - INFO - __main__ - Epoch  51, Step:  109500, Batch Loss:     6.232300, Lr: 0.000061, Tokens per sec:   2771
2023-03-09 18:00:29,325 - INFO - __main__ - Epoch  51, Step:  109600, Batch Loss:     4.668835, Lr: 0.000061, Tokens per sec:   2750
2023-03-09 18:00:49,354 - INFO - __main__ - Epoch  51, Step:  109700, Batch Loss:     4.402480, Lr: 0.000061, Tokens per sec:   2665
2023-03-09 18:01:08,996 - INFO - __main__ - Epoch  51, Step:  109800, Batch Loss:     5.526288, Lr: 0.000061, Tokens per sec:   2749
2023-03-09 18:01:28,694 - INFO - __main__ - Epoch  51, Step:  109900, Batch Loss:     4.860711, Lr: 0.000061, Tokens per sec:   2745
2023-03-09 18:01:48,719 - INFO - __main__ - Epoch  51, Step:  110000, Batch Loss:     4.796548, Lr: 0.000061, Tokens per sec:   2699
2023-03-09 18:02:08,877 - INFO - __main__ - Epoch  51, Step:  110100, Batch Loss:     5.160054, Lr: 0.000061, Tokens per sec:   2688
2023-03-09 18:02:29,003 - INFO - __main__ - Epoch  51, Step:  110200, Batch Loss:     3.851303, Lr: 0.000061, Tokens per sec:   2675
2023-03-09 18:02:48,722 - INFO - __main__ - Epoch  51, Step:  110300, Batch Loss:     4.176581, Lr: 0.000061, Tokens per sec:   2801
2023-03-09 18:03:07,885 - INFO - __main__ - Epoch  51, Step:  110400, Batch Loss:     5.572817, Lr: 0.000061, Tokens per sec:   2754
2023-03-09 18:03:26,748 - INFO - __main__ - Epoch  51, Step:  110500, Batch Loss:     4.749234, Lr: 0.000061, Tokens per sec:   2797
2023-03-09 18:03:46,501 - INFO - __main__ - Epoch  51, Step:  110600, Batch Loss:     5.950171, Lr: 0.000061, Tokens per sec:   2772
2023-03-09 18:04:06,495 - INFO - __main__ - Epoch  51, Step:  110700, Batch Loss:     6.242179, Lr: 0.000061, Tokens per sec:   2695
2023-03-09 18:04:26,330 - INFO - __main__ - Epoch  51, Step:  110800, Batch Loss:     7.640540, Lr: 0.000061, Tokens per sec:   2713
2023-03-09 18:04:46,533 - INFO - __main__ - Epoch  51, Step:  110900, Batch Loss:     4.556096, Lr: 0.000061, Tokens per sec:   2643
2023-03-09 18:05:06,356 - INFO - __main__ - Epoch  51, Step:  111000, Batch Loss:     6.208948, Lr: 0.000061, Tokens per sec:   2710
2023-03-09 18:05:25,979 - INFO - __main__ - Epoch  51, Step:  111100, Batch Loss:     6.507148, Lr: 0.000061, Tokens per sec:   2749
2023-03-09 18:05:31,716 - INFO - __main__ - Epoch  51: total training loss 11303.28
2023-03-09 18:05:31,717 - INFO - __main__ - Epoch 52
2023-03-09 18:05:46,324 - INFO - __main__ - Epoch  52, Step:  111200, Batch Loss:     3.903270, Lr: 0.000060, Tokens per sec:   2670
2023-03-09 18:06:06,466 - INFO - __main__ - Epoch  52, Step:  111300, Batch Loss:     4.480493, Lr: 0.000060, Tokens per sec:   2628
2023-03-09 18:06:26,433 - INFO - __main__ - Epoch  52, Step:  111400, Batch Loss:     4.603614, Lr: 0.000060, Tokens per sec:   2695
2023-03-09 18:06:46,323 - INFO - __main__ - Epoch  52, Step:  111500, Batch Loss:     5.819849, Lr: 0.000060, Tokens per sec:   2727
2023-03-09 18:07:06,542 - INFO - __main__ - Epoch  52, Step:  111600, Batch Loss:     4.280991, Lr: 0.000060, Tokens per sec:   2622
2023-03-09 18:07:26,696 - INFO - __main__ - Epoch  52, Step:  111700, Batch Loss:     7.465047, Lr: 0.000060, Tokens per sec:   2692
2023-03-09 18:07:46,913 - INFO - __main__ - Epoch  52, Step:  111800, Batch Loss:     3.553410, Lr: 0.000060, Tokens per sec:   2731
2023-03-09 18:08:06,962 - INFO - __main__ - Epoch  52, Step:  111900, Batch Loss:     3.649608, Lr: 0.000060, Tokens per sec:   2706
2023-03-09 18:08:26,839 - INFO - __main__ - Epoch  52, Step:  112000, Batch Loss:     5.181306, Lr: 0.000060, Tokens per sec:   2695
2023-03-09 18:08:46,533 - INFO - __main__ - Epoch  52, Step:  112100, Batch Loss:     4.578812, Lr: 0.000060, Tokens per sec:   2742
2023-03-09 18:09:06,753 - INFO - __main__ - Epoch  52, Step:  112200, Batch Loss:     6.686325, Lr: 0.000060, Tokens per sec:   2618
2023-03-09 18:09:26,895 - INFO - __main__ - Epoch  52, Step:  112300, Batch Loss:     5.150447, Lr: 0.000060, Tokens per sec:   2674
2023-03-09 18:09:47,108 - INFO - __main__ - Epoch  52, Step:  112400, Batch Loss:     4.280228, Lr: 0.000060, Tokens per sec:   2643
2023-03-09 18:10:07,182 - INFO - __main__ - Epoch  52, Step:  112500, Batch Loss:     5.877054, Lr: 0.000060, Tokens per sec:   2756
2023-03-09 18:10:26,794 - INFO - __main__ - Epoch  52, Step:  112600, Batch Loss:     6.711002, Lr: 0.000060, Tokens per sec:   2734
2023-03-09 18:10:46,149 - INFO - __main__ - Epoch  52, Step:  112700, Batch Loss:     5.166009, Lr: 0.000060, Tokens per sec:   2783
2023-03-09 18:11:05,814 - INFO - __main__ - Epoch  52, Step:  112800, Batch Loss:     4.162958, Lr: 0.000060, Tokens per sec:   2681
2023-03-09 18:11:25,905 - INFO - __main__ - Epoch  52, Step:  112900, Batch Loss:     4.453246, Lr: 0.000060, Tokens per sec:   2637
2023-03-09 18:11:45,953 - INFO - __main__ - Epoch  52, Step:  113000, Batch Loss:     8.015853, Lr: 0.000060, Tokens per sec:   2732
2023-03-09 18:12:05,770 - INFO - __main__ - Epoch  52, Step:  113100, Batch Loss:     4.974312, Lr: 0.000060, Tokens per sec:   2701
2023-03-09 18:12:25,920 - INFO - __main__ - Epoch  52, Step:  113200, Batch Loss:     4.723149, Lr: 0.000060, Tokens per sec:   2672
2023-03-09 18:12:45,724 - INFO - __main__ - Epoch  52, Step:  113300, Batch Loss:     5.196074, Lr: 0.000060, Tokens per sec:   2715
2023-03-09 18:12:47,384 - INFO - __main__ - Epoch  52: total training loss 10940.73
2023-03-09 18:12:47,385 - INFO - __main__ - Epoch 53
2023-03-09 18:13:06,244 - INFO - __main__ - Epoch  53, Step:  113400, Batch Loss:     3.903564, Lr: 0.000059, Tokens per sec:   2621
2023-03-09 18:13:26,239 - INFO - __main__ - Epoch  53, Step:  113500, Batch Loss:     4.431413, Lr: 0.000059, Tokens per sec:   2653
2023-03-09 18:13:45,279 - INFO - __main__ - Epoch  53, Step:  113600, Batch Loss:     5.415184, Lr: 0.000059, Tokens per sec:   2862
2023-03-09 18:14:04,650 - INFO - __main__ - Epoch  53, Step:  113700, Batch Loss:     3.914322, Lr: 0.000059, Tokens per sec:   2762
2023-03-09 18:14:23,983 - INFO - __main__ - Epoch  53, Step:  113800, Batch Loss:     4.533907, Lr: 0.000059, Tokens per sec:   2776
2023-03-09 18:14:44,139 - INFO - __main__ - Epoch  53, Step:  113900, Batch Loss:     4.799322, Lr: 0.000059, Tokens per sec:   2687
2023-03-09 18:15:04,180 - INFO - __main__ - Epoch  53, Step:  114000, Batch Loss:     5.470303, Lr: 0.000059, Tokens per sec:   2695
2023-03-09 18:15:24,322 - INFO - __main__ - Epoch  53, Step:  114100, Batch Loss:     5.538375, Lr: 0.000059, Tokens per sec:   2706
2023-03-09 18:15:44,445 - INFO - __main__ - Epoch  53, Step:  114200, Batch Loss:     5.534772, Lr: 0.000059, Tokens per sec:   2647
2023-03-09 18:16:03,788 - INFO - __main__ - Epoch  53, Step:  114300, Batch Loss:     4.790986, Lr: 0.000059, Tokens per sec:   2762
2023-03-09 18:16:23,929 - INFO - __main__ - Epoch  53, Step:  114400, Batch Loss:     5.135298, Lr: 0.000059, Tokens per sec:   2700
2023-03-09 18:16:44,015 - INFO - __main__ - Epoch  53, Step:  114500, Batch Loss:     4.244248, Lr: 0.000059, Tokens per sec:   2713
2023-03-09 18:17:04,130 - INFO - __main__ - Epoch  53, Step:  114600, Batch Loss:     4.259125, Lr: 0.000059, Tokens per sec:   2643
2023-03-09 18:17:24,176 - INFO - __main__ - Epoch  53, Step:  114700, Batch Loss:     3.692586, Lr: 0.000059, Tokens per sec:   2691
2023-03-09 18:17:43,559 - INFO - __main__ - Epoch  53, Step:  114800, Batch Loss:     3.684321, Lr: 0.000059, Tokens per sec:   2759
2023-03-09 18:18:02,689 - INFO - __main__ - Epoch  53, Step:  114900, Batch Loss:     5.281945, Lr: 0.000059, Tokens per sec:   2832
2023-03-09 18:18:22,601 - INFO - __main__ - Epoch  53, Step:  115000, Batch Loss:     5.321537, Lr: 0.000059, Tokens per sec:   2739
2023-03-09 18:18:42,561 - INFO - __main__ - Epoch  53, Step:  115100, Batch Loss:     6.439345, Lr: 0.000059, Tokens per sec:   2728
2023-03-09 18:19:02,112 - INFO - __main__ - Epoch  53, Step:  115200, Batch Loss:     4.168498, Lr: 0.000059, Tokens per sec:   2735
2023-03-09 18:19:22,014 - INFO - __main__ - Epoch  53, Step:  115300, Batch Loss:     4.209550, Lr: 0.000059, Tokens per sec:   2687
2023-03-09 18:19:41,635 - INFO - __main__ - Epoch  53, Step:  115400, Batch Loss:     4.427481, Lr: 0.000059, Tokens per sec:   2717
2023-03-09 18:19:59,211 - INFO - __main__ - Epoch  53: total training loss 10667.92
2023-03-09 18:19:59,212 - INFO - __main__ - Epoch 54
2023-03-09 18:20:02,182 - INFO - __main__ - Epoch  54, Step:  115500, Batch Loss:     5.607438, Lr: 0.000059, Tokens per sec:   2395
2023-03-09 18:20:22,260 - INFO - __main__ - Epoch  54, Step:  115600, Batch Loss:     3.582010, Lr: 0.000059, Tokens per sec:   2702
2023-03-09 18:20:41,624 - INFO - __main__ - Epoch  54, Step:  115700, Batch Loss:     4.462799, Lr: 0.000059, Tokens per sec:   2777
2023-03-09 18:21:01,633 - INFO - __main__ - Epoch  54, Step:  115800, Batch Loss:     4.170543, Lr: 0.000059, Tokens per sec:   2693
2023-03-09 18:21:21,631 - INFO - __main__ - Epoch  54, Step:  115900, Batch Loss:     3.618639, Lr: 0.000059, Tokens per sec:   2632
2023-03-09 18:21:41,644 - INFO - __main__ - Epoch  54, Step:  116000, Batch Loss:     3.543839, Lr: 0.000059, Tokens per sec:   2654
2023-03-09 18:22:01,639 - INFO - __main__ - Epoch  54, Step:  116100, Batch Loss:     4.745238, Lr: 0.000059, Tokens per sec:   2670
2023-03-09 18:22:21,671 - INFO - __main__ - Epoch  54, Step:  116200, Batch Loss:     4.217843, Lr: 0.000059, Tokens per sec:   2674
2023-03-09 18:22:41,686 - INFO - __main__ - Epoch  54, Step:  116300, Batch Loss:     6.326845, Lr: 0.000059, Tokens per sec:   2668
2023-03-09 18:23:01,691 - INFO - __main__ - Epoch  54, Step:  116400, Batch Loss:     6.026808, Lr: 0.000059, Tokens per sec:   2768
2023-03-09 18:23:21,729 - INFO - __main__ - Epoch  54, Step:  116500, Batch Loss:     6.223619, Lr: 0.000059, Tokens per sec:   2686
2023-03-09 18:23:41,782 - INFO - __main__ - Epoch  54, Step:  116600, Batch Loss:     3.734439, Lr: 0.000059, Tokens per sec:   2656
2023-03-09 18:24:01,796 - INFO - __main__ - Epoch  54, Step:  116700, Batch Loss:     4.225940, Lr: 0.000059, Tokens per sec:   2719
2023-03-09 18:24:21,813 - INFO - __main__ - Epoch  54, Step:  116800, Batch Loss:     5.595039, Lr: 0.000059, Tokens per sec:   2702
2023-03-09 18:24:41,768 - INFO - __main__ - Epoch  54, Step:  116900, Batch Loss:     4.158884, Lr: 0.000059, Tokens per sec:   2730
2023-03-09 18:25:01,798 - INFO - __main__ - Epoch  54, Step:  117000, Batch Loss:     4.113289, Lr: 0.000059, Tokens per sec:   2714
2023-03-09 18:25:21,808 - INFO - __main__ - Epoch  54, Step:  117100, Batch Loss:     3.422547, Lr: 0.000059, Tokens per sec:   2661
2023-03-09 18:25:41,815 - INFO - __main__ - Epoch  54, Step:  117200, Batch Loss:     4.313223, Lr: 0.000059, Tokens per sec:   2683
2023-03-09 18:26:01,820 - INFO - __main__ - Epoch  54, Step:  117300, Batch Loss:     5.274704, Lr: 0.000059, Tokens per sec:   2671
2023-03-09 18:26:21,830 - INFO - __main__ - Epoch  54, Step:  117400, Batch Loss:     4.122968, Lr: 0.000059, Tokens per sec:   2708
2023-03-09 18:26:41,840 - INFO - __main__ - Epoch  54, Step:  117500, Batch Loss:     5.216533, Lr: 0.000059, Tokens per sec:   2712
2023-03-09 18:27:01,846 - INFO - __main__ - Epoch  54, Step:  117600, Batch Loss:     6.542475, Lr: 0.000059, Tokens per sec:   2688
2023-03-09 18:27:15,071 - INFO - __main__ - Epoch  54: total training loss 10377.09
2023-03-09 18:27:15,073 - INFO - __main__ - Epoch 55
2023-03-09 18:27:22,253 - INFO - __main__ - Epoch  55, Step:  117700, Batch Loss:     5.984181, Lr: 0.000058, Tokens per sec:   2587
2023-03-09 18:27:42,294 - INFO - __main__ - Epoch  55, Step:  117800, Batch Loss:     4.773707, Lr: 0.000058, Tokens per sec:   2660
2023-03-09 18:28:02,321 - INFO - __main__ - Epoch  55, Step:  117900, Batch Loss:     3.391281, Lr: 0.000058, Tokens per sec:   2681
2023-03-09 18:28:22,357 - INFO - __main__ - Epoch  55, Step:  118000, Batch Loss:     4.228452, Lr: 0.000058, Tokens per sec:   2671
2023-03-09 18:28:42,345 - INFO - __main__ - Epoch  55, Step:  118100, Batch Loss:     3.896254, Lr: 0.000058, Tokens per sec:   2720
2023-03-09 18:29:02,362 - INFO - __main__ - Epoch  55, Step:  118200, Batch Loss:     4.610381, Lr: 0.000058, Tokens per sec:   2666
2023-03-09 18:29:22,036 - INFO - __main__ - Epoch  55, Step:  118300, Batch Loss:     4.120234, Lr: 0.000058, Tokens per sec:   2748
2023-03-09 18:29:41,647 - INFO - __main__ - Epoch  55, Step:  118400, Batch Loss:     3.723964, Lr: 0.000058, Tokens per sec:   2749
2023-03-09 18:30:01,662 - INFO - __main__ - Epoch  55, Step:  118500, Batch Loss:     5.722903, Lr: 0.000058, Tokens per sec:   2677
2023-03-09 18:30:21,655 - INFO - __main__ - Epoch  55, Step:  118600, Batch Loss:     3.982671, Lr: 0.000058, Tokens per sec:   2726
2023-03-09 18:30:41,646 - INFO - __main__ - Epoch  55, Step:  118700, Batch Loss:     3.199050, Lr: 0.000058, Tokens per sec:   2741
2023-03-09 18:31:01,669 - INFO - __main__ - Epoch  55, Step:  118800, Batch Loss:     5.627584, Lr: 0.000058, Tokens per sec:   2695
2023-03-09 18:31:21,639 - INFO - __main__ - Epoch  55, Step:  118900, Batch Loss:     4.106480, Lr: 0.000058, Tokens per sec:   2713
2023-03-09 18:31:41,620 - INFO - __main__ - Epoch  55, Step:  119000, Batch Loss:     3.222005, Lr: 0.000058, Tokens per sec:   2695
2023-03-09 18:32:01,619 - INFO - __main__ - Epoch  55, Step:  119100, Batch Loss:     5.208327, Lr: 0.000058, Tokens per sec:   2691
2023-03-09 18:32:20,168 - INFO - __main__ - Epoch  55, Step:  119200, Batch Loss:     4.513919, Lr: 0.000058, Tokens per sec:   2879
2023-03-09 18:32:39,669 - INFO - __main__ - Epoch  55, Step:  119300, Batch Loss:     3.463737, Lr: 0.000058, Tokens per sec:   2758
2023-03-09 18:32:59,665 - INFO - __main__ - Epoch  55, Step:  119400, Batch Loss:     4.604748, Lr: 0.000058, Tokens per sec:   2660
2023-03-09 18:33:19,660 - INFO - __main__ - Epoch  55, Step:  119500, Batch Loss:     3.618623, Lr: 0.000058, Tokens per sec:   2682
2023-03-09 18:33:39,671 - INFO - __main__ - Epoch  55, Step:  119600, Batch Loss:     3.785820, Lr: 0.000058, Tokens per sec:   2674
2023-03-09 18:33:59,725 - INFO - __main__ - Epoch  55, Step:  119700, Batch Loss:     4.517232, Lr: 0.000058, Tokens per sec:   2694
2023-03-09 18:34:19,761 - INFO - __main__ - Epoch  55, Step:  119800, Batch Loss:     5.233824, Lr: 0.000058, Tokens per sec:   2678
2023-03-09 18:34:28,882 - INFO - __main__ - Epoch  55: total training loss 10122.75
2023-03-09 18:34:28,883 - INFO - __main__ - Epoch 56
2023-03-09 18:34:40,155 - INFO - __main__ - Epoch  56, Step:  119900, Batch Loss:     4.924883, Lr: 0.000058, Tokens per sec:   2647
2023-03-09 18:35:00,230 - INFO - __main__ - Epoch  56, Step:  120000, Batch Loss:     5.441293, Lr: 0.000058, Tokens per sec:   2685
2023-03-09 18:35:20,223 - INFO - __main__ - Epoch  56, Step:  120100, Batch Loss:     4.075625, Lr: 0.000058, Tokens per sec:   2726
2023-03-09 18:35:40,240 - INFO - __main__ - Epoch  56, Step:  120200, Batch Loss:     5.902110, Lr: 0.000058, Tokens per sec:   2706
2023-03-09 18:36:00,345 - INFO - __main__ - Epoch  56, Step:  120300, Batch Loss:     5.350497, Lr: 0.000058, Tokens per sec:   2674
2023-03-09 18:36:20,345 - INFO - __main__ - Epoch  56, Step:  120400, Batch Loss:     5.761914, Lr: 0.000058, Tokens per sec:   2669
2023-03-09 18:36:40,355 - INFO - __main__ - Epoch  56, Step:  120500, Batch Loss:     4.725789, Lr: 0.000058, Tokens per sec:   2713
2023-03-09 18:37:00,453 - INFO - __main__ - Epoch  56, Step:  120600, Batch Loss:     4.765401, Lr: 0.000058, Tokens per sec:   2673
2023-03-09 18:37:20,503 - INFO - __main__ - Epoch  56, Step:  120700, Batch Loss:     3.787384, Lr: 0.000058, Tokens per sec:   2654
2023-03-09 18:37:40,499 - INFO - __main__ - Epoch  56, Step:  120800, Batch Loss:     3.952247, Lr: 0.000058, Tokens per sec:   2706
2023-03-09 18:38:00,608 - INFO - __main__ - Epoch  56, Step:  120900, Batch Loss:     4.146720, Lr: 0.000058, Tokens per sec:   2711
2023-03-09 18:38:20,789 - INFO - __main__ - Epoch  56, Step:  121000, Batch Loss:     3.303710, Lr: 0.000058, Tokens per sec:   2679
2023-03-09 18:38:40,707 - INFO - __main__ - Epoch  56, Step:  121100, Batch Loss:     4.745532, Lr: 0.000058, Tokens per sec:   2738
2023-03-09 18:39:00,837 - INFO - __main__ - Epoch  56, Step:  121200, Batch Loss:     3.554728, Lr: 0.000058, Tokens per sec:   2616
2023-03-09 18:39:20,832 - INFO - __main__ - Epoch  56, Step:  121300, Batch Loss:     4.390041, Lr: 0.000058, Tokens per sec:   2698
2023-03-09 18:39:40,800 - INFO - __main__ - Epoch  56, Step:  121400, Batch Loss:     5.307961, Lr: 0.000058, Tokens per sec:   2676
2023-03-09 18:40:00,794 - INFO - __main__ - Epoch  56, Step:  121500, Batch Loss:     6.299205, Lr: 0.000058, Tokens per sec:   2673
2023-03-09 18:40:20,907 - INFO - __main__ - Epoch  56, Step:  121600, Batch Loss:     4.464415, Lr: 0.000058, Tokens per sec:   2645
2023-03-09 18:40:40,986 - INFO - __main__ - Epoch  56, Step:  121700, Batch Loss:     3.890156, Lr: 0.000058, Tokens per sec:   2659
2023-03-09 18:41:01,097 - INFO - __main__ - Epoch  56, Step:  121800, Batch Loss:     3.949168, Lr: 0.000058, Tokens per sec:   2696
2023-03-09 18:41:21,251 - INFO - __main__ - Epoch  56, Step:  121900, Batch Loss:     4.575638, Lr: 0.000058, Tokens per sec:   2656
2023-03-09 18:41:41,102 - INFO - __main__ - Epoch  56, Step:  122000, Batch Loss:     5.027900, Lr: 0.000058, Tokens per sec:   2721
2023-03-09 18:41:45,783 - INFO - __main__ - Epoch  56: total training loss 9883.52
2023-03-09 18:41:45,783 - INFO - __main__ - Epoch 57
2023-03-09 18:42:01,438 - INFO - __main__ - Epoch  57, Step:  122100, Batch Loss:     7.065753, Lr: 0.000057, Tokens per sec:   2630
2023-03-09 18:42:21,452 - INFO - __main__ - Epoch  57, Step:  122200, Batch Loss:     5.163080, Lr: 0.000057, Tokens per sec:   2663
2023-03-09 18:42:41,505 - INFO - __main__ - Epoch  57, Step:  122300, Batch Loss:     3.456715, Lr: 0.000057, Tokens per sec:   2631
2023-03-09 18:43:01,499 - INFO - __main__ - Epoch  57, Step:  122400, Batch Loss:     4.338570, Lr: 0.000057, Tokens per sec:   2736
2023-03-09 18:43:21,614 - INFO - __main__ - Epoch  57, Step:  122500, Batch Loss:     4.153636, Lr: 0.000057, Tokens per sec:   2686
2023-03-09 18:43:41,634 - INFO - __main__ - Epoch  57, Step:  122600, Batch Loss:     4.310959, Lr: 0.000057, Tokens per sec:   2669
2023-03-09 18:44:01,659 - INFO - __main__ - Epoch  57, Step:  122700, Batch Loss:     4.329554, Lr: 0.000057, Tokens per sec:   2664
2023-03-09 18:44:20,791 - INFO - __main__ - Epoch  57, Step:  122800, Batch Loss:     3.594819, Lr: 0.000057, Tokens per sec:   2799
2023-03-09 18:44:40,968 - INFO - __main__ - Epoch  57, Step:  122900, Batch Loss:     3.771740, Lr: 0.000057, Tokens per sec:   2699
2023-03-09 18:45:01,022 - INFO - __main__ - Epoch  57, Step:  123000, Batch Loss:     4.605003, Lr: 0.000057, Tokens per sec:   2673
2023-03-09 18:45:21,183 - INFO - __main__ - Epoch  57, Step:  123100, Batch Loss:     6.332038, Lr: 0.000057, Tokens per sec:   2627
2023-03-09 18:45:40,983 - INFO - __main__ - Epoch  57, Step:  123200, Batch Loss:     4.538500, Lr: 0.000057, Tokens per sec:   2733
2023-03-09 18:46:00,850 - INFO - __main__ - Epoch  57, Step:  123300, Batch Loss:     6.317050, Lr: 0.000057, Tokens per sec:   2653
2023-03-09 18:46:20,913 - INFO - __main__ - Epoch  57, Step:  123400, Batch Loss:     3.938040, Lr: 0.000057, Tokens per sec:   2716
2023-03-09 18:46:40,921 - INFO - __main__ - Epoch  57, Step:  123500, Batch Loss:     3.431417, Lr: 0.000057, Tokens per sec:   2642
2023-03-09 18:47:00,505 - INFO - __main__ - Epoch  57, Step:  123600, Batch Loss:     4.581833, Lr: 0.000057, Tokens per sec:   2827
2023-03-09 18:47:20,615 - INFO - __main__ - Epoch  57, Step:  123700, Batch Loss:     4.442921, Lr: 0.000057, Tokens per sec:   2695
2023-03-09 18:47:40,617 - INFO - __main__ - Epoch  57, Step:  123800, Batch Loss:     5.362719, Lr: 0.000057, Tokens per sec:   2695
2023-03-09 18:48:00,824 - INFO - __main__ - Epoch  57, Step:  123900, Batch Loss:     5.323833, Lr: 0.000057, Tokens per sec:   2661
2023-03-09 18:48:21,057 - INFO - __main__ - Epoch  57, Step:  124000, Batch Loss:     6.324879, Lr: 0.000057, Tokens per sec:   2621
2023-03-09 18:48:42,114 - INFO - __main__ - Epoch  57, Step:  124100, Batch Loss:     3.714153, Lr: 0.000057, Tokens per sec:   2642
2023-03-09 18:49:02,744 - INFO - __main__ - Epoch  57, Step:  124200, Batch Loss:     6.045928, Lr: 0.000057, Tokens per sec:   2614
2023-03-09 18:49:03,456 - INFO - __main__ - Epoch  57: total training loss 9595.02
2023-03-09 18:49:03,457 - INFO - __main__ - Epoch 58
2023-03-09 18:49:23,537 - INFO - __main__ - Epoch  58, Step:  124300, Batch Loss:     3.990963, Lr: 0.000056, Tokens per sec:   2580
2023-03-09 18:49:43,781 - INFO - __main__ - Epoch  58, Step:  124400, Batch Loss:     3.089075, Lr: 0.000056, Tokens per sec:   2630
2023-03-09 18:50:04,026 - INFO - __main__ - Epoch  58, Step:  124500, Batch Loss:     3.544636, Lr: 0.000056, Tokens per sec:   2629
2023-03-09 18:50:24,543 - INFO - __main__ - Epoch  58, Step:  124600, Batch Loss:     3.693819, Lr: 0.000056, Tokens per sec:   2650
2023-03-09 18:50:45,541 - INFO - __main__ - Epoch  58, Step:  124700, Batch Loss:     4.939896, Lr: 0.000056, Tokens per sec:   2522
2023-03-09 18:51:05,896 - INFO - __main__ - Epoch  58, Step:  124800, Batch Loss:     5.962303, Lr: 0.000056, Tokens per sec:   2653
2023-03-09 18:51:26,506 - INFO - __main__ - Epoch  58, Step:  124900, Batch Loss:     3.159818, Lr: 0.000056, Tokens per sec:   2585
2023-03-09 18:51:46,679 - INFO - __main__ - Epoch  58, Step:  125000, Batch Loss:     4.154394, Lr: 0.000056, Tokens per sec:   2662
2023-03-09 18:52:06,825 - INFO - __main__ - Epoch  58, Step:  125100, Batch Loss:     5.773699, Lr: 0.000056, Tokens per sec:   2693
2023-03-09 18:52:26,916 - INFO - __main__ - Epoch  58, Step:  125200, Batch Loss:     3.985459, Lr: 0.000056, Tokens per sec:   2694
2023-03-09 18:52:46,882 - INFO - __main__ - Epoch  58, Step:  125300, Batch Loss:     6.587111, Lr: 0.000056, Tokens per sec:   2712
2023-03-09 18:53:06,807 - INFO - __main__ - Epoch  58, Step:  125400, Batch Loss:     4.497931, Lr: 0.000056, Tokens per sec:   2682
2023-03-09 18:53:27,043 - INFO - __main__ - Epoch  58, Step:  125500, Batch Loss:     5.384384, Lr: 0.000056, Tokens per sec:   2696
2023-03-09 18:53:46,836 - INFO - __main__ - Epoch  58, Step:  125600, Batch Loss:     5.130768, Lr: 0.000056, Tokens per sec:   2771
2023-03-09 18:54:06,610 - INFO - __main__ - Epoch  58, Step:  125700, Batch Loss:     5.399071, Lr: 0.000056, Tokens per sec:   2765
2023-03-09 18:54:26,740 - INFO - __main__ - Epoch  58, Step:  125800, Batch Loss:     5.278166, Lr: 0.000056, Tokens per sec:   2696
2023-03-09 18:54:46,778 - INFO - __main__ - Epoch  58, Step:  125900, Batch Loss:     5.148807, Lr: 0.000056, Tokens per sec:   2700
2023-03-09 18:55:06,606 - INFO - __main__ - Epoch  58, Step:  126000, Batch Loss:     5.093764, Lr: 0.000056, Tokens per sec:   2696
2023-03-09 18:55:26,719 - INFO - __main__ - Epoch  58, Step:  126100, Batch Loss:     5.193462, Lr: 0.000056, Tokens per sec:   2633
2023-03-09 18:55:46,633 - INFO - __main__ - Epoch  58, Step:  126200, Batch Loss:     4.138536, Lr: 0.000056, Tokens per sec:   2666
2023-03-09 18:56:06,761 - INFO - __main__ - Epoch  58, Step:  126300, Batch Loss:     4.164162, Lr: 0.000056, Tokens per sec:   2664
2023-03-09 18:56:23,243 - INFO - __main__ - Epoch  58: total training loss 9361.79
2023-03-09 18:56:23,244 - INFO - __main__ - Epoch 59
2023-03-09 18:56:27,199 - INFO - __main__ - Epoch  59, Step:  126400, Batch Loss:     3.736177, Lr: 0.000056, Tokens per sec:   2331
2023-03-09 18:56:47,293 - INFO - __main__ - Epoch  59, Step:  126500, Batch Loss:     3.564752, Lr: 0.000056, Tokens per sec:   2657
2023-03-09 18:57:07,549 - INFO - __main__ - Epoch  59, Step:  126600, Batch Loss:     3.056057, Lr: 0.000056, Tokens per sec:   2662
2023-03-09 18:57:27,274 - INFO - __main__ - Epoch  59, Step:  126700, Batch Loss:     3.637667, Lr: 0.000056, Tokens per sec:   2731
2023-03-09 18:57:47,468 - INFO - __main__ - Epoch  59, Step:  126800, Batch Loss:     4.630651, Lr: 0.000056, Tokens per sec:   2724
2023-03-09 18:58:07,750 - INFO - __main__ - Epoch  59, Step:  126900, Batch Loss:     3.502335, Lr: 0.000056, Tokens per sec:   2603
2023-03-09 18:58:28,129 - INFO - __main__ - Epoch  59, Step:  127000, Batch Loss:     4.135111, Lr: 0.000056, Tokens per sec:   2634
2023-03-09 18:58:47,993 - INFO - __main__ - Epoch  59, Step:  127100, Batch Loss:     4.679991, Lr: 0.000056, Tokens per sec:   2696
2023-03-09 18:59:07,724 - INFO - __main__ - Epoch  59, Step:  127200, Batch Loss:     4.547178, Lr: 0.000056, Tokens per sec:   2773
2023-03-09 18:59:27,768 - INFO - __main__ - Epoch  59, Step:  127300, Batch Loss:     4.471447, Lr: 0.000056, Tokens per sec:   2681
2023-03-09 18:59:47,671 - INFO - __main__ - Epoch  59, Step:  127400, Batch Loss:     4.819549, Lr: 0.000056, Tokens per sec:   2694
2023-03-09 19:00:07,662 - INFO - __main__ - Epoch  59, Step:  127500, Batch Loss:     4.505501, Lr: 0.000056, Tokens per sec:   2660
2023-03-09 19:00:27,145 - INFO - __main__ - Epoch  59, Step:  127600, Batch Loss:     3.542831, Lr: 0.000056, Tokens per sec:   2753
2023-03-09 19:00:46,157 - INFO - __main__ - Epoch  59, Step:  127700, Batch Loss:     3.773944, Lr: 0.000056, Tokens per sec:   2857
2023-03-09 19:01:06,202 - INFO - __main__ - Epoch  59, Step:  127800, Batch Loss:     4.398382, Lr: 0.000056, Tokens per sec:   2697
2023-03-09 19:01:26,504 - INFO - __main__ - Epoch  59, Step:  127900, Batch Loss:     4.837301, Lr: 0.000056, Tokens per sec:   2674
2023-03-09 19:01:46,092 - INFO - __main__ - Epoch  59, Step:  128000, Batch Loss:     5.844252, Lr: 0.000056, Tokens per sec:   2789
2023-03-09 19:02:05,638 - INFO - __main__ - Epoch  59, Step:  128100, Batch Loss:     3.570200, Lr: 0.000056, Tokens per sec:   2718
2023-03-09 19:02:26,017 - INFO - __main__ - Epoch  59, Step:  128200, Batch Loss:     6.366502, Lr: 0.000056, Tokens per sec:   2693
2023-03-09 19:02:45,868 - INFO - __main__ - Epoch  59, Step:  128300, Batch Loss:     4.232333, Lr: 0.000056, Tokens per sec:   2731
2023-03-09 19:03:05,776 - INFO - __main__ - Epoch  59, Step:  128400, Batch Loss:     2.903659, Lr: 0.000056, Tokens per sec:   2668
2023-03-09 19:03:25,628 - INFO - __main__ - Epoch  59, Step:  128500, Batch Loss:     3.914881, Lr: 0.000056, Tokens per sec:   2699
2023-03-09 19:03:38,006 - INFO - __main__ - Epoch  59: total training loss 9138.26
2023-03-09 19:03:38,007 - INFO - __main__ - Epoch 60
2023-03-09 19:03:46,143 - INFO - __main__ - Epoch  60, Step:  128600, Batch Loss:     3.552290, Lr: 0.000055, Tokens per sec:   2515
2023-03-09 19:04:06,057 - INFO - __main__ - Epoch  60, Step:  128700, Batch Loss:     4.066833, Lr: 0.000055, Tokens per sec:   2721
2023-03-09 19:04:25,774 - INFO - __main__ - Epoch  60, Step:  128800, Batch Loss:     5.245460, Lr: 0.000055, Tokens per sec:   2761
2023-03-09 19:04:45,006 - INFO - __main__ - Epoch  60, Step:  128900, Batch Loss:     2.995968, Lr: 0.000055, Tokens per sec:   2800
2023-03-09 19:05:04,611 - INFO - __main__ - Epoch  60, Step:  129000, Batch Loss:     3.974084, Lr: 0.000055, Tokens per sec:   2663
2023-03-09 19:05:23,831 - INFO - __main__ - Epoch  60, Step:  129100, Batch Loss:     4.072466, Lr: 0.000055, Tokens per sec:   2854
2023-03-09 19:05:43,819 - INFO - __main__ - Epoch  60, Step:  129200, Batch Loss:     3.689410, Lr: 0.000055, Tokens per sec:   2652
2023-03-09 19:06:03,882 - INFO - __main__ - Epoch  60, Step:  129300, Batch Loss:     5.402790, Lr: 0.000055, Tokens per sec:   2694
2023-03-09 19:06:23,458 - INFO - __main__ - Epoch  60, Step:  129400, Batch Loss:     4.097801, Lr: 0.000055, Tokens per sec:   2759
2023-03-09 19:06:43,434 - INFO - __main__ - Epoch  60, Step:  129500, Batch Loss:     5.036869, Lr: 0.000055, Tokens per sec:   2711
2023-03-09 19:07:03,604 - INFO - __main__ - Epoch  60, Step:  129600, Batch Loss:     5.088962, Lr: 0.000055, Tokens per sec:   2706
2023-03-09 19:07:23,629 - INFO - __main__ - Epoch  60, Step:  129700, Batch Loss:     3.923107, Lr: 0.000055, Tokens per sec:   2669
2023-03-09 19:07:43,775 - INFO - __main__ - Epoch  60, Step:  129800, Batch Loss:     4.919520, Lr: 0.000055, Tokens per sec:   2675
2023-03-09 19:08:03,866 - INFO - __main__ - Epoch  60, Step:  129900, Batch Loss:     4.530001, Lr: 0.000055, Tokens per sec:   2677
2023-03-09 19:08:23,925 - INFO - __main__ - Epoch  60, Step:  130000, Batch Loss:     4.564774, Lr: 0.000055, Tokens per sec:   2685
2023-03-09 19:08:43,993 - INFO - __main__ - Epoch  60, Step:  130100, Batch Loss:     4.042426, Lr: 0.000055, Tokens per sec:   2699
2023-03-09 19:09:03,681 - INFO - __main__ - Epoch  60, Step:  130200, Batch Loss:     3.360885, Lr: 0.000055, Tokens per sec:   2755
2023-03-09 19:09:23,657 - INFO - __main__ - Epoch  60, Step:  130300, Batch Loss:     4.862954, Lr: 0.000055, Tokens per sec:   2664
2023-03-09 19:09:43,114 - INFO - __main__ - Epoch  60, Step:  130400, Batch Loss:     3.964437, Lr: 0.000055, Tokens per sec:   2721
2023-03-09 19:10:02,953 - INFO - __main__ - Epoch  60, Step:  130500, Batch Loss:     3.961209, Lr: 0.000055, Tokens per sec:   2738
2023-03-09 19:10:22,982 - INFO - __main__ - Epoch  60, Step:  130600, Batch Loss:     3.902047, Lr: 0.000055, Tokens per sec:   2702
2023-03-09 19:10:43,047 - INFO - __main__ - Epoch  60, Step:  130700, Batch Loss:     3.319248, Lr: 0.000055, Tokens per sec:   2697
2023-03-09 19:10:51,142 - INFO - __main__ - Epoch  60: total training loss 8929.46
2023-03-09 19:10:51,144 - INFO - __main__ - Epoch 61
2023-03-09 19:11:03,612 - INFO - __main__ - Epoch  61, Step:  130800, Batch Loss:     2.648391, Lr: 0.000055, Tokens per sec:   2596
2023-03-09 19:11:23,789 - INFO - __main__ - Epoch  61, Step:  130900, Batch Loss:     4.242349, Lr: 0.000055, Tokens per sec:   2657
2023-03-09 19:11:43,951 - INFO - __main__ - Epoch  61, Step:  131000, Batch Loss:     3.655110, Lr: 0.000055, Tokens per sec:   2658
2023-03-09 19:12:04,106 - INFO - __main__ - Epoch  61, Step:  131100, Batch Loss:     2.565756, Lr: 0.000055, Tokens per sec:   2654
2023-03-09 19:12:23,824 - INFO - __main__ - Epoch  61, Step:  131200, Batch Loss:     3.387233, Lr: 0.000055, Tokens per sec:   2770
2023-03-09 19:12:43,875 - INFO - __main__ - Epoch  61, Step:  131300, Batch Loss:     3.501573, Lr: 0.000055, Tokens per sec:   2709
2023-03-09 19:13:04,003 - INFO - __main__ - Epoch  61, Step:  131400, Batch Loss:     5.545277, Lr: 0.000055, Tokens per sec:   2719
2023-03-09 19:13:24,336 - INFO - __main__ - Epoch  61, Step:  131500, Batch Loss:     4.565934, Lr: 0.000055, Tokens per sec:   2690
2023-03-09 19:13:44,535 - INFO - __main__ - Epoch  61, Step:  131600, Batch Loss:     4.354196, Lr: 0.000055, Tokens per sec:   2700
2023-03-09 19:14:04,352 - INFO - __main__ - Epoch  61, Step:  131700, Batch Loss:     5.105538, Lr: 0.000055, Tokens per sec:   2697
2023-03-09 19:14:24,520 - INFO - __main__ - Epoch  61, Step:  131800, Batch Loss:     5.606730, Lr: 0.000055, Tokens per sec:   2654
2023-03-09 19:14:44,737 - INFO - __main__ - Epoch  61, Step:  131900, Batch Loss:     3.173840, Lr: 0.000055, Tokens per sec:   2655
2023-03-09 19:15:05,016 - INFO - __main__ - Epoch  61, Step:  132000, Batch Loss:     3.866684, Lr: 0.000055, Tokens per sec:   2603
2023-03-09 19:15:24,809 - INFO - __main__ - Epoch  61, Step:  132100, Batch Loss:     3.838980, Lr: 0.000055, Tokens per sec:   2701
2023-03-09 19:15:45,197 - INFO - __main__ - Epoch  61, Step:  132200, Batch Loss:     5.014498, Lr: 0.000055, Tokens per sec:   2627
2023-03-09 19:16:05,519 - INFO - __main__ - Epoch  61, Step:  132300, Batch Loss:     4.215711, Lr: 0.000055, Tokens per sec:   2673
2023-03-09 19:16:25,525 - INFO - __main__ - Epoch  61, Step:  132400, Batch Loss:     3.524191, Lr: 0.000055, Tokens per sec:   2711
2023-03-09 19:16:45,747 - INFO - __main__ - Epoch  61, Step:  132500, Batch Loss:     5.246257, Lr: 0.000055, Tokens per sec:   2659
2023-03-09 19:17:05,497 - INFO - __main__ - Epoch  61, Step:  132600, Batch Loss:     4.530480, Lr: 0.000055, Tokens per sec:   2725
2023-03-09 19:17:25,532 - INFO - __main__ - Epoch  61, Step:  132700, Batch Loss:     6.080976, Lr: 0.000055, Tokens per sec:   2691
2023-03-09 19:17:45,884 - INFO - __main__ - Epoch  61, Step:  132800, Batch Loss:     5.368131, Lr: 0.000055, Tokens per sec:   2616
2023-03-09 19:18:05,676 - INFO - __main__ - Epoch  61, Step:  132900, Batch Loss:     3.791320, Lr: 0.000055, Tokens per sec:   2690
2023-03-09 19:18:09,497 - INFO - __main__ - Epoch  61: total training loss 8714.65
2023-03-09 19:18:09,498 - INFO - __main__ - Epoch 62
2023-03-09 19:18:25,883 - INFO - __main__ - Epoch  62, Step:  133000, Batch Loss:     4.183647, Lr: 0.000054, Tokens per sec:   2738
2023-03-09 19:18:46,089 - INFO - __main__ - Epoch  62, Step:  133100, Batch Loss:     3.693203, Lr: 0.000054, Tokens per sec:   2676
2023-03-09 19:19:06,334 - INFO - __main__ - Epoch  62, Step:  133200, Batch Loss:     2.152145, Lr: 0.000054, Tokens per sec:   2640
2023-03-09 19:19:26,621 - INFO - __main__ - Epoch  62, Step:  133300, Batch Loss:     3.900418, Lr: 0.000054, Tokens per sec:   2690
2023-03-09 19:19:46,950 - INFO - __main__ - Epoch  62, Step:  133400, Batch Loss:     4.546538, Lr: 0.000054, Tokens per sec:   2654
2023-03-09 19:20:07,029 - INFO - __main__ - Epoch  62, Step:  133500, Batch Loss:     2.833167, Lr: 0.000054, Tokens per sec:   2652
2023-03-09 19:20:27,313 - INFO - __main__ - Epoch  62, Step:  133600, Batch Loss:     5.014074, Lr: 0.000054, Tokens per sec:   2636
2023-03-09 19:20:47,419 - INFO - __main__ - Epoch  62, Step:  133700, Batch Loss:     3.480464, Lr: 0.000054, Tokens per sec:   2641
2023-03-09 19:21:07,618 - INFO - __main__ - Epoch  62, Step:  133800, Batch Loss:     3.982989, Lr: 0.000054, Tokens per sec:   2639
2023-03-09 19:21:27,790 - INFO - __main__ - Epoch  62, Step:  133900, Batch Loss:     4.231546, Lr: 0.000054, Tokens per sec:   2679
2023-03-09 19:21:48,265 - INFO - __main__ - Epoch  62, Step:  134000, Batch Loss:     3.442849, Lr: 0.000054, Tokens per sec:   2628
2023-03-09 19:22:08,525 - INFO - __main__ - Epoch  62, Step:  134100, Batch Loss:     3.027163, Lr: 0.000054, Tokens per sec:   2640
2023-03-09 19:22:28,558 - INFO - __main__ - Epoch  62, Step:  134200, Batch Loss:     3.531857, Lr: 0.000054, Tokens per sec:   2670
2023-03-09 19:22:48,746 - INFO - __main__ - Epoch  62, Step:  134300, Batch Loss:     3.793855, Lr: 0.000054, Tokens per sec:   2672
2023-03-09 19:23:09,036 - INFO - __main__ - Epoch  62, Step:  134400, Batch Loss:     3.290809, Lr: 0.000054, Tokens per sec:   2644
2023-03-09 19:23:28,741 - INFO - __main__ - Epoch  62, Step:  134500, Batch Loss:     4.749887, Lr: 0.000054, Tokens per sec:   2724
2023-03-09 19:23:49,028 - INFO - __main__ - Epoch  62, Step:  134600, Batch Loss:     6.270494, Lr: 0.000054, Tokens per sec:   2689
2023-03-09 19:24:09,059 - INFO - __main__ - Epoch  62, Step:  134700, Batch Loss:     3.524758, Lr: 0.000054, Tokens per sec:   2711
2023-03-09 19:24:29,341 - INFO - __main__ - Epoch  62, Step:  134800, Batch Loss:     4.176319, Lr: 0.000054, Tokens per sec:   2640
2023-03-09 19:24:49,570 - INFO - __main__ - Epoch  62, Step:  134900, Batch Loss:     4.001541, Lr: 0.000054, Tokens per sec:   2672
2023-03-09 19:25:09,606 - INFO - __main__ - Epoch  62, Step:  135000, Batch Loss:     5.023246, Lr: 0.000054, Tokens per sec:   2671
2023-03-09 19:25:29,519 - INFO - __main__ - Epoch  62: total training loss 8500.16
2023-03-09 19:25:29,521 - INFO - __main__ - Epoch 63
2023-03-09 19:25:30,270 - INFO - __main__ - Epoch  63, Step:  135100, Batch Loss:     2.999443, Lr: 0.000054, Tokens per sec:   1452
2023-03-09 19:25:50,360 - INFO - __main__ - Epoch  63, Step:  135200, Batch Loss:     3.279803, Lr: 0.000054, Tokens per sec:   2669
2023-03-09 19:26:10,440 - INFO - __main__ - Epoch  63, Step:  135300, Batch Loss:     4.219751, Lr: 0.000054, Tokens per sec:   2703
2023-03-09 19:26:30,679 - INFO - __main__ - Epoch  63, Step:  135400, Batch Loss:     4.442387, Lr: 0.000054, Tokens per sec:   2685
2023-03-09 19:26:50,817 - INFO - __main__ - Epoch  63, Step:  135500, Batch Loss:     5.057776, Lr: 0.000054, Tokens per sec:   2649
2023-03-09 19:27:10,925 - INFO - __main__ - Epoch  63, Step:  135600, Batch Loss:     3.697165, Lr: 0.000054, Tokens per sec:   2689
2023-03-09 19:27:30,975 - INFO - __main__ - Epoch  63, Step:  135700, Batch Loss:     3.465342, Lr: 0.000054, Tokens per sec:   2648
2023-03-09 19:27:50,856 - INFO - __main__ - Epoch  63, Step:  135800, Batch Loss:     4.055632, Lr: 0.000054, Tokens per sec:   2675
2023-03-09 19:28:10,765 - INFO - __main__ - Epoch  63, Step:  135900, Batch Loss:     3.004828, Lr: 0.000054, Tokens per sec:   2720
2023-03-09 19:28:30,162 - INFO - __main__ - Epoch  63, Step:  136000, Batch Loss:     3.904544, Lr: 0.000054, Tokens per sec:   2777
2023-03-09 19:28:50,118 - INFO - __main__ - Epoch  63, Step:  136100, Batch Loss:     4.584797, Lr: 0.000054, Tokens per sec:   2687
2023-03-09 19:29:10,163 - INFO - __main__ - Epoch  63, Step:  136200, Batch Loss:     2.694758, Lr: 0.000054, Tokens per sec:   2702
2023-03-09 19:29:30,312 - INFO - __main__ - Epoch  63, Step:  136300, Batch Loss:     4.414814, Lr: 0.000054, Tokens per sec:   2657
2023-03-09 19:29:50,314 - INFO - __main__ - Epoch  63, Step:  136400, Batch Loss:     3.527762, Lr: 0.000054, Tokens per sec:   2737
2023-03-09 19:30:10,139 - INFO - __main__ - Epoch  63, Step:  136500, Batch Loss:     4.288874, Lr: 0.000054, Tokens per sec:   2731
2023-03-09 19:30:30,482 - INFO - __main__ - Epoch  63, Step:  136600, Batch Loss:     5.612907, Lr: 0.000054, Tokens per sec:   2637
2023-03-09 19:30:50,682 - INFO - __main__ - Epoch  63, Step:  136700, Batch Loss:     3.909880, Lr: 0.000054, Tokens per sec:   2647
2023-03-09 19:31:11,109 - INFO - __main__ - Epoch  63, Step:  136800, Batch Loss:     3.581287, Lr: 0.000054, Tokens per sec:   2624
2023-03-09 19:31:31,107 - INFO - __main__ - Epoch  63, Step:  136900, Batch Loss:     4.060152, Lr: 0.000054, Tokens per sec:   2705
2023-03-09 19:31:50,455 - INFO - __main__ - Epoch  63, Step:  137000, Batch Loss:     3.959953, Lr: 0.000054, Tokens per sec:   2803
2023-03-09 19:32:10,577 - INFO - __main__ - Epoch  63, Step:  137100, Batch Loss:     3.894974, Lr: 0.000054, Tokens per sec:   2676
2023-03-09 19:32:30,928 - INFO - __main__ - Epoch  63, Step:  137200, Batch Loss:     5.526030, Lr: 0.000054, Tokens per sec:   2646
2023-03-09 19:32:46,750 - INFO - __main__ - Epoch  63: total training loss 8319.85
2023-03-09 19:32:46,751 - INFO - __main__ - Epoch 64
2023-03-09 19:32:51,735 - INFO - __main__ - Epoch  64, Step:  137300, Batch Loss:     3.301260, Lr: 0.000053, Tokens per sec:   2440
2023-03-09 19:33:12,068 - INFO - __main__ - Epoch  64, Step:  137400, Batch Loss:     2.769449, Lr: 0.000053, Tokens per sec:   2666
2023-03-09 19:33:32,099 - INFO - __main__ - Epoch  64, Step:  137500, Batch Loss:     3.658489, Lr: 0.000053, Tokens per sec:   2693
2023-03-09 19:33:52,340 - INFO - __main__ - Epoch  64, Step:  137600, Batch Loss:     4.849727, Lr: 0.000053, Tokens per sec:   2664
2023-03-09 19:34:12,547 - INFO - __main__ - Epoch  64, Step:  137700, Batch Loss:     3.983324, Lr: 0.000053, Tokens per sec:   2637
2023-03-09 19:34:32,648 - INFO - __main__ - Epoch  64, Step:  137800, Batch Loss:     2.940967, Lr: 0.000053, Tokens per sec:   2667
2023-03-09 19:34:53,100 - INFO - __main__ - Epoch  64, Step:  137900, Batch Loss:     3.333149, Lr: 0.000053, Tokens per sec:   2708
2023-03-09 19:35:13,374 - INFO - __main__ - Epoch  64, Step:  138000, Batch Loss:     4.446039, Lr: 0.000053, Tokens per sec:   2648
2023-03-09 19:35:33,569 - INFO - __main__ - Epoch  64, Step:  138100, Batch Loss:     4.176865, Lr: 0.000053, Tokens per sec:   2639
2023-03-09 19:35:53,609 - INFO - __main__ - Epoch  64, Step:  138200, Batch Loss:     2.498566, Lr: 0.000053, Tokens per sec:   2681
2023-03-09 19:36:13,846 - INFO - __main__ - Epoch  64, Step:  138300, Batch Loss:     4.302428, Lr: 0.000053, Tokens per sec:   2641
2023-03-09 19:36:33,692 - INFO - __main__ - Epoch  64, Step:  138400, Batch Loss:     3.516901, Lr: 0.000053, Tokens per sec:   2719
2023-03-09 19:36:53,861 - INFO - __main__ - Epoch  64, Step:  138500, Batch Loss:     3.250832, Lr: 0.000053, Tokens per sec:   2704
2023-03-09 19:37:14,175 - INFO - __main__ - Epoch  64, Step:  138600, Batch Loss:     4.293287, Lr: 0.000053, Tokens per sec:   2659
2023-03-09 19:37:34,417 - INFO - __main__ - Epoch  64, Step:  138700, Batch Loss:     2.833835, Lr: 0.000053, Tokens per sec:   2586
2023-03-09 19:37:54,673 - INFO - __main__ - Epoch  64, Step:  138800, Batch Loss:     3.147244, Lr: 0.000053, Tokens per sec:   2644
2023-03-09 19:38:14,862 - INFO - __main__ - Epoch  64, Step:  138900, Batch Loss:     2.368289, Lr: 0.000053, Tokens per sec:   2711
2023-03-09 19:38:34,933 - INFO - __main__ - Epoch  64, Step:  139000, Batch Loss:     3.452831, Lr: 0.000053, Tokens per sec:   2701
2023-03-09 19:38:54,190 - INFO - __main__ - Epoch  64, Step:  139100, Batch Loss:     2.559387, Lr: 0.000053, Tokens per sec:   2816
2023-03-09 19:39:14,285 - INFO - __main__ - Epoch  64, Step:  139200, Batch Loss:     4.384233, Lr: 0.000053, Tokens per sec:   2690
2023-03-09 19:39:34,508 - INFO - __main__ - Epoch  64, Step:  139300, Batch Loss:     3.228953, Lr: 0.000053, Tokens per sec:   2622
2023-03-09 19:39:54,481 - INFO - __main__ - Epoch  64, Step:  139400, Batch Loss:     3.789233, Lr: 0.000053, Tokens per sec:   2725
2023-03-09 19:40:05,893 - INFO - __main__ - Epoch  64: total training loss 8150.71
2023-03-09 19:40:05,895 - INFO - __main__ - Epoch 65
2023-03-09 19:40:15,193 - INFO - __main__ - Epoch  65, Step:  139500, Batch Loss:     2.770820, Lr: 0.000053, Tokens per sec:   2562
2023-03-09 19:40:35,406 - INFO - __main__ - Epoch  65, Step:  139600, Batch Loss:     3.383608, Lr: 0.000053, Tokens per sec:   2643
2023-03-09 19:40:54,885 - INFO - __main__ - Epoch  65, Step:  139700, Batch Loss:     3.063080, Lr: 0.000053, Tokens per sec:   2745
2023-03-09 19:41:14,945 - INFO - __main__ - Epoch  65, Step:  139800, Batch Loss:     3.860187, Lr: 0.000053, Tokens per sec:   2688
2023-03-09 19:41:35,132 - INFO - __main__ - Epoch  65, Step:  139900, Batch Loss:     4.144325, Lr: 0.000053, Tokens per sec:   2651
2023-03-09 19:41:55,440 - INFO - __main__ - Epoch  65, Step:  140000, Batch Loss:     2.524211, Lr: 0.000053, Tokens per sec:   2634
2023-03-09 19:42:15,007 - INFO - __main__ - Epoch  65, Step:  140100, Batch Loss:     3.354504, Lr: 0.000053, Tokens per sec:   2776
2023-03-09 19:42:34,989 - INFO - __main__ - Epoch  65, Step:  140200, Batch Loss:     2.428597, Lr: 0.000053, Tokens per sec:   2667
2023-03-09 19:42:54,851 - INFO - __main__ - Epoch  65, Step:  140300, Batch Loss:     3.535186, Lr: 0.000053, Tokens per sec:   2695
2023-03-09 19:43:15,014 - INFO - __main__ - Epoch  65, Step:  140400, Batch Loss:     3.697458, Lr: 0.000053, Tokens per sec:   2654
2023-03-09 19:43:34,886 - INFO - __main__ - Epoch  65, Step:  140500, Batch Loss:     3.905529, Lr: 0.000053, Tokens per sec:   2704
2023-03-09 19:43:54,898 - INFO - __main__ - Epoch  65, Step:  140600, Batch Loss:     3.640292, Lr: 0.000053, Tokens per sec:   2699
2023-03-09 19:44:14,828 - INFO - __main__ - Epoch  65, Step:  140700, Batch Loss:     3.275785, Lr: 0.000053, Tokens per sec:   2741
2023-03-09 19:44:35,066 - INFO - __main__ - Epoch  65, Step:  140800, Batch Loss:     3.725273, Lr: 0.000053, Tokens per sec:   2598
2023-03-09 19:44:55,283 - INFO - __main__ - Epoch  65, Step:  140900, Batch Loss:     4.152071, Lr: 0.000053, Tokens per sec:   2685
2023-03-09 19:45:15,171 - INFO - __main__ - Epoch  65, Step:  141000, Batch Loss:     4.629314, Lr: 0.000053, Tokens per sec:   2749
2023-03-09 19:45:34,568 - INFO - __main__ - Epoch  65, Step:  141100, Batch Loss:     4.128246, Lr: 0.000053, Tokens per sec:   2789
2023-03-09 19:45:54,279 - INFO - __main__ - Epoch  65, Step:  141200, Batch Loss:     4.113223, Lr: 0.000053, Tokens per sec:   2712
2023-03-09 19:46:14,573 - INFO - __main__ - Epoch  65, Step:  141300, Batch Loss:     3.272767, Lr: 0.000053, Tokens per sec:   2704
2023-03-09 19:46:33,150 - INFO - __main__ - Epoch  65, Step:  141400, Batch Loss:     5.397860, Lr: 0.000053, Tokens per sec:   2894
2023-03-09 19:46:53,319 - INFO - __main__ - Epoch  65, Step:  141500, Batch Loss:     3.015361, Lr: 0.000053, Tokens per sec:   2676
2023-03-09 19:47:12,730 - INFO - __main__ - Epoch  65, Step:  141600, Batch Loss:     4.171730, Lr: 0.000053, Tokens per sec:   2779
2023-03-09 19:47:19,593 - INFO - __main__ - Epoch  65: total training loss 7966.07
2023-03-09 19:47:19,594 - INFO - __main__ - Epoch 66
2023-03-09 19:47:33,051 - INFO - __main__ - Epoch  66, Step:  141700, Batch Loss:     3.874503, Lr: 0.000052, Tokens per sec:   2587
2023-03-09 19:47:53,123 - INFO - __main__ - Epoch  66, Step:  141800, Batch Loss:     4.882300, Lr: 0.000052, Tokens per sec:   2689
2023-03-09 19:48:13,172 - INFO - __main__ - Epoch  66, Step:  141900, Batch Loss:     2.094243, Lr: 0.000052, Tokens per sec:   2722
2023-03-09 19:48:33,248 - INFO - __main__ - Epoch  66, Step:  142000, Batch Loss:     4.905688, Lr: 0.000052, Tokens per sec:   2728
2023-03-09 19:48:53,260 - INFO - __main__ - Epoch  66, Step:  142100, Batch Loss:     2.603925, Lr: 0.000052, Tokens per sec:   2690
2023-03-09 19:49:13,331 - INFO - __main__ - Epoch  66, Step:  142200, Batch Loss:     4.127738, Lr: 0.000052, Tokens per sec:   2707
2023-03-09 19:49:33,359 - INFO - __main__ - Epoch  66, Step:  142300, Batch Loss:     4.411794, Lr: 0.000052, Tokens per sec:   2709
2023-03-09 19:49:53,422 - INFO - __main__ - Epoch  66, Step:  142400, Batch Loss:     3.539984, Lr: 0.000052, Tokens per sec:   2651
2023-03-09 19:50:13,435 - INFO - __main__ - Epoch  66, Step:  142500, Batch Loss:     4.493176, Lr: 0.000052, Tokens per sec:   2725
2023-03-09 19:50:33,494 - INFO - __main__ - Epoch  66, Step:  142600, Batch Loss:     3.702989, Lr: 0.000052, Tokens per sec:   2691
2023-03-09 19:50:53,468 - INFO - __main__ - Epoch  66, Step:  142700, Batch Loss:     2.684934, Lr: 0.000052, Tokens per sec:   2684
2023-03-09 19:51:13,550 - INFO - __main__ - Epoch  66, Step:  142800, Batch Loss:     3.558779, Lr: 0.000052, Tokens per sec:   2701
2023-03-09 19:51:33,600 - INFO - __main__ - Epoch  66, Step:  142900, Batch Loss:     3.716083, Lr: 0.000052, Tokens per sec:   2668
2023-03-09 19:51:53,626 - INFO - __main__ - Epoch  66, Step:  143000, Batch Loss:     5.069925, Lr: 0.000052, Tokens per sec:   2671
2023-03-09 19:52:13,088 - INFO - __main__ - Epoch  66, Step:  143100, Batch Loss:     3.442693, Lr: 0.000052, Tokens per sec:   2741
2023-03-09 19:52:31,375 - INFO - __main__ - Epoch  66, Step:  143200, Batch Loss:     5.096691, Lr: 0.000052, Tokens per sec:   2998
2023-03-09 19:52:49,651 - INFO - __main__ - Epoch  66, Step:  143300, Batch Loss:     2.938301, Lr: 0.000052, Tokens per sec:   2888
2023-03-09 19:53:09,340 - INFO - __main__ - Epoch  66, Step:  143400, Batch Loss:     3.498763, Lr: 0.000052, Tokens per sec:   2751
2023-03-09 19:53:29,381 - INFO - __main__ - Epoch  66, Step:  143500, Batch Loss:     3.940636, Lr: 0.000052, Tokens per sec:   2650
2023-03-09 19:53:49,397 - INFO - __main__ - Epoch  66, Step:  143600, Batch Loss:     4.257648, Lr: 0.000052, Tokens per sec:   2627
2023-03-09 19:54:09,465 - INFO - __main__ - Epoch  66, Step:  143700, Batch Loss:     4.287454, Lr: 0.000052, Tokens per sec:   2720
2023-03-09 19:54:29,461 - INFO - __main__ - Epoch  66, Step:  143800, Batch Loss:     3.112450, Lr: 0.000052, Tokens per sec:   2656
2023-03-09 19:54:32,315 - INFO - __main__ - Epoch  66: total training loss 7763.75
2023-03-09 19:54:32,316 - INFO - __main__ - Epoch 67
2023-03-09 19:54:49,929 - INFO - __main__ - Epoch  67, Step:  143900, Batch Loss:     2.366882, Lr: 0.000052, Tokens per sec:   2640
2023-03-09 19:55:10,011 - INFO - __main__ - Epoch  67, Step:  144000, Batch Loss:     2.309783, Lr: 0.000052, Tokens per sec:   2731
2023-03-09 19:55:30,082 - INFO - __main__ - Epoch  67, Step:  144100, Batch Loss:     4.466054, Lr: 0.000052, Tokens per sec:   2687
2023-03-09 19:55:49,655 - INFO - __main__ - Epoch  67, Step:  144200, Batch Loss:     3.035427, Lr: 0.000052, Tokens per sec:   2717
2023-03-09 19:56:08,347 - INFO - __main__ - Epoch  67, Step:  144300, Batch Loss:     3.680347, Lr: 0.000052, Tokens per sec:   2880
2023-03-09 19:56:28,419 - INFO - __main__ - Epoch  67, Step:  144400, Batch Loss:     2.599028, Lr: 0.000052, Tokens per sec:   2643
2023-03-09 19:56:48,413 - INFO - __main__ - Epoch  67, Step:  144500, Batch Loss:     4.074345, Lr: 0.000052, Tokens per sec:   2655
2023-03-09 19:57:08,379 - INFO - __main__ - Epoch  67, Step:  144600, Batch Loss:     2.992335, Lr: 0.000052, Tokens per sec:   2678
2023-03-09 19:57:28,396 - INFO - __main__ - Epoch  67, Step:  144700, Batch Loss:     3.320189, Lr: 0.000052, Tokens per sec:   2722
2023-03-09 19:57:48,456 - INFO - __main__ - Epoch  67, Step:  144800, Batch Loss:     3.213449, Lr: 0.000052, Tokens per sec:   2684
2023-03-09 19:58:08,468 - INFO - __main__ - Epoch  67, Step:  144900, Batch Loss:     3.239292, Lr: 0.000052, Tokens per sec:   2694
2023-03-09 19:58:28,525 - INFO - __main__ - Epoch  67, Step:  145000, Batch Loss:     4.033698, Lr: 0.000052, Tokens per sec:   2716
2023-03-09 19:58:48,617 - INFO - __main__ - Epoch  67, Step:  145100, Batch Loss:     3.138507, Lr: 0.000052, Tokens per sec:   2667
2023-03-09 19:59:08,629 - INFO - __main__ - Epoch  67, Step:  145200, Batch Loss:     2.056661, Lr: 0.000052, Tokens per sec:   2650
2023-03-09 19:59:28,643 - INFO - __main__ - Epoch  67, Step:  145300, Batch Loss:     2.631406, Lr: 0.000052, Tokens per sec:   2675
2023-03-09 19:59:48,746 - INFO - __main__ - Epoch  67, Step:  145400, Batch Loss:     3.292189, Lr: 0.000052, Tokens per sec:   2699
2023-03-09 20:00:08,790 - INFO - __main__ - Epoch  67, Step:  145500, Batch Loss:     3.643837, Lr: 0.000052, Tokens per sec:   2689
2023-03-09 20:00:28,803 - INFO - __main__ - Epoch  67, Step:  145600, Batch Loss:     3.187569, Lr: 0.000052, Tokens per sec:   2711
2023-03-09 20:00:48,791 - INFO - __main__ - Epoch  67, Step:  145700, Batch Loss:     3.256220, Lr: 0.000052, Tokens per sec:   2699
2023-03-09 20:01:08,809 - INFO - __main__ - Epoch  67, Step:  145800, Batch Loss:     3.390273, Lr: 0.000052, Tokens per sec:   2684
2023-03-09 20:01:28,925 - INFO - __main__ - Epoch  67, Step:  145900, Batch Loss:     4.545695, Lr: 0.000052, Tokens per sec:   2695
2023-03-09 20:01:46,426 - INFO - __main__ - Epoch  67: total training loss 7619.65
2023-03-09 20:01:46,427 - INFO - __main__ - Epoch 68
2023-03-09 20:01:48,200 - INFO - __main__ - Epoch  68, Step:  146000, Batch Loss:     3.264829, Lr: 0.000051, Tokens per sec:   2321
2023-03-09 20:02:08,324 - INFO - __main__ - Epoch  68, Step:  146100, Batch Loss:     3.673693, Lr: 0.000051, Tokens per sec:   2679
2023-03-09 20:02:28,366 - INFO - __main__ - Epoch  68, Step:  146200, Batch Loss:     4.074466, Lr: 0.000051, Tokens per sec:   2701
2023-03-09 20:02:48,361 - INFO - __main__ - Epoch  68, Step:  146300, Batch Loss:     2.665342, Lr: 0.000051, Tokens per sec:   2682
2023-03-09 20:03:07,912 - INFO - __main__ - Epoch  68, Step:  146400, Batch Loss:     2.729315, Lr: 0.000051, Tokens per sec:   2727
2023-03-09 20:03:26,379 - INFO - __main__ - Epoch  68, Step:  146500, Batch Loss:     2.983908, Lr: 0.000051, Tokens per sec:   2915
2023-03-09 20:03:46,237 - INFO - __main__ - Epoch  68, Step:  146600, Batch Loss:     4.451189, Lr: 0.000051, Tokens per sec:   2727
2023-03-09 20:04:05,538 - INFO - __main__ - Epoch  68, Step:  146700, Batch Loss:     3.428355, Lr: 0.000051, Tokens per sec:   2821
2023-03-09 20:04:25,649 - INFO - __main__ - Epoch  68, Step:  146800, Batch Loss:     2.580427, Lr: 0.000051, Tokens per sec:   2650
2023-03-09 20:04:45,781 - INFO - __main__ - Epoch  68, Step:  146900, Batch Loss:     2.227840, Lr: 0.000051, Tokens per sec:   2678
2023-03-09 20:05:05,867 - INFO - __main__ - Epoch  68, Step:  147000, Batch Loss:     3.000965, Lr: 0.000051, Tokens per sec:   2664
2023-03-09 20:05:26,034 - INFO - __main__ - Epoch  68, Step:  147100, Batch Loss:     3.077038, Lr: 0.000051, Tokens per sec:   2665
2023-03-09 20:05:46,141 - INFO - __main__ - Epoch  68, Step:  147200, Batch Loss:     3.907253, Lr: 0.000051, Tokens per sec:   2701
2023-03-09 20:06:06,196 - INFO - __main__ - Epoch  68, Step:  147300, Batch Loss:     4.778244, Lr: 0.000051, Tokens per sec:   2695
2023-03-09 20:06:26,149 - INFO - __main__ - Epoch  68, Step:  147400, Batch Loss:     4.011218, Lr: 0.000051, Tokens per sec:   2631
2023-03-09 20:06:46,284 - INFO - __main__ - Epoch  68, Step:  147500, Batch Loss:     3.337411, Lr: 0.000051, Tokens per sec:   2705
2023-03-09 20:07:06,100 - INFO - __main__ - Epoch  68, Step:  147600, Batch Loss:     3.703631, Lr: 0.000051, Tokens per sec:   2768
2023-03-09 20:07:25,999 - INFO - __main__ - Epoch  68, Step:  147700, Batch Loss:     3.492071, Lr: 0.000051, Tokens per sec:   2686
2023-03-09 20:07:46,018 - INFO - __main__ - Epoch  68, Step:  147800, Batch Loss:     3.341557, Lr: 0.000051, Tokens per sec:   2695
2023-03-09 20:08:06,065 - INFO - __main__ - Epoch  68, Step:  147900, Batch Loss:     3.281276, Lr: 0.000051, Tokens per sec:   2652
2023-03-09 20:08:24,621 - INFO - __main__ - Epoch  68, Step:  148000, Batch Loss:     3.199497, Lr: 0.000051, Tokens per sec:   2903
2023-03-09 20:08:44,230 - INFO - __main__ - Epoch  68, Step:  148100, Batch Loss:     3.678386, Lr: 0.000051, Tokens per sec:   2746
2023-03-09 20:08:57,671 - INFO - __main__ - Epoch  68: total training loss 7471.40
2023-03-09 20:08:57,672 - INFO - __main__ - Epoch 69
2023-03-09 20:09:03,719 - INFO - __main__ - Epoch  69, Step:  148200, Batch Loss:     3.460366, Lr: 0.000050, Tokens per sec:   2512
2023-03-09 20:09:22,498 - INFO - __main__ - Epoch  69, Step:  148300, Batch Loss:     2.461049, Lr: 0.000050, Tokens per sec:   2849
2023-03-09 20:09:42,267 - INFO - __main__ - Epoch  69, Step:  148400, Batch Loss:     4.100674, Lr: 0.000050, Tokens per sec:   2713
2023-03-09 20:10:02,247 - INFO - __main__ - Epoch  69, Step:  148500, Batch Loss:     3.453635, Lr: 0.000050, Tokens per sec:   2675
2023-03-09 20:10:22,339 - INFO - __main__ - Epoch  69, Step:  148600, Batch Loss:     3.766282, Lr: 0.000050, Tokens per sec:   2698
2023-03-09 20:10:41,355 - INFO - __main__ - Epoch  69, Step:  148700, Batch Loss:     2.548038, Lr: 0.000050, Tokens per sec:   2803
2023-03-09 20:11:01,364 - INFO - __main__ - Epoch  69, Step:  148800, Batch Loss:     2.542127, Lr: 0.000050, Tokens per sec:   2694
2023-03-09 20:11:21,361 - INFO - __main__ - Epoch  69, Step:  148900, Batch Loss:     3.447459, Lr: 0.000050, Tokens per sec:   2657
2023-03-09 20:11:40,785 - INFO - __main__ - Epoch  69, Step:  149000, Batch Loss:     2.951555, Lr: 0.000050, Tokens per sec:   2813
2023-03-09 20:12:00,384 - INFO - __main__ - Epoch  69, Step:  149100, Batch Loss:     3.202184, Lr: 0.000050, Tokens per sec:   2743
2023-03-09 20:12:19,746 - INFO - __main__ - Epoch  69, Step:  149200, Batch Loss:     3.333472, Lr: 0.000050, Tokens per sec:   2800
2023-03-09 20:12:38,440 - INFO - __main__ - Epoch  69, Step:  149300, Batch Loss:     4.138767, Lr: 0.000050, Tokens per sec:   2917
2023-03-09 20:12:57,182 - INFO - __main__ - Epoch  69, Step:  149400, Batch Loss:     2.893831, Lr: 0.000050, Tokens per sec:   2865
2023-03-09 20:13:16,739 - INFO - __main__ - Epoch  69, Step:  149500, Batch Loss:     2.706458, Lr: 0.000050, Tokens per sec:   2757
2023-03-09 20:13:36,772 - INFO - __main__ - Epoch  69, Step:  149600, Batch Loss:     2.844486, Lr: 0.000050, Tokens per sec:   2705
2023-03-09 20:13:56,423 - INFO - __main__ - Epoch  69, Step:  149700, Batch Loss:     2.978010, Lr: 0.000050, Tokens per sec:   2767
2023-03-09 20:14:15,739 - INFO - __main__ - Epoch  69, Step:  149800, Batch Loss:     3.087256, Lr: 0.000050, Tokens per sec:   2802
2023-03-09 20:14:35,783 - INFO - __main__ - Epoch  69, Step:  149900, Batch Loss:     2.900066, Lr: 0.000050, Tokens per sec:   2681
2023-03-09 20:14:54,904 - INFO - __main__ - Epoch  69, Step:  150000, Batch Loss:     2.778812, Lr: 0.000050, Tokens per sec:   2820
2023-03-09 20:15:13,249 - INFO - __main__ - Epoch  69, Step:  150100, Batch Loss:     3.387762, Lr: 0.000050, Tokens per sec:   2958
2023-03-09 20:15:31,895 - INFO - __main__ - Epoch  69, Step:  150200, Batch Loss:     3.105372, Lr: 0.000050, Tokens per sec:   2868
2023-03-09 20:15:51,592 - INFO - __main__ - Epoch  69, Step:  150300, Batch Loss:     3.284474, Lr: 0.000050, Tokens per sec:   2676
2023-03-09 20:16:01,911 - INFO - __main__ - Epoch  69: total training loss 7314.15
2023-03-09 20:16:01,911 - INFO - __main__ - Epoch 70
2023-03-09 20:16:12,087 - INFO - __main__ - Epoch  70, Step:  150400, Batch Loss:     2.693634, Lr: 0.000050, Tokens per sec:   2564
2023-03-09 20:16:32,058 - INFO - __main__ - Epoch  70, Step:  150500, Batch Loss:     3.898918, Lr: 0.000050, Tokens per sec:   2712
2023-03-09 20:16:52,134 - INFO - __main__ - Epoch  70, Step:  150600, Batch Loss:     3.925135, Lr: 0.000050, Tokens per sec:   2708
2023-03-09 20:17:12,182 - INFO - __main__ - Epoch  70, Step:  150700, Batch Loss:     2.426115, Lr: 0.000050, Tokens per sec:   2675
2023-03-09 20:17:31,579 - INFO - __main__ - Epoch  70, Step:  150800, Batch Loss:     4.093831, Lr: 0.000050, Tokens per sec:   2727
2023-03-09 20:17:51,356 - INFO - __main__ - Epoch  70, Step:  150900, Batch Loss:     4.795383, Lr: 0.000050, Tokens per sec:   2752
2023-03-09 20:18:11,446 - INFO - __main__ - Epoch  70, Step:  151000, Batch Loss:     3.748849, Lr: 0.000050, Tokens per sec:   2700
2023-03-09 20:18:31,593 - INFO - __main__ - Epoch  70, Step:  151100, Batch Loss:     3.238500, Lr: 0.000050, Tokens per sec:   2719
2023-03-09 20:18:51,620 - INFO - __main__ - Epoch  70, Step:  151200, Batch Loss:     3.438087, Lr: 0.000050, Tokens per sec:   2614
2023-03-09 20:19:11,711 - INFO - __main__ - Epoch  70, Step:  151300, Batch Loss:     4.157083, Lr: 0.000050, Tokens per sec:   2667
2023-03-09 20:19:31,901 - INFO - __main__ - Epoch  70, Step:  151400, Batch Loss:     3.274824, Lr: 0.000050, Tokens per sec:   2644
2023-03-09 20:19:51,877 - INFO - __main__ - Epoch  70, Step:  151500, Batch Loss:     3.215336, Lr: 0.000050, Tokens per sec:   2717
2023-03-09 20:20:11,991 - INFO - __main__ - Epoch  70, Step:  151600, Batch Loss:     3.332321, Lr: 0.000050, Tokens per sec:   2685
2023-03-09 20:20:32,041 - INFO - __main__ - Epoch  70, Step:  151700, Batch Loss:     3.885417, Lr: 0.000050, Tokens per sec:   2676
2023-03-09 20:20:51,986 - INFO - __main__ - Epoch  70, Step:  151800, Batch Loss:     3.168139, Lr: 0.000050, Tokens per sec:   2634
2023-03-09 20:21:11,531 - INFO - __main__ - Epoch  70, Step:  151900, Batch Loss:     3.384882, Lr: 0.000050, Tokens per sec:   2781
2023-03-09 20:21:31,645 - INFO - __main__ - Epoch  70, Step:  152000, Batch Loss:     2.767353, Lr: 0.000050, Tokens per sec:   2660
2023-03-09 20:21:51,363 - INFO - __main__ - Epoch  70, Step:  152100, Batch Loss:     2.858516, Lr: 0.000050, Tokens per sec:   2713
2023-03-09 20:22:10,113 - INFO - __main__ - Epoch  70, Step:  152200, Batch Loss:     2.613812, Lr: 0.000050, Tokens per sec:   2878
2023-03-09 20:22:29,307 - INFO - __main__ - Epoch  70, Step:  152300, Batch Loss:     4.272868, Lr: 0.000050, Tokens per sec:   2828
2023-03-09 20:22:49,369 - INFO - __main__ - Epoch  70, Step:  152400, Batch Loss:     3.144866, Lr: 0.000050, Tokens per sec:   2731
2023-03-09 20:23:09,401 - INFO - __main__ - Epoch  70, Step:  152500, Batch Loss:     2.629574, Lr: 0.000050, Tokens per sec:   2709
2023-03-09 20:23:15,498 - INFO - __main__ - Epoch  70: total training loss 7171.93
2023-03-09 20:23:15,499 - INFO - __main__ - Epoch 71
2023-03-09 20:23:29,884 - INFO - __main__ - Epoch  71, Step:  152600, Batch Loss:     2.131821, Lr: 0.000049, Tokens per sec:   2560
2023-03-09 20:23:49,938 - INFO - __main__ - Epoch  71, Step:  152700, Batch Loss:     3.948502, Lr: 0.000049, Tokens per sec:   2709
2023-03-09 20:24:10,002 - INFO - __main__ - Epoch  71, Step:  152800, Batch Loss:     3.055922, Lr: 0.000049, Tokens per sec:   2644
2023-03-09 20:24:30,080 - INFO - __main__ - Epoch  71, Step:  152900, Batch Loss:     2.603461, Lr: 0.000049, Tokens per sec:   2676
2023-03-09 20:24:50,144 - INFO - __main__ - Epoch  71, Step:  153000, Batch Loss:     3.266624, Lr: 0.000049, Tokens per sec:   2658
2023-03-09 20:25:10,196 - INFO - __main__ - Epoch  71, Step:  153100, Batch Loss:     2.890786, Lr: 0.000049, Tokens per sec:   2673
2023-03-09 20:25:30,230 - INFO - __main__ - Epoch  71, Step:  153200, Batch Loss:     2.189504, Lr: 0.000049, Tokens per sec:   2682
2023-03-09 20:25:50,065 - INFO - __main__ - Epoch  71, Step:  153300, Batch Loss:     2.809251, Lr: 0.000049, Tokens per sec:   2701
2023-03-09 20:26:10,108 - INFO - __main__ - Epoch  71, Step:  153400, Batch Loss:     3.365526, Lr: 0.000049, Tokens per sec:   2707
2023-03-09 20:26:30,253 - INFO - __main__ - Epoch  71, Step:  153500, Batch Loss:     3.207723, Lr: 0.000049, Tokens per sec:   2701
2023-03-09 20:26:50,331 - INFO - __main__ - Epoch  71, Step:  153600, Batch Loss:     3.820187, Lr: 0.000049, Tokens per sec:   2707
2023-03-09 20:27:09,817 - INFO - __main__ - Epoch  71, Step:  153700, Batch Loss:     4.715740, Lr: 0.000049, Tokens per sec:   2773
2023-03-09 20:27:29,826 - INFO - __main__ - Epoch  71, Step:  153800, Batch Loss:     2.149993, Lr: 0.000049, Tokens per sec:   2675
2023-03-09 20:27:49,913 - INFO - __main__ - Epoch  71, Step:  153900, Batch Loss:     2.838518, Lr: 0.000049, Tokens per sec:   2668
2023-03-09 20:28:10,017 - INFO - __main__ - Epoch  71, Step:  154000, Batch Loss:     2.785179, Lr: 0.000049, Tokens per sec:   2655
2023-03-09 20:28:30,055 - INFO - __main__ - Epoch  71, Step:  154100, Batch Loss:     4.850969, Lr: 0.000049, Tokens per sec:   2683
2023-03-09 20:28:50,126 - INFO - __main__ - Epoch  71, Step:  154200, Batch Loss:     3.065510, Lr: 0.000049, Tokens per sec:   2655
2023-03-09 20:29:10,224 - INFO - __main__ - Epoch  71, Step:  154300, Batch Loss:     4.391462, Lr: 0.000049, Tokens per sec:   2695
2023-03-09 20:29:30,264 - INFO - __main__ - Epoch  71, Step:  154400, Batch Loss:     3.401838, Lr: 0.000049, Tokens per sec:   2704
2023-03-09 20:29:50,291 - INFO - __main__ - Epoch  71, Step:  154500, Batch Loss:     2.660809, Lr: 0.000049, Tokens per sec:   2715
2023-03-09 20:30:10,337 - INFO - __main__ - Epoch  71, Step:  154600, Batch Loss:     3.162179, Lr: 0.000049, Tokens per sec:   2708
2023-03-09 20:30:29,739 - INFO - __main__ - Epoch  71, Step:  154700, Batch Loss:     2.946276, Lr: 0.000049, Tokens per sec:   2797
2023-03-09 20:30:31,615 - INFO - __main__ - Epoch  71: total training loss 7002.94
2023-03-09 20:30:31,616 - INFO - __main__ - Epoch 72
2023-03-09 20:30:50,094 - INFO - __main__ - Epoch  72, Step:  154800, Batch Loss:     2.366814, Lr: 0.000049, Tokens per sec:   2649
2023-03-09 20:31:10,504 - INFO - __main__ - Epoch  72, Step:  154900, Batch Loss:     3.282308, Lr: 0.000049, Tokens per sec:   2647
2023-03-09 20:31:30,555 - INFO - __main__ - Epoch  72, Step:  155000, Batch Loss:     2.534728, Lr: 0.000049, Tokens per sec:   2668
2023-03-09 20:31:50,068 - INFO - __main__ - Epoch  72, Step:  155100, Batch Loss:     2.814874, Lr: 0.000049, Tokens per sec:   2750
2023-03-09 20:32:09,408 - INFO - __main__ - Epoch  72, Step:  155200, Batch Loss:     2.923956, Lr: 0.000049, Tokens per sec:   2808
2023-03-09 20:32:29,280 - INFO - __main__ - Epoch  72, Step:  155300, Batch Loss:     2.797931, Lr: 0.000049, Tokens per sec:   2750
2023-03-09 20:32:49,351 - INFO - __main__ - Epoch  72, Step:  155400, Batch Loss:     2.894898, Lr: 0.000049, Tokens per sec:   2683
2023-03-09 20:33:09,423 - INFO - __main__ - Epoch  72, Step:  155500, Batch Loss:     2.533888, Lr: 0.000049, Tokens per sec:   2681
2023-03-09 20:33:29,479 - INFO - __main__ - Epoch  72, Step:  155600, Batch Loss:     2.106007, Lr: 0.000049, Tokens per sec:   2729
2023-03-09 20:33:49,466 - INFO - __main__ - Epoch  72, Step:  155700, Batch Loss:     2.377688, Lr: 0.000049, Tokens per sec:   2693
2023-03-09 20:34:09,424 - INFO - __main__ - Epoch  72, Step:  155800, Batch Loss:     4.274306, Lr: 0.000049, Tokens per sec:   2701
2023-03-09 20:34:29,120 - INFO - __main__ - Epoch  72, Step:  155900, Batch Loss:     1.886541, Lr: 0.000049, Tokens per sec:   2744
2023-03-09 20:34:48,672 - INFO - __main__ - Epoch  72, Step:  156000, Batch Loss:     3.051167, Lr: 0.000049, Tokens per sec:   2733
2023-03-09 20:35:08,601 - INFO - __main__ - Epoch  72, Step:  156100, Batch Loss:     2.316453, Lr: 0.000049, Tokens per sec:   2667
2023-03-09 20:35:28,780 - INFO - __main__ - Epoch  72, Step:  156200, Batch Loss:     3.697874, Lr: 0.000049, Tokens per sec:   2722
2023-03-09 20:35:48,927 - INFO - __main__ - Epoch  72, Step:  156300, Batch Loss:     3.289008, Lr: 0.000049, Tokens per sec:   2667
2023-03-09 20:36:09,185 - INFO - __main__ - Epoch  72, Step:  156400, Batch Loss:     4.128620, Lr: 0.000049, Tokens per sec:   2657
2023-03-09 20:36:28,917 - INFO - __main__ - Epoch  72, Step:  156500, Batch Loss:     3.799552, Lr: 0.000049, Tokens per sec:   2715
2023-03-09 20:36:48,914 - INFO - __main__ - Epoch  72, Step:  156600, Batch Loss:     4.579673, Lr: 0.000049, Tokens per sec:   2664
2023-03-09 20:37:07,846 - INFO - __main__ - Epoch  72, Step:  156700, Batch Loss:     3.304669, Lr: 0.000049, Tokens per sec:   2813
2023-03-09 20:37:27,426 - INFO - __main__ - Epoch  72, Step:  156800, Batch Loss:     3.841233, Lr: 0.000049, Tokens per sec:   2762
2023-03-09 20:37:45,129 - INFO - __main__ - Epoch  72: total training loss 6860.68
2023-03-09 20:37:45,130 - INFO - __main__ - Epoch 73
2023-03-09 20:37:47,799 - INFO - __main__ - Epoch  73, Step:  156900, Batch Loss:     2.203017, Lr: 0.000048, Tokens per sec:   2423
2023-03-09 20:38:07,583 - INFO - __main__ - Epoch  73, Step:  157000, Batch Loss:     2.342789, Lr: 0.000048, Tokens per sec:   2727
2023-03-09 20:38:27,080 - INFO - __main__ - Epoch  73, Step:  157100, Batch Loss:     2.947928, Lr: 0.000048, Tokens per sec:   2743
2023-03-09 20:38:47,241 - INFO - __main__ - Epoch  73, Step:  157200, Batch Loss:     3.267799, Lr: 0.000048, Tokens per sec:   2721
2023-03-09 20:39:06,955 - INFO - __main__ - Epoch  73, Step:  157300, Batch Loss:     2.810904, Lr: 0.000048, Tokens per sec:   2735
2023-03-09 20:39:26,868 - INFO - __main__ - Epoch  73, Step:  157400, Batch Loss:     2.036572, Lr: 0.000048, Tokens per sec:   2709
2023-03-09 20:39:47,165 - INFO - __main__ - Epoch  73, Step:  157500, Batch Loss:     3.115265, Lr: 0.000048, Tokens per sec:   2635
2023-03-09 20:40:07,317 - INFO - __main__ - Epoch  73, Step:  157600, Batch Loss:     3.613649, Lr: 0.000048, Tokens per sec:   2669
2023-03-09 20:40:27,177 - INFO - __main__ - Epoch  73, Step:  157700, Batch Loss:     2.564790, Lr: 0.000048, Tokens per sec:   2702
2023-03-09 20:40:47,397 - INFO - __main__ - Epoch  73, Step:  157800, Batch Loss:     3.282999, Lr: 0.000048, Tokens per sec:   2635
2023-03-09 20:41:06,809 - INFO - __main__ - Epoch  73, Step:  157900, Batch Loss:     2.533687, Lr: 0.000048, Tokens per sec:   2774
2023-03-09 20:41:26,044 - INFO - __main__ - Epoch  73, Step:  158000, Batch Loss:     2.144565, Lr: 0.000048, Tokens per sec:   2816
2023-03-09 20:41:46,317 - INFO - __main__ - Epoch  73, Step:  158100, Batch Loss:     3.579057, Lr: 0.000048, Tokens per sec:   2707
2023-03-09 20:42:06,569 - INFO - __main__ - Epoch  73, Step:  158200, Batch Loss:     3.833034, Lr: 0.000048, Tokens per sec:   2667
2023-03-09 20:42:26,914 - INFO - __main__ - Epoch  73, Step:  158300, Batch Loss:     5.181771, Lr: 0.000048, Tokens per sec:   2640
2023-03-09 20:42:47,143 - INFO - __main__ - Epoch  73, Step:  158400, Batch Loss:     2.900886, Lr: 0.000048, Tokens per sec:   2675
2023-03-09 20:43:07,460 - INFO - __main__ - Epoch  73, Step:  158500, Batch Loss:     2.445781, Lr: 0.000048, Tokens per sec:   2640
2023-03-09 20:43:27,560 - INFO - __main__ - Epoch  73, Step:  158600, Batch Loss:     4.256924, Lr: 0.000048, Tokens per sec:   2661
2023-03-09 20:43:47,624 - INFO - __main__ - Epoch  73, Step:  158700, Batch Loss:     1.907557, Lr: 0.000048, Tokens per sec:   2651
2023-03-09 20:44:07,698 - INFO - __main__ - Epoch  73, Step:  158800, Batch Loss:     4.228012, Lr: 0.000048, Tokens per sec:   2708
2023-03-09 20:44:27,974 - INFO - __main__ - Epoch  73, Step:  158900, Batch Loss:     2.922086, Lr: 0.000048, Tokens per sec:   2624
2023-03-09 20:44:48,270 - INFO - __main__ - Epoch  73, Step:  159000, Batch Loss:     2.232294, Lr: 0.000048, Tokens per sec:   2649
2023-03-09 20:45:01,870 - INFO - __main__ - Epoch  73: total training loss 6710.45
2023-03-09 20:45:01,871 - INFO - __main__ - Epoch 74
2023-03-09 20:45:08,959 - INFO - __main__ - Epoch  74, Step:  159100, Batch Loss:     3.029945, Lr: 0.000048, Tokens per sec:   2603
2023-03-09 20:45:29,102 - INFO - __main__ - Epoch  74, Step:  159200, Batch Loss:     2.375410, Lr: 0.000048, Tokens per sec:   2691
2023-03-09 20:45:49,162 - INFO - __main__ - Epoch  74, Step:  159300, Batch Loss:     2.376437, Lr: 0.000048, Tokens per sec:   2686
2023-03-09 20:46:09,480 - INFO - __main__ - Epoch  74, Step:  159400, Batch Loss:     2.467427, Lr: 0.000048, Tokens per sec:   2688
2023-03-09 20:46:29,468 - INFO - __main__ - Epoch  74, Step:  159500, Batch Loss:     3.261701, Lr: 0.000048, Tokens per sec:   2711
2023-03-09 20:46:49,678 - INFO - __main__ - Epoch  74, Step:  159600, Batch Loss:     2.864219, Lr: 0.000048, Tokens per sec:   2682
2023-03-09 20:47:09,167 - INFO - __main__ - Epoch  74, Step:  159700, Batch Loss:     4.228233, Lr: 0.000048, Tokens per sec:   2739
2023-03-09 20:47:28,670 - INFO - __main__ - Epoch  74, Step:  159800, Batch Loss:     2.880370, Lr: 0.000048, Tokens per sec:   2789
2023-03-09 20:47:48,136 - INFO - __main__ - Epoch  74, Step:  159900, Batch Loss:     2.491727, Lr: 0.000048, Tokens per sec:   2784
2023-03-09 20:48:07,874 - INFO - __main__ - Epoch  74, Step:  160000, Batch Loss:     2.744187, Lr: 0.000048, Tokens per sec:   2725
2023-03-09 20:48:27,629 - INFO - __main__ - Epoch  74, Step:  160100, Batch Loss:     4.492720, Lr: 0.000048, Tokens per sec:   2726
2023-03-09 20:48:47,757 - INFO - __main__ - Epoch  74, Step:  160200, Batch Loss:     2.911038, Lr: 0.000048, Tokens per sec:   2652
2023-03-09 20:49:07,746 - INFO - __main__ - Epoch  74, Step:  160300, Batch Loss:     2.212479, Lr: 0.000048, Tokens per sec:   2709
2023-03-09 20:49:28,020 - INFO - __main__ - Epoch  74, Step:  160400, Batch Loss:     3.268086, Lr: 0.000048, Tokens per sec:   2642
2023-03-09 20:49:47,326 - INFO - __main__ - Epoch  74, Step:  160500, Batch Loss:     2.267396, Lr: 0.000048, Tokens per sec:   2725
2023-03-09 20:50:06,799 - INFO - __main__ - Epoch  74, Step:  160600, Batch Loss:     3.139392, Lr: 0.000048, Tokens per sec:   2813
2023-03-09 20:50:26,818 - INFO - __main__ - Epoch  74, Step:  160700, Batch Loss:     2.762842, Lr: 0.000048, Tokens per sec:   2653
2023-03-09 20:50:46,477 - INFO - __main__ - Epoch  74, Step:  160800, Batch Loss:     2.831177, Lr: 0.000048, Tokens per sec:   2732
2023-03-09 20:51:06,697 - INFO - __main__ - Epoch  74, Step:  160900, Batch Loss:     3.720765, Lr: 0.000048, Tokens per sec:   2669
2023-03-09 20:51:26,964 - INFO - __main__ - Epoch  74, Step:  161000, Batch Loss:     3.271636, Lr: 0.000048, Tokens per sec:   2603
2023-03-09 20:51:47,259 - INFO - __main__ - Epoch  74, Step:  161100, Batch Loss:     3.530386, Lr: 0.000048, Tokens per sec:   2635
2023-03-09 20:52:07,330 - INFO - __main__ - Epoch  74, Step:  161200, Batch Loss:     2.075644, Lr: 0.000048, Tokens per sec:   2664
2023-03-09 20:52:16,800 - INFO - __main__ - Epoch  74: total training loss 6601.51
2023-03-09 20:52:16,801 - INFO - __main__ - Epoch 75
2023-03-09 20:52:28,268 - INFO - __main__ - Epoch  75, Step:  161300, Batch Loss:     3.970355, Lr: 0.000048, Tokens per sec:   2504
2023-03-09 20:52:48,570 - INFO - __main__ - Epoch  75, Step:  161400, Batch Loss:     1.525125, Lr: 0.000048, Tokens per sec:   2651
2023-03-09 20:53:08,498 - INFO - __main__ - Epoch  75, Step:  161500, Batch Loss:     2.308498, Lr: 0.000048, Tokens per sec:   2668
2023-03-09 20:53:28,852 - INFO - __main__ - Epoch  75, Step:  161600, Batch Loss:     2.924185, Lr: 0.000048, Tokens per sec:   2633
2023-03-09 20:53:48,939 - INFO - __main__ - Epoch  75, Step:  161700, Batch Loss:     3.181912, Lr: 0.000048, Tokens per sec:   2723
2023-03-09 20:54:08,809 - INFO - __main__ - Epoch  75, Step:  161800, Batch Loss:     3.575062, Lr: 0.000048, Tokens per sec:   2706
2023-03-09 20:54:28,755 - INFO - __main__ - Epoch  75, Step:  161900, Batch Loss:     2.215497, Lr: 0.000048, Tokens per sec:   2679
2023-03-09 20:54:48,508 - INFO - __main__ - Epoch  75, Step:  162000, Batch Loss:     3.561246, Lr: 0.000048, Tokens per sec:   2713
2023-03-09 20:55:08,831 - INFO - __main__ - Epoch  75, Step:  162100, Batch Loss:     4.089421, Lr: 0.000048, Tokens per sec:   2680
2023-03-09 20:55:29,083 - INFO - __main__ - Epoch  75, Step:  162200, Batch Loss:     3.919251, Lr: 0.000048, Tokens per sec:   2672
2023-03-09 20:55:49,360 - INFO - __main__ - Epoch  75, Step:  162300, Batch Loss:     2.876204, Lr: 0.000048, Tokens per sec:   2693
2023-03-09 20:56:09,490 - INFO - __main__ - Epoch  75, Step:  162400, Batch Loss:     3.715416, Lr: 0.000048, Tokens per sec:   2683
2023-03-09 20:56:29,675 - INFO - __main__ - Epoch  75, Step:  162500, Batch Loss:     3.298709, Lr: 0.000048, Tokens per sec:   2678
2023-03-09 20:56:49,625 - INFO - __main__ - Epoch  75, Step:  162600, Batch Loss:     4.049678, Lr: 0.000048, Tokens per sec:   2743
2023-03-09 20:57:09,834 - INFO - __main__ - Epoch  75, Step:  162700, Batch Loss:     2.690118, Lr: 0.000048, Tokens per sec:   2672
2023-03-09 20:57:29,321 - INFO - __main__ - Epoch  75, Step:  162800, Batch Loss:     3.328001, Lr: 0.000048, Tokens per sec:   2797
2023-03-09 20:57:49,666 - INFO - __main__ - Epoch  75, Step:  162900, Batch Loss:     3.833866, Lr: 0.000048, Tokens per sec:   2620
2023-03-09 20:58:09,930 - INFO - __main__ - Epoch  75, Step:  163000, Batch Loss:     2.397753, Lr: 0.000048, Tokens per sec:   2649
2023-03-09 20:58:29,502 - INFO - __main__ - Epoch  75, Step:  163100, Batch Loss:     3.506663, Lr: 0.000048, Tokens per sec:   2751
2023-03-09 20:58:49,511 - INFO - __main__ - Epoch  75, Step:  163200, Batch Loss:     3.838158, Lr: 0.000048, Tokens per sec:   2645
2023-03-09 20:59:09,724 - INFO - __main__ - Epoch  75, Step:  163300, Batch Loss:     3.835771, Lr: 0.000048, Tokens per sec:   2645
2023-03-09 20:59:29,407 - INFO - __main__ - Epoch  75, Step:  163400, Batch Loss:     2.281980, Lr: 0.000048, Tokens per sec:   2708
2023-03-09 20:59:34,544 - INFO - __main__ - Epoch  75: total training loss 6471.40
2023-03-09 20:59:34,545 - INFO - __main__ - Epoch 76
2023-03-09 20:59:50,222 - INFO - __main__ - Epoch  76, Step:  163500, Batch Loss:     3.188233, Lr: 0.000047, Tokens per sec:   2605
2023-03-09 21:00:10,701 - INFO - __main__ - Epoch  76, Step:  163600, Batch Loss:     2.914962, Lr: 0.000047, Tokens per sec:   2642
2023-03-09 21:00:31,004 - INFO - __main__ - Epoch  76, Step:  163700, Batch Loss:     3.131391, Lr: 0.000047, Tokens per sec:   2640
2023-03-09 21:00:51,253 - INFO - __main__ - Epoch  76, Step:  163800, Batch Loss:     2.911164, Lr: 0.000047, Tokens per sec:   2642
2023-03-09 21:01:11,636 - INFO - __main__ - Epoch  76, Step:  163900, Batch Loss:     2.535673, Lr: 0.000047, Tokens per sec:   2637
2023-03-09 21:01:32,055 - INFO - __main__ - Epoch  76, Step:  164000, Batch Loss:     3.657825, Lr: 0.000047, Tokens per sec:   2592
2023-03-09 21:01:52,239 - INFO - __main__ - Epoch  76, Step:  164100, Batch Loss:     2.467366, Lr: 0.000047, Tokens per sec:   2581
2023-03-09 21:02:12,504 - INFO - __main__ - Epoch  76, Step:  164200, Batch Loss:     2.347411, Lr: 0.000047, Tokens per sec:   2623
2023-03-09 21:02:32,691 - INFO - __main__ - Epoch  76, Step:  164300, Batch Loss:     3.671528, Lr: 0.000047, Tokens per sec:   2700
2023-03-09 21:02:52,972 - INFO - __main__ - Epoch  76, Step:  164400, Batch Loss:     1.794114, Lr: 0.000047, Tokens per sec:   2660
2023-03-09 21:03:13,006 - INFO - __main__ - Epoch  76, Step:  164500, Batch Loss:     2.823714, Lr: 0.000047, Tokens per sec:   2680
2023-03-09 21:03:33,477 - INFO - __main__ - Epoch  76, Step:  164600, Batch Loss:     2.454470, Lr: 0.000047, Tokens per sec:   2678
2023-03-09 21:03:53,498 - INFO - __main__ - Epoch  76, Step:  164700, Batch Loss:     3.149317, Lr: 0.000047, Tokens per sec:   2675
2023-03-09 21:04:13,718 - INFO - __main__ - Epoch  76, Step:  164800, Batch Loss:     3.131967, Lr: 0.000047, Tokens per sec:   2640
2023-03-09 21:04:33,811 - INFO - __main__ - Epoch  76, Step:  164900, Batch Loss:     4.020396, Lr: 0.000047, Tokens per sec:   2695
2023-03-09 21:04:54,112 - INFO - __main__ - Epoch  76, Step:  165000, Batch Loss:     2.580516, Lr: 0.000047, Tokens per sec:   2638
2023-03-09 21:05:14,204 - INFO - __main__ - Epoch  76, Step:  165100, Batch Loss:     2.330738, Lr: 0.000047, Tokens per sec:   2729
2023-03-09 21:05:34,441 - INFO - __main__ - Epoch  76, Step:  165200, Batch Loss:     2.178913, Lr: 0.000047, Tokens per sec:   2688
2023-03-09 21:05:54,832 - INFO - __main__ - Epoch  76, Step:  165300, Batch Loss:     2.385360, Lr: 0.000047, Tokens per sec:   2698
2023-03-09 21:06:15,011 - INFO - __main__ - Epoch  76, Step:  165400, Batch Loss:     3.391760, Lr: 0.000047, Tokens per sec:   2657
2023-03-09 21:06:35,183 - INFO - __main__ - Epoch  76, Step:  165500, Batch Loss:     3.659702, Lr: 0.000047, Tokens per sec:   2637
2023-03-09 21:06:55,282 - INFO - __main__ - Epoch  76, Step:  165600, Batch Loss:     2.955572, Lr: 0.000047, Tokens per sec:   2699
2023-03-09 21:06:56,169 - INFO - __main__ - Epoch  76: total training loss 6327.95
2023-03-09 21:06:56,170 - INFO - __main__ - Epoch 77
2023-03-09 21:07:16,068 - INFO - __main__ - Epoch  77, Step:  165700, Batch Loss:     3.577352, Lr: 0.000047, Tokens per sec:   2633
2023-03-09 21:07:36,616 - INFO - __main__ - Epoch  77, Step:  165800, Batch Loss:     2.481175, Lr: 0.000047, Tokens per sec:   2666
2023-03-09 21:07:56,723 - INFO - __main__ - Epoch  77, Step:  165900, Batch Loss:     3.562650, Lr: 0.000047, Tokens per sec:   2651
2023-03-09 21:08:16,967 - INFO - __main__ - Epoch  77, Step:  166000, Batch Loss:     2.150447, Lr: 0.000047, Tokens per sec:   2649
2023-03-09 21:08:38,193 - INFO - __main__ - Epoch  77, Step:  166100, Batch Loss:     2.665507, Lr: 0.000047, Tokens per sec:   2520
2023-03-09 21:09:00,758 - INFO - __main__ - Epoch  77, Step:  166200, Batch Loss:     3.036103, Lr: 0.000047, Tokens per sec:   2381
2023-03-09 21:09:21,795 - INFO - __main__ - Epoch  77, Step:  166300, Batch Loss:     3.013361, Lr: 0.000047, Tokens per sec:   2587
2023-03-09 21:09:42,105 - INFO - __main__ - Epoch  77, Step:  166400, Batch Loss:     2.726119, Lr: 0.000047, Tokens per sec:   2691
2023-03-09 21:10:03,014 - INFO - __main__ - Epoch  77, Step:  166500, Batch Loss:     3.383219, Lr: 0.000047, Tokens per sec:   2580
2023-03-09 21:10:23,148 - INFO - __main__ - Epoch  77, Step:  166600, Batch Loss:     3.480312, Lr: 0.000047, Tokens per sec:   2643
2023-03-09 21:10:43,149 - INFO - __main__ - Epoch  77, Step:  166700, Batch Loss:     3.726755, Lr: 0.000047, Tokens per sec:   2590
2023-03-09 21:11:03,515 - INFO - __main__ - Epoch  77, Step:  166800, Batch Loss:     3.613020, Lr: 0.000047, Tokens per sec:   2619
2023-03-09 21:11:23,887 - INFO - __main__ - Epoch  77, Step:  166900, Batch Loss:     2.586873, Lr: 0.000047, Tokens per sec:   2658
2023-03-09 21:11:43,987 - INFO - __main__ - Epoch  77, Step:  167000, Batch Loss:     1.883345, Lr: 0.000047, Tokens per sec:   2670
2023-03-09 21:12:03,976 - INFO - __main__ - Epoch  77, Step:  167100, Batch Loss:     2.575411, Lr: 0.000047, Tokens per sec:   2703
2023-03-09 21:12:24,360 - INFO - __main__ - Epoch  77, Step:  167200, Batch Loss:     3.814298, Lr: 0.000047, Tokens per sec:   2678
2023-03-09 21:12:44,730 - INFO - __main__ - Epoch  77, Step:  167300, Batch Loss:     3.525260, Lr: 0.000047, Tokens per sec:   2593
2023-03-09 21:13:04,853 - INFO - __main__ - Epoch  77, Step:  167400, Batch Loss:     2.892748, Lr: 0.000047, Tokens per sec:   2673
2023-03-09 21:13:25,111 - INFO - __main__ - Epoch  77, Step:  167500, Batch Loss:     2.271703, Lr: 0.000047, Tokens per sec:   2644
2023-03-09 21:13:45,448 - INFO - __main__ - Epoch  77, Step:  167600, Batch Loss:     3.344206, Lr: 0.000047, Tokens per sec:   2676
2023-03-09 21:14:05,721 - INFO - __main__ - Epoch  77, Step:  167700, Batch Loss:     2.202068, Lr: 0.000047, Tokens per sec:   2690
2023-03-09 21:14:22,665 - INFO - __main__ - Epoch  77: total training loss 6238.74
2023-03-09 21:14:22,666 - INFO - __main__ - Epoch 78
2023-03-09 21:14:26,605 - INFO - __main__ - Epoch  78, Step:  167800, Batch Loss:     3.191675, Lr: 0.000046, Tokens per sec:   2379
2023-03-09 21:14:46,704 - INFO - __main__ - Epoch  78, Step:  167900, Batch Loss:     1.973758, Lr: 0.000046, Tokens per sec:   2690
2023-03-09 21:15:07,097 - INFO - __main__ - Epoch  78, Step:  168000, Batch Loss:     3.270188, Lr: 0.000046, Tokens per sec:   2625
2023-03-09 21:15:27,229 - INFO - __main__ - Epoch  78, Step:  168100, Batch Loss:     3.492716, Lr: 0.000046, Tokens per sec:   2675
2023-03-09 21:15:47,329 - INFO - __main__ - Epoch  78, Step:  168200, Batch Loss:     2.695726, Lr: 0.000046, Tokens per sec:   2683
2023-03-09 21:16:07,408 - INFO - __main__ - Epoch  78, Step:  168300, Batch Loss:     3.128605, Lr: 0.000046, Tokens per sec:   2671
2023-03-09 21:16:27,782 - INFO - __main__ - Epoch  78, Step:  168400, Batch Loss:     2.514896, Lr: 0.000046, Tokens per sec:   2667
2023-03-09 21:16:48,273 - INFO - __main__ - Epoch  78, Step:  168500, Batch Loss:     2.951978, Lr: 0.000046, Tokens per sec:   2653
2023-03-09 21:17:08,669 - INFO - __main__ - Epoch  78, Step:  168600, Batch Loss:     3.745075, Lr: 0.000046, Tokens per sec:   2579
2023-03-09 21:17:28,751 - INFO - __main__ - Epoch  78, Step:  168700, Batch Loss:     1.712214, Lr: 0.000046, Tokens per sec:   2728
2023-03-09 21:17:48,948 - INFO - __main__ - Epoch  78, Step:  168800, Batch Loss:     2.164039, Lr: 0.000046, Tokens per sec:   2686
2023-03-09 21:18:09,022 - INFO - __main__ - Epoch  78, Step:  168900, Batch Loss:     3.250002, Lr: 0.000046, Tokens per sec:   2706
2023-03-09 21:18:29,011 - INFO - __main__ - Epoch  78, Step:  169000, Batch Loss:     2.392311, Lr: 0.000046, Tokens per sec:   2723
2023-03-09 21:18:49,221 - INFO - __main__ - Epoch  78, Step:  169100, Batch Loss:     3.231158, Lr: 0.000046, Tokens per sec:   2630
2023-03-09 21:19:08,923 - INFO - __main__ - Epoch  78, Step:  169200, Batch Loss:     2.469482, Lr: 0.000046, Tokens per sec:   2745
2023-03-09 21:19:29,297 - INFO - __main__ - Epoch  78, Step:  169300, Batch Loss:     2.862944, Lr: 0.000046, Tokens per sec:   2569
2023-03-09 21:19:49,487 - INFO - __main__ - Epoch  78, Step:  169400, Batch Loss:     2.458876, Lr: 0.000046, Tokens per sec:   2686
2023-03-09 21:20:09,877 - INFO - __main__ - Epoch  78, Step:  169500, Batch Loss:     2.789200, Lr: 0.000046, Tokens per sec:   2640
2023-03-09 21:20:30,187 - INFO - __main__ - Epoch  78, Step:  169600, Batch Loss:     2.527506, Lr: 0.000046, Tokens per sec:   2636
2023-03-09 21:20:50,264 - INFO - __main__ - Epoch  78, Step:  169700, Batch Loss:     3.110484, Lr: 0.000046, Tokens per sec:   2691
2023-03-09 21:21:10,696 - INFO - __main__ - Epoch  78, Step:  169800, Batch Loss:     3.648291, Lr: 0.000046, Tokens per sec:   2631
2023-03-09 21:21:31,034 - INFO - __main__ - Epoch  78, Step:  169900, Batch Loss:     3.546785, Lr: 0.000046, Tokens per sec:   2647
2023-03-09 21:21:43,659 - INFO - __main__ - Epoch  78: total training loss 6076.33
2023-03-09 21:21:43,660 - INFO - __main__ - Epoch 79
2023-03-09 21:21:51,541 - INFO - __main__ - Epoch  79, Step:  170000, Batch Loss:     1.988146, Lr: 0.000046, Tokens per sec:   2606
2023-03-09 21:22:11,823 - INFO - __main__ - Epoch  79, Step:  170100, Batch Loss:     3.359560, Lr: 0.000046, Tokens per sec:   2666
2023-03-09 21:22:32,310 - INFO - __main__ - Epoch  79, Step:  170200, Batch Loss:     3.025992, Lr: 0.000046, Tokens per sec:   2665
2023-03-09 21:22:52,663 - INFO - __main__ - Epoch  79, Step:  170300, Batch Loss:     2.112670, Lr: 0.000046, Tokens per sec:   2629
2023-03-09 21:23:12,612 - INFO - __main__ - Epoch  79, Step:  170400, Batch Loss:     2.673413, Lr: 0.000046, Tokens per sec:   2666
2023-03-09 21:23:32,973 - INFO - __main__ - Epoch  79, Step:  170500, Batch Loss:     2.266347, Lr: 0.000046, Tokens per sec:   2615
2023-03-09 21:23:53,400 - INFO - __main__ - Epoch  79, Step:  170600, Batch Loss:     2.399020, Lr: 0.000046, Tokens per sec:   2650
2023-03-09 21:24:13,840 - INFO - __main__ - Epoch  79, Step:  170700, Batch Loss:     2.722705, Lr: 0.000046, Tokens per sec:   2647
2023-03-09 21:24:34,010 - INFO - __main__ - Epoch  79, Step:  170800, Batch Loss:     2.986985, Lr: 0.000046, Tokens per sec:   2718
2023-03-09 21:24:54,261 - INFO - __main__ - Epoch  79, Step:  170900, Batch Loss:     3.704500, Lr: 0.000046, Tokens per sec:   2643
2023-03-09 21:25:14,564 - INFO - __main__ - Epoch  79, Step:  171000, Batch Loss:     4.014012, Lr: 0.000046, Tokens per sec:   2647
2023-03-09 21:25:34,974 - INFO - __main__ - Epoch  79, Step:  171100, Batch Loss:     3.229359, Lr: 0.000046, Tokens per sec:   2664
2023-03-09 21:25:55,394 - INFO - __main__ - Epoch  79, Step:  171200, Batch Loss:     3.243748, Lr: 0.000046, Tokens per sec:   2653
2023-03-09 21:26:15,712 - INFO - __main__ - Epoch  79, Step:  171300, Batch Loss:     3.453578, Lr: 0.000046, Tokens per sec:   2630
2023-03-09 21:26:35,839 - INFO - __main__ - Epoch  79, Step:  171400, Batch Loss:     2.947659, Lr: 0.000046, Tokens per sec:   2670
2023-03-09 21:26:55,945 - INFO - __main__ - Epoch  79, Step:  171500, Batch Loss:     2.604954, Lr: 0.000046, Tokens per sec:   2636
2023-03-09 21:27:16,220 - INFO - __main__ - Epoch  79, Step:  171600, Batch Loss:     1.916145, Lr: 0.000046, Tokens per sec:   2651
2023-03-09 21:27:36,282 - INFO - __main__ - Epoch  79, Step:  171700, Batch Loss:     2.213389, Lr: 0.000046, Tokens per sec:   2662
2023-03-09 21:27:56,619 - INFO - __main__ - Epoch  79, Step:  171800, Batch Loss:     3.744808, Lr: 0.000046, Tokens per sec:   2636
2023-03-09 21:28:16,600 - INFO - __main__ - Epoch  79, Step:  171900, Batch Loss:     2.938683, Lr: 0.000046, Tokens per sec:   2694
2023-03-09 21:28:36,855 - INFO - __main__ - Epoch  79, Step:  172000, Batch Loss:     2.967969, Lr: 0.000046, Tokens per sec:   2689
2023-03-09 21:28:57,031 - INFO - __main__ - Epoch  79, Step:  172100, Batch Loss:     2.744915, Lr: 0.000046, Tokens per sec:   2662
2023-03-09 21:29:05,373 - INFO - __main__ - Epoch  79: total training loss 5966.78
2023-03-09 21:29:05,373 - INFO - __main__ - Epoch 80
2023-03-09 21:29:17,623 - INFO - __main__ - Epoch  80, Step:  172200, Batch Loss:     3.936999, Lr: 0.000045, Tokens per sec:   2568
2023-03-09 21:29:37,593 - INFO - __main__ - Epoch  80, Step:  172300, Batch Loss:     1.943893, Lr: 0.000045, Tokens per sec:   2702
2023-03-09 21:29:57,554 - INFO - __main__ - Epoch  80, Step:  172400, Batch Loss:     2.901387, Lr: 0.000045, Tokens per sec:   2662
2023-03-09 21:30:17,902 - INFO - __main__ - Epoch  80, Step:  172500, Batch Loss:     3.104690, Lr: 0.000045, Tokens per sec:   2625
2023-03-09 21:30:38,356 - INFO - __main__ - Epoch  80, Step:  172600, Batch Loss:     1.623540, Lr: 0.000045, Tokens per sec:   2642
2023-03-09 21:30:58,574 - INFO - __main__ - Epoch  80, Step:  172700, Batch Loss:     2.467698, Lr: 0.000045, Tokens per sec:   2673
2023-03-09 21:31:18,784 - INFO - __main__ - Epoch  80, Step:  172800, Batch Loss:     2.308491, Lr: 0.000045, Tokens per sec:   2663
2023-03-09 21:31:38,833 - INFO - __main__ - Epoch  80, Step:  172900, Batch Loss:     2.481309, Lr: 0.000045, Tokens per sec:   2623
2023-03-09 21:31:58,995 - INFO - __main__ - Epoch  80, Step:  173000, Batch Loss:     2.636031, Lr: 0.000045, Tokens per sec:   2712
2023-03-09 21:32:19,121 - INFO - __main__ - Epoch  80, Step:  173100, Batch Loss:     1.539398, Lr: 0.000045, Tokens per sec:   2688
2023-03-09 21:32:39,258 - INFO - __main__ - Epoch  80, Step:  173200, Batch Loss:     2.058882, Lr: 0.000045, Tokens per sec:   2714
2023-03-09 21:32:59,333 - INFO - __main__ - Epoch  80, Step:  173300, Batch Loss:     2.655635, Lr: 0.000045, Tokens per sec:   2661
2023-03-09 21:33:19,249 - INFO - __main__ - Epoch  80, Step:  173400, Batch Loss:     2.809412, Lr: 0.000045, Tokens per sec:   2689
2023-03-09 21:33:39,348 - INFO - __main__ - Epoch  80, Step:  173500, Batch Loss:     3.911928, Lr: 0.000045, Tokens per sec:   2705
2023-03-09 21:33:59,443 - INFO - __main__ - Epoch  80, Step:  173600, Batch Loss:     2.688692, Lr: 0.000045, Tokens per sec:   2662
2023-03-09 21:34:19,549 - INFO - __main__ - Epoch  80, Step:  173700, Batch Loss:     3.053053, Lr: 0.000045, Tokens per sec:   2655
2023-03-09 21:34:39,706 - INFO - __main__ - Epoch  80, Step:  173800, Batch Loss:     2.396048, Lr: 0.000045, Tokens per sec:   2701
2023-03-09 21:34:59,739 - INFO - __main__ - Epoch  80, Step:  173900, Batch Loss:     2.380186, Lr: 0.000045, Tokens per sec:   2665
2023-03-09 21:35:19,714 - INFO - __main__ - Epoch  80, Step:  174000, Batch Loss:     2.635681, Lr: 0.000045, Tokens per sec:   2762
2023-03-09 21:35:39,590 - INFO - __main__ - Epoch  80, Step:  174100, Batch Loss:     2.846970, Lr: 0.000045, Tokens per sec:   2734
2023-03-09 21:35:59,757 - INFO - __main__ - Epoch  80, Step:  174200, Batch Loss:     3.416022, Lr: 0.000045, Tokens per sec:   2685
2023-03-09 21:36:19,705 - INFO - __main__ - Epoch  80, Step:  174300, Batch Loss:     2.565013, Lr: 0.000045, Tokens per sec:   2656
2023-03-09 21:36:23,791 - INFO - __main__ - Epoch  80: total training loss 5917.89
2023-03-09 21:36:23,792 - INFO - __main__ - Epoch 81
2023-03-09 21:36:40,024 - INFO - __main__ - Epoch  81, Step:  174400, Batch Loss:     3.860516, Lr: 0.000045, Tokens per sec:   2658
2023-03-09 21:37:00,299 - INFO - __main__ - Epoch  81, Step:  174500, Batch Loss:     2.297451, Lr: 0.000045, Tokens per sec:   2675
2023-03-09 21:37:20,648 - INFO - __main__ - Epoch  81, Step:  174600, Batch Loss:     1.301140, Lr: 0.000045, Tokens per sec:   2631
2023-03-09 21:37:40,383 - INFO - __main__ - Epoch  81, Step:  174700, Batch Loss:     2.446904, Lr: 0.000045, Tokens per sec:   2733
2023-03-09 21:38:00,530 - INFO - __main__ - Epoch  81, Step:  174800, Batch Loss:     2.545745, Lr: 0.000045, Tokens per sec:   2619
2023-03-09 21:38:20,843 - INFO - __main__ - Epoch  81, Step:  174900, Batch Loss:     3.282661, Lr: 0.000045, Tokens per sec:   2639
2023-03-09 21:38:41,291 - INFO - __main__ - Epoch  81, Step:  175000, Batch Loss:     2.213353, Lr: 0.000045, Tokens per sec:   2638
2023-03-09 21:39:01,482 - INFO - __main__ - Epoch  81, Step:  175100, Batch Loss:     3.060381, Lr: 0.000045, Tokens per sec:   2652
2023-03-09 21:39:21,594 - INFO - __main__ - Epoch  81, Step:  175200, Batch Loss:     3.216607, Lr: 0.000045, Tokens per sec:   2614
2023-03-09 21:39:41,691 - INFO - __main__ - Epoch  81, Step:  175300, Batch Loss:     2.191450, Lr: 0.000045, Tokens per sec:   2702
2023-03-09 21:40:01,684 - INFO - __main__ - Epoch  81, Step:  175400, Batch Loss:     2.565233, Lr: 0.000045, Tokens per sec:   2714
2023-03-09 21:40:21,787 - INFO - __main__ - Epoch  81, Step:  175500, Batch Loss:     2.518158, Lr: 0.000045, Tokens per sec:   2699
2023-03-09 21:40:41,709 - INFO - __main__ - Epoch  81, Step:  175600, Batch Loss:     2.251345, Lr: 0.000045, Tokens per sec:   2767
2023-03-09 21:41:01,803 - INFO - __main__ - Epoch  81, Step:  175700, Batch Loss:     1.925725, Lr: 0.000045, Tokens per sec:   2667
2023-03-09 21:41:22,003 - INFO - __main__ - Epoch  81, Step:  175800, Batch Loss:     2.556821, Lr: 0.000045, Tokens per sec:   2666
2023-03-09 21:41:42,274 - INFO - __main__ - Epoch  81, Step:  175900, Batch Loss:     3.574711, Lr: 0.000045, Tokens per sec:   2673
2023-03-09 21:42:02,278 - INFO - __main__ - Epoch  81, Step:  176000, Batch Loss:     1.849524, Lr: 0.000045, Tokens per sec:   2704
2023-03-09 21:42:22,558 - INFO - __main__ - Epoch  81, Step:  176100, Batch Loss:     2.756341, Lr: 0.000045, Tokens per sec:   2633
2023-03-09 21:42:42,858 - INFO - __main__ - Epoch  81, Step:  176200, Batch Loss:     3.355685, Lr: 0.000045, Tokens per sec:   2655
2023-03-09 21:43:03,121 - INFO - __main__ - Epoch  81, Step:  176300, Batch Loss:     1.965219, Lr: 0.000045, Tokens per sec:   2651
2023-03-09 21:43:23,468 - INFO - __main__ - Epoch  81, Step:  176400, Batch Loss:     2.285283, Lr: 0.000045, Tokens per sec:   2682
2023-03-09 21:43:43,752 - INFO - __main__ - Epoch  81: total training loss 5777.80
2023-03-09 21:43:43,753 - INFO - __main__ - Epoch 82
2023-03-09 21:43:44,349 - INFO - __main__ - Epoch  82, Step:  176500, Batch Loss:     3.230021, Lr: 0.000044, Tokens per sec:   1008
2023-03-09 21:44:04,391 - INFO - __main__ - Epoch  82, Step:  176600, Batch Loss:     1.829294, Lr: 0.000044, Tokens per sec:   2632
2023-03-09 21:44:24,418 - INFO - __main__ - Epoch  82, Step:  176700, Batch Loss:     1.975452, Lr: 0.000044, Tokens per sec:   2685
2023-03-09 21:44:44,820 - INFO - __main__ - Epoch  82, Step:  176800, Batch Loss:     2.859793, Lr: 0.000044, Tokens per sec:   2648
2023-03-09 21:45:04,952 - INFO - __main__ - Epoch  82, Step:  176900, Batch Loss:     2.348315, Lr: 0.000044, Tokens per sec:   2647
2023-03-09 21:45:25,118 - INFO - __main__ - Epoch  82, Step:  177000, Batch Loss:     4.202352, Lr: 0.000044, Tokens per sec:   2630
2023-03-09 21:45:45,263 - INFO - __main__ - Epoch  82, Step:  177100, Batch Loss:     3.484131, Lr: 0.000044, Tokens per sec:   2647
2023-03-09 21:46:05,565 - INFO - __main__ - Epoch  82, Step:  177200, Batch Loss:     2.434317, Lr: 0.000044, Tokens per sec:   2710
2023-03-09 21:46:25,483 - INFO - __main__ - Epoch  82, Step:  177300, Batch Loss:     2.993335, Lr: 0.000044, Tokens per sec:   2702
2023-03-09 21:46:45,579 - INFO - __main__ - Epoch  82, Step:  177400, Batch Loss:     3.686491, Lr: 0.000044, Tokens per sec:   2716
2023-03-09 21:47:05,821 - INFO - __main__ - Epoch  82, Step:  177500, Batch Loss:     2.755718, Lr: 0.000044, Tokens per sec:   2653
2023-03-09 21:47:26,072 - INFO - __main__ - Epoch  82, Step:  177600, Batch Loss:     2.328315, Lr: 0.000044, Tokens per sec:   2683
2023-03-09 21:47:46,509 - INFO - __main__ - Epoch  82, Step:  177700, Batch Loss:     1.868513, Lr: 0.000044, Tokens per sec:   2634
2023-03-09 21:48:06,683 - INFO - __main__ - Epoch  82, Step:  177800, Batch Loss:     2.663158, Lr: 0.000044, Tokens per sec:   2669
2023-03-09 21:48:26,675 - INFO - __main__ - Epoch  82, Step:  177900, Batch Loss:     3.318607, Lr: 0.000044, Tokens per sec:   2672
2023-03-09 21:48:47,075 - INFO - __main__ - Epoch  82, Step:  178000, Batch Loss:     2.802785, Lr: 0.000044, Tokens per sec:   2645
2023-03-09 21:49:07,254 - INFO - __main__ - Epoch  82, Step:  178100, Batch Loss:     2.165966, Lr: 0.000044, Tokens per sec:   2690
2023-03-09 21:49:27,427 - INFO - __main__ - Epoch  82, Step:  178200, Batch Loss:     2.714613, Lr: 0.000044, Tokens per sec:   2697
2023-03-09 21:49:47,505 - INFO - __main__ - Epoch  82, Step:  178300, Batch Loss:     3.200237, Lr: 0.000044, Tokens per sec:   2689
2023-03-09 21:50:07,682 - INFO - __main__ - Epoch  82, Step:  178400, Batch Loss:     2.687931, Lr: 0.000044, Tokens per sec:   2675
2023-03-09 21:50:27,979 - INFO - __main__ - Epoch  82, Step:  178500, Batch Loss:     3.123207, Lr: 0.000044, Tokens per sec:   2597
2023-03-09 21:50:47,897 - INFO - __main__ - Epoch  82, Step:  178600, Batch Loss:     1.966493, Lr: 0.000044, Tokens per sec:   2744
2023-03-09 21:51:03,498 - INFO - __main__ - Epoch  82: total training loss 5701.40
2023-03-09 21:51:03,499 - INFO - __main__ - Epoch 83
2023-03-09 21:51:08,013 - INFO - __main__ - Epoch  83, Step:  178700, Batch Loss:     1.229415, Lr: 0.000044, Tokens per sec:   2719
2023-03-09 21:51:28,062 - INFO - __main__ - Epoch  83, Step:  178800, Batch Loss:     3.291843, Lr: 0.000044, Tokens per sec:   2670
2023-03-09 21:51:48,353 - INFO - __main__ - Epoch  83, Step:  178900, Batch Loss:     2.663947, Lr: 0.000044, Tokens per sec:   2684
2023-03-09 21:52:08,630 - INFO - __main__ - Epoch  83, Step:  179000, Batch Loss:     2.778157, Lr: 0.000044, Tokens per sec:   2632
2023-03-09 21:52:29,048 - INFO - __main__ - Epoch  83, Step:  179100, Batch Loss:     3.203852, Lr: 0.000044, Tokens per sec:   2663
2023-03-09 21:52:49,236 - INFO - __main__ - Epoch  83, Step:  179200, Batch Loss:     2.302995, Lr: 0.000044, Tokens per sec:   2671
2023-03-09 21:53:09,465 - INFO - __main__ - Epoch  83, Step:  179300, Batch Loss:     2.338125, Lr: 0.000044, Tokens per sec:   2642
2023-03-09 21:53:29,628 - INFO - __main__ - Epoch  83, Step:  179400, Batch Loss:     3.505745, Lr: 0.000044, Tokens per sec:   2648
2023-03-09 21:53:49,705 - INFO - __main__ - Epoch  83, Step:  179500, Batch Loss:     2.804850, Lr: 0.000044, Tokens per sec:   2620
2023-03-09 21:54:09,856 - INFO - __main__ - Epoch  83, Step:  179600, Batch Loss:     2.077006, Lr: 0.000044, Tokens per sec:   2656
2023-03-09 21:54:29,860 - INFO - __main__ - Epoch  83, Step:  179700, Batch Loss:     1.294152, Lr: 0.000044, Tokens per sec:   2691
2023-03-09 21:54:50,417 - INFO - __main__ - Epoch  83, Step:  179800, Batch Loss:     3.318497, Lr: 0.000044, Tokens per sec:   2646
2023-03-09 21:55:10,639 - INFO - __main__ - Epoch  83, Step:  179900, Batch Loss:     1.967301, Lr: 0.000044, Tokens per sec:   2661
2023-03-09 21:55:31,018 - INFO - __main__ - Epoch  83, Step:  180000, Batch Loss:     3.427914, Lr: 0.000044, Tokens per sec:   2603
2023-03-09 21:55:51,164 - INFO - __main__ - Epoch  83, Step:  180100, Batch Loss:     2.320736, Lr: 0.000044, Tokens per sec:   2694
2023-03-09 21:56:11,292 - INFO - __main__ - Epoch  83, Step:  180200, Batch Loss:     1.960225, Lr: 0.000044, Tokens per sec:   2677
2023-03-09 21:56:31,477 - INFO - __main__ - Epoch  83, Step:  180300, Batch Loss:     3.321970, Lr: 0.000044, Tokens per sec:   2678
2023-03-09 21:56:51,639 - INFO - __main__ - Epoch  83, Step:  180400, Batch Loss:     3.591498, Lr: 0.000044, Tokens per sec:   2652
2023-03-09 21:57:11,337 - INFO - __main__ - Epoch  83, Step:  180500, Batch Loss:     2.476262, Lr: 0.000044, Tokens per sec:   2788
2023-03-09 21:57:31,462 - INFO - __main__ - Epoch  83, Step:  180600, Batch Loss:     2.801503, Lr: 0.000044, Tokens per sec:   2689
2023-03-09 21:57:51,626 - INFO - __main__ - Epoch  83, Step:  180700, Batch Loss:     3.241980, Lr: 0.000044, Tokens per sec:   2656
2023-03-09 21:58:12,002 - INFO - __main__ - Epoch  83, Step:  180800, Batch Loss:     2.783905, Lr: 0.000044, Tokens per sec:   2645
2023-03-09 21:58:23,551 - INFO - __main__ - Epoch  83: total training loss 5579.76
2023-03-09 21:58:23,552 - INFO - __main__ - Epoch 84
2023-03-09 21:58:32,651 - INFO - __main__ - Epoch  84, Step:  180900, Batch Loss:     1.824396, Lr: 0.000043, Tokens per sec:   2560
2023-03-09 21:58:52,951 - INFO - __main__ - Epoch  84, Step:  181000, Batch Loss:     3.640021, Lr: 0.000043, Tokens per sec:   2650
2023-03-09 21:59:13,045 - INFO - __main__ - Epoch  84, Step:  181100, Batch Loss:     2.349996, Lr: 0.000043, Tokens per sec:   2673
2023-03-09 21:59:33,231 - INFO - __main__ - Epoch  84, Step:  181200, Batch Loss:     2.095345, Lr: 0.000043, Tokens per sec:   2658
2023-03-09 21:59:53,427 - INFO - __main__ - Epoch  84, Step:  181300, Batch Loss:     3.041357, Lr: 0.000043, Tokens per sec:   2688
2023-03-09 22:00:13,215 - INFO - __main__ - Epoch  84, Step:  181400, Batch Loss:     2.541364, Lr: 0.000043, Tokens per sec:   2690
2023-03-09 22:00:33,303 - INFO - __main__ - Epoch  84, Step:  181500, Batch Loss:     3.612592, Lr: 0.000043, Tokens per sec:   2705
2023-03-09 22:00:53,525 - INFO - __main__ - Epoch  84, Step:  181600, Batch Loss:     3.550260, Lr: 0.000043, Tokens per sec:   2652
2023-03-09 22:01:13,703 - INFO - __main__ - Epoch  84, Step:  181700, Batch Loss:     2.997368, Lr: 0.000043, Tokens per sec:   2693
2023-03-09 22:01:33,752 - INFO - __main__ - Epoch  84, Step:  181800, Batch Loss:     2.124238, Lr: 0.000043, Tokens per sec:   2667
2023-03-09 22:01:53,655 - INFO - __main__ - Epoch  84, Step:  181900, Batch Loss:     1.606338, Lr: 0.000043, Tokens per sec:   2699
2023-03-09 22:02:13,559 - INFO - __main__ - Epoch  84, Step:  182000, Batch Loss:     2.646334, Lr: 0.000043, Tokens per sec:   2736
2023-03-09 22:02:32,936 - INFO - __main__ - Epoch  84, Step:  182100, Batch Loss:     3.593931, Lr: 0.000043, Tokens per sec:   2793
2023-03-09 22:02:53,147 - INFO - __main__ - Epoch  84, Step:  182200, Batch Loss:     2.712604, Lr: 0.000043, Tokens per sec:   2665
2023-03-09 22:03:12,849 - INFO - __main__ - Epoch  84, Step:  182300, Batch Loss:     3.551925, Lr: 0.000043, Tokens per sec:   2746
2023-03-09 22:03:33,152 - INFO - __main__ - Epoch  84, Step:  182400, Batch Loss:     3.336275, Lr: 0.000043, Tokens per sec:   2670
2023-03-09 22:03:53,530 - INFO - __main__ - Epoch  84, Step:  182500, Batch Loss:     2.761744, Lr: 0.000043, Tokens per sec:   2650
2023-03-09 22:04:13,799 - INFO - __main__ - Epoch  84, Step:  182600, Batch Loss:     1.182518, Lr: 0.000043, Tokens per sec:   2631
2023-03-09 22:04:33,553 - INFO - __main__ - Epoch  84, Step:  182700, Batch Loss:     3.625479, Lr: 0.000043, Tokens per sec:   2679
2023-03-09 22:04:53,772 - INFO - __main__ - Epoch  84, Step:  182800, Batch Loss:     2.181962, Lr: 0.000043, Tokens per sec:   2602
2023-03-09 22:05:13,962 - INFO - __main__ - Epoch  84, Step:  182900, Batch Loss:     1.647644, Lr: 0.000043, Tokens per sec:   2640
2023-03-09 22:05:34,220 - INFO - __main__ - Epoch  84, Step:  183000, Batch Loss:     3.008569, Lr: 0.000043, Tokens per sec:   2692
2023-03-09 22:05:41,611 - INFO - __main__ - Epoch  84: total training loss 5458.80
2023-03-09 22:05:41,612 - INFO - __main__ - Epoch 85
2023-03-09 22:05:54,993 - INFO - __main__ - Epoch  85, Step:  183100, Batch Loss:     2.218618, Lr: 0.000043, Tokens per sec:   2551
2023-03-09 22:06:15,205 - INFO - __main__ - Epoch  85, Step:  183200, Batch Loss:     2.421895, Lr: 0.000043, Tokens per sec:   2656
2023-03-09 22:06:35,590 - INFO - __main__ - Epoch  85, Step:  183300, Batch Loss:     2.345425, Lr: 0.000043, Tokens per sec:   2662
2023-03-09 22:06:55,785 - INFO - __main__ - Epoch  85, Step:  183400, Batch Loss:     2.119921, Lr: 0.000043, Tokens per sec:   2642
2023-03-09 22:07:16,114 - INFO - __main__ - Epoch  85, Step:  183500, Batch Loss:     2.382164, Lr: 0.000043, Tokens per sec:   2657
2023-03-09 22:07:36,488 - INFO - __main__ - Epoch  85, Step:  183600, Batch Loss:     3.382391, Lr: 0.000043, Tokens per sec:   2669
2023-03-09 22:07:56,736 - INFO - __main__ - Epoch  85, Step:  183700, Batch Loss:     3.038115, Lr: 0.000043, Tokens per sec:   2659
2023-03-09 22:08:17,046 - INFO - __main__ - Epoch  85, Step:  183800, Batch Loss:     3.149125, Lr: 0.000043, Tokens per sec:   2645
2023-03-09 22:08:37,314 - INFO - __main__ - Epoch  85, Step:  183900, Batch Loss:     1.892034, Lr: 0.000043, Tokens per sec:   2636
2023-03-09 22:08:57,684 - INFO - __main__ - Epoch  85, Step:  184000, Batch Loss:     3.723082, Lr: 0.000043, Tokens per sec:   2692
2023-03-09 22:09:18,103 - INFO - __main__ - Epoch  85, Step:  184100, Batch Loss:     2.033058, Lr: 0.000043, Tokens per sec:   2678
2023-03-09 22:09:38,557 - INFO - __main__ - Epoch  85, Step:  184200, Batch Loss:     3.025127, Lr: 0.000043, Tokens per sec:   2635
2023-03-09 22:09:58,340 - INFO - __main__ - Epoch  85, Step:  184300, Batch Loss:     3.181017, Lr: 0.000043, Tokens per sec:   2686
2023-03-09 22:10:18,564 - INFO - __main__ - Epoch  85, Step:  184400, Batch Loss:     3.310245, Lr: 0.000043, Tokens per sec:   2625
2023-03-09 22:10:38,975 - INFO - __main__ - Epoch  85, Step:  184500, Batch Loss:     2.505019, Lr: 0.000043, Tokens per sec:   2631
2023-03-09 22:10:59,202 - INFO - __main__ - Epoch  85, Step:  184600, Batch Loss:     1.969622, Lr: 0.000043, Tokens per sec:   2635
2023-03-09 22:11:19,500 - INFO - __main__ - Epoch  85, Step:  184700, Batch Loss:     2.756824, Lr: 0.000043, Tokens per sec:   2622
2023-03-09 22:11:39,720 - INFO - __main__ - Epoch  85, Step:  184800, Batch Loss:     1.664704, Lr: 0.000043, Tokens per sec:   2680
2023-03-09 22:12:00,061 - INFO - __main__ - Epoch  85, Step:  184900, Batch Loss:     3.059397, Lr: 0.000043, Tokens per sec:   2603
2023-03-09 22:12:20,240 - INFO - __main__ - Epoch  85, Step:  185000, Batch Loss:     2.423373, Lr: 0.000043, Tokens per sec:   2712
2023-03-09 22:12:40,487 - INFO - __main__ - Epoch  85, Step:  185100, Batch Loss:     2.232746, Lr: 0.000043, Tokens per sec:   2626
2023-03-09 22:13:00,596 - INFO - __main__ - Epoch  85, Step:  185200, Batch Loss:     2.190448, Lr: 0.000043, Tokens per sec:   2730
2023-03-09 22:13:03,663 - INFO - __main__ - Epoch  85: total training loss 5395.61
2023-03-09 22:13:03,666 - INFO - __main__ - Epoch 86
2023-03-09 22:13:21,320 - INFO - __main__ - Epoch  86, Step:  185300, Batch Loss:     2.653335, Lr: 0.000043, Tokens per sec:   2613
2023-03-09 22:13:41,512 - INFO - __main__ - Epoch  86, Step:  185400, Batch Loss:     1.884283, Lr: 0.000043, Tokens per sec:   2680
2023-03-09 22:14:01,731 - INFO - __main__ - Epoch  86, Step:  185500, Batch Loss:     2.492069, Lr: 0.000043, Tokens per sec:   2636
2023-03-09 22:14:21,956 - INFO - __main__ - Epoch  86, Step:  185600, Batch Loss:     3.211743, Lr: 0.000043, Tokens per sec:   2654
2023-03-09 22:14:41,940 - INFO - __main__ - Epoch  86, Step:  185700, Batch Loss:     2.336972, Lr: 0.000043, Tokens per sec:   2666
2023-03-09 22:15:01,921 - INFO - __main__ - Epoch  86, Step:  185800, Batch Loss:     2.935153, Lr: 0.000043, Tokens per sec:   2713
2023-03-09 22:15:22,288 - INFO - __main__ - Epoch  86, Step:  185900, Batch Loss:     2.590325, Lr: 0.000043, Tokens per sec:   2649
2023-03-09 22:15:42,848 - INFO - __main__ - Epoch  86, Step:  186000, Batch Loss:     2.438294, Lr: 0.000043, Tokens per sec:   2645
2023-03-09 22:16:03,258 - INFO - __main__ - Epoch  86, Step:  186100, Batch Loss:     2.455133, Lr: 0.000043, Tokens per sec:   2601
2023-03-09 22:16:23,439 - INFO - __main__ - Epoch  86, Step:  186200, Batch Loss:     2.372916, Lr: 0.000043, Tokens per sec:   2671
2023-03-09 22:16:43,768 - INFO - __main__ - Epoch  86, Step:  186300, Batch Loss:     2.491158, Lr: 0.000043, Tokens per sec:   2597
2023-03-09 22:17:04,307 - INFO - __main__ - Epoch  86, Step:  186400, Batch Loss:     3.551641, Lr: 0.000043, Tokens per sec:   2649
2023-03-09 22:17:24,820 - INFO - __main__ - Epoch  86, Step:  186500, Batch Loss:     2.585186, Lr: 0.000043, Tokens per sec:   2663
2023-03-09 22:17:44,994 - INFO - __main__ - Epoch  86, Step:  186600, Batch Loss:     3.768396, Lr: 0.000043, Tokens per sec:   2679
2023-03-09 22:18:05,188 - INFO - __main__ - Epoch  86, Step:  186700, Batch Loss:     1.981431, Lr: 0.000043, Tokens per sec:   2699
2023-03-09 22:18:25,292 - INFO - __main__ - Epoch  86, Step:  186800, Batch Loss:     2.452936, Lr: 0.000043, Tokens per sec:   2658
2023-03-09 22:18:45,327 - INFO - __main__ - Epoch  86, Step:  186900, Batch Loss:     2.130226, Lr: 0.000043, Tokens per sec:   2698
2023-03-09 22:19:05,585 - INFO - __main__ - Epoch  86, Step:  187000, Batch Loss:     3.476757, Lr: 0.000043, Tokens per sec:   2666
2023-03-09 22:19:25,845 - INFO - __main__ - Epoch  86, Step:  187100, Batch Loss:     2.871596, Lr: 0.000043, Tokens per sec:   2651
2023-03-09 22:19:46,150 - INFO - __main__ - Epoch  86, Step:  187200, Batch Loss:     3.128808, Lr: 0.000043, Tokens per sec:   2653
2023-03-09 22:20:06,580 - INFO - __main__ - Epoch  86, Step:  187300, Batch Loss:     2.013794, Lr: 0.000043, Tokens per sec:   2624
2023-03-09 22:20:25,655 - INFO - __main__ - Epoch  86: total training loss 5303.36
2023-03-09 22:20:25,656 - INFO - __main__ - Epoch 87
2023-03-09 22:20:27,262 - INFO - __main__ - Epoch  87, Step:  187400, Batch Loss:     1.614567, Lr: 0.000042, Tokens per sec:   1847
2023-03-09 22:20:47,437 - INFO - __main__ - Epoch  87, Step:  187500, Batch Loss:     1.466653, Lr: 0.000042, Tokens per sec:   2663
2023-03-09 22:21:07,595 - INFO - __main__ - Epoch  87, Step:  187600, Batch Loss:     3.222992, Lr: 0.000042, Tokens per sec:   2697
2023-03-09 22:21:27,737 - INFO - __main__ - Epoch  87, Step:  187700, Batch Loss:     2.079655, Lr: 0.000042, Tokens per sec:   2686
2023-03-09 22:21:48,008 - INFO - __main__ - Epoch  87, Step:  187800, Batch Loss:     2.859829, Lr: 0.000042, Tokens per sec:   2691
2023-03-09 22:22:07,988 - INFO - __main__ - Epoch  87, Step:  187900, Batch Loss:     2.586118, Lr: 0.000042, Tokens per sec:   2717
2023-03-09 22:22:28,206 - INFO - __main__ - Epoch  87, Step:  188000, Batch Loss:     4.050590, Lr: 0.000042, Tokens per sec:   2655
2023-03-09 22:22:48,220 - INFO - __main__ - Epoch  87, Step:  188100, Batch Loss:     2.972995, Lr: 0.000042, Tokens per sec:   2716
2023-03-09 22:23:07,923 - INFO - __main__ - Epoch  87, Step:  188200, Batch Loss:     2.276315, Lr: 0.000042, Tokens per sec:   2741
2023-03-09 22:23:28,403 - INFO - __main__ - Epoch  87, Step:  188300, Batch Loss:     2.436169, Lr: 0.000042, Tokens per sec:   2643
2023-03-09 22:23:48,310 - INFO - __main__ - Epoch  87, Step:  188400, Batch Loss:     2.811560, Lr: 0.000042, Tokens per sec:   2711
2023-03-09 22:24:08,516 - INFO - __main__ - Epoch  87, Step:  188500, Batch Loss:     2.940491, Lr: 0.000042, Tokens per sec:   2654
2023-03-09 22:24:28,651 - INFO - __main__ - Epoch  87, Step:  188600, Batch Loss:     1.153295, Lr: 0.000042, Tokens per sec:   2596
2023-03-09 22:24:48,921 - INFO - __main__ - Epoch  87, Step:  188700, Batch Loss:     2.226578, Lr: 0.000042, Tokens per sec:   2653
2023-03-09 22:25:09,125 - INFO - __main__ - Epoch  87, Step:  188800, Batch Loss:     2.853458, Lr: 0.000042, Tokens per sec:   2701
2023-03-09 22:25:28,819 - INFO - __main__ - Epoch  87, Step:  188900, Batch Loss:     2.151713, Lr: 0.000042, Tokens per sec:   2715
2023-03-09 22:25:48,946 - INFO - __main__ - Epoch  87, Step:  189000, Batch Loss:     2.051187, Lr: 0.000042, Tokens per sec:   2703
2023-03-09 22:26:09,149 - INFO - __main__ - Epoch  87, Step:  189100, Batch Loss:     2.393672, Lr: 0.000042, Tokens per sec:   2614
2023-03-09 22:26:29,421 - INFO - __main__ - Epoch  87, Step:  189200, Batch Loss:     2.579839, Lr: 0.000042, Tokens per sec:   2651
2023-03-09 22:26:49,668 - INFO - __main__ - Epoch  87, Step:  189300, Batch Loss:     2.586467, Lr: 0.000042, Tokens per sec:   2658
2023-03-09 22:27:09,739 - INFO - __main__ - Epoch  87, Step:  189400, Batch Loss:     2.621556, Lr: 0.000042, Tokens per sec:   2695
2023-03-09 22:27:29,782 - INFO - __main__ - Epoch  87, Step:  189500, Batch Loss:     2.444283, Lr: 0.000042, Tokens per sec:   2668
2023-03-09 22:27:44,685 - INFO - __main__ - Epoch  87: total training loss 5233.48
2023-03-09 22:27:44,686 - INFO - __main__ - Epoch 88
2023-03-09 22:27:50,560 - INFO - __main__ - Epoch  88, Step:  189600, Batch Loss:     2.449926, Lr: 0.000042, Tokens per sec:   2414
2023-03-09 22:28:10,624 - INFO - __main__ - Epoch  88, Step:  189700, Batch Loss:     2.306424, Lr: 0.000042, Tokens per sec:   2696
2023-03-09 22:28:30,981 - INFO - __main__ - Epoch  88, Step:  189800, Batch Loss:     1.658028, Lr: 0.000042, Tokens per sec:   2668
2023-03-09 22:28:50,978 - INFO - __main__ - Epoch  88, Step:  189900, Batch Loss:     2.755842, Lr: 0.000042, Tokens per sec:   2704
2023-03-09 22:29:11,200 - INFO - __main__ - Epoch  88, Step:  190000, Batch Loss:     3.477677, Lr: 0.000042, Tokens per sec:   2614
2023-03-09 22:29:31,313 - INFO - __main__ - Epoch  88, Step:  190100, Batch Loss:     2.128967, Lr: 0.000042, Tokens per sec:   2689
2023-03-09 22:29:51,382 - INFO - __main__ - Epoch  88, Step:  190200, Batch Loss:     2.518561, Lr: 0.000042, Tokens per sec:   2673
2023-03-09 22:30:11,639 - INFO - __main__ - Epoch  88, Step:  190300, Batch Loss:     2.198247, Lr: 0.000042, Tokens per sec:   2678
2023-03-09 22:30:31,827 - INFO - __main__ - Epoch  88, Step:  190400, Batch Loss:     1.638357, Lr: 0.000042, Tokens per sec:   2696
2023-03-09 22:30:52,058 - INFO - __main__ - Epoch  88, Step:  190500, Batch Loss:     2.253862, Lr: 0.000042, Tokens per sec:   2650
2023-03-09 22:31:12,078 - INFO - __main__ - Epoch  88, Step:  190600, Batch Loss:     2.116980, Lr: 0.000042, Tokens per sec:   2727
2023-03-09 22:31:32,085 - INFO - __main__ - Epoch  88, Step:  190700, Batch Loss:     2.589071, Lr: 0.000042, Tokens per sec:   2687
2023-03-09 22:31:52,120 - INFO - __main__ - Epoch  88, Step:  190800, Batch Loss:     3.182087, Lr: 0.000042, Tokens per sec:   2639
2023-03-09 22:32:12,212 - INFO - __main__ - Epoch  88, Step:  190900, Batch Loss:     2.338364, Lr: 0.000042, Tokens per sec:   2656
2023-03-09 22:32:32,207 - INFO - __main__ - Epoch  88, Step:  191000, Batch Loss:     1.831813, Lr: 0.000042, Tokens per sec:   2695
2023-03-09 22:32:52,255 - INFO - __main__ - Epoch  88, Step:  191100, Batch Loss:     2.796611, Lr: 0.000042, Tokens per sec:   2654
2023-03-09 22:33:12,266 - INFO - __main__ - Epoch  88, Step:  191200, Batch Loss:     2.033166, Lr: 0.000042, Tokens per sec:   2730
2023-03-09 22:33:32,267 - INFO - __main__ - Epoch  88, Step:  191300, Batch Loss:     2.590870, Lr: 0.000042, Tokens per sec:   2693
2023-03-09 22:33:52,361 - INFO - __main__ - Epoch  88, Step:  191400, Batch Loss:     2.201729, Lr: 0.000042, Tokens per sec:   2731
2023-03-09 22:34:12,417 - INFO - __main__ - Epoch  88, Step:  191500, Batch Loss:     1.275362, Lr: 0.000042, Tokens per sec:   2677
2023-03-09 22:34:31,235 - INFO - __main__ - Epoch  88, Step:  191600, Batch Loss:     2.663032, Lr: 0.000042, Tokens per sec:   2878
2023-03-09 22:34:49,514 - INFO - __main__ - Epoch  88, Step:  191700, Batch Loss:     2.459536, Lr: 0.000042, Tokens per sec:   2922
2023-03-09 22:34:59,104 - INFO - __main__ - Epoch  88: total training loss 5139.88
2023-03-09 22:34:59,105 - INFO - __main__ - Epoch 89
2023-03-09 22:35:08,641 - INFO - __main__ - Epoch  89, Step:  191800, Batch Loss:     2.533146, Lr: 0.000041, Tokens per sec:   2772
2023-03-09 22:35:27,525 - INFO - __main__ - Epoch  89, Step:  191900, Batch Loss:     1.236210, Lr: 0.000041, Tokens per sec:   2836
2023-03-09 22:35:46,209 - INFO - __main__ - Epoch  89, Step:  192000, Batch Loss:     2.297480, Lr: 0.000041, Tokens per sec:   2891
2023-03-09 22:36:06,296 - INFO - __main__ - Epoch  89, Step:  192100, Batch Loss:     2.496201, Lr: 0.000041, Tokens per sec:   2722
2023-03-09 22:36:26,339 - INFO - __main__ - Epoch  89, Step:  192200, Batch Loss:     3.397881, Lr: 0.000041, Tokens per sec:   2709
2023-03-09 22:36:46,427 - INFO - __main__ - Epoch  89, Step:  192300, Batch Loss:     1.723683, Lr: 0.000041, Tokens per sec:   2622
2023-03-09 22:37:06,497 - INFO - __main__ - Epoch  89, Step:  192400, Batch Loss:     2.738090, Lr: 0.000041, Tokens per sec:   2722
2023-03-09 22:37:26,547 - INFO - __main__ - Epoch  89, Step:  192500, Batch Loss:     2.162580, Lr: 0.000041, Tokens per sec:   2632
2023-03-09 22:37:46,591 - INFO - __main__ - Epoch  89, Step:  192600, Batch Loss:     3.391053, Lr: 0.000041, Tokens per sec:   2719
2023-03-09 22:38:06,585 - INFO - __main__ - Epoch  89, Step:  192700, Batch Loss:     2.525827, Lr: 0.000041, Tokens per sec:   2667
2023-03-09 22:38:26,717 - INFO - __main__ - Epoch  89, Step:  192800, Batch Loss:     2.057988, Lr: 0.000041, Tokens per sec:   2683
2023-03-09 22:38:46,717 - INFO - __main__ - Epoch  89, Step:  192900, Batch Loss:     2.080544, Lr: 0.000041, Tokens per sec:   2672
2023-03-09 22:39:06,704 - INFO - __main__ - Epoch  89, Step:  193000, Batch Loss:     1.827050, Lr: 0.000041, Tokens per sec:   2665
2023-03-09 22:39:26,758 - INFO - __main__ - Epoch  89, Step:  193100, Batch Loss:     1.823295, Lr: 0.000041, Tokens per sec:   2692
2023-03-09 22:39:46,790 - INFO - __main__ - Epoch  89, Step:  193200, Batch Loss:     1.875525, Lr: 0.000041, Tokens per sec:   2670
2023-03-09 22:40:06,865 - INFO - __main__ - Epoch  89, Step:  193300, Batch Loss:     2.251129, Lr: 0.000041, Tokens per sec:   2718
2023-03-09 22:40:26,902 - INFO - __main__ - Epoch  89, Step:  193400, Batch Loss:     2.056754, Lr: 0.000041, Tokens per sec:   2662
2023-03-09 22:40:46,981 - INFO - __main__ - Epoch  89, Step:  193500, Batch Loss:     2.341354, Lr: 0.000041, Tokens per sec:   2701
2023-03-09 22:41:07,026 - INFO - __main__ - Epoch  89, Step:  193600, Batch Loss:     2.337908, Lr: 0.000041, Tokens per sec:   2643
2023-03-09 22:41:27,097 - INFO - __main__ - Epoch  89, Step:  193700, Batch Loss:     3.906226, Lr: 0.000041, Tokens per sec:   2680
2023-03-09 22:41:47,134 - INFO - __main__ - Epoch  89, Step:  193800, Batch Loss:     2.469625, Lr: 0.000041, Tokens per sec:   2680
2023-03-09 22:42:07,185 - INFO - __main__ - Epoch  89, Step:  193900, Batch Loss:     2.951914, Lr: 0.000041, Tokens per sec:   2753
2023-03-09 22:42:13,447 - INFO - __main__ - Epoch  89: total training loss 5061.51
2023-03-09 22:42:13,448 - INFO - __main__ - Epoch 90
2023-03-09 22:42:27,603 - INFO - __main__ - Epoch  90, Step:  194000, Batch Loss:     2.622931, Lr: 0.000041, Tokens per sec:   2555
2023-03-09 22:42:47,675 - INFO - __main__ - Epoch  90, Step:  194100, Batch Loss:     2.825914, Lr: 0.000041, Tokens per sec:   2688
2023-03-09 22:43:07,705 - INFO - __main__ - Epoch  90, Step:  194200, Batch Loss:     2.818897, Lr: 0.000041, Tokens per sec:   2702
2023-03-09 22:43:27,739 - INFO - __main__ - Epoch  90, Step:  194300, Batch Loss:     2.988425, Lr: 0.000041, Tokens per sec:   2672
2023-03-09 22:43:47,748 - INFO - __main__ - Epoch  90, Step:  194400, Batch Loss:     1.615582, Lr: 0.000041, Tokens per sec:   2687
2023-03-09 22:44:07,749 - INFO - __main__ - Epoch  90, Step:  194500, Batch Loss:     2.439122, Lr: 0.000041, Tokens per sec:   2681
2023-03-09 22:44:27,752 - INFO - __main__ - Epoch  90, Step:  194600, Batch Loss:     1.446514, Lr: 0.000041, Tokens per sec:   2648
2023-03-09 22:44:47,763 - INFO - __main__ - Epoch  90, Step:  194700, Batch Loss:     2.522891, Lr: 0.000041, Tokens per sec:   2668
2023-03-09 22:45:06,735 - INFO - __main__ - Epoch  90, Step:  194800, Batch Loss:     1.348627, Lr: 0.000041, Tokens per sec:   2840
2023-03-09 22:45:26,803 - INFO - __main__ - Epoch  90, Step:  194900, Batch Loss:     1.828248, Lr: 0.000041, Tokens per sec:   2689
2023-03-09 22:45:46,859 - INFO - __main__ - Epoch  90, Step:  195000, Batch Loss:     2.223243, Lr: 0.000041, Tokens per sec:   2690
2023-03-09 22:46:06,950 - INFO - __main__ - Epoch  90, Step:  195100, Batch Loss:     2.366121, Lr: 0.000041, Tokens per sec:   2679
2023-03-09 22:46:26,990 - INFO - __main__ - Epoch  90, Step:  195200, Batch Loss:     2.739540, Lr: 0.000041, Tokens per sec:   2705
2023-03-09 22:46:47,011 - INFO - __main__ - Epoch  90, Step:  195300, Batch Loss:     2.040766, Lr: 0.000041, Tokens per sec:   2709
2023-03-09 22:47:07,073 - INFO - __main__ - Epoch  90, Step:  195400, Batch Loss:     2.636456, Lr: 0.000041, Tokens per sec:   2697
2023-03-09 22:47:27,088 - INFO - __main__ - Epoch  90, Step:  195500, Batch Loss:     2.211463, Lr: 0.000041, Tokens per sec:   2714
2023-03-09 22:47:47,098 - INFO - __main__ - Epoch  90, Step:  195600, Batch Loss:     2.695427, Lr: 0.000041, Tokens per sec:   2712
2023-03-09 22:48:07,128 - INFO - __main__ - Epoch  90, Step:  195700, Batch Loss:     2.007079, Lr: 0.000041, Tokens per sec:   2699
2023-03-09 22:48:27,204 - INFO - __main__ - Epoch  90, Step:  195800, Batch Loss:     2.416751, Lr: 0.000041, Tokens per sec:   2673
2023-03-09 22:48:47,241 - INFO - __main__ - Epoch  90, Step:  195900, Batch Loss:     2.250144, Lr: 0.000041, Tokens per sec:   2736
2023-03-09 22:49:07,232 - INFO - __main__ - Epoch  90, Step:  196000, Batch Loss:     2.482959, Lr: 0.000041, Tokens per sec:   2668
2023-03-09 22:49:27,308 - INFO - __main__ - Epoch  90, Step:  196100, Batch Loss:     1.961803, Lr: 0.000041, Tokens per sec:   2669
2023-03-09 22:49:29,368 - INFO - __main__ - Epoch  90: total training loss 4980.10
2023-03-09 22:49:29,368 - INFO - __main__ - Epoch 91
2023-03-09 22:49:47,801 - INFO - __main__ - Epoch  91, Step:  196200, Batch Loss:     1.759639, Lr: 0.000040, Tokens per sec:   2640
2023-03-09 22:50:07,822 - INFO - __main__ - Epoch  91, Step:  196300, Batch Loss:     2.338328, Lr: 0.000040, Tokens per sec:   2695
2023-03-09 22:50:27,912 - INFO - __main__ - Epoch  91, Step:  196400, Batch Loss:     2.692016, Lr: 0.000040, Tokens per sec:   2692
2023-03-09 22:50:47,981 - INFO - __main__ - Epoch  91, Step:  196500, Batch Loss:     2.594173, Lr: 0.000040, Tokens per sec:   2732
2023-03-09 22:51:08,021 - INFO - __main__ - Epoch  91, Step:  196600, Batch Loss:     1.760376, Lr: 0.000040, Tokens per sec:   2692
2023-03-09 22:51:28,074 - INFO - __main__ - Epoch  91, Step:  196700, Batch Loss:     2.346505, Lr: 0.000040, Tokens per sec:   2710
2023-03-09 22:51:48,184 - INFO - __main__ - Epoch  91, Step:  196800, Batch Loss:     1.616205, Lr: 0.000040, Tokens per sec:   2682
2023-03-09 22:52:07,775 - INFO - __main__ - Epoch  91, Step:  196900, Batch Loss:     1.942264, Lr: 0.000040, Tokens per sec:   2826
2023-03-09 22:52:26,452 - INFO - __main__ - Epoch  91, Step:  197000, Batch Loss:     2.053166, Lr: 0.000040, Tokens per sec:   2862
2023-03-09 22:52:44,886 - INFO - __main__ - Epoch  91, Step:  197100, Batch Loss:     2.526475, Lr: 0.000040, Tokens per sec:   2889
2023-03-09 22:53:03,147 - INFO - __main__ - Epoch  91, Step:  197200, Batch Loss:     1.857832, Lr: 0.000040, Tokens per sec:   2933
2023-03-09 22:53:21,424 - INFO - __main__ - Epoch  91, Step:  197300, Batch Loss:     3.120928, Lr: 0.000040, Tokens per sec:   2951
2023-03-09 22:53:39,675 - INFO - __main__ - Epoch  91, Step:  197400, Batch Loss:     1.510751, Lr: 0.000040, Tokens per sec:   2944
2023-03-09 22:53:58,499 - INFO - __main__ - Epoch  91, Step:  197500, Batch Loss:     2.164244, Lr: 0.000040, Tokens per sec:   2792
2023-03-09 22:54:18,574 - INFO - __main__ - Epoch  91, Step:  197600, Batch Loss:     2.251873, Lr: 0.000040, Tokens per sec:   2711
2023-03-09 22:54:37,620 - INFO - __main__ - Epoch  91, Step:  197700, Batch Loss:     2.903781, Lr: 0.000040, Tokens per sec:   2869
2023-03-09 22:54:55,891 - INFO - __main__ - Epoch  91, Step:  197800, Batch Loss:     2.261407, Lr: 0.000040, Tokens per sec:   2942
2023-03-09 22:55:15,936 - INFO - __main__ - Epoch  91, Step:  197900, Batch Loss:     2.504918, Lr: 0.000040, Tokens per sec:   2681
2023-03-09 22:55:36,005 - INFO - __main__ - Epoch  91, Step:  198000, Batch Loss:     1.366797, Lr: 0.000040, Tokens per sec:   2662
2023-03-09 22:55:56,095 - INFO - __main__ - Epoch  91, Step:  198100, Batch Loss:     2.102751, Lr: 0.000040, Tokens per sec:   2678
2023-03-09 22:56:16,183 - INFO - __main__ - Epoch  91, Step:  198200, Batch Loss:     2.164413, Lr: 0.000040, Tokens per sec:   2643
2023-03-09 22:56:34,147 - INFO - __main__ - Epoch  91: total training loss 4864.58
2023-03-09 22:56:34,148 - INFO - __main__ - Epoch 92
2023-03-09 22:56:36,690 - INFO - __main__ - Epoch  92, Step:  198300, Batch Loss:     2.170544, Lr: 0.000040, Tokens per sec:   2488
2023-03-09 22:56:56,728 - INFO - __main__ - Epoch  92, Step:  198400, Batch Loss:     2.249328, Lr: 0.000040, Tokens per sec:   2670
2023-03-09 22:57:16,765 - INFO - __main__ - Epoch  92, Step:  198500, Batch Loss:     2.270340, Lr: 0.000040, Tokens per sec:   2685
2023-03-09 22:57:35,492 - INFO - __main__ - Epoch  92, Step:  198600, Batch Loss:     2.582493, Lr: 0.000040, Tokens per sec:   2895
2023-03-09 22:57:53,860 - INFO - __main__ - Epoch  92, Step:  198700, Batch Loss:     2.049298, Lr: 0.000040, Tokens per sec:   2979
2023-03-09 22:58:12,422 - INFO - __main__ - Epoch  92, Step:  198800, Batch Loss:     2.602874, Lr: 0.000040, Tokens per sec:   2853
2023-03-09 22:58:32,526 - INFO - __main__ - Epoch  92, Step:  198900, Batch Loss:     1.471906, Lr: 0.000040, Tokens per sec:   2683
2023-03-09 22:58:52,445 - INFO - __main__ - Epoch  92, Step:  199000, Batch Loss:     2.844812, Lr: 0.000040, Tokens per sec:   2680
2023-03-09 22:59:11,858 - INFO - __main__ - Epoch  92, Step:  199100, Batch Loss:     3.427139, Lr: 0.000040, Tokens per sec:   2771
2023-03-09 22:59:31,703 - INFO - __main__ - Epoch  92, Step:  199200, Batch Loss:     2.517726, Lr: 0.000040, Tokens per sec:   2680
2023-03-09 22:59:51,808 - INFO - __main__ - Epoch  92, Step:  199300, Batch Loss:     1.783727, Lr: 0.000040, Tokens per sec:   2706
2023-03-09 23:00:11,822 - INFO - __main__ - Epoch  92, Step:  199400, Batch Loss:     2.219870, Lr: 0.000040, Tokens per sec:   2727
2023-03-09 23:00:31,861 - INFO - __main__ - Epoch  92, Step:  199500, Batch Loss:     2.805599, Lr: 0.000040, Tokens per sec:   2690
2023-03-09 23:00:51,849 - INFO - __main__ - Epoch  92, Step:  199600, Batch Loss:     1.758075, Lr: 0.000040, Tokens per sec:   2672
2023-03-09 23:01:11,870 - INFO - __main__ - Epoch  92, Step:  199700, Batch Loss:     3.077675, Lr: 0.000040, Tokens per sec:   2668
2023-03-09 23:01:30,287 - INFO - __main__ - Epoch  92, Step:  199800, Batch Loss:     2.752139, Lr: 0.000040, Tokens per sec:   2916
2023-03-09 23:01:48,594 - INFO - __main__ - Epoch  92, Step:  199900, Batch Loss:     2.822883, Lr: 0.000040, Tokens per sec:   2997
2023-03-09 23:02:06,960 - INFO - __main__ - Epoch  92, Step:  200000, Batch Loss:     2.370657, Lr: 0.000040, Tokens per sec:   2932
2023-03-09 23:02:25,302 - INFO - __main__ - Epoch  92, Step:  200100, Batch Loss:     1.120585, Lr: 0.000040, Tokens per sec:   2967
2023-03-09 23:02:43,625 - INFO - __main__ - Epoch  92, Step:  200200, Batch Loss:     2.229976, Lr: 0.000040, Tokens per sec:   2893
2023-03-09 23:03:02,011 - INFO - __main__ - Epoch  92, Step:  200300, Batch Loss:     3.472739, Lr: 0.000040, Tokens per sec:   2906
2023-03-09 23:03:20,263 - INFO - __main__ - Epoch  92, Step:  200400, Batch Loss:     2.428025, Lr: 0.000040, Tokens per sec:   2893
2023-03-09 23:03:32,809 - INFO - __main__ - Epoch  92: total training loss 4805.78
2023-03-09 23:03:32,810 - INFO - __main__ - Epoch 93
2023-03-09 23:03:39,559 - INFO - __main__ - Epoch  93, Step:  200500, Batch Loss:     2.666155, Lr: 0.000040, Tokens per sec:   2553
2023-03-09 23:03:59,577 - INFO - __main__ - Epoch  93, Step:  200600, Batch Loss:     2.796344, Lr: 0.000040, Tokens per sec:   2668
2023-03-09 23:04:19,602 - INFO - __main__ - Epoch  93, Step:  200700, Batch Loss:     2.020597, Lr: 0.000040, Tokens per sec:   2670
2023-03-09 23:04:38,407 - INFO - __main__ - Epoch  93, Step:  200800, Batch Loss:     2.207232, Lr: 0.000040, Tokens per sec:   2890
2023-03-09 23:04:56,696 - INFO - __main__ - Epoch  93, Step:  200900, Batch Loss:     2.285740, Lr: 0.000040, Tokens per sec:   2933
2023-03-09 23:05:16,542 - INFO - __main__ - Epoch  93, Step:  201000, Batch Loss:     2.268516, Lr: 0.000040, Tokens per sec:   2748
2023-03-09 23:05:36,319 - INFO - __main__ - Epoch  93, Step:  201100, Batch Loss:     1.298470, Lr: 0.000040, Tokens per sec:   2706
2023-03-09 23:05:56,393 - INFO - __main__ - Epoch  93, Step:  201200, Batch Loss:     2.284281, Lr: 0.000040, Tokens per sec:   2710
2023-03-09 23:06:16,405 - INFO - __main__ - Epoch  93, Step:  201300, Batch Loss:     2.304114, Lr: 0.000040, Tokens per sec:   2697
2023-03-09 23:06:36,464 - INFO - __main__ - Epoch  93, Step:  201400, Batch Loss:     2.920918, Lr: 0.000040, Tokens per sec:   2674
2023-03-09 23:06:56,503 - INFO - __main__ - Epoch  93, Step:  201500, Batch Loss:     1.518672, Lr: 0.000040, Tokens per sec:   2679
2023-03-09 23:07:16,547 - INFO - __main__ - Epoch  93, Step:  201600, Batch Loss:     2.171652, Lr: 0.000040, Tokens per sec:   2674
2023-03-09 23:07:36,532 - INFO - __main__ - Epoch  93, Step:  201700, Batch Loss:     1.886442, Lr: 0.000040, Tokens per sec:   2651
2023-03-09 23:07:56,594 - INFO - __main__ - Epoch  93, Step:  201800, Batch Loss:     1.984588, Lr: 0.000040, Tokens per sec:   2695
2023-03-09 23:08:16,662 - INFO - __main__ - Epoch  93, Step:  201900, Batch Loss:     2.489619, Lr: 0.000040, Tokens per sec:   2689
2023-03-09 23:08:36,702 - INFO - __main__ - Epoch  93, Step:  202000, Batch Loss:     2.282272, Lr: 0.000040, Tokens per sec:   2683
2023-03-09 23:08:56,714 - INFO - __main__ - Epoch  93, Step:  202100, Batch Loss:     1.913748, Lr: 0.000040, Tokens per sec:   2742
2023-03-09 23:09:16,629 - INFO - __main__ - Epoch  93, Step:  202200, Batch Loss:     1.498976, Lr: 0.000040, Tokens per sec:   2638
2023-03-09 23:09:35,378 - INFO - __main__ - Epoch  93, Step:  202300, Batch Loss:     1.989029, Lr: 0.000040, Tokens per sec:   2902
2023-03-09 23:09:55,159 - INFO - __main__ - Epoch  93, Step:  202400, Batch Loss:     2.033599, Lr: 0.000040, Tokens per sec:   2773
2023-03-09 23:10:15,213 - INFO - __main__ - Epoch  93, Step:  202500, Batch Loss:     2.952667, Lr: 0.000040, Tokens per sec:   2645
2023-03-09 23:10:35,241 - INFO - __main__ - Epoch  93, Step:  202600, Batch Loss:     2.113007, Lr: 0.000040, Tokens per sec:   2698
2023-03-09 23:10:44,734 - INFO - __main__ - Epoch  93: total training loss 4743.49
2023-03-09 23:10:44,735 - INFO - __main__ - Epoch 94
2023-03-09 23:10:55,699 - INFO - __main__ - Epoch  94, Step:  202700, Batch Loss:     1.636158, Lr: 0.000039, Tokens per sec:   2651
2023-03-09 23:11:15,816 - INFO - __main__ - Epoch  94, Step:  202800, Batch Loss:     2.424231, Lr: 0.000039, Tokens per sec:   2727
2023-03-09 23:11:34,835 - INFO - __main__ - Epoch  94, Step:  202900, Batch Loss:     2.236786, Lr: 0.000039, Tokens per sec:   2846
2023-03-09 23:11:53,113 - INFO - __main__ - Epoch  94, Step:  203000, Batch Loss:     1.693268, Lr: 0.000039, Tokens per sec:   2943
2023-03-09 23:12:12,669 - INFO - __main__ - Epoch  94, Step:  203100, Batch Loss:     1.651351, Lr: 0.000039, Tokens per sec:   2758
2023-03-09 23:12:30,954 - INFO - __main__ - Epoch  94, Step:  203200, Batch Loss:     2.750503, Lr: 0.000039, Tokens per sec:   3007
2023-03-09 23:12:50,973 - INFO - __main__ - Epoch  94, Step:  203300, Batch Loss:     3.222231, Lr: 0.000039, Tokens per sec:   2717
2023-03-09 23:13:11,453 - INFO - __main__ - Epoch  94, Step:  203400, Batch Loss:     2.162600, Lr: 0.000039, Tokens per sec:   2598
2023-03-09 23:13:31,453 - INFO - __main__ - Epoch  94, Step:  203500, Batch Loss:     2.300019, Lr: 0.000039, Tokens per sec:   2671
2023-03-09 23:13:51,529 - INFO - __main__ - Epoch  94, Step:  203600, Batch Loss:     2.223042, Lr: 0.000039, Tokens per sec:   2719
2023-03-09 23:14:11,590 - INFO - __main__ - Epoch  94, Step:  203700, Batch Loss:     2.554114, Lr: 0.000039, Tokens per sec:   2645
2023-03-09 23:14:31,534 - INFO - __main__ - Epoch  94, Step:  203800, Batch Loss:     2.711205, Lr: 0.000039, Tokens per sec:   2686
2023-03-09 23:14:51,568 - INFO - __main__ - Epoch  94, Step:  203900, Batch Loss:     2.158854, Lr: 0.000039, Tokens per sec:   2676
2023-03-09 23:15:11,594 - INFO - __main__ - Epoch  94, Step:  204000, Batch Loss:     1.997535, Lr: 0.000039, Tokens per sec:   2750
2023-03-09 23:15:31,496 - INFO - __main__ - Epoch  94, Step:  204100, Batch Loss:     1.893677, Lr: 0.000039, Tokens per sec:   2687
2023-03-09 23:15:50,793 - INFO - __main__ - Epoch  94, Step:  204200, Batch Loss:     1.198464, Lr: 0.000039, Tokens per sec:   2819
2023-03-09 23:16:09,152 - INFO - __main__ - Epoch  94, Step:  204300, Batch Loss:     2.169781, Lr: 0.000039, Tokens per sec:   2884
2023-03-09 23:16:28,753 - INFO - __main__ - Epoch  94, Step:  204400, Batch Loss:     2.079639, Lr: 0.000039, Tokens per sec:   2764
2023-03-09 23:16:48,250 - INFO - __main__ - Epoch  94, Step:  204500, Batch Loss:     2.159465, Lr: 0.000039, Tokens per sec:   2721
2023-03-09 23:17:08,242 - INFO - __main__ - Epoch  94, Step:  204600, Batch Loss:     2.194757, Lr: 0.000039, Tokens per sec:   2664
2023-03-09 23:17:28,220 - INFO - __main__ - Epoch  94, Step:  204700, Batch Loss:     2.132701, Lr: 0.000039, Tokens per sec:   2684
2023-03-09 23:17:48,254 - INFO - __main__ - Epoch  94, Step:  204800, Batch Loss:     1.797070, Lr: 0.000039, Tokens per sec:   2622
2023-03-09 23:17:53,499 - INFO - __main__ - Epoch  94: total training loss 4671.23
2023-03-09 23:17:53,500 - INFO - __main__ - Epoch 95
2023-03-09 23:18:08,808 - INFO - __main__ - Epoch  95, Step:  204900, Batch Loss:     2.355136, Lr: 0.000039, Tokens per sec:   2646
2023-03-09 23:18:28,822 - INFO - __main__ - Epoch  95, Step:  205000, Batch Loss:     3.140683, Lr: 0.000039, Tokens per sec:   2694
2023-03-09 23:18:48,817 - INFO - __main__ - Epoch  95, Step:  205100, Batch Loss:     1.502652, Lr: 0.000039, Tokens per sec:   2742
2023-03-09 23:19:08,824 - INFO - __main__ - Epoch  95, Step:  205200, Batch Loss:     1.588009, Lr: 0.000039, Tokens per sec:   2693
2023-03-09 23:19:28,850 - INFO - __main__ - Epoch  95, Step:  205300, Batch Loss:     2.080801, Lr: 0.000039, Tokens per sec:   2719
2023-03-09 23:19:48,791 - INFO - __main__ - Epoch  95, Step:  205400, Batch Loss:     2.082174, Lr: 0.000039, Tokens per sec:   2726
2023-03-09 23:20:08,783 - INFO - __main__ - Epoch  95, Step:  205500, Batch Loss:     2.188136, Lr: 0.000039, Tokens per sec:   2731
2023-03-09 23:20:28,025 - INFO - __main__ - Epoch  95, Step:  205600, Batch Loss:     3.148952, Lr: 0.000039, Tokens per sec:   2762
2023-03-09 23:20:46,321 - INFO - __main__ - Epoch  95, Step:  205700, Batch Loss:     1.850438, Lr: 0.000039, Tokens per sec:   2920
2023-03-09 23:21:04,607 - INFO - __main__ - Epoch  95, Step:  205800, Batch Loss:     2.711712, Lr: 0.000039, Tokens per sec:   2893
2023-03-09 23:21:24,444 - INFO - __main__ - Epoch  95, Step:  205900, Batch Loss:     1.068074, Lr: 0.000039, Tokens per sec:   2716
2023-03-09 23:21:44,501 - INFO - __main__ - Epoch  95, Step:  206000, Batch Loss:     1.823009, Lr: 0.000039, Tokens per sec:   2661
2023-03-09 23:22:04,578 - INFO - __main__ - Epoch  95, Step:  206100, Batch Loss:     1.403308, Lr: 0.000039, Tokens per sec:   2678
2023-03-09 23:22:24,602 - INFO - __main__ - Epoch  95, Step:  206200, Batch Loss:     2.417005, Lr: 0.000039, Tokens per sec:   2692
2023-03-09 23:22:44,719 - INFO - __main__ - Epoch  95, Step:  206300, Batch Loss:     1.805121, Lr: 0.000039, Tokens per sec:   2673
2023-03-09 23:23:04,752 - INFO - __main__ - Epoch  95, Step:  206400, Batch Loss:     2.690885, Lr: 0.000039, Tokens per sec:   2693
2023-03-09 23:23:24,780 - INFO - __main__ - Epoch  95, Step:  206500, Batch Loss:     1.748286, Lr: 0.000039, Tokens per sec:   2636
2023-03-09 23:23:44,799 - INFO - __main__ - Epoch  95, Step:  206600, Batch Loss:     2.493785, Lr: 0.000039, Tokens per sec:   2720
2023-03-09 23:24:04,866 - INFO - __main__ - Epoch  95, Step:  206700, Batch Loss:     2.337262, Lr: 0.000039, Tokens per sec:   2618
2023-03-09 23:24:24,631 - INFO - __main__ - Epoch  95, Step:  206800, Batch Loss:     2.179419, Lr: 0.000039, Tokens per sec:   2734
2023-03-09 23:24:44,221 - INFO - __main__ - Epoch  95, Step:  206900, Batch Loss:     2.071212, Lr: 0.000039, Tokens per sec:   2724
2023-03-09 23:25:03,629 - INFO - __main__ - Epoch  95, Step:  207000, Batch Loss:     2.109913, Lr: 0.000039, Tokens per sec:   2809
2023-03-09 23:25:04,633 - INFO - __main__ - Epoch  95: total training loss 4611.99
2023-03-09 23:25:04,633 - INFO - __main__ - Epoch 96
2023-03-09 23:25:23,965 - INFO - __main__ - Epoch  96, Step:  207100, Batch Loss:     1.304717, Lr: 0.000038, Tokens per sec:   2656
2023-03-09 23:25:44,019 - INFO - __main__ - Epoch  96, Step:  207200, Batch Loss:     1.758160, Lr: 0.000038, Tokens per sec:   2705
2023-03-09 23:26:03,230 - INFO - __main__ - Epoch  96, Step:  207300, Batch Loss:     1.491104, Lr: 0.000038, Tokens per sec:   2744
2023-03-09 23:26:22,150 - INFO - __main__ - Epoch  96, Step:  207400, Batch Loss:     1.918554, Lr: 0.000038, Tokens per sec:   2788
2023-03-09 23:26:42,270 - INFO - __main__ - Epoch  96, Step:  207500, Batch Loss:     3.324386, Lr: 0.000038, Tokens per sec:   2669
2023-03-09 23:27:02,398 - INFO - __main__ - Epoch  96, Step:  207600, Batch Loss:     2.095674, Lr: 0.000038, Tokens per sec:   2642
2023-03-09 23:27:22,297 - INFO - __main__ - Epoch  96, Step:  207700, Batch Loss:     1.256297, Lr: 0.000038, Tokens per sec:   2690
2023-03-09 23:27:42,655 - INFO - __main__ - Epoch  96, Step:  207800, Batch Loss:     1.873831, Lr: 0.000038, Tokens per sec:   2682
2023-03-09 23:28:02,972 - INFO - __main__ - Epoch  96, Step:  207900, Batch Loss:     2.073459, Lr: 0.000038, Tokens per sec:   2634
2023-03-09 23:28:23,282 - INFO - __main__ - Epoch  96, Step:  208000, Batch Loss:     1.989614, Lr: 0.000038, Tokens per sec:   2672
2023-03-09 23:28:43,549 - INFO - __main__ - Epoch  96, Step:  208100, Batch Loss:     1.779400, Lr: 0.000038, Tokens per sec:   2651
2023-03-09 23:29:03,564 - INFO - __main__ - Epoch  96, Step:  208200, Batch Loss:     1.757705, Lr: 0.000038, Tokens per sec:   2680
2023-03-09 23:29:23,900 - INFO - __main__ - Epoch  96, Step:  208300, Batch Loss:     1.298540, Lr: 0.000038, Tokens per sec:   2662
2023-03-09 23:29:44,259 - INFO - __main__ - Epoch  96, Step:  208400, Batch Loss:     2.003688, Lr: 0.000038, Tokens per sec:   2679
2023-03-09 23:30:04,550 - INFO - __main__ - Epoch  96, Step:  208500, Batch Loss:     1.591315, Lr: 0.000038, Tokens per sec:   2667
2023-03-09 23:30:24,526 - INFO - __main__ - Epoch  96, Step:  208600, Batch Loss:     2.268350, Lr: 0.000038, Tokens per sec:   2632
2023-03-09 23:30:44,776 - INFO - __main__ - Epoch  96, Step:  208700, Batch Loss:     2.265358, Lr: 0.000038, Tokens per sec:   2647
2023-03-09 23:31:04,370 - INFO - __main__ - Epoch  96, Step:  208800, Batch Loss:     1.320354, Lr: 0.000038, Tokens per sec:   2768
2023-03-09 23:31:24,483 - INFO - __main__ - Epoch  96, Step:  208900, Batch Loss:     2.110281, Lr: 0.000038, Tokens per sec:   2767
2023-03-09 23:31:44,918 - INFO - __main__ - Epoch  96, Step:  209000, Batch Loss:     2.587948, Lr: 0.000038, Tokens per sec:   2619
2023-03-09 23:32:05,341 - INFO - __main__ - Epoch  96, Step:  209100, Batch Loss:     2.759791, Lr: 0.000038, Tokens per sec:   2638
2023-03-09 23:32:22,423 - INFO - __main__ - Epoch  96: total training loss 4492.05
2023-03-09 23:32:22,424 - INFO - __main__ - Epoch 97
2023-03-09 23:32:26,016 - INFO - __main__ - Epoch  97, Step:  209200, Batch Loss:     1.117468, Lr: 0.000038, Tokens per sec:   2354
2023-03-09 23:32:46,253 - INFO - __main__ - Epoch  97, Step:  209300, Batch Loss:     1.780025, Lr: 0.000038, Tokens per sec:   2681
2023-03-09 23:33:06,421 - INFO - __main__ - Epoch  97, Step:  209400, Batch Loss:     2.324308, Lr: 0.000038, Tokens per sec:   2650
2023-03-09 23:33:26,692 - INFO - __main__ - Epoch  97, Step:  209500, Batch Loss:     2.484368, Lr: 0.000038, Tokens per sec:   2677
2023-03-09 23:33:47,089 - INFO - __main__ - Epoch  97, Step:  209600, Batch Loss:     2.177442, Lr: 0.000038, Tokens per sec:   2642
2023-03-09 23:34:06,905 - INFO - __main__ - Epoch  97, Step:  209700, Batch Loss:     2.218751, Lr: 0.000038, Tokens per sec:   2688
2023-03-09 23:34:27,107 - INFO - __main__ - Epoch  97, Step:  209800, Batch Loss:     2.448527, Lr: 0.000038, Tokens per sec:   2618
2023-03-09 23:34:47,463 - INFO - __main__ - Epoch  97, Step:  209900, Batch Loss:     1.612348, Lr: 0.000038, Tokens per sec:   2683
2023-03-09 23:35:07,626 - INFO - __main__ - Epoch  97, Step:  210000, Batch Loss:     2.578272, Lr: 0.000038, Tokens per sec:   2652
2023-03-09 23:35:27,956 - INFO - __main__ - Epoch  97, Step:  210100, Batch Loss:     1.152573, Lr: 0.000038, Tokens per sec:   2599
2023-03-09 23:35:47,515 - INFO - __main__ - Epoch  97, Step:  210200, Batch Loss:     1.810882, Lr: 0.000038, Tokens per sec:   2759
2023-03-09 23:36:07,819 - INFO - __main__ - Epoch  97, Step:  210300, Batch Loss:     1.523266, Lr: 0.000038, Tokens per sec:   2618
2023-03-09 23:36:27,873 - INFO - __main__ - Epoch  97, Step:  210400, Batch Loss:     1.323969, Lr: 0.000038, Tokens per sec:   2707
2023-03-09 23:36:48,107 - INFO - __main__ - Epoch  97, Step:  210500, Batch Loss:     2.417912, Lr: 0.000038, Tokens per sec:   2679
2023-03-09 23:37:07,844 - INFO - __main__ - Epoch  97, Step:  210600, Batch Loss:     1.788773, Lr: 0.000038, Tokens per sec:   2751
2023-03-09 23:37:28,182 - INFO - __main__ - Epoch  97, Step:  210700, Batch Loss:     1.908337, Lr: 0.000038, Tokens per sec:   2618
2023-03-09 23:37:48,019 - INFO - __main__ - Epoch  97, Step:  210800, Batch Loss:     1.884431, Lr: 0.000038, Tokens per sec:   2756
2023-03-09 23:38:08,342 - INFO - __main__ - Epoch  97, Step:  210900, Batch Loss:     1.666892, Lr: 0.000038, Tokens per sec:   2651
2023-03-09 23:38:28,100 - INFO - __main__ - Epoch  97, Step:  211000, Batch Loss:     1.703868, Lr: 0.000038, Tokens per sec:   2692
2023-03-09 23:38:48,164 - INFO - __main__ - Epoch  97, Step:  211100, Batch Loss:     1.208906, Lr: 0.000038, Tokens per sec:   2665
2023-03-09 23:39:08,449 - INFO - __main__ - Epoch  97, Step:  211200, Batch Loss:     2.460975, Lr: 0.000038, Tokens per sec:   2672
2023-03-09 23:39:28,650 - INFO - __main__ - Epoch  97, Step:  211300, Batch Loss:     2.672367, Lr: 0.000038, Tokens per sec:   2736
2023-03-09 23:39:41,444 - INFO - __main__ - Epoch  97: total training loss 4447.34
2023-03-09 23:39:41,444 - INFO - __main__ - Epoch 98
2023-03-09 23:39:49,260 - INFO - __main__ - Epoch  98, Step:  211400, Batch Loss:     1.911390, Lr: 0.000038, Tokens per sec:   2609
2023-03-09 23:40:09,386 - INFO - __main__ - Epoch  98, Step:  211500, Batch Loss:     2.971373, Lr: 0.000038, Tokens per sec:   2685
2023-03-09 23:40:29,497 - INFO - __main__ - Epoch  98, Step:  211600, Batch Loss:     2.543395, Lr: 0.000038, Tokens per sec:   2670
2023-03-09 23:40:49,802 - INFO - __main__ - Epoch  98, Step:  211700, Batch Loss:     1.940819, Lr: 0.000038, Tokens per sec:   2663
2023-03-09 23:41:09,946 - INFO - __main__ - Epoch  98, Step:  211800, Batch Loss:     1.977017, Lr: 0.000038, Tokens per sec:   2677
2023-03-09 23:41:29,429 - INFO - __main__ - Epoch  98, Step:  211900, Batch Loss:     2.366949, Lr: 0.000038, Tokens per sec:   2779
2023-03-09 23:41:49,420 - INFO - __main__ - Epoch  98, Step:  212000, Batch Loss:     1.635098, Lr: 0.000038, Tokens per sec:   2719
2023-03-09 23:42:09,696 - INFO - __main__ - Epoch  98, Step:  212100, Batch Loss:     2.319440, Lr: 0.000038, Tokens per sec:   2604
2023-03-09 23:42:29,888 - INFO - __main__ - Epoch  98, Step:  212200, Batch Loss:     1.711378, Lr: 0.000038, Tokens per sec:   2667
2023-03-09 23:42:49,928 - INFO - __main__ - Epoch  98, Step:  212300, Batch Loss:     2.344946, Lr: 0.000038, Tokens per sec:   2694
2023-03-09 23:43:10,009 - INFO - __main__ - Epoch  98, Step:  212400, Batch Loss:     1.281310, Lr: 0.000038, Tokens per sec:   2680
2023-03-09 23:43:30,153 - INFO - __main__ - Epoch  98, Step:  212500, Batch Loss:     1.936311, Lr: 0.000038, Tokens per sec:   2656
2023-03-09 23:43:50,391 - INFO - __main__ - Epoch  98, Step:  212600, Batch Loss:     2.251493, Lr: 0.000038, Tokens per sec:   2694
2023-03-09 23:44:10,396 - INFO - __main__ - Epoch  98, Step:  212700, Batch Loss:     1.707166, Lr: 0.000038, Tokens per sec:   2671
2023-03-09 23:44:30,410 - INFO - __main__ - Epoch  98, Step:  212800, Batch Loss:     1.571397, Lr: 0.000038, Tokens per sec:   2719
2023-03-09 23:44:50,427 - INFO - __main__ - Epoch  98, Step:  212900, Batch Loss:     3.205675, Lr: 0.000038, Tokens per sec:   2687
2023-03-09 23:45:10,441 - INFO - __main__ - Epoch  98, Step:  213000, Batch Loss:     1.308169, Lr: 0.000038, Tokens per sec:   2672
2023-03-09 23:45:29,731 - INFO - __main__ - Epoch  98, Step:  213100, Batch Loss:     2.624694, Lr: 0.000038, Tokens per sec:   2734
2023-03-09 23:45:49,753 - INFO - __main__ - Epoch  98, Step:  213200, Batch Loss:     2.449125, Lr: 0.000038, Tokens per sec:   2674
2023-03-09 23:46:09,699 - INFO - __main__ - Epoch  98, Step:  213300, Batch Loss:     1.783917, Lr: 0.000038, Tokens per sec:   2706
2023-03-09 23:46:29,711 - INFO - __main__ - Epoch  98, Step:  213400, Batch Loss:     2.557516, Lr: 0.000038, Tokens per sec:   2727
2023-03-09 23:46:49,707 - INFO - __main__ - Epoch  98, Step:  213500, Batch Loss:     3.040221, Lr: 0.000038, Tokens per sec:   2658
2023-03-09 23:46:58,200 - INFO - __main__ - Epoch  98: total training loss 4367.65
2023-03-09 23:46:58,200 - INFO - __main__ - Epoch 99
2023-03-09 23:47:10,166 - INFO - __main__ - Epoch  99, Step:  213600, Batch Loss:     2.427858, Lr: 0.000037, Tokens per sec:   2584
2023-03-09 23:47:30,194 - INFO - __main__ - Epoch  99, Step:  213700, Batch Loss:     2.629178, Lr: 0.000037, Tokens per sec:   2657
2023-03-09 23:47:50,184 - INFO - __main__ - Epoch  99, Step:  213800, Batch Loss:     2.117148, Lr: 0.000037, Tokens per sec:   2695
2023-03-09 23:48:10,180 - INFO - __main__ - Epoch  99, Step:  213900, Batch Loss:     1.740913, Lr: 0.000037, Tokens per sec:   2726
2023-03-09 23:48:29,993 - INFO - __main__ - Epoch  99, Step:  214000, Batch Loss:     2.321652, Lr: 0.000037, Tokens per sec:   2708
2023-03-09 23:48:50,012 - INFO - __main__ - Epoch  99, Step:  214100, Batch Loss:     1.944063, Lr: 0.000037, Tokens per sec:   2663
2023-03-09 23:49:10,004 - INFO - __main__ - Epoch  99, Step:  214200, Batch Loss:     2.450032, Lr: 0.000037, Tokens per sec:   2703
2023-03-09 23:49:29,991 - INFO - __main__ - Epoch  99, Step:  214300, Batch Loss:     1.737213, Lr: 0.000037, Tokens per sec:   2710
2023-03-09 23:49:49,913 - INFO - __main__ - Epoch  99, Step:  214400, Batch Loss:     1.314721, Lr: 0.000037, Tokens per sec:   2733
2023-03-09 23:50:08,446 - INFO - __main__ - Epoch  99, Step:  214500, Batch Loss:     3.107722, Lr: 0.000037, Tokens per sec:   2921
2023-03-09 23:50:28,064 - INFO - __main__ - Epoch  99, Step:  214600, Batch Loss:     2.250869, Lr: 0.000037, Tokens per sec:   2726
2023-03-09 23:50:48,048 - INFO - __main__ - Epoch  99, Step:  214700, Batch Loss:     1.941783, Lr: 0.000037, Tokens per sec:   2705
2023-03-09 23:51:08,064 - INFO - __main__ - Epoch  99, Step:  214800, Batch Loss:     2.176015, Lr: 0.000037, Tokens per sec:   2710
2023-03-09 23:51:27,637 - INFO - __main__ - Epoch  99, Step:  214900, Batch Loss:     2.065608, Lr: 0.000037, Tokens per sec:   2757
2023-03-09 23:51:47,515 - INFO - __main__ - Epoch  99, Step:  215000, Batch Loss:     1.316254, Lr: 0.000037, Tokens per sec:   2688
2023-03-09 23:52:07,549 - INFO - __main__ - Epoch  99, Step:  215100, Batch Loss:     2.146420, Lr: 0.000037, Tokens per sec:   2712
2023-03-09 23:52:27,488 - INFO - __main__ - Epoch  99, Step:  215200, Batch Loss:     2.276050, Lr: 0.000037, Tokens per sec:   2671
2023-03-09 23:52:47,512 - INFO - __main__ - Epoch  99, Step:  215300, Batch Loss:     2.207174, Lr: 0.000037, Tokens per sec:   2726
2023-03-09 23:53:07,530 - INFO - __main__ - Epoch  99, Step:  215400, Batch Loss:     1.583480, Lr: 0.000037, Tokens per sec:   2685
2023-03-09 23:53:27,527 - INFO - __main__ - Epoch  99, Step:  215500, Batch Loss:     2.638106, Lr: 0.000037, Tokens per sec:   2689
2023-03-09 23:53:47,541 - INFO - __main__ - Epoch  99, Step:  215600, Batch Loss:     3.292229, Lr: 0.000037, Tokens per sec:   2681
2023-03-09 23:54:07,591 - INFO - __main__ - Epoch  99, Step:  215700, Batch Loss:     1.722165, Lr: 0.000037, Tokens per sec:   2641
2023-03-09 23:54:11,833 - INFO - __main__ - Epoch  99: total training loss 4336.68
2023-03-09 23:54:11,834 - INFO - __main__ - Epoch 100
2023-03-09 23:54:27,942 - INFO - __main__ - Epoch 100, Step:  215800, Batch Loss:     1.759390, Lr: 0.000037, Tokens per sec:   2599
2023-03-09 23:54:47,975 - INFO - __main__ - Epoch 100, Step:  215900, Batch Loss:     2.219261, Lr: 0.000037, Tokens per sec:   2685
2023-03-09 23:55:07,970 - INFO - __main__ - Epoch 100, Step:  216000, Batch Loss:     2.355729, Lr: 0.000037, Tokens per sec:   2660
2023-03-09 23:55:28,203 - INFO - __main__ - Epoch 100, Step:  216100, Batch Loss:     1.417836, Lr: 0.000037, Tokens per sec:   2637
2023-03-09 23:55:48,576 - INFO - __main__ - Epoch 100, Step:  216200, Batch Loss:     2.522033, Lr: 0.000037, Tokens per sec:   2670
2023-03-09 23:56:08,706 - INFO - __main__ - Epoch 100, Step:  216300, Batch Loss:     1.878371, Lr: 0.000037, Tokens per sec:   2709
2023-03-09 23:56:28,607 - INFO - __main__ - Epoch 100, Step:  216400, Batch Loss:     2.034457, Lr: 0.000037, Tokens per sec:   2691
2023-03-09 23:56:46,870 - INFO - __main__ - Epoch 100, Step:  216500, Batch Loss:     1.406806, Lr: 0.000037, Tokens per sec:   2957
2023-03-09 23:57:05,753 - INFO - __main__ - Epoch 100, Step:  216600, Batch Loss:     1.529168, Lr: 0.000037, Tokens per sec:   2881
2023-03-09 23:57:25,766 - INFO - __main__ - Epoch 100, Step:  216700, Batch Loss:     2.133771, Lr: 0.000037, Tokens per sec:   2629
2023-03-09 23:57:45,760 - INFO - __main__ - Epoch 100, Step:  216800, Batch Loss:     2.547138, Lr: 0.000037, Tokens per sec:   2721
2023-03-09 23:58:05,812 - INFO - __main__ - Epoch 100, Step:  216900, Batch Loss:     3.091582, Lr: 0.000037, Tokens per sec:   2629
2023-03-09 23:58:25,790 - INFO - __main__ - Epoch 100, Step:  217000, Batch Loss:     1.248586, Lr: 0.000037, Tokens per sec:   2682
2023-03-09 23:58:45,785 - INFO - __main__ - Epoch 100, Step:  217100, Batch Loss:     1.411035, Lr: 0.000037, Tokens per sec:   2662
2023-03-09 23:59:05,772 - INFO - __main__ - Epoch 100, Step:  217200, Batch Loss:     1.768235, Lr: 0.000037, Tokens per sec:   2702
2023-03-09 23:59:25,788 - INFO - __main__ - Epoch 100, Step:  217300, Batch Loss:     1.584813, Lr: 0.000037, Tokens per sec:   2730
2023-03-09 23:59:46,063 - INFO - __main__ - Epoch 100, Step:  217400, Batch Loss:     2.530225, Lr: 0.000037, Tokens per sec:   2677
2023-03-10 00:00:06,084 - INFO - __main__ - Epoch 100, Step:  217500, Batch Loss:     3.052870, Lr: 0.000037, Tokens per sec:   2712
2023-03-10 00:00:26,035 - INFO - __main__ - Epoch 100, Step:  217600, Batch Loss:     1.593029, Lr: 0.000037, Tokens per sec:   2707
2023-03-10 00:00:46,083 - INFO - __main__ - Epoch 100, Step:  217700, Batch Loss:     1.808232, Lr: 0.000037, Tokens per sec:   2711
2023-03-10 00:01:06,092 - INFO - __main__ - Epoch 100, Step:  217800, Batch Loss:     2.672742, Lr: 0.000037, Tokens per sec:   2704
2023-03-10 00:01:26,027 - INFO - __main__ - Epoch 100, Step:  217900, Batch Loss:     0.780923, Lr: 0.000037, Tokens per sec:   2682
2023-03-10 00:01:26,179 - INFO - __main__ - Epoch 100: total training loss 4247.14
2023-03-10 00:01:26,179 - INFO - __main__ - Training ended after 100 epoches!
2023-03-10 00:01:26,179 - INFO - __main__ - Best Validation result (greedy) at step        0:   -inf bleu.
