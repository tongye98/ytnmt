2023-03-08 20:14:44,537 - INFO - root - Hello! This is Tong Ye's Transformer!
2023-03-08 20:15:47,522 - INFO - data - average code tokens = 109.28999515442095
2023-03-08 20:15:47,522 - INFO - data - average ast tokens = 188.85505342888476
2023-03-08 20:15:47,522 - INFO - data - average text tokens = 15.993139680191783
2023-03-08 20:15:47,522 - INFO - data - average position tokens = 188.85505342888476
2023-03-08 20:15:47,522 - INFO - data - average ast edges = 375.7101068577695
2023-03-08 20:15:59,465 - INFO - data - code vocab length = 26684
2023-03-08 20:15:59,466 - INFO - data - text vocab length = 13207
2023-03-08 20:15:59,466 - INFO - data - position vocab length = 20587
2023-03-08 20:16:08,965 - INFO - model - Build Model...
2023-03-08 20:16:09,536 - INFO - model - Total parameters number: 91562496
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.weight               [512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.layer_norm.bias                 [512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.weight [512, 512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.key_project.bias [512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.weight [512, 512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.query_project.bias [512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.weight [512, 512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.value_project.bias [512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.weight [512, 512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.src_src_attenion.output_layer.bias [512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,537 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.weight               [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.layer_norm.bias                 [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.key_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.query_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.value_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.src_src_attenion.output_layer.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.weight               [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.layer_norm.bias                 [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.key_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.query_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.value_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.src_src_attenion.output_layer.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.weight               [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.layer_norm.bias                 [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.key_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.query_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.value_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.src_src_attenion.output_layer.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.weight               [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.layer_norm.bias                 [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.key_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.query_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.value_project.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.weight [512, 512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.src_src_attenion.output_layer.bias [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,538 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.weight               [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.layer_norm.bias                 [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.key_project.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.query_project.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.value_project.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.src_src_attenion.output_layer.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.weight                        [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_encoder.layer_norm.bias                          [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_l.bias                          [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.gnn.lin_r.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.weight                       [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.0.layer_norm.bias                         [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_l.bias                          [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.gnn.lin_r.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.weight                       [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.1.layer_norm.bias                         [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_l.bias                          [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.gnn.lin_r.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.weight                       [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.2.layer_norm.bias                         [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_l.bias                          [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.gnn.lin_r.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.weight                       [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.3.layer_norm.bias                         [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_l.bias                          [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.gnn.lin_r.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.weight                       [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.4.layer_norm.bias                         [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_l.bias                          [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.gnn.lin_r.weight                        [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.weight                       [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): gnn_encoder.layers.5.layer_norm.bias                         [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.trg_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,539 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.src_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.gnn_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.weight               [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm.bias                 [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.weight              [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm2.bias                [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.weight              [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.0.layer_norm3.bias                [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.trg_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.src_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.gnn_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.weight               [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm.bias                 [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.weight              [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm2.bias                [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.weight              [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.1.layer_norm3.bias                [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.trg_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,540 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.src_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.gnn_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.weight               [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm.bias                 [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.weight              [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm2.bias                [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.weight              [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.2.layer_norm3.bias                [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.trg_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.src_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.gnn_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.weight               [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm.bias                 [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.weight              [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm2.bias                [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.weight              [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.3.layer_norm3.bias                [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.trg_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,541 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.src_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.gnn_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.weight               [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm.bias                 [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.weight              [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm2.bias                [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.weight              [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.4.layer_norm3.bias                [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.trg_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.src_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.key_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.query_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.value_project.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.weight [512, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.gnn_trg_attention.output_layer.bias [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.weight  [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.layer_norm.bias    [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.weight      [2048, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.0.bias        [2048]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.weight      [512, 2048]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.feed_forward.pwff.3.bias        [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.weight               [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm.bias                 [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.weight              [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm2.bias                [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.weight              [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layers.5.layer_norm3.bias                [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.weight                        [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): transformer_decoder.layer_norm.bias                          [512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): src_embed.lut.weight                                         [26684, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): code_learnable_embed.learn_lut.weight                        [400, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): position_embed.lut.weight                                    [20587, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): trg_embed.lut.weight                                         [13207, 512]
2023-03-08 20:16:09,542 - DEBUG - model - Trainable parameters(name): trg_learnable_embed.learn_lut.weight                         [50, 512]
2023-03-08 20:16:09,543 - DEBUG - model - Trainable parameters(name): output_layer.weight                                          [13207, 512]
2023-03-08 20:16:09,543 - INFO - model - The model is built.
2023-03-08 20:16:09,544 - INFO - __main__ - ********************1 GPUs are used.********************
2023-03-08 20:16:09,544 - INFO - __main__ - ********************4 num_workers are used.********************
2023-03-08 20:16:10,715 - INFO - __main__ - Adam(lr=0.0001, weight_decay=0, betas=[0.9, 0.999], eps=1e-08)
2023-03-08 20:16:10,715 - INFO - __main__ - Scheduler = StepLR
2023-03-08 20:16:10,715 - INFO - __main__ - Train stats:
	device: cuda
	n_gpu: 1
	batch_size: 32
2023-03-08 20:16:10,715 - INFO - __main__ - Epoch 1
2023-03-08 20:16:32,093 - INFO - __main__ - Epoch   1, Step:     100, Batch Loss:    89.944733, Lr: 0.000100, Tokens per sec:   2477
2023-03-08 20:16:52,047 - INFO - __main__ - Epoch   1, Step:     200, Batch Loss:    85.505989, Lr: 0.000100, Tokens per sec:   2662
2023-03-08 20:17:12,124 - INFO - __main__ - Epoch   1, Step:     300, Batch Loss:    74.764931, Lr: 0.000100, Tokens per sec:   2737
2023-03-08 20:17:32,138 - INFO - __main__ - Epoch   1, Step:     400, Batch Loss:    72.754105, Lr: 0.000100, Tokens per sec:   2713
2023-03-08 20:17:52,149 - INFO - __main__ - Epoch   1, Step:     500, Batch Loss:    67.575134, Lr: 0.000100, Tokens per sec:   2715
2023-03-08 20:18:12,227 - INFO - __main__ - Epoch   1, Step:     600, Batch Loss:    90.420319, Lr: 0.000100, Tokens per sec:   2684
2023-03-08 20:18:32,274 - INFO - __main__ - Epoch   1, Step:     700, Batch Loss:    79.249451, Lr: 0.000100, Tokens per sec:   2675
2023-03-08 20:18:52,208 - INFO - __main__ - Epoch   1, Step:     800, Batch Loss:    70.866356, Lr: 0.000100, Tokens per sec:   2704
2023-03-08 20:19:12,267 - INFO - __main__ - Epoch   1, Step:     900, Batch Loss:    73.645088, Lr: 0.000100, Tokens per sec:   2724
2023-03-08 20:19:32,337 - INFO - __main__ - Epoch   1, Step:    1000, Batch Loss:    82.662933, Lr: 0.000100, Tokens per sec:   2673
2023-03-08 20:19:52,371 - INFO - __main__ - Epoch   1, Step:    1100, Batch Loss:    80.282005, Lr: 0.000100, Tokens per sec:   2638
2023-03-08 20:20:11,913 - INFO - __main__ - Epoch   1, Step:    1200, Batch Loss:    68.731171, Lr: 0.000100, Tokens per sec:   2749
2023-03-08 20:20:31,094 - INFO - __main__ - Epoch   1, Step:    1300, Batch Loss:    74.783272, Lr: 0.000100, Tokens per sec:   2839
2023-03-08 20:20:49,319 - INFO - __main__ - Epoch   1, Step:    1400, Batch Loss:    76.543427, Lr: 0.000100, Tokens per sec:   2908
2023-03-08 20:21:07,474 - INFO - __main__ - Epoch   1, Step:    1500, Batch Loss:    87.421890, Lr: 0.000100, Tokens per sec:   2942
2023-03-08 20:21:25,702 - INFO - __main__ - Epoch   1, Step:    1600, Batch Loss:    97.151138, Lr: 0.000100, Tokens per sec:   2970
2023-03-08 20:21:44,683 - INFO - __main__ - Epoch   1, Step:    1700, Batch Loss:    87.012291, Lr: 0.000100, Tokens per sec:   2883
2023-03-08 20:22:03,197 - INFO - __main__ - Epoch   1, Step:    1800, Batch Loss:    86.923264, Lr: 0.000100, Tokens per sec:   2928
2023-03-08 20:22:23,027 - INFO - __main__ - Epoch   1, Step:    1900, Batch Loss:    90.725258, Lr: 0.000100, Tokens per sec:   2714
2023-03-08 20:22:42,139 - INFO - __main__ - Epoch   1, Step:    2000, Batch Loss:    70.846893, Lr: 0.000100, Tokens per sec:   2828
2023-03-08 20:23:00,390 - INFO - __main__ - Epoch   1, Step:    2100, Batch Loss:    72.648155, Lr: 0.000100, Tokens per sec:   2923
2023-03-08 20:23:15,852 - INFO - __main__ - Epoch   1: total training loss 171250.52
2023-03-08 20:23:15,853 - INFO - __main__ - Epoch 2
2023-03-08 20:23:20,268 - INFO - __main__ - Epoch   2, Step:    2200, Batch Loss:    71.307747, Lr: 0.000099, Tokens per sec:   2538
2023-03-08 20:23:38,570 - INFO - __main__ - Epoch   2, Step:    2300, Batch Loss:    64.151535, Lr: 0.000099, Tokens per sec:   2925
2023-03-08 20:23:56,861 - INFO - __main__ - Epoch   2, Step:    2400, Batch Loss:    61.629501, Lr: 0.000099, Tokens per sec:   2892
2023-03-08 20:24:16,908 - INFO - __main__ - Epoch   2, Step:    2500, Batch Loss:    49.266079, Lr: 0.000099, Tokens per sec:   2724
2023-03-08 20:24:36,947 - INFO - __main__ - Epoch   2, Step:    2600, Batch Loss:    62.434223, Lr: 0.000099, Tokens per sec:   2697
2023-03-08 20:24:55,743 - INFO - __main__ - Epoch   2, Step:    2700, Batch Loss:    58.390175, Lr: 0.000099, Tokens per sec:   2858
2023-03-08 20:25:13,880 - INFO - __main__ - Epoch   2, Step:    2800, Batch Loss:    72.616249, Lr: 0.000099, Tokens per sec:   2991
2023-03-08 20:25:32,090 - INFO - __main__ - Epoch   2, Step:    2900, Batch Loss:    71.712769, Lr: 0.000099, Tokens per sec:   2959
2023-03-08 20:25:50,939 - INFO - __main__ - Epoch   2, Step:    3000, Batch Loss:    67.770470, Lr: 0.000099, Tokens per sec:   2857
2023-03-08 20:26:11,018 - INFO - __main__ - Epoch   2, Step:    3100, Batch Loss:    57.191341, Lr: 0.000099, Tokens per sec:   2685
2023-03-08 20:26:31,047 - INFO - __main__ - Epoch   2, Step:    3200, Batch Loss:    53.149521, Lr: 0.000099, Tokens per sec:   2706
2023-03-08 20:26:50,402 - INFO - __main__ - Epoch   2, Step:    3300, Batch Loss:    60.073299, Lr: 0.000099, Tokens per sec:   2751
2023-03-08 20:27:09,432 - INFO - __main__ - Epoch   2, Step:    3400, Batch Loss:    64.005005, Lr: 0.000099, Tokens per sec:   2825
2023-03-08 20:27:29,477 - INFO - __main__ - Epoch   2, Step:    3500, Batch Loss:    62.666672, Lr: 0.000099, Tokens per sec:   2680
2023-03-08 20:27:49,412 - INFO - __main__ - Epoch   2, Step:    3600, Batch Loss:    49.886700, Lr: 0.000099, Tokens per sec:   2698
2023-03-08 20:28:09,458 - INFO - __main__ - Epoch   2, Step:    3700, Batch Loss:    58.070774, Lr: 0.000099, Tokens per sec:   2750
2023-03-08 20:28:29,480 - INFO - __main__ - Epoch   2, Step:    3800, Batch Loss:    78.276512, Lr: 0.000099, Tokens per sec:   2678
2023-03-08 20:28:49,523 - INFO - __main__ - Epoch   2, Step:    3900, Batch Loss:    57.457813, Lr: 0.000099, Tokens per sec:   2713
2023-03-08 20:29:09,547 - INFO - __main__ - Epoch   2, Step:    4000, Batch Loss:    54.183556, Lr: 0.000099, Tokens per sec:   2687
2023-03-08 20:29:29,566 - INFO - __main__ - Epoch   2, Step:    4100, Batch Loss:    51.554077, Lr: 0.000099, Tokens per sec:   2617
2023-03-08 20:29:49,602 - INFO - __main__ - Epoch   2, Step:    4200, Batch Loss:    67.939873, Lr: 0.000099, Tokens per sec:   2659
2023-03-08 20:30:09,650 - INFO - __main__ - Epoch   2, Step:    4300, Batch Loss:    80.070717, Lr: 0.000099, Tokens per sec:   2715
2023-03-08 20:30:21,259 - INFO - __main__ - Epoch   2: total training loss 139631.23
2023-03-08 20:30:21,260 - INFO - __main__ - Epoch 3
2023-03-08 20:30:29,996 - INFO - __main__ - Epoch   3, Step:    4400, Batch Loss:    70.152153, Lr: 0.000098, Tokens per sec:   2615
2023-03-08 20:30:49,991 - INFO - __main__ - Epoch   3, Step:    4500, Batch Loss:    63.604362, Lr: 0.000098, Tokens per sec:   2691
2023-03-08 20:31:10,032 - INFO - __main__ - Epoch   3, Step:    4600, Batch Loss:    47.129013, Lr: 0.000098, Tokens per sec:   2669
2023-03-08 20:31:30,078 - INFO - __main__ - Epoch   3, Step:    4700, Batch Loss:    53.810604, Lr: 0.000098, Tokens per sec:   2723
2023-03-08 20:31:49,973 - INFO - __main__ - Epoch   3, Step:    4800, Batch Loss:    69.553177, Lr: 0.000098, Tokens per sec:   2742
2023-03-08 20:32:09,998 - INFO - __main__ - Epoch   3, Step:    4900, Batch Loss:    54.840908, Lr: 0.000098, Tokens per sec:   2671
2023-03-08 20:32:30,042 - INFO - __main__ - Epoch   3, Step:    5000, Batch Loss:    44.354282, Lr: 0.000098, Tokens per sec:   2692
2023-03-08 20:32:50,049 - INFO - __main__ - Epoch   3, Step:    5100, Batch Loss:    61.568954, Lr: 0.000098, Tokens per sec:   2693
2023-03-08 20:33:08,827 - INFO - __main__ - Epoch   3, Step:    5200, Batch Loss:    58.506664, Lr: 0.000098, Tokens per sec:   2892
2023-03-08 20:33:26,947 - INFO - __main__ - Epoch   3, Step:    5300, Batch Loss:    54.140598, Lr: 0.000098, Tokens per sec:   2931
2023-03-08 20:33:45,125 - INFO - __main__ - Epoch   3, Step:    5400, Batch Loss:    54.608597, Lr: 0.000098, Tokens per sec:   2961
2023-03-08 20:34:03,236 - INFO - __main__ - Epoch   3, Step:    5500, Batch Loss:    68.613853, Lr: 0.000098, Tokens per sec:   2922
2023-03-08 20:34:23,153 - INFO - __main__ - Epoch   3, Step:    5600, Batch Loss:    52.413628, Lr: 0.000098, Tokens per sec:   2705
2023-03-08 20:34:43,188 - INFO - __main__ - Epoch   3, Step:    5700, Batch Loss:    42.803337, Lr: 0.000098, Tokens per sec:   2704
2023-03-08 20:35:03,225 - INFO - __main__ - Epoch   3, Step:    5800, Batch Loss:    73.467354, Lr: 0.000098, Tokens per sec:   2664
2023-03-08 20:35:23,321 - INFO - __main__ - Epoch   3, Step:    5900, Batch Loss:    58.984695, Lr: 0.000098, Tokens per sec:   2675
2023-03-08 20:35:43,292 - INFO - __main__ - Epoch   3, Step:    6000, Batch Loss:    61.205971, Lr: 0.000098, Tokens per sec:   2722
2023-03-08 20:36:03,339 - INFO - __main__ - Epoch   3, Step:    6100, Batch Loss:    43.784180, Lr: 0.000098, Tokens per sec:   2665
2023-03-08 20:36:23,358 - INFO - __main__ - Epoch   3, Step:    6200, Batch Loss:    55.603390, Lr: 0.000098, Tokens per sec:   2708
2023-03-08 20:36:43,349 - INFO - __main__ - Epoch   3, Step:    6300, Batch Loss:    62.637283, Lr: 0.000098, Tokens per sec:   2687
2023-03-08 20:37:03,441 - INFO - __main__ - Epoch   3, Step:    6400, Batch Loss:    47.941288, Lr: 0.000098, Tokens per sec:   2708
2023-03-08 20:37:21,757 - INFO - __main__ - Epoch   3, Step:    6500, Batch Loss:    61.439144, Lr: 0.000098, Tokens per sec:   2924
2023-03-08 20:37:28,500 - INFO - __main__ - Epoch   3: total training loss 124928.31
2023-03-08 20:37:28,501 - INFO - __main__ - Epoch 4
2023-03-08 20:37:41,406 - INFO - __main__ - Epoch   4, Step:    6600, Batch Loss:    44.270477, Lr: 0.000097, Tokens per sec:   2669
2023-03-08 20:38:01,433 - INFO - __main__ - Epoch   4, Step:    6700, Batch Loss:    57.537689, Lr: 0.000097, Tokens per sec:   2676
2023-03-08 20:38:21,519 - INFO - __main__ - Epoch   4, Step:    6800, Batch Loss:    55.911720, Lr: 0.000097, Tokens per sec:   2702
2023-03-08 20:38:41,491 - INFO - __main__ - Epoch   4, Step:    6900, Batch Loss:    54.756310, Lr: 0.000097, Tokens per sec:   2707
2023-03-08 20:39:01,525 - INFO - __main__ - Epoch   4, Step:    7000, Batch Loss:    53.623749, Lr: 0.000097, Tokens per sec:   2651
2023-03-08 20:39:20,739 - INFO - __main__ - Epoch   4, Step:    7100, Batch Loss:    63.120731, Lr: 0.000097, Tokens per sec:   2857
2023-03-08 20:39:39,804 - INFO - __main__ - Epoch   4, Step:    7200, Batch Loss:    47.296112, Lr: 0.000097, Tokens per sec:   2779
2023-03-08 20:39:59,824 - INFO - __main__ - Epoch   4, Step:    7300, Batch Loss:    58.214466, Lr: 0.000097, Tokens per sec:   2722
2023-03-08 20:40:19,838 - INFO - __main__ - Epoch   4, Step:    7400, Batch Loss:    53.373585, Lr: 0.000097, Tokens per sec:   2677
2023-03-08 20:40:39,831 - INFO - __main__ - Epoch   4, Step:    7500, Batch Loss:    46.911194, Lr: 0.000097, Tokens per sec:   2744
2023-03-08 20:40:59,846 - INFO - __main__ - Epoch   4, Step:    7600, Batch Loss:    58.095833, Lr: 0.000097, Tokens per sec:   2713
2023-03-08 20:41:19,842 - INFO - __main__ - Epoch   4, Step:    7700, Batch Loss:    49.872391, Lr: 0.000097, Tokens per sec:   2643
2023-03-08 20:41:39,773 - INFO - __main__ - Epoch   4, Step:    7800, Batch Loss:    52.510307, Lr: 0.000097, Tokens per sec:   2682
2023-03-08 20:41:59,761 - INFO - __main__ - Epoch   4, Step:    7900, Batch Loss:    65.531929, Lr: 0.000097, Tokens per sec:   2669
2023-03-08 20:42:19,767 - INFO - __main__ - Epoch   4, Step:    8000, Batch Loss:    58.523296, Lr: 0.000097, Tokens per sec:   2689
2023-03-08 20:42:39,753 - INFO - __main__ - Epoch   4, Step:    8100, Batch Loss:    56.372894, Lr: 0.000097, Tokens per sec:   2709
2023-03-08 20:42:59,385 - INFO - __main__ - Epoch   4, Step:    8200, Batch Loss:    43.561768, Lr: 0.000097, Tokens per sec:   2718
2023-03-08 20:43:18,595 - INFO - __main__ - Epoch   4, Step:    8300, Batch Loss:    41.725986, Lr: 0.000097, Tokens per sec:   2789
2023-03-08 20:43:38,613 - INFO - __main__ - Epoch   4, Step:    8400, Batch Loss:    45.761143, Lr: 0.000097, Tokens per sec:   2680
2023-03-08 20:43:56,950 - INFO - __main__ - Epoch   4, Step:    8500, Batch Loss:    53.227062, Lr: 0.000097, Tokens per sec:   2982
2023-03-08 20:44:15,154 - INFO - __main__ - Epoch   4, Step:    8600, Batch Loss:    38.464973, Lr: 0.000097, Tokens per sec:   2914
2023-03-08 20:44:34,130 - INFO - __main__ - Epoch   4, Step:    8700, Batch Loss:    43.207390, Lr: 0.000097, Tokens per sec:   2846
2023-03-08 20:44:37,363 - INFO - __main__ - Epoch   4: total training loss 114155.38
2023-03-08 20:44:37,364 - INFO - __main__ - Epoch 5
2023-03-08 20:44:54,548 - INFO - __main__ - Epoch   5, Step:    8800, Batch Loss:    48.857903, Lr: 0.000096, Tokens per sec:   2610
2023-03-08 20:45:14,268 - INFO - __main__ - Epoch   5, Step:    8900, Batch Loss:    57.412441, Lr: 0.000096, Tokens per sec:   2754
2023-03-08 20:45:34,257 - INFO - __main__ - Epoch   5, Step:    9000, Batch Loss:    49.510540, Lr: 0.000096, Tokens per sec:   2700
2023-03-08 20:45:54,266 - INFO - __main__ - Epoch   5, Step:    9100, Batch Loss:    41.860832, Lr: 0.000096, Tokens per sec:   2642
2023-03-08 20:46:14,297 - INFO - __main__ - Epoch   5, Step:    9200, Batch Loss:    58.123341, Lr: 0.000096, Tokens per sec:   2719
2023-03-08 20:46:34,273 - INFO - __main__ - Epoch   5, Step:    9300, Batch Loss:    43.811142, Lr: 0.000096, Tokens per sec:   2659
2023-03-08 20:46:54,154 - INFO - __main__ - Epoch   5, Step:    9400, Batch Loss:    41.367001, Lr: 0.000096, Tokens per sec:   2703
2023-03-08 20:47:13,980 - INFO - __main__ - Epoch   5, Step:    9500, Batch Loss:    41.685169, Lr: 0.000096, Tokens per sec:   2687
2023-03-08 20:47:33,718 - INFO - __main__ - Epoch   5, Step:    9600, Batch Loss:    63.753742, Lr: 0.000096, Tokens per sec:   2701
2023-03-08 20:47:51,877 - INFO - __main__ - Epoch   5, Step:    9700, Batch Loss:    51.437599, Lr: 0.000096, Tokens per sec:   3019
2023-03-08 20:48:09,931 - INFO - __main__ - Epoch   5, Step:    9800, Batch Loss:    36.610527, Lr: 0.000096, Tokens per sec:   2980
2023-03-08 20:48:27,936 - INFO - __main__ - Epoch   5, Step:    9900, Batch Loss:    50.655605, Lr: 0.000096, Tokens per sec:   2939
2023-03-08 20:48:45,952 - INFO - __main__ - Epoch   5, Step:   10000, Batch Loss:    48.248417, Lr: 0.000096, Tokens per sec:   3003
2023-03-08 20:49:05,928 - INFO - __main__ - Epoch   5, Step:   10100, Batch Loss:    54.466240, Lr: 0.000096, Tokens per sec:   2731
2023-03-08 20:49:25,600 - INFO - __main__ - Epoch   5, Step:   10200, Batch Loss:    50.376144, Lr: 0.000096, Tokens per sec:   2756
2023-03-08 20:49:45,627 - INFO - __main__ - Epoch   5, Step:   10300, Batch Loss:    49.249405, Lr: 0.000096, Tokens per sec:   2708
2023-03-08 20:50:05,467 - INFO - __main__ - Epoch   5, Step:   10400, Batch Loss:    55.954170, Lr: 0.000096, Tokens per sec:   2719
2023-03-08 20:50:25,461 - INFO - __main__ - Epoch   5, Step:   10500, Batch Loss:    50.645439, Lr: 0.000096, Tokens per sec:   2720
2023-03-08 20:50:45,452 - INFO - __main__ - Epoch   5, Step:   10600, Batch Loss:    36.216633, Lr: 0.000096, Tokens per sec:   2708
2023-03-08 20:51:05,391 - INFO - __main__ - Epoch   5, Step:   10700, Batch Loss:    47.476353, Lr: 0.000096, Tokens per sec:   2681
2023-03-08 20:51:24,580 - INFO - __main__ - Epoch   5, Step:   10800, Batch Loss:    37.920357, Lr: 0.000096, Tokens per sec:   2815
2023-03-08 20:51:43,375 - INFO - __main__ - Epoch   5: total training loss 105355.56
2023-03-08 20:51:43,376 - INFO - __main__ - Epoch 6
2023-03-08 20:51:44,661 - INFO - __main__ - Epoch   6, Step:   10900, Batch Loss:    52.040760, Lr: 0.000095, Tokens per sec:   2353
2023-03-08 20:52:04,630 - INFO - __main__ - Epoch   6, Step:   11000, Batch Loss:    31.802094, Lr: 0.000095, Tokens per sec:   2743
2023-03-08 20:52:24,606 - INFO - __main__ - Epoch   6, Step:   11100, Batch Loss:    50.885056, Lr: 0.000095, Tokens per sec:   2721
2023-03-08 20:52:44,561 - INFO - __main__ - Epoch   6, Step:   11200, Batch Loss:    42.963699, Lr: 0.000095, Tokens per sec:   2699
2023-03-08 20:53:04,550 - INFO - __main__ - Epoch   6, Step:   11300, Batch Loss:    63.746590, Lr: 0.000095, Tokens per sec:   2689
2023-03-08 20:53:24,535 - INFO - __main__ - Epoch   6, Step:   11400, Batch Loss:    34.599213, Lr: 0.000095, Tokens per sec:   2703
2023-03-08 20:53:44,494 - INFO - __main__ - Epoch   6, Step:   11500, Batch Loss:    32.659698, Lr: 0.000095, Tokens per sec:   2686
2023-03-08 20:54:04,446 - INFO - __main__ - Epoch   6, Step:   11600, Batch Loss:    38.297527, Lr: 0.000095, Tokens per sec:   2663
2023-03-08 20:54:24,473 - INFO - __main__ - Epoch   6, Step:   11700, Batch Loss:    48.917995, Lr: 0.000095, Tokens per sec:   2678
2023-03-08 20:54:44,460 - INFO - __main__ - Epoch   6, Step:   11800, Batch Loss:    36.260830, Lr: 0.000095, Tokens per sec:   2674
2023-03-08 20:55:04,430 - INFO - __main__ - Epoch   6, Step:   11900, Batch Loss:    47.687691, Lr: 0.000095, Tokens per sec:   2678
2023-03-08 20:55:24,382 - INFO - __main__ - Epoch   6, Step:   12000, Batch Loss:    48.493515, Lr: 0.000095, Tokens per sec:   2694
2023-03-08 20:55:44,415 - INFO - __main__ - Epoch   6, Step:   12100, Batch Loss:    40.877232, Lr: 0.000095, Tokens per sec:   2675
2023-03-08 20:56:04,371 - INFO - __main__ - Epoch   6, Step:   12200, Batch Loss:    35.759907, Lr: 0.000095, Tokens per sec:   2715
2023-03-08 20:56:24,321 - INFO - __main__ - Epoch   6, Step:   12300, Batch Loss:    55.749672, Lr: 0.000095, Tokens per sec:   2748
2023-03-08 20:56:43,476 - INFO - __main__ - Epoch   6, Step:   12400, Batch Loss:    37.488796, Lr: 0.000095, Tokens per sec:   2807
2023-03-08 20:57:02,427 - INFO - __main__ - Epoch   6, Step:   12500, Batch Loss:    41.820396, Lr: 0.000095, Tokens per sec:   2811
2023-03-08 20:57:22,427 - INFO - __main__ - Epoch   6, Step:   12600, Batch Loss:    38.172764, Lr: 0.000095, Tokens per sec:   2706
2023-03-08 20:57:42,378 - INFO - __main__ - Epoch   6, Step:   12700, Batch Loss:    53.751644, Lr: 0.000095, Tokens per sec:   2696
2023-03-08 20:58:02,325 - INFO - __main__ - Epoch   6, Step:   12800, Batch Loss:    47.982613, Lr: 0.000095, Tokens per sec:   2679
2023-03-08 20:58:20,901 - INFO - __main__ - Epoch   6, Step:   12900, Batch Loss:    39.821526, Lr: 0.000095, Tokens per sec:   2874
2023-03-08 20:58:40,657 - INFO - __main__ - Epoch   6, Step:   13000, Batch Loss:    44.624191, Lr: 0.000095, Tokens per sec:   2741
2023-03-08 20:58:55,450 - INFO - __main__ - Epoch   6: total training loss 97661.34
2023-03-08 20:58:55,451 - INFO - __main__ - Epoch 7
2023-03-08 20:59:00,932 - INFO - __main__ - Epoch   7, Step:   13100, Batch Loss:    34.428165, Lr: 0.000094, Tokens per sec:   2636
2023-03-08 20:59:20,299 - INFO - __main__ - Epoch   7, Step:   13200, Batch Loss:    41.670330, Lr: 0.000094, Tokens per sec:   2772
2023-03-08 20:59:38,278 - INFO - __main__ - Epoch   7, Step:   13300, Batch Loss:    48.045063, Lr: 0.000094, Tokens per sec:   3029
2023-03-08 20:59:56,252 - INFO - __main__ - Epoch   7, Step:   13400, Batch Loss:    48.474903, Lr: 0.000094, Tokens per sec:   2973
2023-03-08 21:00:14,233 - INFO - __main__ - Epoch   7, Step:   13500, Batch Loss:    39.320736, Lr: 0.000094, Tokens per sec:   3000
2023-03-08 21:00:32,220 - INFO - __main__ - Epoch   7, Step:   13600, Batch Loss:    32.806889, Lr: 0.000094, Tokens per sec:   2969
2023-03-08 21:00:50,203 - INFO - __main__ - Epoch   7, Step:   13700, Batch Loss:    38.756130, Lr: 0.000094, Tokens per sec:   2959
2023-03-08 21:01:10,190 - INFO - __main__ - Epoch   7, Step:   13800, Batch Loss:    54.959419, Lr: 0.000094, Tokens per sec:   2650
2023-03-08 21:01:29,890 - INFO - __main__ - Epoch   7, Step:   13900, Batch Loss:    37.004776, Lr: 0.000094, Tokens per sec:   2765
2023-03-08 21:01:49,922 - INFO - __main__ - Epoch   7, Step:   14000, Batch Loss:    48.962364, Lr: 0.000094, Tokens per sec:   2711
2023-03-08 21:02:09,728 - INFO - __main__ - Epoch   7, Step:   14100, Batch Loss:    40.710888, Lr: 0.000094, Tokens per sec:   2713
2023-03-08 21:02:28,619 - INFO - __main__ - Epoch   7, Step:   14200, Batch Loss:    29.792990, Lr: 0.000094, Tokens per sec:   2881
2023-03-08 21:02:47,699 - INFO - __main__ - Epoch   7, Step:   14300, Batch Loss:    39.638390, Lr: 0.000094, Tokens per sec:   2818
2023-03-08 21:03:05,992 - INFO - __main__ - Epoch   7, Step:   14400, Batch Loss:    47.500237, Lr: 0.000094, Tokens per sec:   2972
2023-03-08 21:03:24,046 - INFO - __main__ - Epoch   7, Step:   14500, Batch Loss:    48.965397, Lr: 0.000094, Tokens per sec:   3030
2023-03-08 21:03:42,344 - INFO - __main__ - Epoch   7, Step:   14600, Batch Loss:    47.382877, Lr: 0.000094, Tokens per sec:   2944
2023-03-08 21:04:02,074 - INFO - __main__ - Epoch   7, Step:   14700, Batch Loss:    40.912758, Lr: 0.000094, Tokens per sec:   2714
2023-03-08 21:04:20,989 - INFO - __main__ - Epoch   7, Step:   14800, Batch Loss:    43.490620, Lr: 0.000094, Tokens per sec:   2836
2023-03-08 21:04:40,983 - INFO - __main__ - Epoch   7, Step:   14900, Batch Loss:    42.751976, Lr: 0.000094, Tokens per sec:   2681
2023-03-08 21:05:00,978 - INFO - __main__ - Epoch   7, Step:   15000, Batch Loss:    36.223354, Lr: 0.000094, Tokens per sec:   2670
2023-03-08 21:05:20,950 - INFO - __main__ - Epoch   7, Step:   15100, Batch Loss:    30.135004, Lr: 0.000094, Tokens per sec:   2671
2023-03-08 21:05:40,681 - INFO - __main__ - Epoch   7, Step:   15200, Batch Loss:    42.352177, Lr: 0.000094, Tokens per sec:   2701
2023-03-08 21:05:50,426 - INFO - __main__ - Epoch   7: total training loss 90848.53
2023-03-08 21:05:50,427 - INFO - __main__ - Epoch 8
2023-03-08 21:06:00,199 - INFO - __main__ - Epoch   8, Step:   15300, Batch Loss:    43.262421, Lr: 0.000093, Tokens per sec:   2537
2023-03-08 21:06:20,306 - INFO - __main__ - Epoch   8, Step:   15400, Batch Loss:    42.323071, Lr: 0.000093, Tokens per sec:   2720
2023-03-08 21:06:40,284 - INFO - __main__ - Epoch   8, Step:   15500, Batch Loss:    56.369930, Lr: 0.000093, Tokens per sec:   2695
2023-03-08 21:07:00,262 - INFO - __main__ - Epoch   8, Step:   15600, Batch Loss:    43.638142, Lr: 0.000093, Tokens per sec:   2724
2023-03-08 21:07:20,289 - INFO - __main__ - Epoch   8, Step:   15700, Batch Loss:    40.392151, Lr: 0.000093, Tokens per sec:   2717
2023-03-08 21:07:40,311 - INFO - __main__ - Epoch   8, Step:   15800, Batch Loss:    32.108162, Lr: 0.000093, Tokens per sec:   2680
2023-03-08 21:08:00,039 - INFO - __main__ - Epoch   8, Step:   15900, Batch Loss:    54.355743, Lr: 0.000093, Tokens per sec:   2726
2023-03-08 21:08:19,995 - INFO - __main__ - Epoch   8, Step:   16000, Batch Loss:    37.055050, Lr: 0.000093, Tokens per sec:   2666
2023-03-08 21:08:39,982 - INFO - __main__ - Epoch   8, Step:   16100, Batch Loss:    41.423237, Lr: 0.000093, Tokens per sec:   2707
2023-03-08 21:08:59,903 - INFO - __main__ - Epoch   8, Step:   16200, Batch Loss:    39.772465, Lr: 0.000093, Tokens per sec:   2724
2023-03-08 21:09:19,921 - INFO - __main__ - Epoch   8, Step:   16300, Batch Loss:    35.561859, Lr: 0.000093, Tokens per sec:   2640
2023-03-08 21:09:39,987 - INFO - __main__ - Epoch   8, Step:   16400, Batch Loss:    29.509302, Lr: 0.000093, Tokens per sec:   2706
2023-03-08 21:10:00,016 - INFO - __main__ - Epoch   8, Step:   16500, Batch Loss:    44.035141, Lr: 0.000093, Tokens per sec:   2698
2023-03-08 21:10:19,992 - INFO - __main__ - Epoch   8, Step:   16600, Batch Loss:    46.223419, Lr: 0.000093, Tokens per sec:   2683
2023-03-08 21:10:40,000 - INFO - __main__ - Epoch   8, Step:   16700, Batch Loss:    45.915150, Lr: 0.000093, Tokens per sec:   2649
2023-03-08 21:10:59,971 - INFO - __main__ - Epoch   8, Step:   16800, Batch Loss:    41.297146, Lr: 0.000093, Tokens per sec:   2704
2023-03-08 21:11:20,035 - INFO - __main__ - Epoch   8, Step:   16900, Batch Loss:    31.816198, Lr: 0.000093, Tokens per sec:   2688
2023-03-08 21:11:39,752 - INFO - __main__ - Epoch   8, Step:   17000, Batch Loss:    45.253395, Lr: 0.000093, Tokens per sec:   2754
2023-03-08 21:11:59,775 - INFO - __main__ - Epoch   8, Step:   17100, Batch Loss:    50.476318, Lr: 0.000093, Tokens per sec:   2669
2023-03-08 21:12:19,798 - INFO - __main__ - Epoch   8, Step:   17200, Batch Loss:    34.094814, Lr: 0.000093, Tokens per sec:   2697
2023-03-08 21:12:39,861 - INFO - __main__ - Epoch   8, Step:   17300, Batch Loss:    43.261528, Lr: 0.000093, Tokens per sec:   2657
2023-03-08 21:12:59,729 - INFO - __main__ - Epoch   8, Step:   17400, Batch Loss:    46.745983, Lr: 0.000093, Tokens per sec:   2731
2023-03-08 21:13:06,122 - INFO - __main__ - Epoch   8: total training loss 84692.28
2023-03-08 21:13:06,123 - INFO - __main__ - Epoch 9
2023-03-08 21:13:19,978 - INFO - __main__ - Epoch   9, Step:   17500, Batch Loss:    36.612961, Lr: 0.000092, Tokens per sec:   2628
2023-03-08 21:13:39,711 - INFO - __main__ - Epoch   9, Step:   17600, Batch Loss:    30.575031, Lr: 0.000092, Tokens per sec:   2725
2023-03-08 21:13:59,739 - INFO - __main__ - Epoch   9, Step:   17700, Batch Loss:    32.691826, Lr: 0.000092, Tokens per sec:   2724
2023-03-08 21:14:19,722 - INFO - __main__ - Epoch   9, Step:   17800, Batch Loss:    26.616438, Lr: 0.000092, Tokens per sec:   2716
2023-03-08 21:14:39,192 - INFO - __main__ - Epoch   9, Step:   17900, Batch Loss:    33.270542, Lr: 0.000092, Tokens per sec:   2742
2023-03-08 21:14:57,708 - INFO - __main__ - Epoch   9, Step:   18000, Batch Loss:    34.620182, Lr: 0.000092, Tokens per sec:   2901
2023-03-08 21:15:15,905 - INFO - __main__ - Epoch   9, Step:   18100, Batch Loss:    41.945107, Lr: 0.000092, Tokens per sec:   2973
2023-03-08 21:15:35,910 - INFO - __main__ - Epoch   9, Step:   18200, Batch Loss:    25.038666, Lr: 0.000092, Tokens per sec:   2761
2023-03-08 21:15:55,959 - INFO - __main__ - Epoch   9, Step:   18300, Batch Loss:    21.761707, Lr: 0.000092, Tokens per sec:   2685
2023-03-08 21:16:16,039 - INFO - __main__ - Epoch   9, Step:   18400, Batch Loss:    42.756874, Lr: 0.000092, Tokens per sec:   2660
2023-03-08 21:16:35,131 - INFO - __main__ - Epoch   9, Step:   18500, Batch Loss:    42.026592, Lr: 0.000092, Tokens per sec:   2837
2023-03-08 21:16:54,807 - INFO - __main__ - Epoch   9, Step:   18600, Batch Loss:    53.819733, Lr: 0.000092, Tokens per sec:   2732
2023-03-08 21:17:14,798 - INFO - __main__ - Epoch   9, Step:   18700, Batch Loss:    21.653999, Lr: 0.000092, Tokens per sec:   2708
2023-03-08 21:17:34,752 - INFO - __main__ - Epoch   9, Step:   18800, Batch Loss:    43.266502, Lr: 0.000092, Tokens per sec:   2685
2023-03-08 21:17:54,089 - INFO - __main__ - Epoch   9, Step:   18900, Batch Loss:    36.895897, Lr: 0.000092, Tokens per sec:   2772
2023-03-08 21:18:12,204 - INFO - __main__ - Epoch   9, Step:   19000, Batch Loss:    42.137974, Lr: 0.000092, Tokens per sec:   2964
2023-03-08 21:18:31,642 - INFO - __main__ - Epoch   9, Step:   19100, Batch Loss:    39.756184, Lr: 0.000092, Tokens per sec:   2795
2023-03-08 21:18:51,629 - INFO - __main__ - Epoch   9, Step:   19200, Batch Loss:    42.196484, Lr: 0.000092, Tokens per sec:   2674
2023-03-08 21:19:11,743 - INFO - __main__ - Epoch   9, Step:   19300, Batch Loss:    27.495745, Lr: 0.000092, Tokens per sec:   2641
2023-03-08 21:19:31,694 - INFO - __main__ - Epoch   9, Step:   19400, Batch Loss:    35.571693, Lr: 0.000092, Tokens per sec:   2714
2023-03-08 21:19:50,682 - INFO - __main__ - Epoch   9, Step:   19500, Batch Loss:    33.454723, Lr: 0.000092, Tokens per sec:   2801
2023-03-08 21:20:10,523 - INFO - __main__ - Epoch   9, Step:   19600, Batch Loss:    34.789639, Lr: 0.000092, Tokens per sec:   2695
2023-03-08 21:20:12,787 - INFO - __main__ - Epoch   9: total training loss 79059.48
2023-03-08 21:20:12,788 - INFO - __main__ - Epoch 10
2023-03-08 21:20:30,762 - INFO - __main__ - Epoch  10, Step:   19700, Batch Loss:    32.959179, Lr: 0.000091, Tokens per sec:   2661
2023-03-08 21:20:50,756 - INFO - __main__ - Epoch  10, Step:   19800, Batch Loss:    37.209919, Lr: 0.000091, Tokens per sec:   2714
2023-03-08 21:21:10,477 - INFO - __main__ - Epoch  10, Step:   19900, Batch Loss:    29.934195, Lr: 0.000091, Tokens per sec:   2780
2023-03-08 21:21:30,237 - INFO - __main__ - Epoch  10, Step:   20000, Batch Loss:    30.433018, Lr: 0.000091, Tokens per sec:   2691
2023-03-08 21:21:50,024 - INFO - __main__ - Epoch  10, Step:   20100, Batch Loss:    36.376743, Lr: 0.000091, Tokens per sec:   2754
2023-03-08 21:22:10,056 - INFO - __main__ - Epoch  10, Step:   20200, Batch Loss:    36.425457, Lr: 0.000091, Tokens per sec:   2706
2023-03-08 21:22:29,842 - INFO - __main__ - Epoch  10, Step:   20300, Batch Loss:    38.631092, Lr: 0.000091, Tokens per sec:   2743
2023-03-08 21:22:49,984 - INFO - __main__ - Epoch  10, Step:   20400, Batch Loss:    34.264530, Lr: 0.000091, Tokens per sec:   2621
2023-03-08 21:23:09,291 - INFO - __main__ - Epoch  10, Step:   20500, Batch Loss:    23.428501, Lr: 0.000091, Tokens per sec:   2812
2023-03-08 21:23:29,284 - INFO - __main__ - Epoch  10, Step:   20600, Batch Loss:    42.368950, Lr: 0.000091, Tokens per sec:   2721
2023-03-08 21:23:49,314 - INFO - __main__ - Epoch  10, Step:   20700, Batch Loss:    41.786514, Lr: 0.000091, Tokens per sec:   2668
2023-03-08 21:24:09,352 - INFO - __main__ - Epoch  10, Step:   20800, Batch Loss:    43.363861, Lr: 0.000091, Tokens per sec:   2664
2023-03-08 21:24:29,119 - INFO - __main__ - Epoch  10, Step:   20900, Batch Loss:    30.994913, Lr: 0.000091, Tokens per sec:   2709
2023-03-08 21:24:49,044 - INFO - __main__ - Epoch  10, Step:   21000, Batch Loss:    30.856798, Lr: 0.000091, Tokens per sec:   2724
2023-03-08 21:25:08,850 - INFO - __main__ - Epoch  10, Step:   21100, Batch Loss:    39.365261, Lr: 0.000091, Tokens per sec:   2677
2023-03-08 21:25:28,498 - INFO - __main__ - Epoch  10, Step:   21200, Batch Loss:    24.606544, Lr: 0.000091, Tokens per sec:   2720
2023-03-08 21:25:48,561 - INFO - __main__ - Epoch  10, Step:   21300, Batch Loss:    34.631664, Lr: 0.000091, Tokens per sec:   2663
2023-03-08 21:26:08,629 - INFO - __main__ - Epoch  10, Step:   21400, Batch Loss:    41.017963, Lr: 0.000091, Tokens per sec:   2696
2023-03-08 21:26:28,678 - INFO - __main__ - Epoch  10, Step:   21500, Batch Loss:    36.853836, Lr: 0.000091, Tokens per sec:   2678
2023-03-08 21:26:48,736 - INFO - __main__ - Epoch  10, Step:   21600, Batch Loss:    15.654819, Lr: 0.000091, Tokens per sec:   2655
2023-03-08 21:27:08,733 - INFO - __main__ - Epoch  10, Step:   21700, Batch Loss:    38.280563, Lr: 0.000091, Tokens per sec:   2748
2023-03-08 21:27:26,418 - INFO - __main__ - Epoch  10: total training loss 73920.25
2023-03-08 21:27:26,419 - INFO - __main__ - Epoch 11
2023-03-08 21:27:28,743 - INFO - __main__ - Epoch  11, Step:   21800, Batch Loss:    26.452251, Lr: 0.000090, Tokens per sec:   2230
2023-03-08 21:27:48,836 - INFO - __main__ - Epoch  11, Step:   21900, Batch Loss:    28.486479, Lr: 0.000090, Tokens per sec:   2704
2023-03-08 21:28:08,915 - INFO - __main__ - Epoch  11, Step:   22000, Batch Loss:    33.992905, Lr: 0.000090, Tokens per sec:   2619
2023-03-08 21:28:29,125 - INFO - __main__ - Epoch  11, Step:   22100, Batch Loss:    35.507622, Lr: 0.000090, Tokens per sec:   2666
2023-03-08 21:28:49,319 - INFO - __main__ - Epoch  11, Step:   22200, Batch Loss:    24.555248, Lr: 0.000090, Tokens per sec:   2658
2023-03-08 21:29:09,283 - INFO - __main__ - Epoch  11, Step:   22300, Batch Loss:    26.940113, Lr: 0.000090, Tokens per sec:   2701
2023-03-08 21:29:29,326 - INFO - __main__ - Epoch  11, Step:   22400, Batch Loss:    28.993940, Lr: 0.000090, Tokens per sec:   2684
2023-03-08 21:29:49,202 - INFO - __main__ - Epoch  11, Step:   22500, Batch Loss:    29.935328, Lr: 0.000090, Tokens per sec:   2735
2023-03-08 21:30:08,869 - INFO - __main__ - Epoch  11, Step:   22600, Batch Loss:    26.792183, Lr: 0.000090, Tokens per sec:   2719
2023-03-08 21:30:29,005 - INFO - __main__ - Epoch  11, Step:   22700, Batch Loss:    35.327690, Lr: 0.000090, Tokens per sec:   2713
2023-03-08 21:30:48,619 - INFO - __main__ - Epoch  11, Step:   22800, Batch Loss:    30.375389, Lr: 0.000090, Tokens per sec:   2796
2023-03-08 21:31:08,608 - INFO - __main__ - Epoch  11, Step:   22900, Batch Loss:    37.542774, Lr: 0.000090, Tokens per sec:   2653
2023-03-08 21:31:28,326 - INFO - __main__ - Epoch  11, Step:   23000, Batch Loss:    24.027830, Lr: 0.000090, Tokens per sec:   2752
2023-03-08 21:31:48,277 - INFO - __main__ - Epoch  11, Step:   23100, Batch Loss:    24.515066, Lr: 0.000090, Tokens per sec:   2692
2023-03-08 21:32:08,477 - INFO - __main__ - Epoch  11, Step:   23200, Batch Loss:    36.522758, Lr: 0.000090, Tokens per sec:   2681
2023-03-08 21:32:28,510 - INFO - __main__ - Epoch  11, Step:   23300, Batch Loss:    31.046669, Lr: 0.000090, Tokens per sec:   2657
2023-03-08 21:32:48,340 - INFO - __main__ - Epoch  11, Step:   23400, Batch Loss:    33.601032, Lr: 0.000090, Tokens per sec:   2696
2023-03-08 21:33:07,827 - INFO - __main__ - Epoch  11, Step:   23500, Batch Loss:    33.816837, Lr: 0.000090, Tokens per sec:   2774
2023-03-08 21:33:27,308 - INFO - __main__ - Epoch  11, Step:   23600, Batch Loss:    28.377895, Lr: 0.000090, Tokens per sec:   2791
2023-03-08 21:33:47,280 - INFO - __main__ - Epoch  11, Step:   23700, Batch Loss:    25.837059, Lr: 0.000090, Tokens per sec:   2637
2023-03-08 21:34:07,279 - INFO - __main__ - Epoch  11, Step:   23800, Batch Loss:    34.304607, Lr: 0.000090, Tokens per sec:   2679
2023-03-08 21:34:27,292 - INFO - __main__ - Epoch  11, Step:   23900, Batch Loss:    33.932297, Lr: 0.000090, Tokens per sec:   2728
2023-03-08 21:34:41,182 - INFO - __main__ - Epoch  11: total training loss 69143.32
2023-03-08 21:34:41,183 - INFO - __main__ - Epoch 12
2023-03-08 21:34:47,712 - INFO - __main__ - Epoch  12, Step:   24000, Batch Loss:    23.860292, Lr: 0.000090, Tokens per sec:   2543
2023-03-08 21:35:07,748 - INFO - __main__ - Epoch  12, Step:   24100, Batch Loss:    31.514936, Lr: 0.000090, Tokens per sec:   2670
2023-03-08 21:35:27,775 - INFO - __main__ - Epoch  12, Step:   24200, Batch Loss:    29.839901, Lr: 0.000090, Tokens per sec:   2650
2023-03-08 21:35:46,387 - INFO - __main__ - Epoch  12, Step:   24300, Batch Loss:    32.859547, Lr: 0.000090, Tokens per sec:   2886
2023-03-08 21:36:06,235 - INFO - __main__ - Epoch  12, Step:   24400, Batch Loss:    29.341002, Lr: 0.000090, Tokens per sec:   2707
2023-03-08 21:36:26,193 - INFO - __main__ - Epoch  12, Step:   24500, Batch Loss:    27.909874, Lr: 0.000090, Tokens per sec:   2715
2023-03-08 21:36:46,188 - INFO - __main__ - Epoch  12, Step:   24600, Batch Loss:    22.132298, Lr: 0.000090, Tokens per sec:   2664
2023-03-08 21:37:05,717 - INFO - __main__ - Epoch  12, Step:   24700, Batch Loss:    23.717800, Lr: 0.000090, Tokens per sec:   2763
2023-03-08 21:37:25,776 - INFO - __main__ - Epoch  12, Step:   24800, Batch Loss:    31.694323, Lr: 0.000090, Tokens per sec:   2693
2023-03-08 21:37:45,775 - INFO - __main__ - Epoch  12, Step:   24900, Batch Loss:    33.630127, Lr: 0.000090, Tokens per sec:   2715
2023-03-08 21:38:05,799 - INFO - __main__ - Epoch  12, Step:   25000, Batch Loss:    15.079338, Lr: 0.000090, Tokens per sec:   2667
2023-03-08 21:38:24,951 - INFO - __main__ - Epoch  12, Step:   25100, Batch Loss:    33.751839, Lr: 0.000090, Tokens per sec:   2805
2023-03-08 21:38:45,047 - INFO - __main__ - Epoch  12, Step:   25200, Batch Loss:    31.018627, Lr: 0.000090, Tokens per sec:   2713
2023-03-08 21:39:05,043 - INFO - __main__ - Epoch  12, Step:   25300, Batch Loss:    24.338976, Lr: 0.000090, Tokens per sec:   2656
2023-03-08 21:39:24,993 - INFO - __main__ - Epoch  12, Step:   25400, Batch Loss:    24.863012, Lr: 0.000090, Tokens per sec:   2736
2023-03-08 21:39:44,963 - INFO - __main__ - Epoch  12, Step:   25500, Batch Loss:    22.948099, Lr: 0.000090, Tokens per sec:   2672
2023-03-08 21:40:04,958 - INFO - __main__ - Epoch  12, Step:   25600, Batch Loss:    29.296335, Lr: 0.000090, Tokens per sec:   2667
2023-03-08 21:40:24,919 - INFO - __main__ - Epoch  12, Step:   25700, Batch Loss:    40.577251, Lr: 0.000090, Tokens per sec:   2713
2023-03-08 21:40:44,731 - INFO - __main__ - Epoch  12, Step:   25800, Batch Loss:    31.477701, Lr: 0.000090, Tokens per sec:   2736
2023-03-08 21:41:04,784 - INFO - __main__ - Epoch  12, Step:   25900, Batch Loss:    35.386005, Lr: 0.000090, Tokens per sec:   2691
2023-03-08 21:41:24,757 - INFO - __main__ - Epoch  12, Step:   26000, Batch Loss:    24.992453, Lr: 0.000090, Tokens per sec:   2681
2023-03-08 21:41:44,863 - INFO - __main__ - Epoch  12, Step:   26100, Batch Loss:    21.654005, Lr: 0.000090, Tokens per sec:   2731
2023-03-08 21:41:54,474 - INFO - __main__ - Epoch  12: total training loss 64807.64
2023-03-08 21:41:54,475 - INFO - __main__ - Epoch 13
2023-03-08 21:42:04,754 - INFO - __main__ - Epoch  13, Step:   26200, Batch Loss:    19.094494, Lr: 0.000089, Tokens per sec:   2733
2023-03-08 21:42:24,840 - INFO - __main__ - Epoch  13, Step:   26300, Batch Loss:    22.597229, Lr: 0.000089, Tokens per sec:   2669
2023-03-08 21:42:44,013 - INFO - __main__ - Epoch  13, Step:   26400, Batch Loss:    29.651110, Lr: 0.000089, Tokens per sec:   2807
2023-03-08 21:43:04,161 - INFO - __main__ - Epoch  13, Step:   26500, Batch Loss:    20.863699, Lr: 0.000089, Tokens per sec:   2658
2023-03-08 21:43:24,308 - INFO - __main__ - Epoch  13, Step:   26600, Batch Loss:    30.218334, Lr: 0.000089, Tokens per sec:   2675
2023-03-08 21:43:44,467 - INFO - __main__ - Epoch  13, Step:   26700, Batch Loss:    21.844576, Lr: 0.000089, Tokens per sec:   2693
2023-03-08 21:44:04,257 - INFO - __main__ - Epoch  13, Step:   26800, Batch Loss:    30.826445, Lr: 0.000089, Tokens per sec:   2749
2023-03-08 21:44:24,225 - INFO - __main__ - Epoch  13, Step:   26900, Batch Loss:    28.293360, Lr: 0.000089, Tokens per sec:   2689
2023-03-08 21:44:44,300 - INFO - __main__ - Epoch  13, Step:   27000, Batch Loss:    29.145828, Lr: 0.000089, Tokens per sec:   2700
2023-03-08 21:45:04,343 - INFO - __main__ - Epoch  13, Step:   27100, Batch Loss:    22.897026, Lr: 0.000089, Tokens per sec:   2710
2023-03-08 21:45:24,395 - INFO - __main__ - Epoch  13, Step:   27200, Batch Loss:    27.026834, Lr: 0.000089, Tokens per sec:   2706
2023-03-08 21:45:43,889 - INFO - __main__ - Epoch  13, Step:   27300, Batch Loss:    35.064796, Lr: 0.000089, Tokens per sec:   2757
2023-03-08 21:46:02,772 - INFO - __main__ - Epoch  13, Step:   27400, Batch Loss:    32.742165, Lr: 0.000089, Tokens per sec:   2822
2023-03-08 21:46:22,421 - INFO - __main__ - Epoch  13, Step:   27500, Batch Loss:    24.476881, Lr: 0.000089, Tokens per sec:   2748
2023-03-08 21:46:42,053 - INFO - __main__ - Epoch  13, Step:   27600, Batch Loss:    30.061575, Lr: 0.000089, Tokens per sec:   2743
2023-03-08 21:47:01,813 - INFO - __main__ - Epoch  13, Step:   27700, Batch Loss:    26.442484, Lr: 0.000089, Tokens per sec:   2742
2023-03-08 21:47:21,606 - INFO - __main__ - Epoch  13, Step:   27800, Batch Loss:    29.061741, Lr: 0.000089, Tokens per sec:   2743
2023-03-08 21:47:41,656 - INFO - __main__ - Epoch  13, Step:   27900, Batch Loss:    22.120993, Lr: 0.000089, Tokens per sec:   2688
2023-03-08 21:48:01,265 - INFO - __main__ - Epoch  13, Step:   28000, Batch Loss:    21.985353, Lr: 0.000089, Tokens per sec:   2693
2023-03-08 21:48:19,984 - INFO - __main__ - Epoch  13, Step:   28100, Batch Loss:    25.725527, Lr: 0.000089, Tokens per sec:   2806
2023-03-08 21:48:39,816 - INFO - __main__ - Epoch  13, Step:   28200, Batch Loss:    32.014465, Lr: 0.000089, Tokens per sec:   2703
2023-03-08 21:48:58,813 - INFO - __main__ - Epoch  13, Step:   28300, Batch Loss:    23.957579, Lr: 0.000089, Tokens per sec:   2836
2023-03-08 21:49:04,288 - INFO - __main__ - Epoch  13: total training loss 60672.00
2023-03-08 21:49:04,289 - INFO - __main__ - Epoch 14
2023-03-08 21:49:18,954 - INFO - __main__ - Epoch  14, Step:   28400, Batch Loss:    23.571726, Lr: 0.000088, Tokens per sec:   2702
2023-03-08 21:49:39,100 - INFO - __main__ - Epoch  14, Step:   28500, Batch Loss:    30.746790, Lr: 0.000088, Tokens per sec:   2689
2023-03-08 21:49:58,801 - INFO - __main__ - Epoch  14, Step:   28600, Batch Loss:    28.466608, Lr: 0.000088, Tokens per sec:   2728
2023-03-08 21:50:18,648 - INFO - __main__ - Epoch  14, Step:   28700, Batch Loss:    24.551708, Lr: 0.000088, Tokens per sec:   2701
2023-03-08 21:50:37,319 - INFO - __main__ - Epoch  14, Step:   28800, Batch Loss:    27.922466, Lr: 0.000088, Tokens per sec:   2875
2023-03-08 21:50:56,175 - INFO - __main__ - Epoch  14, Step:   28900, Batch Loss:    31.041353, Lr: 0.000088, Tokens per sec:   2827
2023-03-08 21:51:15,831 - INFO - __main__ - Epoch  14, Step:   29000, Batch Loss:    26.752979, Lr: 0.000088, Tokens per sec:   2766
2023-03-08 21:51:35,858 - INFO - __main__ - Epoch  14, Step:   29100, Batch Loss:    21.266142, Lr: 0.000088, Tokens per sec:   2682
2023-03-08 21:51:55,516 - INFO - __main__ - Epoch  14, Step:   29200, Batch Loss:    24.732437, Lr: 0.000088, Tokens per sec:   2751
2023-03-08 21:52:15,362 - INFO - __main__ - Epoch  14, Step:   29300, Batch Loss:    19.421682, Lr: 0.000088, Tokens per sec:   2687
2023-03-08 21:52:35,371 - INFO - __main__ - Epoch  14, Step:   29400, Batch Loss:    34.397957, Lr: 0.000088, Tokens per sec:   2686
2023-03-08 21:52:54,277 - INFO - __main__ - Epoch  14, Step:   29500, Batch Loss:    27.374994, Lr: 0.000088, Tokens per sec:   2859
2023-03-08 21:53:13,692 - INFO - __main__ - Epoch  14, Step:   29600, Batch Loss:    25.146408, Lr: 0.000088, Tokens per sec:   2782
2023-03-08 21:53:31,955 - INFO - __main__ - Epoch  14, Step:   29700, Batch Loss:    29.714514, Lr: 0.000088, Tokens per sec:   2921
2023-03-08 21:53:50,724 - INFO - __main__ - Epoch  14, Step:   29800, Batch Loss:    28.150333, Lr: 0.000088, Tokens per sec:   2846
2023-03-08 21:54:10,073 - INFO - __main__ - Epoch  14, Step:   29900, Batch Loss:    31.023481, Lr: 0.000088, Tokens per sec:   2826
2023-03-08 21:54:29,390 - INFO - __main__ - Epoch  14, Step:   30000, Batch Loss:    22.155254, Lr: 0.000088, Tokens per sec:   2746
2023-03-08 21:54:49,384 - INFO - __main__ - Epoch  14, Step:   30100, Batch Loss:    28.431332, Lr: 0.000088, Tokens per sec:   2727
2023-03-08 21:55:08,476 - INFO - __main__ - Epoch  14, Step:   30200, Batch Loss:    26.138498, Lr: 0.000088, Tokens per sec:   2853
2023-03-08 21:55:28,493 - INFO - __main__ - Epoch  14, Step:   30300, Batch Loss:    25.143150, Lr: 0.000088, Tokens per sec:   2689
2023-03-08 21:55:48,261 - INFO - __main__ - Epoch  14, Step:   30400, Batch Loss:    28.659935, Lr: 0.000088, Tokens per sec:   2708
2023-03-08 21:56:07,264 - INFO - __main__ - Epoch  14, Step:   30500, Batch Loss:    32.430801, Lr: 0.000088, Tokens per sec:   2815
2023-03-08 21:56:08,519 - INFO - __main__ - Epoch  14: total training loss 56936.03
2023-03-08 21:56:08,521 - INFO - __main__ - Epoch 15
2023-03-08 21:56:27,723 - INFO - __main__ - Epoch  15, Step:   30600, Batch Loss:    21.715431, Lr: 0.000087, Tokens per sec:   2620
2023-03-08 21:56:47,683 - INFO - __main__ - Epoch  15, Step:   30700, Batch Loss:    22.111124, Lr: 0.000087, Tokens per sec:   2700
2023-03-08 21:57:07,537 - INFO - __main__ - Epoch  15, Step:   30800, Batch Loss:    13.211436, Lr: 0.000087, Tokens per sec:   2658
2023-03-08 21:57:27,586 - INFO - __main__ - Epoch  15, Step:   30900, Batch Loss:    23.570663, Lr: 0.000087, Tokens per sec:   2634
2023-03-08 21:57:47,164 - INFO - __main__ - Epoch  15, Step:   31000, Batch Loss:    26.253983, Lr: 0.000087, Tokens per sec:   2776
2023-03-08 21:58:07,227 - INFO - __main__ - Epoch  15, Step:   31100, Batch Loss:    24.296671, Lr: 0.000087, Tokens per sec:   2706
2023-03-08 21:58:27,153 - INFO - __main__ - Epoch  15, Step:   31200, Batch Loss:    22.783379, Lr: 0.000087, Tokens per sec:   2724
2023-03-08 21:58:47,089 - INFO - __main__ - Epoch  15, Step:   31300, Batch Loss:    36.028076, Lr: 0.000087, Tokens per sec:   2699
2023-03-08 21:59:07,023 - INFO - __main__ - Epoch  15, Step:   31400, Batch Loss:    27.069912, Lr: 0.000087, Tokens per sec:   2712
2023-03-08 21:59:26,981 - INFO - __main__ - Epoch  15, Step:   31500, Batch Loss:    24.591286, Lr: 0.000087, Tokens per sec:   2707
2023-03-08 21:59:46,990 - INFO - __main__ - Epoch  15, Step:   31600, Batch Loss:    32.575947, Lr: 0.000087, Tokens per sec:   2664
2023-03-08 22:00:06,720 - INFO - __main__ - Epoch  15, Step:   31700, Batch Loss:    35.360939, Lr: 0.000087, Tokens per sec:   2734
2023-03-08 22:00:26,234 - INFO - __main__ - Epoch  15, Step:   31800, Batch Loss:    28.710623, Lr: 0.000087, Tokens per sec:   2747
2023-03-08 22:00:46,268 - INFO - __main__ - Epoch  15, Step:   31900, Batch Loss:    25.995304, Lr: 0.000087, Tokens per sec:   2693
2023-03-08 22:01:06,341 - INFO - __main__ - Epoch  15, Step:   32000, Batch Loss:    17.129953, Lr: 0.000087, Tokens per sec:   2703
2023-03-08 22:01:26,407 - INFO - __main__ - Epoch  15, Step:   32100, Batch Loss:    19.993109, Lr: 0.000087, Tokens per sec:   2674
2023-03-08 22:01:45,290 - INFO - __main__ - Epoch  15, Step:   32200, Batch Loss:    29.086992, Lr: 0.000087, Tokens per sec:   2874
2023-03-08 22:02:04,735 - INFO - __main__ - Epoch  15, Step:   32300, Batch Loss:    20.164919, Lr: 0.000087, Tokens per sec:   2763
2023-03-08 22:02:23,539 - INFO - __main__ - Epoch  15, Step:   32400, Batch Loss:    23.975761, Lr: 0.000087, Tokens per sec:   2888
2023-03-08 22:02:43,560 - INFO - __main__ - Epoch  15, Step:   32500, Batch Loss:    23.879753, Lr: 0.000087, Tokens per sec:   2710
2023-03-08 22:03:03,276 - INFO - __main__ - Epoch  15, Step:   32600, Batch Loss:    28.770269, Lr: 0.000087, Tokens per sec:   2742
2023-03-08 22:03:20,372 - INFO - __main__ - Epoch  15: total training loss 53486.77
2023-03-08 22:03:20,374 - INFO - __main__ - Epoch 16
2023-03-08 22:03:23,699 - INFO - __main__ - Epoch  16, Step:   32700, Batch Loss:    22.140795, Lr: 0.000086, Tokens per sec:   2497
2023-03-08 22:03:43,780 - INFO - __main__ - Epoch  16, Step:   32800, Batch Loss:    24.515795, Lr: 0.000086, Tokens per sec:   2674
2023-03-08 22:04:03,951 - INFO - __main__ - Epoch  16, Step:   32900, Batch Loss:    20.714428, Lr: 0.000086, Tokens per sec:   2673
2023-03-08 22:04:23,929 - INFO - __main__ - Epoch  16, Step:   33000, Batch Loss:    31.899862, Lr: 0.000086, Tokens per sec:   2710
2023-03-08 22:04:44,042 - INFO - __main__ - Epoch  16, Step:   33100, Batch Loss:    25.079500, Lr: 0.000086, Tokens per sec:   2662
2023-03-08 22:05:03,811 - INFO - __main__ - Epoch  16, Step:   33200, Batch Loss:    20.476603, Lr: 0.000086, Tokens per sec:   2749
2023-03-08 22:05:23,405 - INFO - __main__ - Epoch  16, Step:   33300, Batch Loss:    23.500999, Lr: 0.000086, Tokens per sec:   2701
2023-03-08 22:05:42,819 - INFO - __main__ - Epoch  16, Step:   33400, Batch Loss:    22.295918, Lr: 0.000086, Tokens per sec:   2772
2023-03-08 22:06:02,515 - INFO - __main__ - Epoch  16, Step:   33500, Batch Loss:    20.091129, Lr: 0.000086, Tokens per sec:   2731
2023-03-08 22:06:21,929 - INFO - __main__ - Epoch  16, Step:   33600, Batch Loss:    24.465261, Lr: 0.000086, Tokens per sec:   2764
2023-03-08 22:06:40,558 - INFO - __main__ - Epoch  16, Step:   33700, Batch Loss:    20.497269, Lr: 0.000086, Tokens per sec:   2876
2023-03-08 22:07:00,170 - INFO - __main__ - Epoch  16, Step:   33800, Batch Loss:    22.180717, Lr: 0.000086, Tokens per sec:   2750
2023-03-08 22:07:20,246 - INFO - __main__ - Epoch  16, Step:   33900, Batch Loss:    32.762833, Lr: 0.000086, Tokens per sec:   2717
2023-03-08 22:07:39,798 - INFO - __main__ - Epoch  16, Step:   34000, Batch Loss:    15.568074, Lr: 0.000086, Tokens per sec:   2681
2023-03-08 22:07:58,738 - INFO - __main__ - Epoch  16, Step:   34100, Batch Loss:    23.821058, Lr: 0.000086, Tokens per sec:   2863
2023-03-08 22:08:17,887 - INFO - __main__ - Epoch  16, Step:   34200, Batch Loss:    25.024626, Lr: 0.000086, Tokens per sec:   2835
2023-03-08 22:08:36,223 - INFO - __main__ - Epoch  16, Step:   34300, Batch Loss:    33.458820, Lr: 0.000086, Tokens per sec:   2896
2023-03-08 22:08:56,246 - INFO - __main__ - Epoch  16, Step:   34400, Batch Loss:    20.119972, Lr: 0.000086, Tokens per sec:   2684
2023-03-08 22:09:16,421 - INFO - __main__ - Epoch  16, Step:   34500, Batch Loss:    20.228855, Lr: 0.000086, Tokens per sec:   2724
2023-03-08 22:09:36,301 - INFO - __main__ - Epoch  16, Step:   34600, Batch Loss:    24.160339, Lr: 0.000086, Tokens per sec:   2723
2023-03-08 22:09:55,708 - INFO - __main__ - Epoch  16, Step:   34700, Batch Loss:    19.689270, Lr: 0.000086, Tokens per sec:   2787
2023-03-08 22:10:15,074 - INFO - __main__ - Epoch  16, Step:   34800, Batch Loss:    16.700609, Lr: 0.000086, Tokens per sec:   2777
2023-03-08 22:10:27,231 - INFO - __main__ - Epoch  16: total training loss 50220.23
2023-03-08 22:10:27,232 - INFO - __main__ - Epoch 17
2023-03-08 22:10:34,761 - INFO - __main__ - Epoch  17, Step:   34900, Batch Loss:    15.800947, Lr: 0.000085, Tokens per sec:   2574
2023-03-08 22:10:53,579 - INFO - __main__ - Epoch  17, Step:   35000, Batch Loss:    16.395355, Lr: 0.000085, Tokens per sec:   2831
2023-03-08 22:11:12,714 - INFO - __main__ - Epoch  17, Step:   35100, Batch Loss:    13.101055, Lr: 0.000085, Tokens per sec:   2769
2023-03-08 22:11:31,907 - INFO - __main__ - Epoch  17, Step:   35200, Batch Loss:    16.600939, Lr: 0.000085, Tokens per sec:   2817
2023-03-08 22:11:51,461 - INFO - __main__ - Epoch  17, Step:   35300, Batch Loss:    23.386990, Lr: 0.000085, Tokens per sec:   2753
2023-03-08 22:12:11,435 - INFO - __main__ - Epoch  17, Step:   35400, Batch Loss:    23.879019, Lr: 0.000085, Tokens per sec:   2668
2023-03-08 22:12:31,228 - INFO - __main__ - Epoch  17, Step:   35500, Batch Loss:    14.955989, Lr: 0.000085, Tokens per sec:   2714
2023-03-08 22:12:50,419 - INFO - __main__ - Epoch  17, Step:   35600, Batch Loss:    22.852531, Lr: 0.000085, Tokens per sec:   2852
2023-03-08 22:13:09,455 - INFO - __main__ - Epoch  17, Step:   35700, Batch Loss:    19.053871, Lr: 0.000085, Tokens per sec:   2913
2023-03-08 22:13:29,585 - INFO - __main__ - Epoch  17, Step:   35800, Batch Loss:    32.554699, Lr: 0.000085, Tokens per sec:   2709
2023-03-08 22:13:49,132 - INFO - __main__ - Epoch  17, Step:   35900, Batch Loss:    21.420002, Lr: 0.000085, Tokens per sec:   2761
2023-03-08 22:14:08,780 - INFO - __main__ - Epoch  17, Step:   36000, Batch Loss:    12.870533, Lr: 0.000085, Tokens per sec:   2712
2023-03-08 22:14:28,889 - INFO - __main__ - Epoch  17, Step:   36100, Batch Loss:    22.281874, Lr: 0.000085, Tokens per sec:   2678
2023-03-08 22:14:48,863 - INFO - __main__ - Epoch  17, Step:   36200, Batch Loss:    26.687387, Lr: 0.000085, Tokens per sec:   2719
2023-03-08 22:15:08,057 - INFO - __main__ - Epoch  17, Step:   36300, Batch Loss:    18.929928, Lr: 0.000085, Tokens per sec:   2834
2023-03-08 22:15:28,097 - INFO - __main__ - Epoch  17, Step:   36400, Batch Loss:    21.061638, Lr: 0.000085, Tokens per sec:   2650
2023-03-08 22:15:47,592 - INFO - __main__ - Epoch  17, Step:   36500, Batch Loss:    32.002171, Lr: 0.000085, Tokens per sec:   2739
2023-03-08 22:16:06,768 - INFO - __main__ - Epoch  17, Step:   36600, Batch Loss:    26.063145, Lr: 0.000085, Tokens per sec:   2824
2023-03-08 22:16:26,231 - INFO - __main__ - Epoch  17, Step:   36700, Batch Loss:    22.443951, Lr: 0.000085, Tokens per sec:   2740
2023-03-08 22:16:45,908 - INFO - __main__ - Epoch  17, Step:   36800, Batch Loss:    17.078659, Lr: 0.000085, Tokens per sec:   2697
2023-03-08 22:17:05,052 - INFO - __main__ - Epoch  17, Step:   36900, Batch Loss:    23.717793, Lr: 0.000085, Tokens per sec:   2848
2023-03-08 22:17:24,718 - INFO - __main__ - Epoch  17, Step:   37000, Batch Loss:    23.380304, Lr: 0.000085, Tokens per sec:   2725
2023-03-08 22:17:32,845 - INFO - __main__ - Epoch  17: total training loss 47194.82
2023-03-08 22:17:32,845 - INFO - __main__ - Epoch 18
2023-03-08 22:17:44,378 - INFO - __main__ - Epoch  18, Step:   37100, Batch Loss:    14.659310, Lr: 0.000084, Tokens per sec:   2619
2023-03-08 22:18:04,398 - INFO - __main__ - Epoch  18, Step:   37200, Batch Loss:    15.457350, Lr: 0.000084, Tokens per sec:   2704
2023-03-08 22:18:24,101 - INFO - __main__ - Epoch  18, Step:   37300, Batch Loss:    26.737194, Lr: 0.000084, Tokens per sec:   2724
2023-03-08 22:18:42,988 - INFO - __main__ - Epoch  18, Step:   37400, Batch Loss:    24.097338, Lr: 0.000084, Tokens per sec:   2877
2023-03-08 22:19:02,210 - INFO - __main__ - Epoch  18, Step:   37500, Batch Loss:    24.558060, Lr: 0.000084, Tokens per sec:   2800
2023-03-08 22:19:21,978 - INFO - __main__ - Epoch  18, Step:   37600, Batch Loss:    20.065008, Lr: 0.000084, Tokens per sec:   2692
2023-03-08 22:19:41,793 - INFO - __main__ - Epoch  18, Step:   37700, Batch Loss:    27.519650, Lr: 0.000084, Tokens per sec:   2736
2023-03-08 22:20:01,764 - INFO - __main__ - Epoch  18, Step:   37800, Batch Loss:    18.031820, Lr: 0.000084, Tokens per sec:   2672
2023-03-08 22:20:21,431 - INFO - __main__ - Epoch  18, Step:   37900, Batch Loss:    28.537159, Lr: 0.000084, Tokens per sec:   2758
2023-03-08 22:20:41,245 - INFO - __main__ - Epoch  18, Step:   38000, Batch Loss:    15.758056, Lr: 0.000084, Tokens per sec:   2710
2023-03-08 22:21:01,450 - INFO - __main__ - Epoch  18, Step:   38100, Batch Loss:    12.276988, Lr: 0.000084, Tokens per sec:   2650
2023-03-08 22:21:20,980 - INFO - __main__ - Epoch  18, Step:   38200, Batch Loss:    19.318810, Lr: 0.000084, Tokens per sec:   2753
2023-03-08 22:21:40,874 - INFO - __main__ - Epoch  18, Step:   38300, Batch Loss:    22.437925, Lr: 0.000084, Tokens per sec:   2688
2023-03-08 22:22:00,929 - INFO - __main__ - Epoch  18, Step:   38400, Batch Loss:    20.948553, Lr: 0.000084, Tokens per sec:   2683
2023-03-08 22:22:20,847 - INFO - __main__ - Epoch  18, Step:   38500, Batch Loss:    18.871246, Lr: 0.000084, Tokens per sec:   2707
2023-03-08 22:22:40,766 - INFO - __main__ - Epoch  18, Step:   38600, Batch Loss:    27.026518, Lr: 0.000084, Tokens per sec:   2715
2023-03-08 22:23:00,859 - INFO - __main__ - Epoch  18, Step:   38700, Batch Loss:    23.841715, Lr: 0.000084, Tokens per sec:   2670
2023-03-08 22:23:21,004 - INFO - __main__ - Epoch  18, Step:   38800, Batch Loss:    18.886148, Lr: 0.000084, Tokens per sec:   2669
2023-03-08 22:23:41,267 - INFO - __main__ - Epoch  18, Step:   38900, Batch Loss:    18.605618, Lr: 0.000084, Tokens per sec:   2626
2023-03-08 22:24:01,276 - INFO - __main__ - Epoch  18, Step:   39000, Batch Loss:    14.147551, Lr: 0.000084, Tokens per sec:   2710
2023-03-08 22:24:21,451 - INFO - __main__ - Epoch  18, Step:   39100, Batch Loss:    19.328777, Lr: 0.000084, Tokens per sec:   2732
2023-03-08 22:24:41,567 - INFO - __main__ - Epoch  18, Step:   39200, Batch Loss:    23.258280, Lr: 0.000084, Tokens per sec:   2656
2023-03-08 22:24:45,910 - INFO - __main__ - Epoch  18: total training loss 44439.23
2023-03-08 22:24:45,911 - INFO - __main__ - Epoch 19
2023-03-08 22:25:01,946 - INFO - __main__ - Epoch  19, Step:   39300, Batch Loss:    18.002195, Lr: 0.000083, Tokens per sec:   2619
2023-03-08 22:25:22,152 - INFO - __main__ - Epoch  19, Step:   39400, Batch Loss:    20.309994, Lr: 0.000083, Tokens per sec:   2646
2023-03-08 22:25:42,173 - INFO - __main__ - Epoch  19, Step:   39500, Batch Loss:    12.336169, Lr: 0.000083, Tokens per sec:   2688
2023-03-08 22:26:02,378 - INFO - __main__ - Epoch  19, Step:   39600, Batch Loss:    24.352905, Lr: 0.000083, Tokens per sec:   2700
2023-03-08 22:26:22,196 - INFO - __main__ - Epoch  19, Step:   39700, Batch Loss:    17.054911, Lr: 0.000083, Tokens per sec:   2696
2023-03-08 22:26:42,153 - INFO - __main__ - Epoch  19, Step:   39800, Batch Loss:    17.486603, Lr: 0.000083, Tokens per sec:   2713
2023-03-08 22:27:02,348 - INFO - __main__ - Epoch  19, Step:   39900, Batch Loss:    15.877488, Lr: 0.000083, Tokens per sec:   2650
2023-03-08 22:27:22,519 - INFO - __main__ - Epoch  19, Step:   40000, Batch Loss:    19.600119, Lr: 0.000083, Tokens per sec:   2661
2023-03-08 22:27:42,280 - INFO - __main__ - Epoch  19, Step:   40100, Batch Loss:    21.819395, Lr: 0.000083, Tokens per sec:   2776
2023-03-08 22:28:02,032 - INFO - __main__ - Epoch  19, Step:   40200, Batch Loss:    16.285883, Lr: 0.000083, Tokens per sec:   2754
2023-03-08 22:28:21,988 - INFO - __main__ - Epoch  19, Step:   40300, Batch Loss:    20.080507, Lr: 0.000083, Tokens per sec:   2668
2023-03-08 22:28:41,842 - INFO - __main__ - Epoch  19, Step:   40400, Batch Loss:    23.091961, Lr: 0.000083, Tokens per sec:   2688
2023-03-08 22:29:00,869 - INFO - __main__ - Epoch  19, Step:   40500, Batch Loss:    19.944042, Lr: 0.000083, Tokens per sec:   2828
2023-03-08 22:29:20,478 - INFO - __main__ - Epoch  19, Step:   40600, Batch Loss:    21.928408, Lr: 0.000083, Tokens per sec:   2736
2023-03-08 22:29:39,412 - INFO - __main__ - Epoch  19, Step:   40700, Batch Loss:    26.613035, Lr: 0.000083, Tokens per sec:   2857
2023-03-08 22:29:59,156 - INFO - __main__ - Epoch  19, Step:   40800, Batch Loss:    21.588268, Lr: 0.000083, Tokens per sec:   2715
2023-03-08 22:30:19,001 - INFO - __main__ - Epoch  19, Step:   40900, Batch Loss:    22.551935, Lr: 0.000083, Tokens per sec:   2702
2023-03-08 22:30:39,189 - INFO - __main__ - Epoch  19, Step:   41000, Batch Loss:    20.409374, Lr: 0.000083, Tokens per sec:   2620
2023-03-08 22:30:58,662 - INFO - __main__ - Epoch  19, Step:   41100, Batch Loss:    19.646564, Lr: 0.000083, Tokens per sec:   2733
2023-03-08 22:31:18,739 - INFO - __main__ - Epoch  19, Step:   41200, Batch Loss:    14.653378, Lr: 0.000083, Tokens per sec:   2736
2023-03-08 22:31:38,931 - INFO - __main__ - Epoch  19, Step:   41300, Batch Loss:    18.413771, Lr: 0.000083, Tokens per sec:   2699
2023-03-08 22:31:59,097 - INFO - __main__ - Epoch  19, Step:   41400, Batch Loss:    22.897747, Lr: 0.000083, Tokens per sec:   2669
2023-03-08 22:31:59,323 - INFO - __main__ - Epoch  19: total training loss 41836.45
2023-03-08 22:31:59,324 - INFO - __main__ - Epoch 20
2023-03-08 22:32:19,263 - INFO - __main__ - Epoch  20, Step:   41500, Batch Loss:    14.778024, Lr: 0.000083, Tokens per sec:   2656
2023-03-08 22:32:39,405 - INFO - __main__ - Epoch  20, Step:   41600, Batch Loss:    14.538691, Lr: 0.000083, Tokens per sec:   2706
2023-03-08 22:32:59,290 - INFO - __main__ - Epoch  20, Step:   41700, Batch Loss:    19.051929, Lr: 0.000083, Tokens per sec:   2698
2023-03-08 22:33:19,308 - INFO - __main__ - Epoch  20, Step:   41800, Batch Loss:    21.154331, Lr: 0.000083, Tokens per sec:   2713
2023-03-08 22:33:39,343 - INFO - __main__ - Epoch  20, Step:   41900, Batch Loss:    13.460037, Lr: 0.000083, Tokens per sec:   2677
2023-03-08 22:33:59,653 - INFO - __main__ - Epoch  20, Step:   42000, Batch Loss:    18.573839, Lr: 0.000083, Tokens per sec:   2667
2023-03-08 22:34:19,931 - INFO - __main__ - Epoch  20, Step:   42100, Batch Loss:    20.286201, Lr: 0.000083, Tokens per sec:   2693
2023-03-08 22:34:39,502 - INFO - __main__ - Epoch  20, Step:   42200, Batch Loss:    15.677726, Lr: 0.000083, Tokens per sec:   2734
2023-03-08 22:34:59,450 - INFO - __main__ - Epoch  20, Step:   42300, Batch Loss:    20.880219, Lr: 0.000083, Tokens per sec:   2674
2023-03-08 22:35:18,469 - INFO - __main__ - Epoch  20, Step:   42400, Batch Loss:    15.279637, Lr: 0.000083, Tokens per sec:   2835
2023-03-08 22:35:38,547 - INFO - __main__ - Epoch  20, Step:   42500, Batch Loss:    22.557734, Lr: 0.000083, Tokens per sec:   2657
2023-03-08 22:35:58,611 - INFO - __main__ - Epoch  20, Step:   42600, Batch Loss:    16.364182, Lr: 0.000083, Tokens per sec:   2667
2023-03-08 22:36:18,249 - INFO - __main__ - Epoch  20, Step:   42700, Batch Loss:    16.554897, Lr: 0.000083, Tokens per sec:   2741
2023-03-08 22:36:38,060 - INFO - __main__ - Epoch  20, Step:   42800, Batch Loss:    15.536493, Lr: 0.000083, Tokens per sec:   2770
2023-03-08 22:36:57,931 - INFO - __main__ - Epoch  20, Step:   42900, Batch Loss:    17.646849, Lr: 0.000083, Tokens per sec:   2741
2023-03-08 22:37:17,165 - INFO - __main__ - Epoch  20, Step:   43000, Batch Loss:    15.978123, Lr: 0.000083, Tokens per sec:   2764
2023-03-08 22:37:36,995 - INFO - __main__ - Epoch  20, Step:   43100, Batch Loss:    19.852669, Lr: 0.000083, Tokens per sec:   2734
2023-03-08 22:37:57,038 - INFO - __main__ - Epoch  20, Step:   43200, Batch Loss:    19.942659, Lr: 0.000083, Tokens per sec:   2667
2023-03-08 22:38:17,145 - INFO - __main__ - Epoch  20, Step:   43300, Batch Loss:    19.485043, Lr: 0.000083, Tokens per sec:   2666
2023-03-08 22:38:36,582 - INFO - __main__ - Epoch  20, Step:   43400, Batch Loss:    17.053825, Lr: 0.000083, Tokens per sec:   2738
2023-03-08 22:38:56,642 - INFO - __main__ - Epoch  20, Step:   43500, Batch Loss:    26.607439, Lr: 0.000083, Tokens per sec:   2666
2023-03-08 22:39:12,696 - INFO - __main__ - Epoch  20: total training loss 39422.53
2023-03-08 22:39:12,697 - INFO - __main__ - Epoch 21
2023-03-08 22:39:17,365 - INFO - __main__ - Epoch  21, Step:   43600, Batch Loss:    13.320331, Lr: 0.000082, Tokens per sec:   2255
2023-03-08 22:39:36,532 - INFO - __main__ - Epoch  21, Step:   43700, Batch Loss:    14.808584, Lr: 0.000082, Tokens per sec:   2853
2023-03-08 22:39:55,594 - INFO - __main__ - Epoch  21, Step:   43800, Batch Loss:    12.955886, Lr: 0.000082, Tokens per sec:   2824
2023-03-08 22:40:14,722 - INFO - __main__ - Epoch  21, Step:   43900, Batch Loss:    11.929680, Lr: 0.000082, Tokens per sec:   2790
2023-03-08 22:40:34,132 - INFO - __main__ - Epoch  21, Step:   44000, Batch Loss:    16.530849, Lr: 0.000082, Tokens per sec:   2780
2023-03-08 22:40:53,406 - INFO - __main__ - Epoch  21, Step:   44100, Batch Loss:    14.365303, Lr: 0.000082, Tokens per sec:   2797
2023-03-08 22:41:12,830 - INFO - __main__ - Epoch  21, Step:   44200, Batch Loss:    12.297285, Lr: 0.000082, Tokens per sec:   2724
2023-03-08 22:41:32,339 - INFO - __main__ - Epoch  21, Step:   44300, Batch Loss:    15.427265, Lr: 0.000082, Tokens per sec:   2717
2023-03-08 22:41:51,828 - INFO - __main__ - Epoch  21, Step:   44400, Batch Loss:    14.565321, Lr: 0.000082, Tokens per sec:   2779
2023-03-08 22:42:10,789 - INFO - __main__ - Epoch  21, Step:   44500, Batch Loss:    15.015160, Lr: 0.000082, Tokens per sec:   2847
2023-03-08 22:42:30,789 - INFO - __main__ - Epoch  21, Step:   44600, Batch Loss:    20.596552, Lr: 0.000082, Tokens per sec:   2699
2023-03-08 22:42:49,522 - INFO - __main__ - Epoch  21, Step:   44700, Batch Loss:    17.265886, Lr: 0.000082, Tokens per sec:   2903
2023-03-08 22:43:09,368 - INFO - __main__ - Epoch  21, Step:   44800, Batch Loss:    25.998684, Lr: 0.000082, Tokens per sec:   2677
2023-03-08 22:43:29,352 - INFO - __main__ - Epoch  21, Step:   44900, Batch Loss:    20.436829, Lr: 0.000082, Tokens per sec:   2728
2023-03-08 22:43:48,686 - INFO - __main__ - Epoch  21, Step:   45000, Batch Loss:    20.395790, Lr: 0.000082, Tokens per sec:   2802
2023-03-08 22:44:08,551 - INFO - __main__ - Epoch  21, Step:   45100, Batch Loss:    15.628490, Lr: 0.000082, Tokens per sec:   2713
2023-03-08 22:44:27,989 - INFO - __main__ - Epoch  21, Step:   45200, Batch Loss:    20.957554, Lr: 0.000082, Tokens per sec:   2809
2023-03-08 22:44:47,656 - INFO - __main__ - Epoch  21, Step:   45300, Batch Loss:    16.442799, Lr: 0.000082, Tokens per sec:   2744
2023-03-08 22:45:07,162 - INFO - __main__ - Epoch  21, Step:   45400, Batch Loss:    20.707067, Lr: 0.000082, Tokens per sec:   2715
2023-03-08 22:45:25,908 - INFO - __main__ - Epoch  21, Step:   45500, Batch Loss:    19.653276, Lr: 0.000082, Tokens per sec:   2869
2023-03-08 22:45:44,422 - INFO - __main__ - Epoch  21, Step:   45600, Batch Loss:    15.053095, Lr: 0.000082, Tokens per sec:   2895
2023-03-08 22:46:03,410 - INFO - __main__ - Epoch  21, Step:   45700, Batch Loss:    20.106096, Lr: 0.000082, Tokens per sec:   2840
2023-03-08 22:46:14,490 - INFO - __main__ - Epoch  21: total training loss 37061.88
2023-03-08 22:46:14,491 - INFO - __main__ - Epoch 22
2023-03-08 22:46:22,730 - INFO - __main__ - Epoch  22, Step:   45800, Batch Loss:    15.327670, Lr: 0.000081, Tokens per sec:   2710
2023-03-08 22:46:41,343 - INFO - __main__ - Epoch  22, Step:   45900, Batch Loss:    14.176955, Lr: 0.000081, Tokens per sec:   2874
2023-03-08 22:47:01,024 - INFO - __main__ - Epoch  22, Step:   46000, Batch Loss:    16.465830, Lr: 0.000081, Tokens per sec:   2739
2023-03-08 22:47:20,691 - INFO - __main__ - Epoch  22, Step:   46100, Batch Loss:    12.911942, Lr: 0.000081, Tokens per sec:   2738
2023-03-08 22:47:39,668 - INFO - __main__ - Epoch  22, Step:   46200, Batch Loss:    14.321618, Lr: 0.000081, Tokens per sec:   2893
2023-03-08 22:47:59,471 - INFO - __main__ - Epoch  22, Step:   46300, Batch Loss:    18.451988, Lr: 0.000081, Tokens per sec:   2759
2023-03-08 22:48:18,251 - INFO - __main__ - Epoch  22, Step:   46400, Batch Loss:    16.414639, Lr: 0.000081, Tokens per sec:   2870
2023-03-08 22:48:38,316 - INFO - __main__ - Epoch  22, Step:   46500, Batch Loss:    19.499449, Lr: 0.000081, Tokens per sec:   2674
2023-03-08 22:48:57,954 - INFO - __main__ - Epoch  22, Step:   46600, Batch Loss:    20.434599, Lr: 0.000081, Tokens per sec:   2698
2023-03-08 22:49:16,478 - INFO - __main__ - Epoch  22, Step:   46700, Batch Loss:    18.489790, Lr: 0.000081, Tokens per sec:   2902
2023-03-08 22:49:34,862 - INFO - __main__ - Epoch  22, Step:   46800, Batch Loss:    15.012292, Lr: 0.000081, Tokens per sec:   2881
2023-03-08 22:49:54,205 - INFO - __main__ - Epoch  22, Step:   46900, Batch Loss:    16.132715, Lr: 0.000081, Tokens per sec:   2779
2023-03-08 22:50:14,342 - INFO - __main__ - Epoch  22, Step:   47000, Batch Loss:    21.199459, Lr: 0.000081, Tokens per sec:   2690
2023-03-08 22:50:33,236 - INFO - __main__ - Epoch  22, Step:   47100, Batch Loss:    16.162504, Lr: 0.000081, Tokens per sec:   2826
2023-03-08 22:50:52,508 - INFO - __main__ - Epoch  22, Step:   47200, Batch Loss:    12.145176, Lr: 0.000081, Tokens per sec:   2792
2023-03-08 22:51:12,465 - INFO - __main__ - Epoch  22, Step:   47300, Batch Loss:    11.111737, Lr: 0.000081, Tokens per sec:   2677
2023-03-08 22:51:32,235 - INFO - __main__ - Epoch  22, Step:   47400, Batch Loss:    15.232732, Lr: 0.000081, Tokens per sec:   2696
2023-03-08 22:51:52,318 - INFO - __main__ - Epoch  22, Step:   47500, Batch Loss:    21.639490, Lr: 0.000081, Tokens per sec:   2705
2023-03-08 22:52:11,396 - INFO - __main__ - Epoch  22, Step:   47600, Batch Loss:    17.405775, Lr: 0.000081, Tokens per sec:   2836
2023-03-08 22:52:29,949 - INFO - __main__ - Epoch  22, Step:   47700, Batch Loss:    19.701609, Lr: 0.000081, Tokens per sec:   2929
2023-03-08 22:52:48,122 - INFO - __main__ - Epoch  22, Step:   47800, Batch Loss:    19.015909, Lr: 0.000081, Tokens per sec:   2961
2023-03-08 22:53:07,617 - INFO - __main__ - Epoch  22, Step:   47900, Batch Loss:    15.125396, Lr: 0.000081, Tokens per sec:   2753
2023-03-08 22:53:14,592 - INFO - __main__ - Epoch  22: total training loss 34997.80
2023-03-08 22:53:14,594 - INFO - __main__ - Epoch 23
2023-03-08 22:53:26,706 - INFO - __main__ - Epoch  23, Step:   48000, Batch Loss:    14.604170, Lr: 0.000080, Tokens per sec:   2755
2023-03-08 22:53:45,409 - INFO - __main__ - Epoch  23, Step:   48100, Batch Loss:    13.487583, Lr: 0.000080, Tokens per sec:   2883
2023-03-08 22:54:04,456 - INFO - __main__ - Epoch  23, Step:   48200, Batch Loss:    12.643847, Lr: 0.000080, Tokens per sec:   2870
2023-03-08 22:54:24,074 - INFO - __main__ - Epoch  23, Step:   48300, Batch Loss:    11.363135, Lr: 0.000080, Tokens per sec:   2723
2023-03-08 22:54:42,929 - INFO - __main__ - Epoch  23, Step:   48400, Batch Loss:    12.353004, Lr: 0.000080, Tokens per sec:   2896
2023-03-08 22:55:01,245 - INFO - __main__ - Epoch  23, Step:   48500, Batch Loss:    18.641844, Lr: 0.000080, Tokens per sec:   2954
2023-03-08 22:55:20,731 - INFO - __main__ - Epoch  23, Step:   48600, Batch Loss:    17.405455, Lr: 0.000080, Tokens per sec:   2776
2023-03-08 22:55:39,254 - INFO - __main__ - Epoch  23, Step:   48700, Batch Loss:    15.178113, Lr: 0.000080, Tokens per sec:   2913
2023-03-08 22:55:57,361 - INFO - __main__ - Epoch  23, Step:   48800, Batch Loss:    11.614120, Lr: 0.000080, Tokens per sec:   2957
2023-03-08 22:56:15,896 - INFO - __main__ - Epoch  23, Step:   48900, Batch Loss:    12.969172, Lr: 0.000080, Tokens per sec:   2911
2023-03-08 22:56:34,629 - INFO - __main__ - Epoch  23, Step:   49000, Batch Loss:    14.849451, Lr: 0.000080, Tokens per sec:   2879
2023-03-08 22:56:53,958 - INFO - __main__ - Epoch  23, Step:   49100, Batch Loss:    17.132799, Lr: 0.000080, Tokens per sec:   2771
2023-03-08 22:57:13,941 - INFO - __main__ - Epoch  23, Step:   49200, Batch Loss:     8.731442, Lr: 0.000080, Tokens per sec:   2699
2023-03-08 22:57:32,982 - INFO - __main__ - Epoch  23, Step:   49300, Batch Loss:    18.316439, Lr: 0.000080, Tokens per sec:   2829
2023-03-08 22:57:52,983 - INFO - __main__ - Epoch  23, Step:   49400, Batch Loss:    17.238997, Lr: 0.000080, Tokens per sec:   2730
2023-03-08 22:58:12,440 - INFO - __main__ - Epoch  23, Step:   49500, Batch Loss:    16.588316, Lr: 0.000080, Tokens per sec:   2765
2023-03-08 22:58:32,336 - INFO - __main__ - Epoch  23, Step:   49600, Batch Loss:    12.037064, Lr: 0.000080, Tokens per sec:   2725
2023-03-08 22:58:52,397 - INFO - __main__ - Epoch  23, Step:   49700, Batch Loss:    17.407793, Lr: 0.000080, Tokens per sec:   2657
2023-03-08 22:59:12,021 - INFO - __main__ - Epoch  23, Step:   49800, Batch Loss:    16.446440, Lr: 0.000080, Tokens per sec:   2759
2023-03-08 22:59:32,052 - INFO - __main__ - Epoch  23, Step:   49900, Batch Loss:    14.148675, Lr: 0.000080, Tokens per sec:   2655
2023-03-08 22:59:52,063 - INFO - __main__ - Epoch  23, Step:   50000, Batch Loss:    14.151253, Lr: 0.000080, Tokens per sec:   2643
2023-03-08 23:00:10,865 - INFO - __main__ - Epoch  23, Step:   50100, Batch Loss:    15.742617, Lr: 0.000080, Tokens per sec:   2814
2023-03-08 23:00:14,047 - INFO - __main__ - Epoch  23: total training loss 33011.17
2023-03-08 23:00:14,048 - INFO - __main__ - Epoch 24
2023-03-08 23:00:31,175 - INFO - __main__ - Epoch  24, Step:   50200, Batch Loss:    11.693776, Lr: 0.000079, Tokens per sec:   2634
2023-03-08 23:00:50,688 - INFO - __main__ - Epoch  24, Step:   50300, Batch Loss:    16.471457, Lr: 0.000079, Tokens per sec:   2749
2023-03-08 23:01:09,698 - INFO - __main__ - Epoch  24, Step:   50400, Batch Loss:    15.691955, Lr: 0.000079, Tokens per sec:   2866
2023-03-08 23:01:29,548 - INFO - __main__ - Epoch  24, Step:   50500, Batch Loss:    10.569538, Lr: 0.000079, Tokens per sec:   2689
2023-03-08 23:01:48,174 - INFO - __main__ - Epoch  24, Step:   50600, Batch Loss:    12.494018, Lr: 0.000079, Tokens per sec:   2906
2023-03-08 23:02:07,521 - INFO - __main__ - Epoch  24, Step:   50700, Batch Loss:    13.238125, Lr: 0.000079, Tokens per sec:   2771
2023-03-08 23:02:26,854 - INFO - __main__ - Epoch  24, Step:   50800, Batch Loss:    12.483884, Lr: 0.000079, Tokens per sec:   2809
2023-03-08 23:02:46,826 - INFO - __main__ - Epoch  24, Step:   50900, Batch Loss:    12.317654, Lr: 0.000079, Tokens per sec:   2645
2023-03-08 23:03:05,982 - INFO - __main__ - Epoch  24, Step:   51000, Batch Loss:    13.544000, Lr: 0.000079, Tokens per sec:   2776
2023-03-08 23:03:25,652 - INFO - __main__ - Epoch  24, Step:   51100, Batch Loss:    13.572015, Lr: 0.000079, Tokens per sec:   2744
2023-03-08 23:03:44,944 - INFO - __main__ - Epoch  24, Step:   51200, Batch Loss:    13.870172, Lr: 0.000079, Tokens per sec:   2830
2023-03-08 23:04:04,643 - INFO - __main__ - Epoch  24, Step:   51300, Batch Loss:    10.562524, Lr: 0.000079, Tokens per sec:   2749
2023-03-08 23:04:24,513 - INFO - __main__ - Epoch  24, Step:   51400, Batch Loss:     9.010950, Lr: 0.000079, Tokens per sec:   2725
2023-03-08 23:04:43,552 - INFO - __main__ - Epoch  24, Step:   51500, Batch Loss:    11.264457, Lr: 0.000079, Tokens per sec:   2840
2023-03-08 23:05:03,331 - INFO - __main__ - Epoch  24, Step:   51600, Batch Loss:    18.149332, Lr: 0.000079, Tokens per sec:   2722
2023-03-08 23:05:22,376 - INFO - __main__ - Epoch  24, Step:   51700, Batch Loss:    15.983282, Lr: 0.000079, Tokens per sec:   2789
2023-03-08 23:05:41,515 - INFO - __main__ - Epoch  24, Step:   51800, Batch Loss:    14.601056, Lr: 0.000079, Tokens per sec:   2776
2023-03-08 23:06:00,064 - INFO - __main__ - Epoch  24, Step:   51900, Batch Loss:    13.514510, Lr: 0.000079, Tokens per sec:   2951
2023-03-08 23:06:19,763 - INFO - __main__ - Epoch  24, Step:   52000, Batch Loss:    15.905426, Lr: 0.000079, Tokens per sec:   2721
2023-03-08 23:06:38,176 - INFO - __main__ - Epoch  24, Step:   52100, Batch Loss:    10.642423, Lr: 0.000079, Tokens per sec:   2926
2023-03-08 23:06:56,969 - INFO - __main__ - Epoch  24, Step:   52200, Batch Loss:    10.858172, Lr: 0.000079, Tokens per sec:   2861
2023-03-08 23:07:15,791 - INFO - __main__ - Epoch  24: total training loss 31121.76
2023-03-08 23:07:15,793 - INFO - __main__ - Epoch 25
2023-03-08 23:07:16,898 - INFO - __main__ - Epoch  25, Step:   52300, Batch Loss:    10.944318, Lr: 0.000079, Tokens per sec:   1972
2023-03-08 23:07:36,387 - INFO - __main__ - Epoch  25, Step:   52400, Batch Loss:    14.145499, Lr: 0.000079, Tokens per sec:   2764
2023-03-08 23:07:55,583 - INFO - __main__ - Epoch  25, Step:   52500, Batch Loss:    10.750647, Lr: 0.000079, Tokens per sec:   2805
2023-03-08 23:08:14,559 - INFO - __main__ - Epoch  25, Step:   52600, Batch Loss:    12.932961, Lr: 0.000079, Tokens per sec:   2785
2023-03-08 23:08:33,308 - INFO - __main__ - Epoch  25, Step:   52700, Batch Loss:    16.901764, Lr: 0.000079, Tokens per sec:   2868
2023-03-08 23:08:52,324 - INFO - __main__ - Epoch  25, Step:   52800, Batch Loss:    16.202459, Lr: 0.000079, Tokens per sec:   2823
2023-03-08 23:09:12,382 - INFO - __main__ - Epoch  25, Step:   52900, Batch Loss:    16.893452, Lr: 0.000079, Tokens per sec:   2669
2023-03-08 23:09:31,831 - INFO - __main__ - Epoch  25, Step:   53000, Batch Loss:    13.222214, Lr: 0.000079, Tokens per sec:   2722
2023-03-08 23:09:51,748 - INFO - __main__ - Epoch  25, Step:   53100, Batch Loss:    20.136530, Lr: 0.000079, Tokens per sec:   2759
2023-03-08 23:10:10,020 - INFO - __main__ - Epoch  25, Step:   53200, Batch Loss:    12.420831, Lr: 0.000079, Tokens per sec:   2936
2023-03-08 23:10:29,402 - INFO - __main__ - Epoch  25, Step:   53300, Batch Loss:    11.556481, Lr: 0.000079, Tokens per sec:   2752
2023-03-08 23:10:49,411 - INFO - __main__ - Epoch  25, Step:   53400, Batch Loss:    17.636641, Lr: 0.000079, Tokens per sec:   2734
2023-03-08 23:11:08,605 - INFO - __main__ - Epoch  25, Step:   53500, Batch Loss:    16.841763, Lr: 0.000079, Tokens per sec:   2727
2023-03-08 23:11:27,016 - INFO - __main__ - Epoch  25, Step:   53600, Batch Loss:     8.118913, Lr: 0.000079, Tokens per sec:   2864
2023-03-08 23:11:45,706 - INFO - __main__ - Epoch  25, Step:   53700, Batch Loss:    11.352219, Lr: 0.000079, Tokens per sec:   2914
2023-03-08 23:12:04,843 - INFO - __main__ - Epoch  25, Step:   53800, Batch Loss:    17.290052, Lr: 0.000079, Tokens per sec:   2794
2023-03-08 23:12:23,997 - INFO - __main__ - Epoch  25, Step:   53900, Batch Loss:    14.210566, Lr: 0.000079, Tokens per sec:   2850
2023-03-08 23:12:43,017 - INFO - __main__ - Epoch  25, Step:   54000, Batch Loss:    15.187595, Lr: 0.000079, Tokens per sec:   2900
2023-03-08 23:13:02,814 - INFO - __main__ - Epoch  25, Step:   54100, Batch Loss:    15.095132, Lr: 0.000079, Tokens per sec:   2727
2023-03-08 23:13:22,111 - INFO - __main__ - Epoch  25, Step:   54200, Batch Loss:    12.848710, Lr: 0.000079, Tokens per sec:   2824
2023-03-08 23:13:40,695 - INFO - __main__ - Epoch  25, Step:   54300, Batch Loss:    22.073233, Lr: 0.000079, Tokens per sec:   2904
2023-03-08 23:14:00,291 - INFO - __main__ - Epoch  25, Step:   54400, Batch Loss:    16.146877, Lr: 0.000079, Tokens per sec:   2761
2023-03-08 23:14:15,215 - INFO - __main__ - Epoch  25: total training loss 29446.45
2023-03-08 23:14:15,215 - INFO - __main__ - Epoch 26
2023-03-08 23:14:20,120 - INFO - __main__ - Epoch  26, Step:   54500, Batch Loss:    10.650559, Lr: 0.000078, Tokens per sec:   2799
2023-03-08 23:14:40,111 - INFO - __main__ - Epoch  26, Step:   54600, Batch Loss:     6.871123, Lr: 0.000078, Tokens per sec:   2645
2023-03-08 23:15:00,168 - INFO - __main__ - Epoch  26, Step:   54700, Batch Loss:     9.659056, Lr: 0.000078, Tokens per sec:   2689
2023-03-08 23:15:19,140 - INFO - __main__ - Epoch  26, Step:   54800, Batch Loss:    12.236602, Lr: 0.000078, Tokens per sec:   2880
2023-03-08 23:15:38,616 - INFO - __main__ - Epoch  26, Step:   54900, Batch Loss:    15.086138, Lr: 0.000078, Tokens per sec:   2759
2023-03-08 23:15:57,766 - INFO - __main__ - Epoch  26, Step:   55000, Batch Loss:    12.179810, Lr: 0.000078, Tokens per sec:   2787
2023-03-08 23:16:17,163 - INFO - __main__ - Epoch  26, Step:   55100, Batch Loss:     8.113141, Lr: 0.000078, Tokens per sec:   2750
2023-03-08 23:16:36,858 - INFO - __main__ - Epoch  26, Step:   55200, Batch Loss:    13.503821, Lr: 0.000078, Tokens per sec:   2724
2023-03-08 23:16:56,089 - INFO - __main__ - Epoch  26, Step:   55300, Batch Loss:    12.085193, Lr: 0.000078, Tokens per sec:   2801
2023-03-08 23:17:14,746 - INFO - __main__ - Epoch  26, Step:   55400, Batch Loss:    11.971494, Lr: 0.000078, Tokens per sec:   2862
2023-03-08 23:17:33,129 - INFO - __main__ - Epoch  26, Step:   55500, Batch Loss:    10.006350, Lr: 0.000078, Tokens per sec:   2961
2023-03-08 23:17:52,690 - INFO - __main__ - Epoch  26, Step:   55600, Batch Loss:    10.321898, Lr: 0.000078, Tokens per sec:   2747
2023-03-08 23:18:11,483 - INFO - __main__ - Epoch  26, Step:   55700, Batch Loss:     9.990176, Lr: 0.000078, Tokens per sec:   2873
2023-03-08 23:18:31,520 - INFO - __main__ - Epoch  26, Step:   55800, Batch Loss:    13.098373, Lr: 0.000078, Tokens per sec:   2679
2023-03-08 23:18:51,233 - INFO - __main__ - Epoch  26, Step:   55900, Batch Loss:    15.277160, Lr: 0.000078, Tokens per sec:   2721
2023-03-08 23:19:10,866 - INFO - __main__ - Epoch  26, Step:   56000, Batch Loss:    16.770220, Lr: 0.000078, Tokens per sec:   2717
2023-03-08 23:19:30,987 - INFO - __main__ - Epoch  26, Step:   56100, Batch Loss:    11.898173, Lr: 0.000078, Tokens per sec:   2708
2023-03-08 23:19:50,136 - INFO - __main__ - Epoch  26, Step:   56200, Batch Loss:    12.683731, Lr: 0.000078, Tokens per sec:   2844
2023-03-08 23:20:09,964 - INFO - __main__ - Epoch  26, Step:   56300, Batch Loss:    11.506776, Lr: 0.000078, Tokens per sec:   2728
2023-03-08 23:20:29,523 - INFO - __main__ - Epoch  26, Step:   56400, Batch Loss:    17.843010, Lr: 0.000078, Tokens per sec:   2763
2023-03-08 23:20:49,101 - INFO - __main__ - Epoch  26, Step:   56500, Batch Loss:    10.592609, Lr: 0.000078, Tokens per sec:   2739
2023-03-08 23:21:09,091 - INFO - __main__ - Epoch  26, Step:   56600, Batch Loss:    12.355199, Lr: 0.000078, Tokens per sec:   2701
2023-03-08 23:21:19,937 - INFO - __main__ - Epoch  26: total training loss 27827.48
2023-03-08 23:21:19,937 - INFO - __main__ - Epoch 27
2023-03-08 23:21:29,477 - INFO - __main__ - Epoch  27, Step:   56700, Batch Loss:    13.197954, Lr: 0.000077, Tokens per sec:   2628
2023-03-08 23:21:49,418 - INFO - __main__ - Epoch  27, Step:   56800, Batch Loss:    12.111670, Lr: 0.000077, Tokens per sec:   2710
2023-03-08 23:22:07,768 - INFO - __main__ - Epoch  27, Step:   56900, Batch Loss:     9.038171, Lr: 0.000077, Tokens per sec:   2903
2023-03-08 23:22:27,499 - INFO - __main__ - Epoch  27, Step:   57000, Batch Loss:    12.988016, Lr: 0.000077, Tokens per sec:   2722
2023-03-08 23:22:47,091 - INFO - __main__ - Epoch  27, Step:   57100, Batch Loss:     7.346001, Lr: 0.000077, Tokens per sec:   2769
2023-03-08 23:23:06,492 - INFO - __main__ - Epoch  27, Step:   57200, Batch Loss:    14.121510, Lr: 0.000077, Tokens per sec:   2750
2023-03-08 23:23:25,729 - INFO - __main__ - Epoch  27, Step:   57300, Batch Loss:    14.676263, Lr: 0.000077, Tokens per sec:   2820
2023-03-08 23:23:45,163 - INFO - __main__ - Epoch  27, Step:   57400, Batch Loss:    15.168360, Lr: 0.000077, Tokens per sec:   2843
2023-03-08 23:24:04,352 - INFO - __main__ - Epoch  27, Step:   57500, Batch Loss:    11.935987, Lr: 0.000077, Tokens per sec:   2773
2023-03-08 23:24:23,006 - INFO - __main__ - Epoch  27, Step:   57600, Batch Loss:     8.333058, Lr: 0.000077, Tokens per sec:   2910
2023-03-08 23:24:42,999 - INFO - __main__ - Epoch  27, Step:   57700, Batch Loss:    13.210191, Lr: 0.000077, Tokens per sec:   2760
2023-03-08 23:25:02,060 - INFO - __main__ - Epoch  27, Step:   57800, Batch Loss:     9.960489, Lr: 0.000077, Tokens per sec:   2808
2023-03-08 23:25:21,075 - INFO - __main__ - Epoch  27, Step:   57900, Batch Loss:     8.528950, Lr: 0.000077, Tokens per sec:   2829
2023-03-08 23:25:41,156 - INFO - __main__ - Epoch  27, Step:   58000, Batch Loss:    10.835855, Lr: 0.000077, Tokens per sec:   2679
2023-03-08 23:26:01,315 - INFO - __main__ - Epoch  27, Step:   58100, Batch Loss:    13.037764, Lr: 0.000077, Tokens per sec:   2670
2023-03-08 23:26:20,881 - INFO - __main__ - Epoch  27, Step:   58200, Batch Loss:    13.070947, Lr: 0.000077, Tokens per sec:   2738
2023-03-08 23:26:40,999 - INFO - __main__ - Epoch  27, Step:   58300, Batch Loss:    14.350964, Lr: 0.000077, Tokens per sec:   2705
2023-03-08 23:27:00,013 - INFO - __main__ - Epoch  27, Step:   58400, Batch Loss:    10.644334, Lr: 0.000077, Tokens per sec:   2815
2023-03-08 23:27:19,068 - INFO - __main__ - Epoch  27, Step:   58500, Batch Loss:    11.041706, Lr: 0.000077, Tokens per sec:   2767
2023-03-08 23:27:39,003 - INFO - __main__ - Epoch  27, Step:   58600, Batch Loss:     9.423356, Lr: 0.000077, Tokens per sec:   2671
2023-03-08 23:27:57,996 - INFO - __main__ - Epoch  27, Step:   58700, Batch Loss:    14.159819, Lr: 0.000077, Tokens per sec:   2803
2023-03-08 23:28:18,200 - INFO - __main__ - Epoch  27, Step:   58800, Batch Loss:    12.500776, Lr: 0.000077, Tokens per sec:   2654
2023-03-08 23:28:24,911 - INFO - __main__ - Epoch  27: total training loss 26346.03
2023-03-08 23:28:24,912 - INFO - __main__ - Epoch 28
2023-03-08 23:28:38,420 - INFO - __main__ - Epoch  28, Step:   58900, Batch Loss:    13.565956, Lr: 0.000076, Tokens per sec:   2648
2023-03-08 23:28:58,494 - INFO - __main__ - Epoch  28, Step:   59000, Batch Loss:     7.584461, Lr: 0.000076, Tokens per sec:   2701
2023-03-08 23:29:18,648 - INFO - __main__ - Epoch  28, Step:   59100, Batch Loss:    13.783503, Lr: 0.000076, Tokens per sec:   2671
2023-03-08 23:29:37,923 - INFO - __main__ - Epoch  28, Step:   59200, Batch Loss:    11.928669, Lr: 0.000076, Tokens per sec:   2828
2023-03-08 23:29:57,526 - INFO - __main__ - Epoch  28, Step:   59300, Batch Loss:     8.850772, Lr: 0.000076, Tokens per sec:   2744
2023-03-08 23:30:17,272 - INFO - __main__ - Epoch  28, Step:   59400, Batch Loss:    10.979712, Lr: 0.000076, Tokens per sec:   2723
2023-03-08 23:30:36,859 - INFO - __main__ - Epoch  28, Step:   59500, Batch Loss:     8.362232, Lr: 0.000076, Tokens per sec:   2758
2023-03-08 23:30:56,995 - INFO - __main__ - Epoch  28, Step:   59600, Batch Loss:    10.774478, Lr: 0.000076, Tokens per sec:   2708
2023-03-08 23:31:16,832 - INFO - __main__ - Epoch  28, Step:   59700, Batch Loss:    11.330980, Lr: 0.000076, Tokens per sec:   2704
2023-03-08 23:31:36,964 - INFO - __main__ - Epoch  28, Step:   59800, Batch Loss:     9.205150, Lr: 0.000076, Tokens per sec:   2675
2023-03-08 23:31:56,795 - INFO - __main__ - Epoch  28, Step:   59900, Batch Loss:    14.018781, Lr: 0.000076, Tokens per sec:   2648
2023-03-08 23:32:16,389 - INFO - __main__ - Epoch  28, Step:   60000, Batch Loss:    11.831053, Lr: 0.000076, Tokens per sec:   2761
2023-03-08 23:32:35,826 - INFO - __main__ - Epoch  28, Step:   60100, Batch Loss:    13.235975, Lr: 0.000076, Tokens per sec:   2818
2023-03-08 23:32:55,472 - INFO - __main__ - Epoch  28, Step:   60200, Batch Loss:    14.532455, Lr: 0.000076, Tokens per sec:   2752
2023-03-08 23:33:15,276 - INFO - __main__ - Epoch  28, Step:   60300, Batch Loss:    12.914917, Lr: 0.000076, Tokens per sec:   2641
2023-03-08 23:33:34,766 - INFO - __main__ - Epoch  28, Step:   60400, Batch Loss:    11.369298, Lr: 0.000076, Tokens per sec:   2777
2023-03-08 23:33:53,872 - INFO - __main__ - Epoch  28, Step:   60500, Batch Loss:    12.014248, Lr: 0.000076, Tokens per sec:   2803
2023-03-08 23:34:13,598 - INFO - __main__ - Epoch  28, Step:   60600, Batch Loss:    12.041125, Lr: 0.000076, Tokens per sec:   2745
2023-03-08 23:34:33,668 - INFO - __main__ - Epoch  28, Step:   60700, Batch Loss:    12.837564, Lr: 0.000076, Tokens per sec:   2690
2023-03-08 23:34:52,583 - INFO - __main__ - Epoch  28, Step:   60800, Batch Loss:    13.532817, Lr: 0.000076, Tokens per sec:   2884
2023-03-08 23:35:12,613 - INFO - __main__ - Epoch  28, Step:   60900, Batch Loss:    11.017015, Lr: 0.000076, Tokens per sec:   2668
2023-03-08 23:35:31,825 - INFO - __main__ - Epoch  28, Step:   61000, Batch Loss:    12.687185, Lr: 0.000076, Tokens per sec:   2780
2023-03-08 23:35:34,266 - INFO - __main__ - Epoch  28: total training loss 24929.95
2023-03-08 23:35:34,267 - INFO - __main__ - Epoch 29
2023-03-08 23:35:52,161 - INFO - __main__ - Epoch  29, Step:   61100, Batch Loss:     7.798164, Lr: 0.000075, Tokens per sec:   2644
2023-03-08 23:36:12,074 - INFO - __main__ - Epoch  29, Step:   61200, Batch Loss:     9.710211, Lr: 0.000075, Tokens per sec:   2726
2023-03-08 23:36:31,311 - INFO - __main__ - Epoch  29, Step:   61300, Batch Loss:    10.477191, Lr: 0.000075, Tokens per sec:   2728
2023-03-08 23:36:51,219 - INFO - __main__ - Epoch  29, Step:   61400, Batch Loss:    13.327585, Lr: 0.000075, Tokens per sec:   2696
2023-03-08 23:37:10,495 - INFO - __main__ - Epoch  29, Step:   61500, Batch Loss:     7.322387, Lr: 0.000075, Tokens per sec:   2839
2023-03-08 23:37:29,607 - INFO - __main__ - Epoch  29, Step:   61600, Batch Loss:    13.240816, Lr: 0.000075, Tokens per sec:   2859
2023-03-08 23:37:49,741 - INFO - __main__ - Epoch  29, Step:   61700, Batch Loss:    12.431943, Lr: 0.000075, Tokens per sec:   2698
2023-03-08 23:38:09,589 - INFO - __main__ - Epoch  29, Step:   61800, Batch Loss:    10.947279, Lr: 0.000075, Tokens per sec:   2745
2023-03-08 23:38:29,703 - INFO - __main__ - Epoch  29, Step:   61900, Batch Loss:    13.511522, Lr: 0.000075, Tokens per sec:   2718
2023-03-08 23:38:48,963 - INFO - __main__ - Epoch  29, Step:   62000, Batch Loss:    11.960282, Lr: 0.000075, Tokens per sec:   2738
2023-03-08 23:39:07,398 - INFO - __main__ - Epoch  29, Step:   62100, Batch Loss:    10.940179, Lr: 0.000075, Tokens per sec:   2880
2023-03-08 23:39:27,436 - INFO - __main__ - Epoch  29, Step:   62200, Batch Loss:     9.075761, Lr: 0.000075, Tokens per sec:   2711
2023-03-08 23:39:46,897 - INFO - __main__ - Epoch  29, Step:   62300, Batch Loss:     7.226062, Lr: 0.000075, Tokens per sec:   2751
2023-03-08 23:40:06,118 - INFO - __main__ - Epoch  29, Step:   62400, Batch Loss:    12.289565, Lr: 0.000075, Tokens per sec:   2820
2023-03-08 23:40:25,692 - INFO - __main__ - Epoch  29, Step:   62500, Batch Loss:    12.448747, Lr: 0.000075, Tokens per sec:   2745
2023-03-08 23:40:44,827 - INFO - __main__ - Epoch  29, Step:   62600, Batch Loss:    11.942476, Lr: 0.000075, Tokens per sec:   2796
2023-03-08 23:41:04,817 - INFO - __main__ - Epoch  29, Step:   62700, Batch Loss:    12.734066, Lr: 0.000075, Tokens per sec:   2702
2023-03-08 23:41:24,806 - INFO - __main__ - Epoch  29, Step:   62800, Batch Loss:    12.776346, Lr: 0.000075, Tokens per sec:   2656
2023-03-08 23:41:44,942 - INFO - __main__ - Epoch  29, Step:   62900, Batch Loss:    14.203669, Lr: 0.000075, Tokens per sec:   2659
2023-03-08 23:42:04,953 - INFO - __main__ - Epoch  29, Step:   63000, Batch Loss:    12.503203, Lr: 0.000075, Tokens per sec:   2667
2023-03-08 23:42:24,474 - INFO - __main__ - Epoch  29, Step:   63100, Batch Loss:    11.808008, Lr: 0.000075, Tokens per sec:   2730
2023-03-08 23:42:42,648 - INFO - __main__ - Epoch  29: total training loss 23655.55
2023-03-08 23:42:42,649 - INFO - __main__ - Epoch 30
2023-03-08 23:42:44,607 - INFO - __main__ - Epoch  30, Step:   63200, Batch Loss:     9.342772, Lr: 0.000075, Tokens per sec:   2289
2023-03-08 23:43:04,501 - INFO - __main__ - Epoch  30, Step:   63300, Batch Loss:     9.717226, Lr: 0.000075, Tokens per sec:   2755
2023-03-08 23:43:24,319 - INFO - __main__ - Epoch  30, Step:   63400, Batch Loss:     8.213570, Lr: 0.000075, Tokens per sec:   2702
2023-03-08 23:43:44,474 - INFO - __main__ - Epoch  30, Step:   63500, Batch Loss:    11.792185, Lr: 0.000075, Tokens per sec:   2668
2023-03-08 23:44:03,402 - INFO - __main__ - Epoch  30, Step:   63600, Batch Loss:    10.280519, Lr: 0.000075, Tokens per sec:   2844
2023-03-08 23:44:23,435 - INFO - __main__ - Epoch  30, Step:   63700, Batch Loss:    11.151363, Lr: 0.000075, Tokens per sec:   2719
2023-03-08 23:44:43,250 - INFO - __main__ - Epoch  30, Step:   63800, Batch Loss:    11.845471, Lr: 0.000075, Tokens per sec:   2754
2023-03-08 23:45:02,328 - INFO - __main__ - Epoch  30, Step:   63900, Batch Loss:     9.746356, Lr: 0.000075, Tokens per sec:   2824
2023-03-08 23:45:22,492 - INFO - __main__ - Epoch  30, Step:   64000, Batch Loss:     8.375049, Lr: 0.000075, Tokens per sec:   2670
2023-03-08 23:45:42,110 - INFO - __main__ - Epoch  30, Step:   64100, Batch Loss:     8.502117, Lr: 0.000075, Tokens per sec:   2766
2023-03-08 23:46:02,003 - INFO - __main__ - Epoch  30, Step:   64200, Batch Loss:    11.364527, Lr: 0.000075, Tokens per sec:   2697
2023-03-08 23:46:20,897 - INFO - __main__ - Epoch  30, Step:   64300, Batch Loss:    13.552729, Lr: 0.000075, Tokens per sec:   2804
2023-03-08 23:46:40,654 - INFO - __main__ - Epoch  30, Step:   64400, Batch Loss:    12.313690, Lr: 0.000075, Tokens per sec:   2711
2023-03-08 23:46:58,915 - INFO - __main__ - Epoch  30, Step:   64500, Batch Loss:    13.804634, Lr: 0.000075, Tokens per sec:   2961
2023-03-08 23:47:17,690 - INFO - __main__ - Epoch  30, Step:   64600, Batch Loss:    10.936671, Lr: 0.000075, Tokens per sec:   2862
2023-03-08 23:47:37,116 - INFO - __main__ - Epoch  30, Step:   64700, Batch Loss:     9.751731, Lr: 0.000075, Tokens per sec:   2702
2023-03-08 23:47:57,205 - INFO - __main__ - Epoch  30, Step:   64800, Batch Loss:     6.531558, Lr: 0.000075, Tokens per sec:   2674
2023-03-08 23:48:16,597 - INFO - __main__ - Epoch  30, Step:   64900, Batch Loss:    14.996123, Lr: 0.000075, Tokens per sec:   2836
2023-03-08 23:48:35,387 - INFO - __main__ - Epoch  30, Step:   65000, Batch Loss:    11.613547, Lr: 0.000075, Tokens per sec:   2839
2023-03-08 23:48:54,022 - INFO - __main__ - Epoch  30, Step:   65100, Batch Loss:     9.748968, Lr: 0.000075, Tokens per sec:   2897
2023-03-08 23:49:13,285 - INFO - __main__ - Epoch  30, Step:   65200, Batch Loss:    10.841055, Lr: 0.000075, Tokens per sec:   2756
2023-03-08 23:49:31,732 - INFO - __main__ - Epoch  30, Step:   65300, Batch Loss:    12.607737, Lr: 0.000075, Tokens per sec:   2957
2023-03-08 23:49:45,203 - INFO - __main__ - Epoch  30: total training loss 22470.99
2023-03-08 23:49:45,204 - INFO - __main__ - Epoch 31
2023-03-08 23:49:51,526 - INFO - __main__ - Epoch  31, Step:   65400, Batch Loss:    10.926242, Lr: 0.000074, Tokens per sec:   2542
2023-03-08 23:50:11,612 - INFO - __main__ - Epoch  31, Step:   65500, Batch Loss:     6.811091, Lr: 0.000074, Tokens per sec:   2686
2023-03-08 23:50:31,454 - INFO - __main__ - Epoch  31, Step:   65600, Batch Loss:     9.392221, Lr: 0.000074, Tokens per sec:   2722
2023-03-08 23:50:50,709 - INFO - __main__ - Epoch  31, Step:   65700, Batch Loss:     7.859895, Lr: 0.000074, Tokens per sec:   2801
2023-03-08 23:51:09,637 - INFO - __main__ - Epoch  31, Step:   65800, Batch Loss:     8.407098, Lr: 0.000074, Tokens per sec:   2797
2023-03-08 23:51:29,577 - INFO - __main__ - Epoch  31, Step:   65900, Batch Loss:     7.168077, Lr: 0.000074, Tokens per sec:   2672
2023-03-08 23:51:49,731 - INFO - __main__ - Epoch  31, Step:   66000, Batch Loss:    10.732478, Lr: 0.000074, Tokens per sec:   2650
2023-03-08 23:52:08,725 - INFO - __main__ - Epoch  31, Step:   66100, Batch Loss:    10.598899, Lr: 0.000074, Tokens per sec:   2843
2023-03-08 23:52:28,877 - INFO - __main__ - Epoch  31, Step:   66200, Batch Loss:     7.288949, Lr: 0.000074, Tokens per sec:   2681
2023-03-08 23:52:48,949 - INFO - __main__ - Epoch  31, Step:   66300, Batch Loss:     9.883529, Lr: 0.000074, Tokens per sec:   2633
2023-03-08 23:53:08,198 - INFO - __main__ - Epoch  31, Step:   66400, Batch Loss:    11.248019, Lr: 0.000074, Tokens per sec:   2854
2023-03-08 23:53:28,399 - INFO - __main__ - Epoch  31, Step:   66500, Batch Loss:     7.922163, Lr: 0.000074, Tokens per sec:   2679
2023-03-08 23:53:47,591 - INFO - __main__ - Epoch  31, Step:   66600, Batch Loss:    10.916348, Lr: 0.000074, Tokens per sec:   2782
2023-03-08 23:54:07,634 - INFO - __main__ - Epoch  31, Step:   66700, Batch Loss:     9.435699, Lr: 0.000074, Tokens per sec:   2678
2023-03-08 23:54:26,970 - INFO - __main__ - Epoch  31, Step:   66800, Batch Loss:     7.078419, Lr: 0.000074, Tokens per sec:   2792
2023-03-08 23:54:46,009 - INFO - __main__ - Epoch  31, Step:   66900, Batch Loss:    10.187682, Lr: 0.000074, Tokens per sec:   2821
2023-03-08 23:55:05,581 - INFO - __main__ - Epoch  31, Step:   67000, Batch Loss:    11.036167, Lr: 0.000074, Tokens per sec:   2820
2023-03-08 23:55:24,532 - INFO - __main__ - Epoch  31, Step:   67100, Batch Loss:    10.556065, Lr: 0.000074, Tokens per sec:   2844
2023-03-08 23:55:43,914 - INFO - __main__ - Epoch  31, Step:   67200, Batch Loss:    12.095286, Lr: 0.000074, Tokens per sec:   2764
2023-03-08 23:56:03,994 - INFO - __main__ - Epoch  31, Step:   67300, Batch Loss:    13.510117, Lr: 0.000074, Tokens per sec:   2700
2023-03-08 23:56:22,655 - INFO - __main__ - Epoch  31, Step:   67400, Batch Loss:     8.955346, Lr: 0.000074, Tokens per sec:   2870
2023-03-08 23:56:42,731 - INFO - __main__ - Epoch  31, Step:   67500, Batch Loss:    10.491879, Lr: 0.000074, Tokens per sec:   2659
2023-03-08 23:56:52,048 - INFO - __main__ - Epoch  31: total training loss 21351.90
2023-03-08 23:56:52,049 - INFO - __main__ - Epoch 32
2023-03-08 23:57:02,381 - INFO - __main__ - Epoch  32, Step:   67600, Batch Loss:     6.832062, Lr: 0.000073, Tokens per sec:   2640
2023-03-08 23:57:22,427 - INFO - __main__ - Epoch  32, Step:   67700, Batch Loss:     7.796649, Lr: 0.000073, Tokens per sec:   2710
2023-03-08 23:57:41,874 - INFO - __main__ - Epoch  32, Step:   67800, Batch Loss:     6.744711, Lr: 0.000073, Tokens per sec:   2800
2023-03-08 23:58:01,098 - INFO - __main__ - Epoch  32, Step:   67900, Batch Loss:     9.783670, Lr: 0.000073, Tokens per sec:   2804
2023-03-08 23:58:21,300 - INFO - __main__ - Epoch  32, Step:   68000, Batch Loss:     6.959796, Lr: 0.000073, Tokens per sec:   2586
2023-03-08 23:58:40,478 - INFO - __main__ - Epoch  32, Step:   68100, Batch Loss:     9.770059, Lr: 0.000073, Tokens per sec:   2830
2023-03-08 23:59:00,153 - INFO - __main__ - Epoch  32, Step:   68200, Batch Loss:     7.085128, Lr: 0.000073, Tokens per sec:   2735
2023-03-08 23:59:20,050 - INFO - __main__ - Epoch  32, Step:   68300, Batch Loss:    12.794972, Lr: 0.000073, Tokens per sec:   2680
2023-03-08 23:59:39,575 - INFO - __main__ - Epoch  32, Step:   68400, Batch Loss:     8.333617, Lr: 0.000073, Tokens per sec:   2737
2023-03-08 23:59:59,492 - INFO - __main__ - Epoch  32, Step:   68500, Batch Loss:    10.945029, Lr: 0.000073, Tokens per sec:   2726
2023-03-09 00:00:19,181 - INFO - __main__ - Epoch  32, Step:   68600, Batch Loss:    10.083432, Lr: 0.000073, Tokens per sec:   2749
2023-03-09 00:00:38,666 - INFO - __main__ - Epoch  32, Step:   68700, Batch Loss:    12.337741, Lr: 0.000073, Tokens per sec:   2695
2023-03-09 00:00:58,032 - INFO - __main__ - Epoch  32, Step:   68800, Batch Loss:    10.287167, Lr: 0.000073, Tokens per sec:   2812
2023-03-09 00:01:17,619 - INFO - __main__ - Epoch  32, Step:   68900, Batch Loss:     8.094075, Lr: 0.000073, Tokens per sec:   2762
2023-03-09 00:01:37,003 - INFO - __main__ - Epoch  32, Step:   69000, Batch Loss:    10.496754, Lr: 0.000073, Tokens per sec:   2781
2023-03-09 00:01:56,815 - INFO - __main__ - Epoch  32, Step:   69100, Batch Loss:    10.556905, Lr: 0.000073, Tokens per sec:   2717
2023-03-09 00:02:16,352 - INFO - __main__ - Epoch  32, Step:   69200, Batch Loss:     7.673466, Lr: 0.000073, Tokens per sec:   2822
2023-03-09 00:02:36,299 - INFO - __main__ - Epoch  32, Step:   69300, Batch Loss:    11.225593, Lr: 0.000073, Tokens per sec:   2707
2023-03-09 00:02:56,210 - INFO - __main__ - Epoch  32, Step:   69400, Batch Loss:     8.153441, Lr: 0.000073, Tokens per sec:   2710
2023-03-09 00:03:15,628 - INFO - __main__ - Epoch  32, Step:   69500, Batch Loss:     6.519147, Lr: 0.000073, Tokens per sec:   2773
2023-03-09 00:03:35,793 - INFO - __main__ - Epoch  32, Step:   69600, Batch Loss:     8.911356, Lr: 0.000073, Tokens per sec:   2693
2023-03-09 00:03:55,135 - INFO - __main__ - Epoch  32, Step:   69700, Batch Loss:     8.575937, Lr: 0.000073, Tokens per sec:   2704
2023-03-09 00:04:00,804 - INFO - __main__ - Epoch  32: total training loss 20309.65
2023-03-09 00:04:00,804 - INFO - __main__ - Epoch 33
2023-03-09 00:04:15,234 - INFO - __main__ - Epoch  33, Step:   69800, Batch Loss:     8.039273, Lr: 0.000072, Tokens per sec:   2700
2023-03-09 00:04:34,890 - INFO - __main__ - Epoch  33, Step:   69900, Batch Loss:     8.284665, Lr: 0.000072, Tokens per sec:   2722
2023-03-09 00:04:54,635 - INFO - __main__ - Epoch  33, Step:   70000, Batch Loss:     5.766699, Lr: 0.000072, Tokens per sec:   2742
2023-03-09 00:05:14,655 - INFO - __main__ - Epoch  33, Step:   70100, Batch Loss:     8.241466, Lr: 0.000072, Tokens per sec:   2626
2023-03-09 00:05:34,721 - INFO - __main__ - Epoch  33, Step:   70200, Batch Loss:     9.173536, Lr: 0.000072, Tokens per sec:   2713
2023-03-09 00:05:54,777 - INFO - __main__ - Epoch  33, Step:   70300, Batch Loss:     7.482161, Lr: 0.000072, Tokens per sec:   2678
2023-03-09 00:06:14,811 - INFO - __main__ - Epoch  33, Step:   70400, Batch Loss:     5.238360, Lr: 0.000072, Tokens per sec:   2676
2023-03-09 00:06:34,831 - INFO - __main__ - Epoch  33, Step:   70500, Batch Loss:     7.658025, Lr: 0.000072, Tokens per sec:   2680
2023-03-09 00:06:54,882 - INFO - __main__ - Epoch  33, Step:   70600, Batch Loss:     6.868290, Lr: 0.000072, Tokens per sec:   2727
2023-03-09 00:07:14,172 - INFO - __main__ - Epoch  33, Step:   70700, Batch Loss:    11.881351, Lr: 0.000072, Tokens per sec:   2783
2023-03-09 00:07:33,385 - INFO - __main__ - Epoch  33, Step:   70800, Batch Loss:    12.073897, Lr: 0.000072, Tokens per sec:   2786
2023-03-09 00:07:52,713 - INFO - __main__ - Epoch  33, Step:   70900, Batch Loss:     9.676188, Lr: 0.000072, Tokens per sec:   2764
2023-03-09 00:08:12,910 - INFO - __main__ - Epoch  33, Step:   71000, Batch Loss:     7.616311, Lr: 0.000072, Tokens per sec:   2640
2023-03-09 00:08:33,023 - INFO - __main__ - Epoch  33, Step:   71100, Batch Loss:     7.366020, Lr: 0.000072, Tokens per sec:   2657
2023-03-09 00:08:52,525 - INFO - __main__ - Epoch  33, Step:   71200, Batch Loss:     8.064341, Lr: 0.000072, Tokens per sec:   2748
2023-03-09 00:09:12,339 - INFO - __main__ - Epoch  33, Step:   71300, Batch Loss:     8.010123, Lr: 0.000072, Tokens per sec:   2734
2023-03-09 00:09:32,531 - INFO - __main__ - Epoch  33, Step:   71400, Batch Loss:     7.707477, Lr: 0.000072, Tokens per sec:   2650
2023-03-09 00:09:52,462 - INFO - __main__ - Epoch  33, Step:   71500, Batch Loss:    11.068862, Lr: 0.000072, Tokens per sec:   2712
2023-03-09 00:10:12,471 - INFO - __main__ - Epoch  33, Step:   71600, Batch Loss:     7.656793, Lr: 0.000072, Tokens per sec:   2722
2023-03-09 00:10:31,940 - INFO - __main__ - Epoch  33, Step:   71700, Batch Loss:     9.728567, Lr: 0.000072, Tokens per sec:   2764
2023-03-09 00:10:51,329 - INFO - __main__ - Epoch  33, Step:   71800, Batch Loss:    10.476609, Lr: 0.000072, Tokens per sec:   2875
2023-03-09 00:11:11,520 - INFO - __main__ - Epoch  33, Step:   71900, Batch Loss:     9.463852, Lr: 0.000072, Tokens per sec:   2651
2023-03-09 00:11:13,021 - INFO - __main__ - Epoch  33: total training loss 19371.01
2023-03-09 00:11:13,022 - INFO - __main__ - Epoch 34
2023-03-09 00:11:32,091 - INFO - __main__ - Epoch  34, Step:   72000, Batch Loss:    10.220478, Lr: 0.000072, Tokens per sec:   2617
2023-03-09 00:11:51,962 - INFO - __main__ - Epoch  34, Step:   72100, Batch Loss:     7.632922, Lr: 0.000072, Tokens per sec:   2716
2023-03-09 00:12:11,856 - INFO - __main__ - Epoch  34, Step:   72200, Batch Loss:     7.489744, Lr: 0.000072, Tokens per sec:   2710
2023-03-09 00:12:31,919 - INFO - __main__ - Epoch  34, Step:   72300, Batch Loss:     7.550673, Lr: 0.000072, Tokens per sec:   2690
2023-03-09 00:12:51,507 - INFO - __main__ - Epoch  34, Step:   72400, Batch Loss:    10.596489, Lr: 0.000072, Tokens per sec:   2776
2023-03-09 00:13:11,502 - INFO - __main__ - Epoch  34, Step:   72500, Batch Loss:     7.721653, Lr: 0.000072, Tokens per sec:   2625
2023-03-09 00:13:31,601 - INFO - __main__ - Epoch  34, Step:   72600, Batch Loss:     9.837453, Lr: 0.000072, Tokens per sec:   2757
2023-03-09 00:13:51,564 - INFO - __main__ - Epoch  34, Step:   72700, Batch Loss:     6.488426, Lr: 0.000072, Tokens per sec:   2639
2023-03-09 00:14:11,643 - INFO - __main__ - Epoch  34, Step:   72800, Batch Loss:     8.794167, Lr: 0.000072, Tokens per sec:   2680
2023-03-09 00:14:30,574 - INFO - __main__ - Epoch  34, Step:   72900, Batch Loss:     8.536942, Lr: 0.000072, Tokens per sec:   2816
2023-03-09 00:14:50,424 - INFO - __main__ - Epoch  34, Step:   73000, Batch Loss:     6.586534, Lr: 0.000072, Tokens per sec:   2697
2023-03-09 00:15:08,944 - INFO - __main__ - Epoch  34, Step:   73100, Batch Loss:     8.527575, Lr: 0.000072, Tokens per sec:   2886
2023-03-09 00:15:27,240 - INFO - __main__ - Epoch  34, Step:   73200, Batch Loss:     6.355002, Lr: 0.000072, Tokens per sec:   2970
2023-03-09 00:15:45,467 - INFO - __main__ - Epoch  34, Step:   73300, Batch Loss:     7.008007, Lr: 0.000072, Tokens per sec:   3000
2023-03-09 00:16:03,427 - INFO - __main__ - Epoch  34, Step:   73400, Batch Loss:     7.632201, Lr: 0.000072, Tokens per sec:   2987
2023-03-09 00:16:23,204 - INFO - __main__ - Epoch  34, Step:   73500, Batch Loss:     9.905097, Lr: 0.000072, Tokens per sec:   2774
2023-03-09 00:16:43,154 - INFO - __main__ - Epoch  34, Step:   73600, Batch Loss:     7.684144, Lr: 0.000072, Tokens per sec:   2654
2023-03-09 00:17:02,862 - INFO - __main__ - Epoch  34, Step:   73700, Batch Loss:     7.595865, Lr: 0.000072, Tokens per sec:   2737
2023-03-09 00:17:22,855 - INFO - __main__ - Epoch  34, Step:   73800, Batch Loss:     6.963394, Lr: 0.000072, Tokens per sec:   2692
2023-03-09 00:17:42,813 - INFO - __main__ - Epoch  34, Step:   73900, Batch Loss:    12.028400, Lr: 0.000072, Tokens per sec:   2700
2023-03-09 00:18:02,806 - INFO - __main__ - Epoch  34, Step:   74000, Batch Loss:    10.433181, Lr: 0.000072, Tokens per sec:   2707
2023-03-09 00:18:20,067 - INFO - __main__ - Epoch  34: total training loss 18501.74
2023-03-09 00:18:20,068 - INFO - __main__ - Epoch 35
2023-03-09 00:18:22,961 - INFO - __main__ - Epoch  35, Step:   74100, Batch Loss:     7.627185, Lr: 0.000071, Tokens per sec:   2661
2023-03-09 00:18:41,139 - INFO - __main__ - Epoch  35, Step:   74200, Batch Loss:     7.080854, Lr: 0.000071, Tokens per sec:   3013
2023-03-09 00:18:59,117 - INFO - __main__ - Epoch  35, Step:   74300, Batch Loss:     5.851202, Lr: 0.000071, Tokens per sec:   3011
2023-03-09 00:19:17,099 - INFO - __main__ - Epoch  35, Step:   74400, Batch Loss:     7.959996, Lr: 0.000071, Tokens per sec:   2941
2023-03-09 00:19:35,176 - INFO - __main__ - Epoch  35, Step:   74500, Batch Loss:     8.574307, Lr: 0.000071, Tokens per sec:   3001
2023-03-09 00:19:53,170 - INFO - __main__ - Epoch  35, Step:   74600, Batch Loss:     7.483292, Lr: 0.000071, Tokens per sec:   3008
2023-03-09 00:20:11,188 - INFO - __main__ - Epoch  35, Step:   74700, Batch Loss:     8.149190, Lr: 0.000071, Tokens per sec:   2963
2023-03-09 00:20:29,340 - INFO - __main__ - Epoch  35, Step:   74800, Batch Loss:     7.861397, Lr: 0.000071, Tokens per sec:   2939
2023-03-09 00:20:47,594 - INFO - __main__ - Epoch  35, Step:   74900, Batch Loss:     7.213374, Lr: 0.000071, Tokens per sec:   2995
2023-03-09 00:21:06,027 - INFO - __main__ - Epoch  35, Step:   75000, Batch Loss:     7.587176, Lr: 0.000071, Tokens per sec:   2909
2023-03-09 00:21:26,064 - INFO - __main__ - Epoch  35, Step:   75100, Batch Loss:     7.995495, Lr: 0.000071, Tokens per sec:   2716
2023-03-09 00:21:46,012 - INFO - __main__ - Epoch  35, Step:   75200, Batch Loss:     6.705354, Lr: 0.000071, Tokens per sec:   2685
2023-03-09 00:22:04,549 - INFO - __main__ - Epoch  35, Step:   75300, Batch Loss:     9.868283, Lr: 0.000071, Tokens per sec:   2914
2023-03-09 00:22:24,298 - INFO - __main__ - Epoch  35, Step:   75400, Batch Loss:     8.948380, Lr: 0.000071, Tokens per sec:   2737
2023-03-09 00:22:43,439 - INFO - __main__ - Epoch  35, Step:   75500, Batch Loss:    10.104175, Lr: 0.000071, Tokens per sec:   2806
2023-03-09 00:23:01,668 - INFO - __main__ - Epoch  35, Step:   75600, Batch Loss:    10.451848, Lr: 0.000071, Tokens per sec:   2967
2023-03-09 00:23:21,456 - INFO - __main__ - Epoch  35, Step:   75700, Batch Loss:     8.258775, Lr: 0.000071, Tokens per sec:   2727
2023-03-09 00:23:40,743 - INFO - __main__ - Epoch  35, Step:   75800, Batch Loss:     9.701321, Lr: 0.000071, Tokens per sec:   2771
2023-03-09 00:24:00,577 - INFO - __main__ - Epoch  35, Step:   75900, Batch Loss:    13.784507, Lr: 0.000071, Tokens per sec:   2676
2023-03-09 00:24:18,711 - INFO - __main__ - Epoch  35, Step:   76000, Batch Loss:     8.616263, Lr: 0.000071, Tokens per sec:   2964
2023-03-09 00:24:37,124 - INFO - __main__ - Epoch  35, Step:   76100, Batch Loss:    10.101554, Lr: 0.000071, Tokens per sec:   2903
2023-03-09 00:24:55,060 - INFO - __main__ - Epoch  35, Step:   76200, Batch Loss:     9.624916, Lr: 0.000071, Tokens per sec:   2977
2023-03-09 00:25:06,832 - INFO - __main__ - Epoch  35: total training loss 17581.02
2023-03-09 00:25:06,834 - INFO - __main__ - Epoch 36
2023-03-09 00:25:14,149 - INFO - __main__ - Epoch  36, Step:   76300, Batch Loss:     7.253615, Lr: 0.000070, Tokens per sec:   2681
2023-03-09 00:25:33,407 - INFO - __main__ - Epoch  36, Step:   76400, Batch Loss:     8.553115, Lr: 0.000070, Tokens per sec:   2780
2023-03-09 00:25:51,355 - INFO - __main__ - Epoch  36, Step:   76500, Batch Loss:    10.155415, Lr: 0.000070, Tokens per sec:   3044
2023-03-09 00:26:11,092 - INFO - __main__ - Epoch  36, Step:   76600, Batch Loss:     9.055925, Lr: 0.000070, Tokens per sec:   2755
2023-03-09 00:26:31,069 - INFO - __main__ - Epoch  36, Step:   76700, Batch Loss:     6.346561, Lr: 0.000070, Tokens per sec:   2675
2023-03-09 00:26:49,717 - INFO - __main__ - Epoch  36, Step:   76800, Batch Loss:     9.698603, Lr: 0.000070, Tokens per sec:   2925
2023-03-09 00:27:08,006 - INFO - __main__ - Epoch  36, Step:   76900, Batch Loss:     6.359492, Lr: 0.000070, Tokens per sec:   2913
2023-03-09 00:27:27,558 - INFO - __main__ - Epoch  36, Step:   77000, Batch Loss:     6.444879, Lr: 0.000070, Tokens per sec:   2791
2023-03-09 00:27:47,558 - INFO - __main__ - Epoch  36, Step:   77100, Batch Loss:     4.591507, Lr: 0.000070, Tokens per sec:   2708
2023-03-09 00:28:07,557 - INFO - __main__ - Epoch  36, Step:   77200, Batch Loss:     8.486866, Lr: 0.000070, Tokens per sec:   2654
2023-03-09 00:28:27,529 - INFO - __main__ - Epoch  36, Step:   77300, Batch Loss:     7.051916, Lr: 0.000070, Tokens per sec:   2697
2023-03-09 00:28:46,902 - INFO - __main__ - Epoch  36, Step:   77400, Batch Loss:    10.582694, Lr: 0.000070, Tokens per sec:   2774
2023-03-09 00:29:05,152 - INFO - __main__ - Epoch  36, Step:   77500, Batch Loss:     8.611285, Lr: 0.000070, Tokens per sec:   2895
2023-03-09 00:29:23,254 - INFO - __main__ - Epoch  36, Step:   77600, Batch Loss:     7.170778, Lr: 0.000070, Tokens per sec:   2974
2023-03-09 00:29:41,403 - INFO - __main__ - Epoch  36, Step:   77700, Batch Loss:    10.845500, Lr: 0.000070, Tokens per sec:   2943
2023-03-09 00:29:59,469 - INFO - __main__ - Epoch  36, Step:   77800, Batch Loss:     5.331683, Lr: 0.000070, Tokens per sec:   2974
2023-03-09 00:30:19,191 - INFO - __main__ - Epoch  36, Step:   77900, Batch Loss:     8.180883, Lr: 0.000070, Tokens per sec:   2725
2023-03-09 00:30:39,204 - INFO - __main__ - Epoch  36, Step:   78000, Batch Loss:     6.066439, Lr: 0.000070, Tokens per sec:   2691
2023-03-09 00:30:59,195 - INFO - __main__ - Epoch  36, Step:   78100, Batch Loss:     8.664187, Lr: 0.000070, Tokens per sec:   2704
2023-03-09 00:31:19,196 - INFO - __main__ - Epoch  36, Step:   78200, Batch Loss:     6.934807, Lr: 0.000070, Tokens per sec:   2661
2023-03-09 00:31:39,248 - INFO - __main__ - Epoch  36, Step:   78300, Batch Loss:     9.192556, Lr: 0.000070, Tokens per sec:   2696
2023-03-09 00:31:59,222 - INFO - __main__ - Epoch  36, Step:   78400, Batch Loss:     8.916033, Lr: 0.000070, Tokens per sec:   2699
2023-03-09 00:32:08,112 - INFO - __main__ - Epoch  36: total training loss 16825.35
2023-03-09 00:32:08,113 - INFO - __main__ - Epoch 37
2023-03-09 00:32:19,738 - INFO - __main__ - Epoch  37, Step:   78500, Batch Loss:     8.362007, Lr: 0.000070, Tokens per sec:   2635
2023-03-09 00:32:38,869 - INFO - __main__ - Epoch  37, Step:   78600, Batch Loss:     6.494880, Lr: 0.000070, Tokens per sec:   2768
2023-03-09 00:32:57,037 - INFO - __main__ - Epoch  37, Step:   78700, Batch Loss:     6.925882, Lr: 0.000070, Tokens per sec:   2944
2023-03-09 00:33:15,195 - INFO - __main__ - Epoch  37, Step:   78800, Batch Loss:     6.692493, Lr: 0.000070, Tokens per sec:   2965
2023-03-09 00:33:33,299 - INFO - __main__ - Epoch  37, Step:   78900, Batch Loss:     6.343007, Lr: 0.000070, Tokens per sec:   2954
2023-03-09 00:33:51,524 - INFO - __main__ - Epoch  37, Step:   79000, Batch Loss:     7.236837, Lr: 0.000070, Tokens per sec:   2983
2023-03-09 00:34:10,500 - INFO - __main__ - Epoch  37, Step:   79100, Batch Loss:     6.847707, Lr: 0.000070, Tokens per sec:   2790
2023-03-09 00:34:30,528 - INFO - __main__ - Epoch  37, Step:   79200, Batch Loss:     5.632042, Lr: 0.000070, Tokens per sec:   2687
2023-03-09 00:34:50,555 - INFO - __main__ - Epoch  37, Step:   79300, Batch Loss:     8.090962, Lr: 0.000070, Tokens per sec:   2688
2023-03-09 00:35:10,599 - INFO - __main__ - Epoch  37, Step:   79400, Batch Loss:     6.607980, Lr: 0.000070, Tokens per sec:   2704
2023-03-09 00:35:30,591 - INFO - __main__ - Epoch  37, Step:   79500, Batch Loss:     9.108122, Lr: 0.000070, Tokens per sec:   2655
2023-03-09 00:35:50,590 - INFO - __main__ - Epoch  37, Step:   79600, Batch Loss:     9.987233, Lr: 0.000070, Tokens per sec:   2684
2023-03-09 00:36:09,962 - INFO - __main__ - Epoch  37, Step:   79700, Batch Loss:     6.050207, Lr: 0.000070, Tokens per sec:   2794
2023-03-09 00:36:28,131 - INFO - __main__ - Epoch  37, Step:   79800, Batch Loss:     6.474663, Lr: 0.000070, Tokens per sec:   2962
2023-03-09 00:36:48,033 - INFO - __main__ - Epoch  37, Step:   79900, Batch Loss:     9.282734, Lr: 0.000070, Tokens per sec:   2681
2023-03-09 00:37:06,266 - INFO - __main__ - Epoch  37, Step:   80000, Batch Loss:     5.605784, Lr: 0.000070, Tokens per sec:   2967
2023-03-09 00:37:25,724 - INFO - __main__ - Epoch  37, Step:   80100, Batch Loss:     5.772567, Lr: 0.000070, Tokens per sec:   2824
2023-03-09 00:37:45,744 - INFO - __main__ - Epoch  37, Step:   80200, Batch Loss:     6.243434, Lr: 0.000070, Tokens per sec:   2716
2023-03-09 00:38:05,730 - INFO - __main__ - Epoch  37, Step:   80300, Batch Loss:     9.168492, Lr: 0.000070, Tokens per sec:   2735
2023-03-09 00:38:25,695 - INFO - __main__ - Epoch  37, Step:   80400, Batch Loss:     5.535389, Lr: 0.000070, Tokens per sec:   2670
2023-03-09 00:38:45,727 - INFO - __main__ - Epoch  37, Step:   80500, Batch Loss:     8.724562, Lr: 0.000070, Tokens per sec:   2711
2023-03-09 00:39:05,725 - INFO - __main__ - Epoch  37, Step:   80600, Batch Loss:     7.856358, Lr: 0.000070, Tokens per sec:   2703
2023-03-09 00:39:10,346 - INFO - __main__ - Epoch  37: total training loss 16130.85
2023-03-09 00:39:10,347 - INFO - __main__ - Epoch 38
2023-03-09 00:39:25,995 - INFO - __main__ - Epoch  38, Step:   80700, Batch Loss:     4.950535, Lr: 0.000069, Tokens per sec:   2645
2023-03-09 00:39:44,964 - INFO - __main__ - Epoch  38, Step:   80800, Batch Loss:     5.865993, Lr: 0.000069, Tokens per sec:   2824
2023-03-09 00:40:04,222 - INFO - __main__ - Epoch  38, Step:   80900, Batch Loss:     5.791627, Lr: 0.000069, Tokens per sec:   2800
2023-03-09 00:40:23,303 - INFO - __main__ - Epoch  38, Step:   81000, Batch Loss:     6.335270, Lr: 0.000069, Tokens per sec:   2805
2023-03-09 00:40:41,654 - INFO - __main__ - Epoch  38, Step:   81100, Batch Loss:     7.116954, Lr: 0.000069, Tokens per sec:   2924
2023-03-09 00:41:00,188 - INFO - __main__ - Epoch  38, Step:   81200, Batch Loss:     5.669040, Lr: 0.000069, Tokens per sec:   2929
2023-03-09 00:41:20,083 - INFO - __main__ - Epoch  38, Step:   81300, Batch Loss:     8.064227, Lr: 0.000069, Tokens per sec:   2702
2023-03-09 00:41:40,056 - INFO - __main__ - Epoch  38, Step:   81400, Batch Loss:     7.306670, Lr: 0.000069, Tokens per sec:   2729
2023-03-09 00:41:58,290 - INFO - __main__ - Epoch  38, Step:   81500, Batch Loss:     5.547151, Lr: 0.000069, Tokens per sec:   2965
2023-03-09 00:42:16,221 - INFO - __main__ - Epoch  38, Step:   81600, Batch Loss:     6.355601, Lr: 0.000069, Tokens per sec:   3015
2023-03-09 00:42:35,318 - INFO - __main__ - Epoch  38, Step:   81700, Batch Loss:     7.983307, Lr: 0.000069, Tokens per sec:   2772
2023-03-09 00:42:53,558 - INFO - __main__ - Epoch  38, Step:   81800, Batch Loss:     7.265420, Lr: 0.000069, Tokens per sec:   2958
2023-03-09 00:43:11,544 - INFO - __main__ - Epoch  38, Step:   81900, Batch Loss:     7.202196, Lr: 0.000069, Tokens per sec:   2956
2023-03-09 00:43:30,439 - INFO - __main__ - Epoch  38, Step:   82000, Batch Loss:     7.089283, Lr: 0.000069, Tokens per sec:   2809
2023-03-09 00:43:49,797 - INFO - __main__ - Epoch  38, Step:   82100, Batch Loss:     7.967985, Lr: 0.000069, Tokens per sec:   2787
2023-03-09 00:44:08,110 - INFO - __main__ - Epoch  38, Step:   82200, Batch Loss:     8.158966, Lr: 0.000069, Tokens per sec:   2949
2023-03-09 00:44:26,774 - INFO - __main__ - Epoch  38, Step:   82300, Batch Loss:     5.225009, Lr: 0.000069, Tokens per sec:   2922
2023-03-09 00:44:45,657 - INFO - __main__ - Epoch  38, Step:   82400, Batch Loss:     8.257629, Lr: 0.000069, Tokens per sec:   2864
2023-03-09 00:45:05,632 - INFO - __main__ - Epoch  38, Step:   82500, Batch Loss:     6.053390, Lr: 0.000069, Tokens per sec:   2713
2023-03-09 00:45:25,544 - INFO - __main__ - Epoch  38, Step:   82600, Batch Loss:     9.874291, Lr: 0.000069, Tokens per sec:   2664
2023-03-09 00:45:45,517 - INFO - __main__ - Epoch  38, Step:   82700, Batch Loss:     9.736639, Lr: 0.000069, Tokens per sec:   2698
2023-03-09 00:46:05,455 - INFO - __main__ - Epoch  38, Step:   82800, Batch Loss:     7.801636, Lr: 0.000069, Tokens per sec:   2729
2023-03-09 00:46:05,942 - INFO - __main__ - Epoch  38: total training loss 15497.09
2023-03-09 00:46:05,943 - INFO - __main__ - Epoch 39
2023-03-09 00:46:24,904 - INFO - __main__ - Epoch  39, Step:   82900, Batch Loss:     6.267604, Lr: 0.000068, Tokens per sec:   2747
2023-03-09 00:46:42,947 - INFO - __main__ - Epoch  39, Step:   83000, Batch Loss:     5.870799, Lr: 0.000068, Tokens per sec:   2972
2023-03-09 00:47:01,067 - INFO - __main__ - Epoch  39, Step:   83100, Batch Loss:     4.344253, Lr: 0.000068, Tokens per sec:   3013
2023-03-09 00:47:19,107 - INFO - __main__ - Epoch  39, Step:   83200, Batch Loss:     5.232058, Lr: 0.000068, Tokens per sec:   2979
2023-03-09 00:47:37,198 - INFO - __main__ - Epoch  39, Step:   83300, Batch Loss:     7.261363, Lr: 0.000068, Tokens per sec:   3016
2023-03-09 00:47:55,241 - INFO - __main__ - Epoch  39, Step:   83400, Batch Loss:     7.707433, Lr: 0.000068, Tokens per sec:   3023
2023-03-09 00:48:14,974 - INFO - __main__ - Epoch  39, Step:   83500, Batch Loss:     6.036409, Lr: 0.000068, Tokens per sec:   2695
2023-03-09 00:48:34,970 - INFO - __main__ - Epoch  39, Step:   83600, Batch Loss:     5.139828, Lr: 0.000068, Tokens per sec:   2701
2023-03-09 00:48:54,959 - INFO - __main__ - Epoch  39, Step:   83700, Batch Loss:     8.187771, Lr: 0.000068, Tokens per sec:   2692
2023-03-09 00:49:14,948 - INFO - __main__ - Epoch  39, Step:   83800, Batch Loss:     6.554912, Lr: 0.000068, Tokens per sec:   2695
2023-03-09 00:49:34,949 - INFO - __main__ - Epoch  39, Step:   83900, Batch Loss:     8.637917, Lr: 0.000068, Tokens per sec:   2720
2023-03-09 00:49:54,205 - INFO - __main__ - Epoch  39, Step:   84000, Batch Loss:     6.817701, Lr: 0.000068, Tokens per sec:   2821
2023-03-09 00:50:14,166 - INFO - __main__ - Epoch  39, Step:   84100, Batch Loss:     7.712757, Lr: 0.000068, Tokens per sec:   2697
2023-03-09 00:50:34,084 - INFO - __main__ - Epoch  39, Step:   84200, Batch Loss:     5.322019, Lr: 0.000068, Tokens per sec:   2690
2023-03-09 00:50:53,359 - INFO - __main__ - Epoch  39, Step:   84300, Batch Loss:     7.150746, Lr: 0.000068, Tokens per sec:   2745
2023-03-09 00:51:13,416 - INFO - __main__ - Epoch  39, Step:   84400, Batch Loss:    10.620062, Lr: 0.000068, Tokens per sec:   2696
2023-03-09 00:51:33,328 - INFO - __main__ - Epoch  39, Step:   84500, Batch Loss:     6.246420, Lr: 0.000068, Tokens per sec:   2689
2023-03-09 00:51:53,328 - INFO - __main__ - Epoch  39, Step:   84600, Batch Loss:     6.884135, Lr: 0.000068, Tokens per sec:   2664
2023-03-09 00:52:13,011 - INFO - __main__ - Epoch  39, Step:   84700, Batch Loss:     7.119628, Lr: 0.000068, Tokens per sec:   2754
2023-03-09 00:52:31,031 - INFO - __main__ - Epoch  39, Step:   84800, Batch Loss:     9.649857, Lr: 0.000068, Tokens per sec:   3022
2023-03-09 00:52:49,044 - INFO - __main__ - Epoch  39, Step:   84900, Batch Loss:     8.423461, Lr: 0.000068, Tokens per sec:   2946
2023-03-09 00:53:03,662 - INFO - __main__ - Epoch  39: total training loss 14861.63
2023-03-09 00:53:03,663 - INFO - __main__ - Epoch 40
2023-03-09 00:53:07,782 - INFO - __main__ - Epoch  40, Step:   85000, Batch Loss:     3.861689, Lr: 0.000068, Tokens per sec:   2442
2023-03-09 00:53:27,674 - INFO - __main__ - Epoch  40, Step:   85100, Batch Loss:     4.748182, Lr: 0.000068, Tokens per sec:   2676
2023-03-09 00:53:47,667 - INFO - __main__ - Epoch  40, Step:   85200, Batch Loss:     6.117016, Lr: 0.000068, Tokens per sec:   2677
2023-03-09 00:54:07,050 - INFO - __main__ - Epoch  40, Step:   85300, Batch Loss:     5.783389, Lr: 0.000068, Tokens per sec:   2759
2023-03-09 00:54:25,391 - INFO - __main__ - Epoch  40, Step:   85400, Batch Loss:     5.887295, Lr: 0.000068, Tokens per sec:   2931
2023-03-09 00:54:43,475 - INFO - __main__ - Epoch  40, Step:   85500, Batch Loss:     4.874142, Lr: 0.000068, Tokens per sec:   2964
2023-03-09 00:55:02,304 - INFO - __main__ - Epoch  40, Step:   85600, Batch Loss:     5.268268, Lr: 0.000068, Tokens per sec:   2843
2023-03-09 00:55:22,234 - INFO - __main__ - Epoch  40, Step:   85700, Batch Loss:     6.017600, Lr: 0.000068, Tokens per sec:   2719
2023-03-09 00:55:41,843 - INFO - __main__ - Epoch  40, Step:   85800, Batch Loss:    10.525353, Lr: 0.000068, Tokens per sec:   2782
2023-03-09 00:56:01,204 - INFO - __main__ - Epoch  40, Step:   85900, Batch Loss:     5.899469, Lr: 0.000068, Tokens per sec:   2763
2023-03-09 00:56:21,160 - INFO - __main__ - Epoch  40, Step:   86000, Batch Loss:     6.216311, Lr: 0.000068, Tokens per sec:   2706
2023-03-09 00:56:41,111 - INFO - __main__ - Epoch  40, Step:   86100, Batch Loss:     7.292754, Lr: 0.000068, Tokens per sec:   2745
2023-03-09 00:56:59,547 - INFO - __main__ - Epoch  40, Step:   86200, Batch Loss:     7.862648, Lr: 0.000068, Tokens per sec:   2922
2023-03-09 00:57:18,140 - INFO - __main__ - Epoch  40, Step:   86300, Batch Loss:     5.248489, Lr: 0.000068, Tokens per sec:   2912
2023-03-09 00:57:37,516 - INFO - __main__ - Epoch  40, Step:   86400, Batch Loss:     6.535721, Lr: 0.000068, Tokens per sec:   2750
2023-03-09 00:57:56,273 - INFO - __main__ - Epoch  40, Step:   86500, Batch Loss:     6.507796, Lr: 0.000068, Tokens per sec:   2865
2023-03-09 00:58:14,801 - INFO - __main__ - Epoch  40, Step:   86600, Batch Loss:     6.978240, Lr: 0.000068, Tokens per sec:   2883
2023-03-09 00:58:32,750 - INFO - __main__ - Epoch  40, Step:   86700, Batch Loss:     6.262428, Lr: 0.000068, Tokens per sec:   2987
2023-03-09 00:58:51,427 - INFO - __main__ - Epoch  40, Step:   86800, Batch Loss:     6.892502, Lr: 0.000068, Tokens per sec:   2954
2023-03-09 00:59:09,878 - INFO - __main__ - Epoch  40, Step:   86900, Batch Loss:     8.287833, Lr: 0.000068, Tokens per sec:   2936
2023-03-09 00:59:27,835 - INFO - __main__ - Epoch  40, Step:   87000, Batch Loss:     6.980051, Lr: 0.000068, Tokens per sec:   2973
2023-03-09 00:59:45,954 - INFO - __main__ - Epoch  40, Step:   87100, Batch Loss:     4.105891, Lr: 0.000068, Tokens per sec:   2938
2023-03-09 00:59:57,295 - INFO - __main__ - Epoch  40: total training loss 14295.29
2023-03-09 00:59:57,297 - INFO - __main__ - Epoch 41
2023-03-09 01:00:05,257 - INFO - __main__ - Epoch  41, Step:   87200, Batch Loss:     6.501528, Lr: 0.000067, Tokens per sec:   2683
2023-03-09 01:00:24,059 - INFO - __main__ - Epoch  41, Step:   87300, Batch Loss:     6.829215, Lr: 0.000067, Tokens per sec:   2875
2023-03-09 01:00:44,038 - INFO - __main__ - Epoch  41, Step:   87400, Batch Loss:     6.433887, Lr: 0.000067, Tokens per sec:   2718
2023-03-09 01:01:02,930 - INFO - __main__ - Epoch  41, Step:   87500, Batch Loss:     4.127584, Lr: 0.000067, Tokens per sec:   2826
2023-03-09 01:01:22,060 - INFO - __main__ - Epoch  41, Step:   87600, Batch Loss:     6.427687, Lr: 0.000067, Tokens per sec:   2861
2023-03-09 01:01:42,035 - INFO - __main__ - Epoch  41, Step:   87700, Batch Loss:     4.532934, Lr: 0.000067, Tokens per sec:   2650
2023-03-09 01:02:00,404 - INFO - __main__ - Epoch  41, Step:   87800, Batch Loss:     3.282817, Lr: 0.000067, Tokens per sec:   2986
2023-03-09 01:02:18,985 - INFO - __main__ - Epoch  41, Step:   87900, Batch Loss:     4.264226, Lr: 0.000067, Tokens per sec:   2896
2023-03-09 01:02:38,960 - INFO - __main__ - Epoch  41, Step:   88000, Batch Loss:     5.612425, Lr: 0.000067, Tokens per sec:   2689
2023-03-09 01:02:58,913 - INFO - __main__ - Epoch  41, Step:   88100, Batch Loss:     5.214246, Lr: 0.000067, Tokens per sec:   2657
2023-03-09 01:03:18,882 - INFO - __main__ - Epoch  41, Step:   88200, Batch Loss:     7.361035, Lr: 0.000067, Tokens per sec:   2691
2023-03-09 01:03:38,678 - INFO - __main__ - Epoch  41, Step:   88300, Batch Loss:     3.847510, Lr: 0.000067, Tokens per sec:   2663
2023-03-09 01:03:58,473 - INFO - __main__ - Epoch  41, Step:   88400, Batch Loss:     7.809521, Lr: 0.000067, Tokens per sec:   2737
2023-03-09 01:04:18,120 - INFO - __main__ - Epoch  41, Step:   88500, Batch Loss:     7.718741, Lr: 0.000067, Tokens per sec:   2717
2023-03-09 01:04:38,065 - INFO - __main__ - Epoch  41, Step:   88600, Batch Loss:     6.131479, Lr: 0.000067, Tokens per sec:   2695
2023-03-09 01:04:56,166 - INFO - __main__ - Epoch  41, Step:   88700, Batch Loss:     6.292593, Lr: 0.000067, Tokens per sec:   2998
2023-03-09 01:05:14,135 - INFO - __main__ - Epoch  41, Step:   88800, Batch Loss:     7.966109, Lr: 0.000067, Tokens per sec:   2968
2023-03-09 01:05:32,624 - INFO - __main__ - Epoch  41, Step:   88900, Batch Loss:     5.147748, Lr: 0.000067, Tokens per sec:   2970
2023-03-09 01:05:52,492 - INFO - __main__ - Epoch  41, Step:   89000, Batch Loss:     4.780237, Lr: 0.000067, Tokens per sec:   2709
2023-03-09 01:06:11,285 - INFO - __main__ - Epoch  41, Step:   89100, Batch Loss:     5.326712, Lr: 0.000067, Tokens per sec:   2902
2023-03-09 01:06:31,003 - INFO - __main__ - Epoch  41, Step:   89200, Batch Loss:     5.877409, Lr: 0.000067, Tokens per sec:   2722
2023-03-09 01:06:50,639 - INFO - __main__ - Epoch  41, Step:   89300, Batch Loss:     7.086519, Lr: 0.000067, Tokens per sec:   2735
2023-03-09 01:06:58,480 - INFO - __main__ - Epoch  41: total training loss 13791.12
2023-03-09 01:06:58,481 - INFO - __main__ - Epoch 42
2023-03-09 01:07:10,994 - INFO - __main__ - Epoch  42, Step:   89400, Batch Loss:     4.891880, Lr: 0.000066, Tokens per sec:   2581
2023-03-09 01:07:30,939 - INFO - __main__ - Epoch  42, Step:   89500, Batch Loss:     3.473462, Lr: 0.000066, Tokens per sec:   2673
2023-03-09 01:07:50,720 - INFO - __main__ - Epoch  42, Step:   89600, Batch Loss:     7.385296, Lr: 0.000066, Tokens per sec:   2690
2023-03-09 01:08:10,714 - INFO - __main__ - Epoch  42, Step:   89700, Batch Loss:     6.318305, Lr: 0.000066, Tokens per sec:   2695
2023-03-09 01:08:30,687 - INFO - __main__ - Epoch  42, Step:   89800, Batch Loss:     7.307734, Lr: 0.000066, Tokens per sec:   2685
2023-03-09 01:08:50,671 - INFO - __main__ - Epoch  42, Step:   89900, Batch Loss:     6.711007, Lr: 0.000066, Tokens per sec:   2740
2023-03-09 01:09:10,671 - INFO - __main__ - Epoch  42, Step:   90000, Batch Loss:     4.342995, Lr: 0.000066, Tokens per sec:   2720
2023-03-09 01:09:30,556 - INFO - __main__ - Epoch  42, Step:   90100, Batch Loss:     6.651540, Lr: 0.000066, Tokens per sec:   2701
2023-03-09 01:09:50,583 - INFO - __main__ - Epoch  42, Step:   90200, Batch Loss:     5.088219, Lr: 0.000066, Tokens per sec:   2645
2023-03-09 01:10:10,654 - INFO - __main__ - Epoch  42, Step:   90300, Batch Loss:     4.707132, Lr: 0.000066, Tokens per sec:   2743
2023-03-09 01:10:30,525 - INFO - __main__ - Epoch  42, Step:   90400, Batch Loss:     4.215363, Lr: 0.000066, Tokens per sec:   2673
2023-03-09 01:10:49,242 - INFO - __main__ - Epoch  42, Step:   90500, Batch Loss:     5.648202, Lr: 0.000066, Tokens per sec:   2872
2023-03-09 01:11:07,466 - INFO - __main__ - Epoch  42, Step:   90600, Batch Loss:     6.774614, Lr: 0.000066, Tokens per sec:   2927
2023-03-09 01:11:26,246 - INFO - __main__ - Epoch  42, Step:   90700, Batch Loss:     3.799852, Lr: 0.000066, Tokens per sec:   2869
2023-03-09 01:11:45,654 - INFO - __main__ - Epoch  42, Step:   90800, Batch Loss:     7.282623, Lr: 0.000066, Tokens per sec:   2782
2023-03-09 01:12:05,743 - INFO - __main__ - Epoch  42, Step:   90900, Batch Loss:     6.316599, Lr: 0.000066, Tokens per sec:   2669
2023-03-09 01:12:25,731 - INFO - __main__ - Epoch  42, Step:   91000, Batch Loss:     4.836501, Lr: 0.000066, Tokens per sec:   2684
2023-03-09 01:12:44,924 - INFO - __main__ - Epoch  42, Step:   91100, Batch Loss:     6.150802, Lr: 0.000066, Tokens per sec:   2849
2023-03-09 01:13:04,879 - INFO - __main__ - Epoch  42, Step:   91200, Batch Loss:     6.852106, Lr: 0.000066, Tokens per sec:   2684
2023-03-09 01:13:24,912 - INFO - __main__ - Epoch  42, Step:   91300, Batch Loss:     6.092174, Lr: 0.000066, Tokens per sec:   2663
2023-03-09 01:13:44,888 - INFO - __main__ - Epoch  42, Step:   91400, Batch Loss:     8.468011, Lr: 0.000066, Tokens per sec:   2705
2023-03-09 01:14:04,905 - INFO - __main__ - Epoch  42, Step:   91500, Batch Loss:     6.328516, Lr: 0.000066, Tokens per sec:   2743
2023-03-09 01:14:08,552 - INFO - __main__ - Epoch  42: total training loss 13221.26
2023-03-09 01:14:08,553 - INFO - __main__ - Epoch 43
2023-03-09 01:14:25,274 - INFO - __main__ - Epoch  43, Step:   91600, Batch Loss:     7.150401, Lr: 0.000066, Tokens per sec:   2649
2023-03-09 01:14:43,905 - INFO - __main__ - Epoch  43, Step:   91700, Batch Loss:     4.896695, Lr: 0.000066, Tokens per sec:   2882
2023-03-09 01:15:03,941 - INFO - __main__ - Epoch  43, Step:   91800, Batch Loss:     3.783389, Lr: 0.000066, Tokens per sec:   2656
2023-03-09 01:15:22,553 - INFO - __main__ - Epoch  43, Step:   91900, Batch Loss:     3.918783, Lr: 0.000066, Tokens per sec:   2905
2023-03-09 01:15:42,286 - INFO - __main__ - Epoch  43, Step:   92000, Batch Loss:     6.098608, Lr: 0.000066, Tokens per sec:   2779
2023-03-09 01:16:02,202 - INFO - __main__ - Epoch  43, Step:   92100, Batch Loss:     2.295653, Lr: 0.000066, Tokens per sec:   2722
2023-03-09 01:16:22,129 - INFO - __main__ - Epoch  43, Step:   92200, Batch Loss:     5.669984, Lr: 0.000066, Tokens per sec:   2712
2023-03-09 01:16:41,367 - INFO - __main__ - Epoch  43, Step:   92300, Batch Loss:     6.038382, Lr: 0.000066, Tokens per sec:   2783
2023-03-09 01:17:00,245 - INFO - __main__ - Epoch  43, Step:   92400, Batch Loss:     6.324387, Lr: 0.000066, Tokens per sec:   2833
2023-03-09 01:17:19,401 - INFO - __main__ - Epoch  43, Step:   92500, Batch Loss:     7.953789, Lr: 0.000066, Tokens per sec:   2843
2023-03-09 01:17:37,411 - INFO - __main__ - Epoch  43, Step:   92600, Batch Loss:     5.873692, Lr: 0.000066, Tokens per sec:   2972
2023-03-09 01:17:56,499 - INFO - __main__ - Epoch  43, Step:   92700, Batch Loss:     6.136534, Lr: 0.000066, Tokens per sec:   2811
2023-03-09 01:18:16,520 - INFO - __main__ - Epoch  43, Step:   92800, Batch Loss:     4.899069, Lr: 0.000066, Tokens per sec:   2689
2023-03-09 01:18:36,493 - INFO - __main__ - Epoch  43, Step:   92900, Batch Loss:     4.945300, Lr: 0.000066, Tokens per sec:   2744
2023-03-09 01:18:56,267 - INFO - __main__ - Epoch  43, Step:   93000, Batch Loss:     5.813730, Lr: 0.000066, Tokens per sec:   2716
2023-03-09 01:19:14,524 - INFO - __main__ - Epoch  43, Step:   93100, Batch Loss:     7.461460, Lr: 0.000066, Tokens per sec:   2950
2023-03-09 01:19:32,740 - INFO - __main__ - Epoch  43, Step:   93200, Batch Loss:     7.204975, Lr: 0.000066, Tokens per sec:   2930
2023-03-09 01:19:50,786 - INFO - __main__ - Epoch  43, Step:   93300, Batch Loss:     6.436614, Lr: 0.000066, Tokens per sec:   2953
2023-03-09 01:20:08,742 - INFO - __main__ - Epoch  43, Step:   93400, Batch Loss:     4.964510, Lr: 0.000066, Tokens per sec:   2956
2023-03-09 01:20:27,179 - INFO - __main__ - Epoch  43, Step:   93500, Batch Loss:     8.384987, Lr: 0.000066, Tokens per sec:   2940
2023-03-09 01:20:46,887 - INFO - __main__ - Epoch  43, Step:   93600, Batch Loss:     5.171874, Lr: 0.000066, Tokens per sec:   2660
2023-03-09 01:21:05,960 - INFO - __main__ - Epoch  43: total training loss 12738.07
2023-03-09 01:21:05,961 - INFO - __main__ - Epoch 44
2023-03-09 01:21:06,861 - INFO - __main__ - Epoch  44, Step:   93700, Batch Loss:     4.847341, Lr: 0.000065, Tokens per sec:   1636
2023-03-09 01:21:26,882 - INFO - __main__ - Epoch  44, Step:   93800, Batch Loss:     4.473395, Lr: 0.000065, Tokens per sec:   2712
2023-03-09 01:21:46,912 - INFO - __main__ - Epoch  44, Step:   93900, Batch Loss:     3.623973, Lr: 0.000065, Tokens per sec:   2711
2023-03-09 01:22:06,902 - INFO - __main__ - Epoch  44, Step:   94000, Batch Loss:     4.169925, Lr: 0.000065, Tokens per sec:   2639
2023-03-09 01:22:26,925 - INFO - __main__ - Epoch  44, Step:   94100, Batch Loss:     5.089506, Lr: 0.000065, Tokens per sec:   2657
2023-03-09 01:22:46,528 - INFO - __main__ - Epoch  44, Step:   94200, Batch Loss:     4.933184, Lr: 0.000065, Tokens per sec:   2701
2023-03-09 01:23:04,589 - INFO - __main__ - Epoch  44, Step:   94300, Batch Loss:     7.459325, Lr: 0.000065, Tokens per sec:   2995
2023-03-09 01:23:23,099 - INFO - __main__ - Epoch  44, Step:   94400, Batch Loss:     5.901272, Lr: 0.000065, Tokens per sec:   2884
2023-03-09 01:23:41,380 - INFO - __main__ - Epoch  44, Step:   94500, Batch Loss:     9.419219, Lr: 0.000065, Tokens per sec:   2942
2023-03-09 01:23:59,427 - INFO - __main__ - Epoch  44, Step:   94600, Batch Loss:     5.502504, Lr: 0.000065, Tokens per sec:   2978
2023-03-09 01:24:17,744 - INFO - __main__ - Epoch  44, Step:   94700, Batch Loss:     3.810772, Lr: 0.000065, Tokens per sec:   2946
2023-03-09 01:24:36,255 - INFO - __main__ - Epoch  44, Step:   94800, Batch Loss:     7.346193, Lr: 0.000065, Tokens per sec:   2933
2023-03-09 01:24:56,249 - INFO - __main__ - Epoch  44, Step:   94900, Batch Loss:     9.245394, Lr: 0.000065, Tokens per sec:   2747
2023-03-09 01:25:15,863 - INFO - __main__ - Epoch  44, Step:   95000, Batch Loss:     6.196193, Lr: 0.000065, Tokens per sec:   2774
2023-03-09 01:25:34,157 - INFO - __main__ - Epoch  44, Step:   95100, Batch Loss:     4.361753, Lr: 0.000065, Tokens per sec:   2986
2023-03-09 01:25:52,629 - INFO - __main__ - Epoch  44, Step:   95200, Batch Loss:     4.941038, Lr: 0.000065, Tokens per sec:   2916
2023-03-09 01:26:11,490 - INFO - __main__ - Epoch  44, Step:   95300, Batch Loss:     6.503948, Lr: 0.000065, Tokens per sec:   2868
2023-03-09 01:26:29,940 - INFO - __main__ - Epoch  44, Step:   95400, Batch Loss:     5.672209, Lr: 0.000065, Tokens per sec:   2922
2023-03-09 01:26:48,414 - INFO - __main__ - Epoch  44, Step:   95500, Batch Loss:     7.843083, Lr: 0.000065, Tokens per sec:   2876
2023-03-09 01:27:08,412 - INFO - __main__ - Epoch  44, Step:   95600, Batch Loss:     7.066602, Lr: 0.000065, Tokens per sec:   2662
2023-03-09 01:27:28,310 - INFO - __main__ - Epoch  44, Step:   95700, Batch Loss:     6.321248, Lr: 0.000065, Tokens per sec:   2714
2023-03-09 01:27:48,312 - INFO - __main__ - Epoch  44, Step:   95800, Batch Loss:     7.899281, Lr: 0.000065, Tokens per sec:   2650
2023-03-09 01:28:03,406 - INFO - __main__ - Epoch  44: total training loss 12291.39
2023-03-09 01:28:03,407 - INFO - __main__ - Epoch 45
2023-03-09 01:28:08,242 - INFO - __main__ - Epoch  45, Step:   95900, Batch Loss:     4.213621, Lr: 0.000064, Tokens per sec:   2597
2023-03-09 01:28:27,923 - INFO - __main__ - Epoch  45, Step:   96000, Batch Loss:     5.525191, Lr: 0.000064, Tokens per sec:   2719
2023-03-09 01:28:46,671 - INFO - __main__ - Epoch  45, Step:   96100, Batch Loss:     5.597036, Lr: 0.000064, Tokens per sec:   2855
2023-03-09 01:29:06,655 - INFO - __main__ - Epoch  45, Step:   96200, Batch Loss:     5.170329, Lr: 0.000064, Tokens per sec:   2710
2023-03-09 01:29:26,101 - INFO - __main__ - Epoch  45, Step:   96300, Batch Loss:     3.774573, Lr: 0.000064, Tokens per sec:   2753
2023-03-09 01:29:44,284 - INFO - __main__ - Epoch  45, Step:   96400, Batch Loss:     5.415068, Lr: 0.000064, Tokens per sec:   2971
2023-03-09 01:30:03,096 - INFO - __main__ - Epoch  45, Step:   96500, Batch Loss:     4.732349, Lr: 0.000064, Tokens per sec:   2821
2023-03-09 01:30:21,710 - INFO - __main__ - Epoch  45, Step:   96600, Batch Loss:     4.488712, Lr: 0.000064, Tokens per sec:   2884
2023-03-09 01:30:39,904 - INFO - __main__ - Epoch  45, Step:   96700, Batch Loss:     6.028690, Lr: 0.000064, Tokens per sec:   2961
2023-03-09 01:30:58,511 - INFO - __main__ - Epoch  45, Step:   96800, Batch Loss:     6.074880, Lr: 0.000064, Tokens per sec:   2928
2023-03-09 01:31:17,072 - INFO - __main__ - Epoch  45, Step:   96900, Batch Loss:     4.194029, Lr: 0.000064, Tokens per sec:   2931
2023-03-09 01:31:36,266 - INFO - __main__ - Epoch  45, Step:   97000, Batch Loss:     5.038362, Lr: 0.000064, Tokens per sec:   2759
2023-03-09 01:31:54,651 - INFO - __main__ - Epoch  45, Step:   97100, Batch Loss:     5.874870, Lr: 0.000064, Tokens per sec:   2920
2023-03-09 01:32:13,771 - INFO - __main__ - Epoch  45, Step:   97200, Batch Loss:     6.719581, Lr: 0.000064, Tokens per sec:   2885
2023-03-09 01:32:32,341 - INFO - __main__ - Epoch  45, Step:   97300, Batch Loss:     4.310212, Lr: 0.000064, Tokens per sec:   2895
2023-03-09 01:32:50,958 - INFO - __main__ - Epoch  45, Step:   97400, Batch Loss:     5.044551, Lr: 0.000064, Tokens per sec:   2889
2023-03-09 01:33:09,730 - INFO - __main__ - Epoch  45, Step:   97500, Batch Loss:     4.020742, Lr: 0.000064, Tokens per sec:   2845
2023-03-09 01:33:28,401 - INFO - __main__ - Epoch  45, Step:   97600, Batch Loss:     4.814735, Lr: 0.000064, Tokens per sec:   2844
2023-03-09 01:33:47,035 - INFO - __main__ - Epoch  45, Step:   97700, Batch Loss:     4.613291, Lr: 0.000064, Tokens per sec:   2886
2023-03-09 01:34:06,592 - INFO - __main__ - Epoch  45, Step:   97800, Batch Loss:     7.467018, Lr: 0.000064, Tokens per sec:   2738
2023-03-09 01:34:26,610 - INFO - __main__ - Epoch  45, Step:   97900, Batch Loss:    10.871973, Lr: 0.000064, Tokens per sec:   2756
2023-03-09 01:34:46,380 - INFO - __main__ - Epoch  45, Step:   98000, Batch Loss:     6.811210, Lr: 0.000064, Tokens per sec:   2756
2023-03-09 01:34:57,389 - INFO - __main__ - Epoch  45: total training loss 11865.26
2023-03-09 01:34:57,390 - INFO - __main__ - Epoch 46
2023-03-09 01:35:06,767 - INFO - __main__ - Epoch  46, Step:   98100, Batch Loss:     4.724780, Lr: 0.000064, Tokens per sec:   2576
2023-03-09 01:35:26,807 - INFO - __main__ - Epoch  46, Step:   98200, Batch Loss:     3.775509, Lr: 0.000064, Tokens per sec:   2693
2023-03-09 01:35:46,858 - INFO - __main__ - Epoch  46, Step:   98300, Batch Loss:     4.101702, Lr: 0.000064, Tokens per sec:   2702
2023-03-09 01:36:06,686 - INFO - __main__ - Epoch  46, Step:   98400, Batch Loss:     6.140569, Lr: 0.000064, Tokens per sec:   2713
2023-03-09 01:36:26,734 - INFO - __main__ - Epoch  46, Step:   98500, Batch Loss:     4.543269, Lr: 0.000064, Tokens per sec:   2712
2023-03-09 01:36:46,792 - INFO - __main__ - Epoch  46, Step:   98600, Batch Loss:     4.967992, Lr: 0.000064, Tokens per sec:   2676
2023-03-09 01:37:06,349 - INFO - __main__ - Epoch  46, Step:   98700, Batch Loss:     5.577562, Lr: 0.000064, Tokens per sec:   2757
2023-03-09 01:37:25,293 - INFO - __main__ - Epoch  46, Step:   98800, Batch Loss:     4.141094, Lr: 0.000064, Tokens per sec:   2889
2023-03-09 01:37:45,285 - INFO - __main__ - Epoch  46, Step:   98900, Batch Loss:     5.448645, Lr: 0.000064, Tokens per sec:   2640
2023-03-09 01:38:05,295 - INFO - __main__ - Epoch  46, Step:   99000, Batch Loss:     7.172646, Lr: 0.000064, Tokens per sec:   2659
2023-03-09 01:38:25,333 - INFO - __main__ - Epoch  46, Step:   99100, Batch Loss:     3.986171, Lr: 0.000064, Tokens per sec:   2685
2023-03-09 01:38:45,133 - INFO - __main__ - Epoch  46, Step:   99200, Batch Loss:     5.944046, Lr: 0.000064, Tokens per sec:   2781
2023-03-09 01:39:04,214 - INFO - __main__ - Epoch  46, Step:   99300, Batch Loss:     3.972310, Lr: 0.000064, Tokens per sec:   2788
2023-03-09 01:39:24,269 - INFO - __main__ - Epoch  46, Step:   99400, Batch Loss:     7.555029, Lr: 0.000064, Tokens per sec:   2713
2023-03-09 01:39:44,281 - INFO - __main__ - Epoch  46, Step:   99500, Batch Loss:     5.405257, Lr: 0.000064, Tokens per sec:   2673
2023-03-09 01:40:03,784 - INFO - __main__ - Epoch  46, Step:   99600, Batch Loss:     5.672512, Lr: 0.000064, Tokens per sec:   2766
2023-03-09 01:40:23,690 - INFO - __main__ - Epoch  46, Step:   99700, Batch Loss:     5.608131, Lr: 0.000064, Tokens per sec:   2724
2023-03-09 01:40:43,716 - INFO - __main__ - Epoch  46, Step:   99800, Batch Loss:     6.256657, Lr: 0.000064, Tokens per sec:   2677
2023-03-09 01:41:03,206 - INFO - __main__ - Epoch  46, Step:   99900, Batch Loss:     6.571679, Lr: 0.000064, Tokens per sec:   2756
2023-03-09 01:41:21,771 - INFO - __main__ - Epoch  46, Step:  100000, Batch Loss:     5.710934, Lr: 0.000064, Tokens per sec:   2889
2023-03-09 01:41:40,285 - INFO - __main__ - Epoch  46, Step:  100100, Batch Loss:     4.421712, Lr: 0.000064, Tokens per sec:   2851
2023-03-09 01:42:00,167 - INFO - __main__ - Epoch  46, Step:  100200, Batch Loss:     5.829193, Lr: 0.000064, Tokens per sec:   2736
2023-03-09 01:42:06,596 - INFO - __main__ - Epoch  46: total training loss 11484.44
2023-03-09 01:42:06,597 - INFO - __main__ - Epoch 47
2023-03-09 01:42:20,246 - INFO - __main__ - Epoch  47, Step:  100300, Batch Loss:     7.423074, Lr: 0.000063, Tokens per sec:   2582
2023-03-09 01:42:40,446 - INFO - __main__ - Epoch  47, Step:  100400, Batch Loss:     5.389026, Lr: 0.000063, Tokens per sec:   2655
2023-03-09 01:43:00,646 - INFO - __main__ - Epoch  47, Step:  100500, Batch Loss:     2.521686, Lr: 0.000063, Tokens per sec:   2695
2023-03-09 01:43:20,912 - INFO - __main__ - Epoch  47, Step:  100600, Batch Loss:     3.680943, Lr: 0.000063, Tokens per sec:   2665
2023-03-09 01:43:41,131 - INFO - __main__ - Epoch  47, Step:  100700, Batch Loss:     8.814684, Lr: 0.000063, Tokens per sec:   2669
2023-03-09 01:44:00,993 - INFO - __main__ - Epoch  47, Step:  100800, Batch Loss:     4.064120, Lr: 0.000063, Tokens per sec:   2723
2023-03-09 01:44:20,999 - INFO - __main__ - Epoch  47, Step:  100900, Batch Loss:     5.293577, Lr: 0.000063, Tokens per sec:   2698
2023-03-09 01:44:41,181 - INFO - __main__ - Epoch  47, Step:  101000, Batch Loss:     4.703496, Lr: 0.000063, Tokens per sec:   2627
2023-03-09 01:45:01,387 - INFO - __main__ - Epoch  47, Step:  101100, Batch Loss:     5.085697, Lr: 0.000063, Tokens per sec:   2677
2023-03-09 01:45:21,576 - INFO - __main__ - Epoch  47, Step:  101200, Batch Loss:     3.494799, Lr: 0.000063, Tokens per sec:   2706
2023-03-09 01:45:41,729 - INFO - __main__ - Epoch  47, Step:  101300, Batch Loss:     5.260929, Lr: 0.000063, Tokens per sec:   2635
2023-03-09 01:46:01,839 - INFO - __main__ - Epoch  47, Step:  101400, Batch Loss:     7.221900, Lr: 0.000063, Tokens per sec:   2685
2023-03-09 01:46:22,215 - INFO - __main__ - Epoch  47, Step:  101500, Batch Loss:     3.936296, Lr: 0.000063, Tokens per sec:   2655
2023-03-09 01:46:42,170 - INFO - __main__ - Epoch  47, Step:  101600, Batch Loss:     4.551049, Lr: 0.000063, Tokens per sec:   2687
2023-03-09 01:47:02,301 - INFO - __main__ - Epoch  47, Step:  101700, Batch Loss:     7.690184, Lr: 0.000063, Tokens per sec:   2632
2023-03-09 01:47:22,459 - INFO - __main__ - Epoch  47, Step:  101800, Batch Loss:     3.795338, Lr: 0.000063, Tokens per sec:   2693
2023-03-09 01:47:42,764 - INFO - __main__ - Epoch  47, Step:  101900, Batch Loss:     3.455896, Lr: 0.000063, Tokens per sec:   2613
2023-03-09 01:48:03,026 - INFO - __main__ - Epoch  47, Step:  102000, Batch Loss:     6.565786, Lr: 0.000063, Tokens per sec:   2637
2023-03-09 01:48:23,270 - INFO - __main__ - Epoch  47, Step:  102100, Batch Loss:     6.456286, Lr: 0.000063, Tokens per sec:   2693
2023-03-09 01:48:43,601 - INFO - __main__ - Epoch  47, Step:  102200, Batch Loss:     6.001505, Lr: 0.000063, Tokens per sec:   2612
2023-03-09 01:49:03,847 - INFO - __main__ - Epoch  47, Step:  102300, Batch Loss:     4.129250, Lr: 0.000063, Tokens per sec:   2669
2023-03-09 01:49:24,021 - INFO - __main__ - Epoch  47, Step:  102400, Batch Loss:     6.145808, Lr: 0.000063, Tokens per sec:   2706
2023-03-09 01:49:26,674 - INFO - __main__ - Epoch  47: total training loss 11127.24
2023-03-09 01:49:26,675 - INFO - __main__ - Epoch 48
2023-03-09 01:49:44,657 - INFO - __main__ - Epoch  48, Step:  102500, Batch Loss:     4.754237, Lr: 0.000062, Tokens per sec:   2590
2023-03-09 01:50:04,908 - INFO - __main__ - Epoch  48, Step:  102600, Batch Loss:     4.988129, Lr: 0.000062, Tokens per sec:   2662
2023-03-09 01:50:24,529 - INFO - __main__ - Epoch  48, Step:  102700, Batch Loss:     3.188942, Lr: 0.000062, Tokens per sec:   2743
2023-03-09 01:50:44,728 - INFO - __main__ - Epoch  48, Step:  102800, Batch Loss:     5.885510, Lr: 0.000062, Tokens per sec:   2660
2023-03-09 01:51:04,951 - INFO - __main__ - Epoch  48, Step:  102900, Batch Loss:     6.222781, Lr: 0.000062, Tokens per sec:   2668
2023-03-09 01:51:25,109 - INFO - __main__ - Epoch  48, Step:  103000, Batch Loss:     3.473802, Lr: 0.000062, Tokens per sec:   2687
2023-03-09 01:51:45,234 - INFO - __main__ - Epoch  48, Step:  103100, Batch Loss:     3.079038, Lr: 0.000062, Tokens per sec:   2648
2023-03-09 01:52:05,333 - INFO - __main__ - Epoch  48, Step:  103200, Batch Loss:     3.762685, Lr: 0.000062, Tokens per sec:   2677
2023-03-09 01:52:25,537 - INFO - __main__ - Epoch  48, Step:  103300, Batch Loss:     5.075342, Lr: 0.000062, Tokens per sec:   2653
2023-03-09 01:52:44,352 - INFO - __main__ - Epoch  48, Step:  103400, Batch Loss:     4.358325, Lr: 0.000062, Tokens per sec:   2863
2023-03-09 01:53:04,489 - INFO - __main__ - Epoch  48, Step:  103500, Batch Loss:     4.917901, Lr: 0.000062, Tokens per sec:   2663
2023-03-09 01:53:24,526 - INFO - __main__ - Epoch  48, Step:  103600, Batch Loss:     5.150320, Lr: 0.000062, Tokens per sec:   2661
2023-03-09 01:53:44,666 - INFO - __main__ - Epoch  48, Step:  103700, Batch Loss:     5.601421, Lr: 0.000062, Tokens per sec:   2720
2023-03-09 01:54:04,780 - INFO - __main__ - Epoch  48, Step:  103800, Batch Loss:     6.292839, Lr: 0.000062, Tokens per sec:   2683
2023-03-09 01:54:24,970 - INFO - __main__ - Epoch  48, Step:  103900, Batch Loss:     5.976792, Lr: 0.000062, Tokens per sec:   2635
2023-03-09 01:54:45,075 - INFO - __main__ - Epoch  48, Step:  104000, Batch Loss:     4.151616, Lr: 0.000062, Tokens per sec:   2659
2023-03-09 01:55:05,146 - INFO - __main__ - Epoch  48, Step:  104100, Batch Loss:     5.447201, Lr: 0.000062, Tokens per sec:   2687
2023-03-09 01:55:25,214 - INFO - __main__ - Epoch  48, Step:  104200, Batch Loss:     4.888189, Lr: 0.000062, Tokens per sec:   2707
2023-03-09 01:55:45,373 - INFO - __main__ - Epoch  48, Step:  104300, Batch Loss:     5.375117, Lr: 0.000062, Tokens per sec:   2639
2023-03-09 01:56:05,587 - INFO - __main__ - Epoch  48, Step:  104400, Batch Loss:     4.602054, Lr: 0.000062, Tokens per sec:   2683
2023-03-09 01:56:24,875 - INFO - __main__ - Epoch  48, Step:  104500, Batch Loss:     6.971199, Lr: 0.000062, Tokens per sec:   2840
2023-03-09 01:56:43,417 - INFO - __main__ - Epoch  48: total training loss 10714.52
2023-03-09 01:56:43,418 - INFO - __main__ - Epoch 49
2023-03-09 01:56:45,395 - INFO - __main__ - Epoch  49, Step:  104600, Batch Loss:     5.029809, Lr: 0.000062, Tokens per sec:   2151
2023-03-09 01:57:05,088 - INFO - __main__ - Epoch  49, Step:  104700, Batch Loss:     4.193305, Lr: 0.000062, Tokens per sec:   2716
2023-03-09 01:57:23,594 - INFO - __main__ - Epoch  49, Step:  104800, Batch Loss:     4.585478, Lr: 0.000062, Tokens per sec:   2920
2023-03-09 01:57:43,393 - INFO - __main__ - Epoch  49, Step:  104900, Batch Loss:     5.072709, Lr: 0.000062, Tokens per sec:   2737
2023-03-09 01:58:01,912 - INFO - __main__ - Epoch  49, Step:  105000, Batch Loss:     4.056286, Lr: 0.000062, Tokens per sec:   2931
2023-03-09 01:58:22,025 - INFO - __main__ - Epoch  49, Step:  105100, Batch Loss:     3.580239, Lr: 0.000062, Tokens per sec:   2660
2023-03-09 01:58:42,095 - INFO - __main__ - Epoch  49, Step:  105200, Batch Loss:     4.520843, Lr: 0.000062, Tokens per sec:   2649
2023-03-09 01:59:02,186 - INFO - __main__ - Epoch  49, Step:  105300, Batch Loss:     2.980725, Lr: 0.000062, Tokens per sec:   2693
2023-03-09 01:59:21,677 - INFO - __main__ - Epoch  49, Step:  105400, Batch Loss:     4.649419, Lr: 0.000062, Tokens per sec:   2793
2023-03-09 01:59:41,004 - INFO - __main__ - Epoch  49, Step:  105500, Batch Loss:     4.513903, Lr: 0.000062, Tokens per sec:   2776
2023-03-09 02:00:00,531 - INFO - __main__ - Epoch  49, Step:  105600, Batch Loss:     3.469089, Lr: 0.000062, Tokens per sec:   2759
2023-03-09 02:00:20,702 - INFO - __main__ - Epoch  49, Step:  105700, Batch Loss:     6.030118, Lr: 0.000062, Tokens per sec:   2689
2023-03-09 02:00:40,891 - INFO - __main__ - Epoch  49, Step:  105800, Batch Loss:     4.451059, Lr: 0.000062, Tokens per sec:   2686
2023-03-09 02:01:01,068 - INFO - __main__ - Epoch  49, Step:  105900, Batch Loss:     3.502740, Lr: 0.000062, Tokens per sec:   2699
2023-03-09 02:01:20,748 - INFO - __main__ - Epoch  49, Step:  106000, Batch Loss:     5.870799, Lr: 0.000062, Tokens per sec:   2702
2023-03-09 02:01:40,806 - INFO - __main__ - Epoch  49, Step:  106100, Batch Loss:     4.168284, Lr: 0.000062, Tokens per sec:   2687
2023-03-09 02:02:00,379 - INFO - __main__ - Epoch  49, Step:  106200, Batch Loss:     3.986190, Lr: 0.000062, Tokens per sec:   2759
2023-03-09 02:02:19,783 - INFO - __main__ - Epoch  49, Step:  106300, Batch Loss:     4.192984, Lr: 0.000062, Tokens per sec:   2778
2023-03-09 02:02:39,603 - INFO - __main__ - Epoch  49, Step:  106400, Batch Loss:     4.791098, Lr: 0.000062, Tokens per sec:   2710
2023-03-09 02:02:59,025 - INFO - __main__ - Epoch  49, Step:  106500, Batch Loss:     5.983653, Lr: 0.000062, Tokens per sec:   2749
2023-03-09 02:03:17,945 - INFO - __main__ - Epoch  49, Step:  106600, Batch Loss:     6.127279, Lr: 0.000062, Tokens per sec:   2889
2023-03-09 02:03:38,100 - INFO - __main__ - Epoch  49, Step:  106700, Batch Loss:     4.561564, Lr: 0.000062, Tokens per sec:   2588
2023-03-09 02:03:52,289 - INFO - __main__ - Epoch  49: total training loss 10395.08
2023-03-09 02:03:52,291 - INFO - __main__ - Epoch 50
2023-03-09 02:03:58,072 - INFO - __main__ - Epoch  50, Step:  106800, Batch Loss:     3.933717, Lr: 0.000061, Tokens per sec:   2740
2023-03-09 02:04:17,307 - INFO - __main__ - Epoch  50, Step:  106900, Batch Loss:     4.734536, Lr: 0.000061, Tokens per sec:   2803
2023-03-09 02:04:35,928 - INFO - __main__ - Epoch  50, Step:  107000, Batch Loss:     4.494354, Lr: 0.000061, Tokens per sec:   2966
2023-03-09 02:04:54,982 - INFO - __main__ - Epoch  50, Step:  107100, Batch Loss:     4.195075, Lr: 0.000061, Tokens per sec:   2831
2023-03-09 02:05:13,695 - INFO - __main__ - Epoch  50, Step:  107200, Batch Loss:     4.769907, Lr: 0.000061, Tokens per sec:   2855
2023-03-09 02:05:33,264 - INFO - __main__ - Epoch  50, Step:  107300, Batch Loss:     3.336251, Lr: 0.000061, Tokens per sec:   2734
2023-03-09 02:05:53,342 - INFO - __main__ - Epoch  50, Step:  107400, Batch Loss:     3.564273, Lr: 0.000061, Tokens per sec:   2661
2023-03-09 02:06:13,526 - INFO - __main__ - Epoch  50, Step:  107500, Batch Loss:     4.553559, Lr: 0.000061, Tokens per sec:   2693
2023-03-09 02:06:33,656 - INFO - __main__ - Epoch  50, Step:  107600, Batch Loss:     4.099818, Lr: 0.000061, Tokens per sec:   2682
2023-03-09 02:06:53,756 - INFO - __main__ - Epoch  50, Step:  107700, Batch Loss:     3.180672, Lr: 0.000061, Tokens per sec:   2696
2023-03-09 02:07:13,940 - INFO - __main__ - Epoch  50, Step:  107800, Batch Loss:     3.823384, Lr: 0.000061, Tokens per sec:   2620
2023-03-09 02:07:33,751 - INFO - __main__ - Epoch  50, Step:  107900, Batch Loss:     5.314452, Lr: 0.000061, Tokens per sec:   2702
2023-03-09 02:07:53,798 - INFO - __main__ - Epoch  50, Step:  108000, Batch Loss:     4.187522, Lr: 0.000061, Tokens per sec:   2720
2023-03-09 02:08:13,408 - INFO - __main__ - Epoch  50, Step:  108100, Batch Loss:     4.205338, Lr: 0.000061, Tokens per sec:   2761
2023-03-09 02:08:32,013 - INFO - __main__ - Epoch  50, Step:  108200, Batch Loss:     3.172813, Lr: 0.000061, Tokens per sec:   2864
2023-03-09 02:08:51,867 - INFO - __main__ - Epoch  50, Step:  108300, Batch Loss:     3.256502, Lr: 0.000061, Tokens per sec:   2734
2023-03-09 02:09:11,709 - INFO - __main__ - Epoch  50, Step:  108400, Batch Loss:     4.710080, Lr: 0.000061, Tokens per sec:   2695
2023-03-09 02:09:31,592 - INFO - __main__ - Epoch  50, Step:  108500, Batch Loss:     4.813640, Lr: 0.000061, Tokens per sec:   2719
2023-03-09 02:09:50,743 - INFO - __main__ - Epoch  50, Step:  108600, Batch Loss:     4.118677, Lr: 0.000061, Tokens per sec:   2793
2023-03-09 02:10:10,247 - INFO - __main__ - Epoch  50, Step:  108700, Batch Loss:     3.584436, Lr: 0.000061, Tokens per sec:   2749
2023-03-09 02:10:30,089 - INFO - __main__ - Epoch  50, Step:  108800, Batch Loss:     6.393856, Lr: 0.000061, Tokens per sec:   2709
2023-03-09 02:10:50,265 - INFO - __main__ - Epoch  50, Step:  108900, Batch Loss:     3.426339, Lr: 0.000061, Tokens per sec:   2676
2023-03-09 02:11:00,442 - INFO - __main__ - Epoch  50: total training loss 10093.54
2023-03-09 02:11:00,443 - INFO - __main__ - Epoch 51
2023-03-09 02:11:10,755 - INFO - __main__ - Epoch  51, Step:  109000, Batch Loss:     4.950895, Lr: 0.000061, Tokens per sec:   2599
2023-03-09 02:11:30,824 - INFO - __main__ - Epoch  51, Step:  109100, Batch Loss:     3.455328, Lr: 0.000061, Tokens per sec:   2641
2023-03-09 02:11:50,664 - INFO - __main__ - Epoch  51, Step:  109200, Batch Loss:     4.185758, Lr: 0.000061, Tokens per sec:   2721
2023-03-09 02:12:10,015 - INFO - __main__ - Epoch  51, Step:  109300, Batch Loss:     4.805851, Lr: 0.000061, Tokens per sec:   2814
2023-03-09 02:12:30,214 - INFO - __main__ - Epoch  51, Step:  109400, Batch Loss:     6.024281, Lr: 0.000061, Tokens per sec:   2678
2023-03-09 02:12:50,448 - INFO - __main__ - Epoch  51, Step:  109500, Batch Loss:     6.042474, Lr: 0.000061, Tokens per sec:   2672
2023-03-09 02:13:10,161 - INFO - __main__ - Epoch  51, Step:  109600, Batch Loss:     3.976523, Lr: 0.000061, Tokens per sec:   2706
2023-03-09 02:13:30,350 - INFO - __main__ - Epoch  51, Step:  109700, Batch Loss:     4.481359, Lr: 0.000061, Tokens per sec:   2644
2023-03-09 02:13:50,430 - INFO - __main__ - Epoch  51, Step:  109800, Batch Loss:     4.007102, Lr: 0.000061, Tokens per sec:   2689
2023-03-09 02:14:10,482 - INFO - __main__ - Epoch  51, Step:  109900, Batch Loss:     4.502893, Lr: 0.000061, Tokens per sec:   2697
2023-03-09 02:14:30,525 - INFO - __main__ - Epoch  51, Step:  110000, Batch Loss:     4.868783, Lr: 0.000061, Tokens per sec:   2697
2023-03-09 02:14:50,642 - INFO - __main__ - Epoch  51, Step:  110100, Batch Loss:     4.433469, Lr: 0.000061, Tokens per sec:   2693
2023-03-09 02:15:10,684 - INFO - __main__ - Epoch  51, Step:  110200, Batch Loss:     3.825022, Lr: 0.000061, Tokens per sec:   2687
2023-03-09 02:15:30,569 - INFO - __main__ - Epoch  51, Step:  110300, Batch Loss:     3.323192, Lr: 0.000061, Tokens per sec:   2777
2023-03-09 02:15:50,770 - INFO - __main__ - Epoch  51, Step:  110400, Batch Loss:     3.654454, Lr: 0.000061, Tokens per sec:   2612
2023-03-09 02:16:10,971 - INFO - __main__ - Epoch  51, Step:  110500, Batch Loss:     4.265810, Lr: 0.000061, Tokens per sec:   2611
2023-03-09 02:16:31,235 - INFO - __main__ - Epoch  51, Step:  110600, Batch Loss:     4.655205, Lr: 0.000061, Tokens per sec:   2702
2023-03-09 02:16:51,400 - INFO - __main__ - Epoch  51, Step:  110700, Batch Loss:     4.439331, Lr: 0.000061, Tokens per sec:   2672
2023-03-09 02:17:11,523 - INFO - __main__ - Epoch  51, Step:  110800, Batch Loss:     6.124950, Lr: 0.000061, Tokens per sec:   2674
2023-03-09 02:17:31,761 - INFO - __main__ - Epoch  51, Step:  110900, Batch Loss:     4.681679, Lr: 0.000061, Tokens per sec:   2638
2023-03-09 02:17:51,831 - INFO - __main__ - Epoch  51, Step:  111000, Batch Loss:     5.357004, Lr: 0.000061, Tokens per sec:   2677
2023-03-09 02:18:12,122 - INFO - __main__ - Epoch  51, Step:  111100, Batch Loss:     6.432217, Lr: 0.000061, Tokens per sec:   2659
2023-03-09 02:18:18,076 - INFO - __main__ - Epoch  51: total training loss 9777.44
2023-03-09 02:18:18,077 - INFO - __main__ - Epoch 52
2023-03-09 02:18:32,756 - INFO - __main__ - Epoch  52, Step:  111200, Batch Loss:     3.532636, Lr: 0.000060, Tokens per sec:   2658
2023-03-09 02:18:53,005 - INFO - __main__ - Epoch  52, Step:  111300, Batch Loss:     3.266397, Lr: 0.000060, Tokens per sec:   2614
2023-03-09 02:19:13,141 - INFO - __main__ - Epoch  52, Step:  111400, Batch Loss:     3.061255, Lr: 0.000060, Tokens per sec:   2673
2023-03-09 02:19:33,214 - INFO - __main__ - Epoch  52, Step:  111500, Batch Loss:     3.748210, Lr: 0.000060, Tokens per sec:   2702
2023-03-09 02:19:53,268 - INFO - __main__ - Epoch  52, Step:  111600, Batch Loss:     3.274064, Lr: 0.000060, Tokens per sec:   2643
2023-03-09 02:20:13,331 - INFO - __main__ - Epoch  52, Step:  111700, Batch Loss:     6.103246, Lr: 0.000060, Tokens per sec:   2704
2023-03-09 02:20:32,493 - INFO - __main__ - Epoch  52, Step:  111800, Batch Loss:     3.243480, Lr: 0.000060, Tokens per sec:   2882
2023-03-09 02:20:50,563 - INFO - __main__ - Epoch  52, Step:  111900, Batch Loss:     2.873627, Lr: 0.000060, Tokens per sec:   3003
2023-03-09 02:21:09,363 - INFO - __main__ - Epoch  52, Step:  112000, Batch Loss:     5.688518, Lr: 0.000060, Tokens per sec:   2850
2023-03-09 02:21:28,856 - INFO - __main__ - Epoch  52, Step:  112100, Batch Loss:     4.362529, Lr: 0.000060, Tokens per sec:   2770
2023-03-09 02:21:47,290 - INFO - __main__ - Epoch  52, Step:  112200, Batch Loss:     4.507890, Lr: 0.000060, Tokens per sec:   2872
2023-03-09 02:22:05,604 - INFO - __main__ - Epoch  52, Step:  112300, Batch Loss:     3.699958, Lr: 0.000060, Tokens per sec:   2941
2023-03-09 02:22:25,176 - INFO - __main__ - Epoch  52, Step:  112400, Batch Loss:     4.193989, Lr: 0.000060, Tokens per sec:   2730
2023-03-09 02:22:45,232 - INFO - __main__ - Epoch  52, Step:  112500, Batch Loss:     4.470118, Lr: 0.000060, Tokens per sec:   2759
2023-03-09 02:23:05,182 - INFO - __main__ - Epoch  52, Step:  112600, Batch Loss:     5.612014, Lr: 0.000060, Tokens per sec:   2687
2023-03-09 02:23:23,341 - INFO - __main__ - Epoch  52, Step:  112700, Batch Loss:     4.232194, Lr: 0.000060, Tokens per sec:   2966
2023-03-09 02:23:42,172 - INFO - __main__ - Epoch  52, Step:  112800, Batch Loss:     4.855330, Lr: 0.000060, Tokens per sec:   2800
2023-03-09 02:24:02,196 - INFO - __main__ - Epoch  52, Step:  112900, Batch Loss:     4.543836, Lr: 0.000060, Tokens per sec:   2646
2023-03-09 02:24:22,198 - INFO - __main__ - Epoch  52, Step:  113000, Batch Loss:     6.300472, Lr: 0.000060, Tokens per sec:   2738
2023-03-09 02:24:42,218 - INFO - __main__ - Epoch  52, Step:  113100, Batch Loss:     4.248314, Lr: 0.000060, Tokens per sec:   2673
2023-03-09 02:25:02,209 - INFO - __main__ - Epoch  52, Step:  113200, Batch Loss:     3.520375, Lr: 0.000060, Tokens per sec:   2694
2023-03-09 02:25:22,152 - INFO - __main__ - Epoch  52, Step:  113300, Batch Loss:     4.489597, Lr: 0.000060, Tokens per sec:   2696
2023-03-09 02:25:23,822 - INFO - __main__ - Epoch  52: total training loss 9504.62
2023-03-09 02:25:23,823 - INFO - __main__ - Epoch 53
2023-03-09 02:25:42,513 - INFO - __main__ - Epoch  53, Step:  113400, Batch Loss:     3.910340, Lr: 0.000059, Tokens per sec:   2645
2023-03-09 02:26:02,525 - INFO - __main__ - Epoch  53, Step:  113500, Batch Loss:     3.818967, Lr: 0.000059, Tokens per sec:   2651
2023-03-09 02:26:20,950 - INFO - __main__ - Epoch  53, Step:  113600, Batch Loss:     4.506651, Lr: 0.000059, Tokens per sec:   2957
2023-03-09 02:26:39,034 - INFO - __main__ - Epoch  53, Step:  113700, Batch Loss:     3.736857, Lr: 0.000059, Tokens per sec:   2959
2023-03-09 02:26:57,135 - INFO - __main__ - Epoch  53, Step:  113800, Batch Loss:     3.641317, Lr: 0.000059, Tokens per sec:   2965
2023-03-09 02:27:15,386 - INFO - __main__ - Epoch  53, Step:  113900, Batch Loss:     4.129976, Lr: 0.000059, Tokens per sec:   2967
2023-03-09 02:27:34,238 - INFO - __main__ - Epoch  53, Step:  114000, Batch Loss:     4.687329, Lr: 0.000059, Tokens per sec:   2865
2023-03-09 02:27:53,031 - INFO - __main__ - Epoch  53, Step:  114100, Batch Loss:     5.311117, Lr: 0.000059, Tokens per sec:   2900
2023-03-09 02:28:12,085 - INFO - __main__ - Epoch  53, Step:  114200, Batch Loss:     4.459944, Lr: 0.000059, Tokens per sec:   2795
2023-03-09 02:28:31,085 - INFO - __main__ - Epoch  53, Step:  114300, Batch Loss:     4.896182, Lr: 0.000059, Tokens per sec:   2812
2023-03-09 02:28:49,304 - INFO - __main__ - Epoch  53, Step:  114400, Batch Loss:     5.043085, Lr: 0.000059, Tokens per sec:   2985
2023-03-09 02:29:07,385 - INFO - __main__ - Epoch  53, Step:  114500, Batch Loss:     3.116329, Lr: 0.000059, Tokens per sec:   3014
2023-03-09 02:29:25,501 - INFO - __main__ - Epoch  53, Step:  114600, Batch Loss:     4.561799, Lr: 0.000059, Tokens per sec:   2935
2023-03-09 02:29:44,001 - INFO - __main__ - Epoch  53, Step:  114700, Batch Loss:     3.252106, Lr: 0.000059, Tokens per sec:   2916
2023-03-09 02:30:03,315 - INFO - __main__ - Epoch  53, Step:  114800, Batch Loss:     4.657523, Lr: 0.000059, Tokens per sec:   2769
2023-03-09 02:30:22,633 - INFO - __main__ - Epoch  53, Step:  114900, Batch Loss:     3.421875, Lr: 0.000059, Tokens per sec:   2805
2023-03-09 02:30:41,571 - INFO - __main__ - Epoch  53, Step:  115000, Batch Loss:     4.639241, Lr: 0.000059, Tokens per sec:   2880
2023-03-09 02:31:00,214 - INFO - __main__ - Epoch  53, Step:  115100, Batch Loss:     5.013091, Lr: 0.000059, Tokens per sec:   2921
2023-03-09 02:31:18,321 - INFO - __main__ - Epoch  53, Step:  115200, Batch Loss:     3.687295, Lr: 0.000059, Tokens per sec:   2953
2023-03-09 02:31:37,017 - INFO - __main__ - Epoch  53, Step:  115300, Batch Loss:     3.766680, Lr: 0.000059, Tokens per sec:   2860
2023-03-09 02:31:55,074 - INFO - __main__ - Epoch  53, Step:  115400, Batch Loss:     2.645546, Lr: 0.000059, Tokens per sec:   2953
2023-03-09 02:32:10,853 - INFO - __main__ - Epoch  53: total training loss 9228.63
2023-03-09 02:32:10,854 - INFO - __main__ - Epoch 54
2023-03-09 02:32:13,755 - INFO - __main__ - Epoch  54, Step:  115500, Batch Loss:     5.155324, Lr: 0.000059, Tokens per sec:   2451
2023-03-09 02:32:33,782 - INFO - __main__ - Epoch  54, Step:  115600, Batch Loss:     3.070174, Lr: 0.000059, Tokens per sec:   2709
2023-03-09 02:32:53,793 - INFO - __main__ - Epoch  54, Step:  115700, Batch Loss:     3.884319, Lr: 0.000059, Tokens per sec:   2687
2023-03-09 02:33:13,144 - INFO - __main__ - Epoch  54, Step:  115800, Batch Loss:     3.571499, Lr: 0.000059, Tokens per sec:   2785
2023-03-09 02:33:31,293 - INFO - __main__ - Epoch  54, Step:  115900, Batch Loss:     2.766521, Lr: 0.000059, Tokens per sec:   2901
2023-03-09 02:33:50,081 - INFO - __main__ - Epoch  54, Step:  116000, Batch Loss:     4.333234, Lr: 0.000059, Tokens per sec:   2828
2023-03-09 02:34:10,092 - INFO - __main__ - Epoch  54, Step:  116100, Batch Loss:     5.155659, Lr: 0.000059, Tokens per sec:   2668
2023-03-09 02:34:30,064 - INFO - __main__ - Epoch  54, Step:  116200, Batch Loss:     2.906715, Lr: 0.000059, Tokens per sec:   2682
2023-03-09 02:34:50,080 - INFO - __main__ - Epoch  54, Step:  116300, Batch Loss:     5.688354, Lr: 0.000059, Tokens per sec:   2668
2023-03-09 02:35:10,069 - INFO - __main__ - Epoch  54, Step:  116400, Batch Loss:     4.998017, Lr: 0.000059, Tokens per sec:   2771
2023-03-09 02:35:30,085 - INFO - __main__ - Epoch  54, Step:  116500, Batch Loss:     4.522508, Lr: 0.000059, Tokens per sec:   2689
2023-03-09 02:35:48,455 - INFO - __main__ - Epoch  54, Step:  116600, Batch Loss:     3.146497, Lr: 0.000059, Tokens per sec:   2899
2023-03-09 02:36:06,666 - INFO - __main__ - Epoch  54, Step:  116700, Batch Loss:     3.619548, Lr: 0.000059, Tokens per sec:   2989
2023-03-09 02:36:26,686 - INFO - __main__ - Epoch  54, Step:  116800, Batch Loss:     3.636998, Lr: 0.000059, Tokens per sec:   2701
2023-03-09 02:36:46,627 - INFO - __main__ - Epoch  54, Step:  116900, Batch Loss:     3.873891, Lr: 0.000059, Tokens per sec:   2732
2023-03-09 02:37:05,154 - INFO - __main__ - Epoch  54, Step:  117000, Batch Loss:     3.998097, Lr: 0.000059, Tokens per sec:   2934
2023-03-09 02:37:25,192 - INFO - __main__ - Epoch  54, Step:  117100, Batch Loss:     4.217280, Lr: 0.000059, Tokens per sec:   2658
2023-03-09 02:37:43,688 - INFO - __main__ - Epoch  54, Step:  117200, Batch Loss:     3.939982, Lr: 0.000059, Tokens per sec:   2902
2023-03-09 02:38:02,418 - INFO - __main__ - Epoch  54, Step:  117300, Batch Loss:     4.106791, Lr: 0.000059, Tokens per sec:   2853
2023-03-09 02:38:21,667 - INFO - __main__ - Epoch  54, Step:  117400, Batch Loss:     4.020239, Lr: 0.000059, Tokens per sec:   2815
2023-03-09 02:38:39,758 - INFO - __main__ - Epoch  54, Step:  117500, Batch Loss:     5.078035, Lr: 0.000059, Tokens per sec:   3000
2023-03-09 02:38:59,640 - INFO - __main__ - Epoch  54, Step:  117600, Batch Loss:     4.973742, Lr: 0.000059, Tokens per sec:   2704
2023-03-09 02:39:12,861 - INFO - __main__ - Epoch  54: total training loss 8959.23
2023-03-09 02:39:12,862 - INFO - __main__ - Epoch 55
2023-03-09 02:39:19,991 - INFO - __main__ - Epoch  55, Step:  117700, Batch Loss:     5.708639, Lr: 0.000058, Tokens per sec:   2605
2023-03-09 02:39:38,798 - INFO - __main__ - Epoch  55, Step:  117800, Batch Loss:     3.036914, Lr: 0.000058, Tokens per sec:   2834
2023-03-09 02:39:56,843 - INFO - __main__ - Epoch  55, Step:  117900, Batch Loss:     2.770563, Lr: 0.000058, Tokens per sec:   2976
2023-03-09 02:40:14,844 - INFO - __main__ - Epoch  55, Step:  118000, Batch Loss:     3.614564, Lr: 0.000058, Tokens per sec:   2973
2023-03-09 02:40:32,820 - INFO - __main__ - Epoch  55, Step:  118100, Batch Loss:     3.009968, Lr: 0.000058, Tokens per sec:   3024
2023-03-09 02:40:51,587 - INFO - __main__ - Epoch  55, Step:  118200, Batch Loss:     3.912270, Lr: 0.000058, Tokens per sec:   2843
2023-03-09 02:41:10,592 - INFO - __main__ - Epoch  55, Step:  118300, Batch Loss:     3.191376, Lr: 0.000058, Tokens per sec:   2845
2023-03-09 02:41:30,610 - INFO - __main__ - Epoch  55, Step:  118400, Batch Loss:     3.039630, Lr: 0.000058, Tokens per sec:   2693
2023-03-09 02:41:50,202 - INFO - __main__ - Epoch  55, Step:  118500, Batch Loss:     4.599869, Lr: 0.000058, Tokens per sec:   2735
2023-03-09 02:42:09,140 - INFO - __main__ - Epoch  55, Step:  118600, Batch Loss:     3.658099, Lr: 0.000058, Tokens per sec:   2878
2023-03-09 02:42:27,102 - INFO - __main__ - Epoch  55, Step:  118700, Batch Loss:     2.903909, Lr: 0.000058, Tokens per sec:   3050
2023-03-09 02:42:45,069 - INFO - __main__ - Epoch  55, Step:  118800, Batch Loss:     5.002691, Lr: 0.000058, Tokens per sec:   3003
2023-03-09 02:43:03,161 - INFO - __main__ - Epoch  55, Step:  118900, Batch Loss:     3.845671, Lr: 0.000058, Tokens per sec:   2995
2023-03-09 02:43:23,114 - INFO - __main__ - Epoch  55, Step:  119000, Batch Loss:     2.221587, Lr: 0.000058, Tokens per sec:   2699
2023-03-09 02:43:43,084 - INFO - __main__ - Epoch  55, Step:  119100, Batch Loss:     4.462491, Lr: 0.000058, Tokens per sec:   2695
2023-03-09 02:44:03,085 - INFO - __main__ - Epoch  55, Step:  119200, Batch Loss:     4.311192, Lr: 0.000058, Tokens per sec:   2670
2023-03-09 02:44:21,523 - INFO - __main__ - Epoch  55, Step:  119300, Batch Loss:     4.377944, Lr: 0.000058, Tokens per sec:   2917
2023-03-09 02:44:39,480 - INFO - __main__ - Epoch  55, Step:  119400, Batch Loss:     4.424653, Lr: 0.000058, Tokens per sec:   2962
2023-03-09 02:44:58,278 - INFO - __main__ - Epoch  55, Step:  119500, Batch Loss:     3.476784, Lr: 0.000058, Tokens per sec:   2852
2023-03-09 02:45:17,494 - INFO - __main__ - Epoch  55, Step:  119600, Batch Loss:     3.429423, Lr: 0.000058, Tokens per sec:   2785
2023-03-09 02:45:37,151 - INFO - __main__ - Epoch  55, Step:  119700, Batch Loss:     3.812284, Lr: 0.000058, Tokens per sec:   2748
2023-03-09 02:45:55,905 - INFO - __main__ - Epoch  55, Step:  119800, Batch Loss:     4.925767, Lr: 0.000058, Tokens per sec:   2862
2023-03-09 02:46:04,702 - INFO - __main__ - Epoch  55: total training loss 8740.33
2023-03-09 02:46:04,703 - INFO - __main__ - Epoch 56
2023-03-09 02:46:14,854 - INFO - __main__ - Epoch  56, Step:  119900, Batch Loss:     3.432053, Lr: 0.000058, Tokens per sec:   2937
2023-03-09 02:46:32,887 - INFO - __main__ - Epoch  56, Step:  120000, Batch Loss:     3.589574, Lr: 0.000058, Tokens per sec:   2989
2023-03-09 02:46:51,219 - INFO - __main__ - Epoch  56, Step:  120100, Batch Loss:     4.683925, Lr: 0.000058, Tokens per sec:   2973
2023-03-09 02:47:09,683 - INFO - __main__ - Epoch  56, Step:  120200, Batch Loss:     4.368644, Lr: 0.000058, Tokens per sec:   2933
2023-03-09 02:47:28,121 - INFO - __main__ - Epoch  56, Step:  120300, Batch Loss:     4.382720, Lr: 0.000058, Tokens per sec:   2916
2023-03-09 02:47:48,149 - INFO - __main__ - Epoch  56, Step:  120400, Batch Loss:     4.934213, Lr: 0.000058, Tokens per sec:   2665
2023-03-09 02:48:07,049 - INFO - __main__ - Epoch  56, Step:  120500, Batch Loss:     3.407608, Lr: 0.000058, Tokens per sec:   2872
2023-03-09 02:48:25,337 - INFO - __main__ - Epoch  56, Step:  120600, Batch Loss:     4.693661, Lr: 0.000058, Tokens per sec:   2938
2023-03-09 02:48:43,347 - INFO - __main__ - Epoch  56, Step:  120700, Batch Loss:     3.011244, Lr: 0.000058, Tokens per sec:   2955
2023-03-09 02:49:01,899 - INFO - __main__ - Epoch  56, Step:  120800, Batch Loss:     2.695389, Lr: 0.000058, Tokens per sec:   2917
2023-03-09 02:49:21,914 - INFO - __main__ - Epoch  56, Step:  120900, Batch Loss:     2.433877, Lr: 0.000058, Tokens per sec:   2724
2023-03-09 02:49:41,931 - INFO - __main__ - Epoch  56, Step:  121000, Batch Loss:     3.052627, Lr: 0.000058, Tokens per sec:   2701
2023-03-09 02:50:01,945 - INFO - __main__ - Epoch  56, Step:  121100, Batch Loss:     3.320775, Lr: 0.000058, Tokens per sec:   2725
2023-03-09 02:50:21,948 - INFO - __main__ - Epoch  56, Step:  121200, Batch Loss:     4.266062, Lr: 0.000058, Tokens per sec:   2632
2023-03-09 02:50:41,954 - INFO - __main__ - Epoch  56, Step:  121300, Batch Loss:     5.533902, Lr: 0.000058, Tokens per sec:   2696
2023-03-09 02:51:01,685 - INFO - __main__ - Epoch  56, Step:  121400, Batch Loss:     3.711943, Lr: 0.000058, Tokens per sec:   2709
2023-03-09 02:51:20,434 - INFO - __main__ - Epoch  56, Step:  121500, Batch Loss:     4.843450, Lr: 0.000058, Tokens per sec:   2851
2023-03-09 02:51:39,572 - INFO - __main__ - Epoch  56, Step:  121600, Batch Loss:     3.092989, Lr: 0.000058, Tokens per sec:   2780
2023-03-09 02:51:58,348 - INFO - __main__ - Epoch  56, Step:  121700, Batch Loss:     3.757780, Lr: 0.000058, Tokens per sec:   2844
2023-03-09 02:52:16,545 - INFO - __main__ - Epoch  56, Step:  121800, Batch Loss:     3.981836, Lr: 0.000058, Tokens per sec:   2980
2023-03-09 02:52:34,675 - INFO - __main__ - Epoch  56, Step:  121900, Batch Loss:     3.293902, Lr: 0.000058, Tokens per sec:   2952
2023-03-09 02:52:52,819 - INFO - __main__ - Epoch  56, Step:  122000, Batch Loss:     5.282500, Lr: 0.000058, Tokens per sec:   2978
2023-03-09 02:52:57,370 - INFO - __main__ - Epoch  56: total training loss 8519.34
2023-03-09 02:52:57,371 - INFO - __main__ - Epoch 57
2023-03-09 02:53:12,908 - INFO - __main__ - Epoch  57, Step:  122100, Batch Loss:     5.130038, Lr: 0.000057, Tokens per sec:   2651
2023-03-09 02:53:32,134 - INFO - __main__ - Epoch  57, Step:  122200, Batch Loss:     5.053036, Lr: 0.000057, Tokens per sec:   2772
2023-03-09 02:53:50,291 - INFO - __main__ - Epoch  57, Step:  122300, Batch Loss:     3.151592, Lr: 0.000057, Tokens per sec:   2905
2023-03-09 02:54:08,404 - INFO - __main__ - Epoch  57, Step:  122400, Batch Loss:     3.743417, Lr: 0.000057, Tokens per sec:   3020
2023-03-09 02:54:26,601 - INFO - __main__ - Epoch  57, Step:  122500, Batch Loss:     3.261639, Lr: 0.000057, Tokens per sec:   2969
2023-03-09 02:54:46,189 - INFO - __main__ - Epoch  57, Step:  122600, Batch Loss:     4.337764, Lr: 0.000057, Tokens per sec:   2727
2023-03-09 02:55:06,251 - INFO - __main__ - Epoch  57, Step:  122700, Batch Loss:     3.783937, Lr: 0.000057, Tokens per sec:   2659
2023-03-09 02:55:25,271 - INFO - __main__ - Epoch  57, Step:  122800, Batch Loss:     3.046740, Lr: 0.000057, Tokens per sec:   2816
2023-03-09 02:55:43,353 - INFO - __main__ - Epoch  57, Step:  122900, Batch Loss:     4.315699, Lr: 0.000057, Tokens per sec:   3012
2023-03-09 02:56:01,350 - INFO - __main__ - Epoch  57, Step:  123000, Batch Loss:     4.297472, Lr: 0.000057, Tokens per sec:   2979
2023-03-09 02:56:19,403 - INFO - __main__ - Epoch  57, Step:  123100, Batch Loss:     4.979715, Lr: 0.000057, Tokens per sec:   2933
2023-03-09 02:56:37,530 - INFO - __main__ - Epoch  57, Step:  123200, Batch Loss:     4.330013, Lr: 0.000057, Tokens per sec:   2986
2023-03-09 02:56:55,718 - INFO - __main__ - Epoch  57, Step:  123300, Batch Loss:     4.108423, Lr: 0.000057, Tokens per sec:   2898
2023-03-09 02:57:13,776 - INFO - __main__ - Epoch  57, Step:  123400, Batch Loss:     3.986093, Lr: 0.000057, Tokens per sec:   3018
2023-03-09 02:57:32,995 - INFO - __main__ - Epoch  57, Step:  123500, Batch Loss:     4.704996, Lr: 0.000057, Tokens per sec:   2751
2023-03-09 02:57:53,041 - INFO - __main__ - Epoch  57, Step:  123600, Batch Loss:     4.063435, Lr: 0.000057, Tokens per sec:   2762
2023-03-09 02:58:13,018 - INFO - __main__ - Epoch  57, Step:  123700, Batch Loss:     3.993762, Lr: 0.000057, Tokens per sec:   2713
2023-03-09 02:58:33,014 - INFO - __main__ - Epoch  57, Step:  123800, Batch Loss:     4.360440, Lr: 0.000057, Tokens per sec:   2695
2023-03-09 02:58:52,976 - INFO - __main__ - Epoch  57, Step:  123900, Batch Loss:     4.593191, Lr: 0.000057, Tokens per sec:   2693
2023-03-09 02:59:13,029 - INFO - __main__ - Epoch  57, Step:  124000, Batch Loss:     4.070398, Lr: 0.000057, Tokens per sec:   2644
2023-03-09 02:59:33,120 - INFO - __main__ - Epoch  57, Step:  124100, Batch Loss:     3.035697, Lr: 0.000057, Tokens per sec:   2769
2023-03-09 02:59:53,006 - INFO - __main__ - Epoch  57, Step:  124200, Batch Loss:     5.573926, Lr: 0.000057, Tokens per sec:   2712
2023-03-09 02:59:53,656 - INFO - __main__ - Epoch  57: total training loss 8263.75
2023-03-09 02:59:53,657 - INFO - __main__ - Epoch 58
2023-03-09 03:00:13,123 - INFO - __main__ - Epoch  58, Step:  124300, Batch Loss:     3.710348, Lr: 0.000056, Tokens per sec:   2661
2023-03-09 03:00:33,149 - INFO - __main__ - Epoch  58, Step:  124400, Batch Loss:     3.350530, Lr: 0.000056, Tokens per sec:   2659
2023-03-09 03:00:53,158 - INFO - __main__ - Epoch  58, Step:  124500, Batch Loss:     2.849984, Lr: 0.000056, Tokens per sec:   2660
2023-03-09 03:01:13,184 - INFO - __main__ - Epoch  58, Step:  124600, Batch Loss:     2.663132, Lr: 0.000056, Tokens per sec:   2716
2023-03-09 03:01:33,218 - INFO - __main__ - Epoch  58, Step:  124700, Batch Loss:     4.063061, Lr: 0.000056, Tokens per sec:   2643
2023-03-09 03:01:53,172 - INFO - __main__ - Epoch  58, Step:  124800, Batch Loss:     5.476633, Lr: 0.000056, Tokens per sec:   2706
2023-03-09 03:02:12,976 - INFO - __main__ - Epoch  58, Step:  124900, Batch Loss:     2.494220, Lr: 0.000056, Tokens per sec:   2690
2023-03-09 03:02:32,242 - INFO - __main__ - Epoch  58, Step:  125000, Batch Loss:     4.463682, Lr: 0.000056, Tokens per sec:   2787
2023-03-09 03:02:50,649 - INFO - __main__ - Epoch  58, Step:  125100, Batch Loss:     4.023117, Lr: 0.000056, Tokens per sec:   2947
2023-03-09 03:03:10,668 - INFO - __main__ - Epoch  58, Step:  125200, Batch Loss:     3.751575, Lr: 0.000056, Tokens per sec:   2703
2023-03-09 03:03:28,787 - INFO - __main__ - Epoch  58, Step:  125300, Batch Loss:     4.866775, Lr: 0.000056, Tokens per sec:   2988
2023-03-09 03:03:47,171 - INFO - __main__ - Epoch  58, Step:  125400, Batch Loss:     3.771052, Lr: 0.000056, Tokens per sec:   2907
2023-03-09 03:04:05,186 - INFO - __main__ - Epoch  58, Step:  125500, Batch Loss:     3.897078, Lr: 0.000056, Tokens per sec:   3028
2023-03-09 03:04:23,139 - INFO - __main__ - Epoch  58, Step:  125600, Batch Loss:     4.645627, Lr: 0.000056, Tokens per sec:   3056
2023-03-09 03:04:41,345 - INFO - __main__ - Epoch  58, Step:  125700, Batch Loss:     5.618631, Lr: 0.000056, Tokens per sec:   3003
2023-03-09 03:04:59,383 - INFO - __main__ - Epoch  58, Step:  125800, Batch Loss:     4.441036, Lr: 0.000056, Tokens per sec:   3009
2023-03-09 03:05:17,367 - INFO - __main__ - Epoch  58, Step:  125900, Batch Loss:     3.566576, Lr: 0.000056, Tokens per sec:   3009
2023-03-09 03:05:35,399 - INFO - __main__ - Epoch  58, Step:  126000, Batch Loss:     4.628036, Lr: 0.000056, Tokens per sec:   2965
2023-03-09 03:05:53,420 - INFO - __main__ - Epoch  58, Step:  126100, Batch Loss:     4.834115, Lr: 0.000056, Tokens per sec:   2938
2023-03-09 03:06:12,230 - INFO - __main__ - Epoch  58, Step:  126200, Batch Loss:     3.803491, Lr: 0.000056, Tokens per sec:   2823
2023-03-09 03:06:30,281 - INFO - __main__ - Epoch  58, Step:  126300, Batch Loss:     4.264162, Lr: 0.000056, Tokens per sec:   2971
2023-03-09 03:06:45,135 - INFO - __main__ - Epoch  58: total training loss 8041.09
2023-03-09 03:06:45,136 - INFO - __main__ - Epoch 59
2023-03-09 03:06:49,056 - INFO - __main__ - Epoch  59, Step:  126400, Batch Loss:     4.468468, Lr: 0.000056, Tokens per sec:   2351
2023-03-09 03:07:09,046 - INFO - __main__ - Epoch  59, Step:  126500, Batch Loss:     3.678878, Lr: 0.000056, Tokens per sec:   2670
2023-03-09 03:07:27,935 - INFO - __main__ - Epoch  59, Step:  126600, Batch Loss:     2.744275, Lr: 0.000056, Tokens per sec:   2854
2023-03-09 03:07:46,036 - INFO - __main__ - Epoch  59, Step:  126700, Batch Loss:     2.223373, Lr: 0.000056, Tokens per sec:   2976
2023-03-09 03:08:04,715 - INFO - __main__ - Epoch  59, Step:  126800, Batch Loss:     3.136094, Lr: 0.000056, Tokens per sec:   2944
2023-03-09 03:08:22,859 - INFO - __main__ - Epoch  59, Step:  126900, Batch Loss:     3.491219, Lr: 0.000056, Tokens per sec:   2909
2023-03-09 03:08:41,331 - INFO - __main__ - Epoch  59, Step:  127000, Batch Loss:     2.763816, Lr: 0.000056, Tokens per sec:   2906
2023-03-09 03:08:59,849 - INFO - __main__ - Epoch  59, Step:  127100, Batch Loss:     3.614648, Lr: 0.000056, Tokens per sec:   2892
2023-03-09 03:09:19,234 - INFO - __main__ - Epoch  59, Step:  127200, Batch Loss:     3.005733, Lr: 0.000056, Tokens per sec:   2823
2023-03-09 03:09:38,034 - INFO - __main__ - Epoch  59, Step:  127300, Batch Loss:     3.531124, Lr: 0.000056, Tokens per sec:   2858
2023-03-09 03:09:57,150 - INFO - __main__ - Epoch  59, Step:  127400, Batch Loss:     3.873175, Lr: 0.000056, Tokens per sec:   2805
2023-03-09 03:10:15,184 - INFO - __main__ - Epoch  59, Step:  127500, Batch Loss:     3.805520, Lr: 0.000056, Tokens per sec:   2948
2023-03-09 03:10:33,309 - INFO - __main__ - Epoch  59, Step:  127600, Batch Loss:     4.106783, Lr: 0.000056, Tokens per sec:   2959
2023-03-09 03:10:51,375 - INFO - __main__ - Epoch  59, Step:  127700, Batch Loss:     3.555954, Lr: 0.000056, Tokens per sec:   3007
2023-03-09 03:11:09,398 - INFO - __main__ - Epoch  59, Step:  127800, Batch Loss:     2.791430, Lr: 0.000056, Tokens per sec:   3000
2023-03-09 03:11:27,543 - INFO - __main__ - Epoch  59, Step:  127900, Batch Loss:     3.155426, Lr: 0.000056, Tokens per sec:   2992
2023-03-09 03:11:45,636 - INFO - __main__ - Epoch  59, Step:  128000, Batch Loss:     4.262050, Lr: 0.000056, Tokens per sec:   3019
2023-03-09 03:12:05,229 - INFO - __main__ - Epoch  59, Step:  128100, Batch Loss:     3.757292, Lr: 0.000056, Tokens per sec:   2711
2023-03-09 03:12:25,265 - INFO - __main__ - Epoch  59, Step:  128200, Batch Loss:     4.852622, Lr: 0.000056, Tokens per sec:   2740
2023-03-09 03:12:44,072 - INFO - __main__ - Epoch  59, Step:  128300, Batch Loss:     3.773803, Lr: 0.000056, Tokens per sec:   2882
2023-03-09 03:13:03,014 - INFO - __main__ - Epoch  59, Step:  128400, Batch Loss:     2.235424, Lr: 0.000056, Tokens per sec:   2804
2023-03-09 03:13:22,217 - INFO - __main__ - Epoch  59, Step:  128500, Batch Loss:     3.027441, Lr: 0.000056, Tokens per sec:   2790
2023-03-09 03:13:33,502 - INFO - __main__ - Epoch  59: total training loss 7831.48
2023-03-09 03:13:33,503 - INFO - __main__ - Epoch 60
2023-03-09 03:13:41,661 - INFO - __main__ - Epoch  60, Step:  128600, Batch Loss:     3.569823, Lr: 0.000055, Tokens per sec:   2508
2023-03-09 03:14:01,627 - INFO - __main__ - Epoch  60, Step:  128700, Batch Loss:     4.379428, Lr: 0.000055, Tokens per sec:   2714
2023-03-09 03:14:21,662 - INFO - __main__ - Epoch  60, Step:  128800, Batch Loss:     3.414397, Lr: 0.000055, Tokens per sec:   2717
2023-03-09 03:14:41,617 - INFO - __main__ - Epoch  60, Step:  128900, Batch Loss:     2.395334, Lr: 0.000055, Tokens per sec:   2698
2023-03-09 03:15:01,650 - INFO - __main__ - Epoch  60, Step:  129000, Batch Loss:     3.008913, Lr: 0.000055, Tokens per sec:   2606
2023-03-09 03:15:21,665 - INFO - __main__ - Epoch  60, Step:  129100, Batch Loss:     3.206748, Lr: 0.000055, Tokens per sec:   2741
2023-03-09 03:15:41,364 - INFO - __main__ - Epoch  60, Step:  129200, Batch Loss:     4.097778, Lr: 0.000055, Tokens per sec:   2691
2023-03-09 03:15:59,434 - INFO - __main__ - Epoch  60, Step:  129300, Batch Loss:     3.776343, Lr: 0.000055, Tokens per sec:   2992
2023-03-09 03:16:19,049 - INFO - __main__ - Epoch  60, Step:  129400, Batch Loss:     2.972454, Lr: 0.000055, Tokens per sec:   2754
2023-03-09 03:16:39,117 - INFO - __main__ - Epoch  60, Step:  129500, Batch Loss:     4.954616, Lr: 0.000055, Tokens per sec:   2699
2023-03-09 03:16:59,153 - INFO - __main__ - Epoch  60, Step:  129600, Batch Loss:     4.069876, Lr: 0.000055, Tokens per sec:   2725
2023-03-09 03:17:18,652 - INFO - __main__ - Epoch  60, Step:  129700, Batch Loss:     3.195678, Lr: 0.000055, Tokens per sec:   2741
2023-03-09 03:17:36,709 - INFO - __main__ - Epoch  60, Step:  129800, Batch Loss:     3.748873, Lr: 0.000055, Tokens per sec:   2984
2023-03-09 03:17:56,614 - INFO - __main__ - Epoch  60, Step:  129900, Batch Loss:     2.944067, Lr: 0.000055, Tokens per sec:   2702
2023-03-09 03:18:16,369 - INFO - __main__ - Epoch  60, Step:  130000, Batch Loss:     4.637944, Lr: 0.000055, Tokens per sec:   2727
2023-03-09 03:18:34,383 - INFO - __main__ - Epoch  60, Step:  130100, Batch Loss:     3.745130, Lr: 0.000055, Tokens per sec:   3007
2023-03-09 03:18:52,410 - INFO - __main__ - Epoch  60, Step:  130200, Batch Loss:     3.066994, Lr: 0.000055, Tokens per sec:   3009
2023-03-09 03:19:11,807 - INFO - __main__ - Epoch  60, Step:  130300, Batch Loss:     4.202190, Lr: 0.000055, Tokens per sec:   2744
2023-03-09 03:19:31,044 - INFO - __main__ - Epoch  60, Step:  130400, Batch Loss:     2.853429, Lr: 0.000055, Tokens per sec:   2752
2023-03-09 03:19:50,318 - INFO - __main__ - Epoch  60, Step:  130500, Batch Loss:     2.989889, Lr: 0.000055, Tokens per sec:   2818
2023-03-09 03:20:10,278 - INFO - __main__ - Epoch  60, Step:  130600, Batch Loss:     2.919206, Lr: 0.000055, Tokens per sec:   2711
2023-03-09 03:20:28,447 - INFO - __main__ - Epoch  60, Step:  130700, Batch Loss:     4.021456, Lr: 0.000055, Tokens per sec:   2978
2023-03-09 03:20:35,740 - INFO - __main__ - Epoch  60: total training loss 7607.49
2023-03-09 03:20:35,741 - INFO - __main__ - Epoch 61
2023-03-09 03:20:46,990 - INFO - __main__ - Epoch  61, Step:  130800, Batch Loss:     2.370540, Lr: 0.000055, Tokens per sec:   2876
2023-03-09 03:21:05,299 - INFO - __main__ - Epoch  61, Step:  130900, Batch Loss:     2.977130, Lr: 0.000055, Tokens per sec:   2928
2023-03-09 03:21:23,375 - INFO - __main__ - Epoch  61, Step:  131000, Batch Loss:     2.520939, Lr: 0.000055, Tokens per sec:   2965
2023-03-09 03:21:41,479 - INFO - __main__ - Epoch  61, Step:  131100, Batch Loss:     2.379715, Lr: 0.000055, Tokens per sec:   2954
2023-03-09 03:21:59,500 - INFO - __main__ - Epoch  61, Step:  131200, Batch Loss:     3.043395, Lr: 0.000055, Tokens per sec:   3031
2023-03-09 03:22:17,929 - INFO - __main__ - Epoch  61, Step:  131300, Batch Loss:     2.936474, Lr: 0.000055, Tokens per sec:   2947
2023-03-09 03:22:37,564 - INFO - __main__ - Epoch  61, Step:  131400, Batch Loss:     4.445303, Lr: 0.000055, Tokens per sec:   2788
2023-03-09 03:22:57,199 - INFO - __main__ - Epoch  61, Step:  131500, Batch Loss:     3.167882, Lr: 0.000055, Tokens per sec:   2785
2023-03-09 03:23:15,293 - INFO - __main__ - Epoch  61, Step:  131600, Batch Loss:     3.723821, Lr: 0.000055, Tokens per sec:   3014
2023-03-09 03:23:33,913 - INFO - __main__ - Epoch  61, Step:  131700, Batch Loss:     4.104674, Lr: 0.000055, Tokens per sec:   2870
2023-03-09 03:23:53,335 - INFO - __main__ - Epoch  61, Step:  131800, Batch Loss:     6.023716, Lr: 0.000055, Tokens per sec:   2756
2023-03-09 03:24:11,416 - INFO - __main__ - Epoch  61, Step:  131900, Batch Loss:     3.165483, Lr: 0.000055, Tokens per sec:   2969
2023-03-09 03:24:30,909 - INFO - __main__ - Epoch  61, Step:  132000, Batch Loss:     3.184557, Lr: 0.000055, Tokens per sec:   2708
2023-03-09 03:24:50,860 - INFO - __main__ - Epoch  61, Step:  132100, Batch Loss:     2.768056, Lr: 0.000055, Tokens per sec:   2680
2023-03-09 03:25:10,903 - INFO - __main__ - Epoch  61, Step:  132200, Batch Loss:     3.245807, Lr: 0.000055, Tokens per sec:   2672
2023-03-09 03:25:30,926 - INFO - __main__ - Epoch  61, Step:  132300, Batch Loss:     2.764287, Lr: 0.000055, Tokens per sec:   2713
2023-03-09 03:25:49,743 - INFO - __main__ - Epoch  61, Step:  132400, Batch Loss:     3.170670, Lr: 0.000055, Tokens per sec:   2883
2023-03-09 03:26:08,139 - INFO - __main__ - Epoch  61, Step:  132500, Batch Loss:     4.349988, Lr: 0.000055, Tokens per sec:   2923
2023-03-09 03:26:26,190 - INFO - __main__ - Epoch  61, Step:  132600, Batch Loss:     4.421863, Lr: 0.000055, Tokens per sec:   2981
2023-03-09 03:26:46,083 - INFO - __main__ - Epoch  61, Step:  132700, Batch Loss:     4.230950, Lr: 0.000055, Tokens per sec:   2710
2023-03-09 03:27:04,659 - INFO - __main__ - Epoch  61, Step:  132800, Batch Loss:     3.972223, Lr: 0.000055, Tokens per sec:   2867
2023-03-09 03:27:24,551 - INFO - __main__ - Epoch  61, Step:  132900, Batch Loss:     3.701878, Lr: 0.000055, Tokens per sec:   2677
2023-03-09 03:27:28,415 - INFO - __main__ - Epoch  61: total training loss 7444.83
2023-03-09 03:27:28,416 - INFO - __main__ - Epoch 62
2023-03-09 03:27:44,925 - INFO - __main__ - Epoch  62, Step:  133000, Batch Loss:     3.973458, Lr: 0.000054, Tokens per sec:   2717
2023-03-09 03:28:04,901 - INFO - __main__ - Epoch  62, Step:  133100, Batch Loss:     2.690240, Lr: 0.000054, Tokens per sec:   2707
2023-03-09 03:28:23,328 - INFO - __main__ - Epoch  62, Step:  133200, Batch Loss:     2.290988, Lr: 0.000054, Tokens per sec:   2900
2023-03-09 03:28:41,311 - INFO - __main__ - Epoch  62, Step:  133300, Batch Loss:     3.316415, Lr: 0.000054, Tokens per sec:   3035
2023-03-09 03:29:00,592 - INFO - __main__ - Epoch  62, Step:  133400, Batch Loss:     3.402286, Lr: 0.000054, Tokens per sec:   2798
2023-03-09 03:29:20,575 - INFO - __main__ - Epoch  62, Step:  133500, Batch Loss:     3.233025, Lr: 0.000054, Tokens per sec:   2665
2023-03-09 03:29:40,591 - INFO - __main__ - Epoch  62, Step:  133600, Batch Loss:     4.359134, Lr: 0.000054, Tokens per sec:   2671
2023-03-09 03:30:00,483 - INFO - __main__ - Epoch  62, Step:  133700, Batch Loss:     3.011330, Lr: 0.000054, Tokens per sec:   2669
2023-03-09 03:30:20,238 - INFO - __main__ - Epoch  62, Step:  133800, Batch Loss:     4.428425, Lr: 0.000054, Tokens per sec:   2699
2023-03-09 03:30:40,199 - INFO - __main__ - Epoch  62, Step:  133900, Batch Loss:     3.327981, Lr: 0.000054, Tokens per sec:   2707
2023-03-09 03:30:58,599 - INFO - __main__ - Epoch  62, Step:  134000, Batch Loss:     3.277363, Lr: 0.000054, Tokens per sec:   2924
2023-03-09 03:31:16,557 - INFO - __main__ - Epoch  62, Step:  134100, Batch Loss:     3.078083, Lr: 0.000054, Tokens per sec:   2978
2023-03-09 03:31:34,668 - INFO - __main__ - Epoch  62, Step:  134200, Batch Loss:     2.540622, Lr: 0.000054, Tokens per sec:   2954
2023-03-09 03:31:53,148 - INFO - __main__ - Epoch  62, Step:  134300, Batch Loss:     3.625588, Lr: 0.000054, Tokens per sec:   2919
2023-03-09 03:32:13,197 - INFO - __main__ - Epoch  62, Step:  134400, Batch Loss:     3.412804, Lr: 0.000054, Tokens per sec:   2676
2023-03-09 03:32:32,772 - INFO - __main__ - Epoch  62, Step:  134500, Batch Loss:     3.425518, Lr: 0.000054, Tokens per sec:   2743
2023-03-09 03:32:51,109 - INFO - __main__ - Epoch  62, Step:  134600, Batch Loss:     4.927933, Lr: 0.000054, Tokens per sec:   2975
2023-03-09 03:33:09,757 - INFO - __main__ - Epoch  62, Step:  134700, Batch Loss:     3.917120, Lr: 0.000054, Tokens per sec:   2912
2023-03-09 03:33:29,775 - INFO - __main__ - Epoch  62, Step:  134800, Batch Loss:     3.794614, Lr: 0.000054, Tokens per sec:   2675
2023-03-09 03:33:49,773 - INFO - __main__ - Epoch  62, Step:  134900, Batch Loss:     3.042269, Lr: 0.000054, Tokens per sec:   2703
2023-03-09 03:34:09,742 - INFO - __main__ - Epoch  62, Step:  135000, Batch Loss:     4.167318, Lr: 0.000054, Tokens per sec:   2680
2023-03-09 03:34:29,236 - INFO - __main__ - Epoch  62: total training loss 7287.38
2023-03-09 03:34:29,238 - INFO - __main__ - Epoch 63
2023-03-09 03:34:29,937 - INFO - __main__ - Epoch  63, Step:  135100, Batch Loss:     2.550733, Lr: 0.000054, Tokens per sec:   1557
2023-03-09 03:34:49,289 - INFO - __main__ - Epoch  63, Step:  135200, Batch Loss:     3.464050, Lr: 0.000054, Tokens per sec:   2771
2023-03-09 03:35:07,779 - INFO - __main__ - Epoch  63, Step:  135300, Batch Loss:     4.554857, Lr: 0.000054, Tokens per sec:   2935
2023-03-09 03:35:25,919 - INFO - __main__ - Epoch  63, Step:  135400, Batch Loss:     3.426849, Lr: 0.000054, Tokens per sec:   2996
2023-03-09 03:35:44,512 - INFO - __main__ - Epoch  63, Step:  135500, Batch Loss:     3.933282, Lr: 0.000054, Tokens per sec:   2869
2023-03-09 03:36:03,341 - INFO - __main__ - Epoch  63, Step:  135600, Batch Loss:     2.658707, Lr: 0.000054, Tokens per sec:   2871
2023-03-09 03:36:23,297 - INFO - __main__ - Epoch  63, Step:  135700, Batch Loss:     2.905207, Lr: 0.000054, Tokens per sec:   2661
2023-03-09 03:36:43,281 - INFO - __main__ - Epoch  63, Step:  135800, Batch Loss:     2.525995, Lr: 0.000054, Tokens per sec:   2662
2023-03-09 03:37:03,347 - INFO - __main__ - Epoch  63, Step:  135900, Batch Loss:     3.790576, Lr: 0.000054, Tokens per sec:   2699
2023-03-09 03:37:23,301 - INFO - __main__ - Epoch  63, Step:  136000, Batch Loss:     3.052635, Lr: 0.000054, Tokens per sec:   2699
2023-03-09 03:37:43,342 - INFO - __main__ - Epoch  63, Step:  136100, Batch Loss:     3.639424, Lr: 0.000054, Tokens per sec:   2676
2023-03-09 03:38:01,672 - INFO - __main__ - Epoch  63, Step:  136200, Batch Loss:     2.781455, Lr: 0.000054, Tokens per sec:   2955
2023-03-09 03:38:20,641 - INFO - __main__ - Epoch  63, Step:  136300, Batch Loss:     3.880548, Lr: 0.000054, Tokens per sec:   2822
2023-03-09 03:38:39,297 - INFO - __main__ - Epoch  63, Step:  136400, Batch Loss:     2.545279, Lr: 0.000054, Tokens per sec:   2935
2023-03-09 03:38:59,090 - INFO - __main__ - Epoch  63, Step:  136500, Batch Loss:     3.373358, Lr: 0.000054, Tokens per sec:   2735
2023-03-09 03:39:18,531 - INFO - __main__ - Epoch  63, Step:  136600, Batch Loss:     4.037910, Lr: 0.000054, Tokens per sec:   2759
2023-03-09 03:39:37,964 - INFO - __main__ - Epoch  63, Step:  136700, Batch Loss:     2.706691, Lr: 0.000054, Tokens per sec:   2751
2023-03-09 03:39:57,295 - INFO - __main__ - Epoch  63, Step:  136800, Batch Loss:     2.858312, Lr: 0.000054, Tokens per sec:   2773
2023-03-09 03:40:17,227 - INFO - __main__ - Epoch  63, Step:  136900, Batch Loss:     3.238574, Lr: 0.000054, Tokens per sec:   2714
2023-03-09 03:40:37,236 - INFO - __main__ - Epoch  63, Step:  137000, Batch Loss:     4.110871, Lr: 0.000054, Tokens per sec:   2711
2023-03-09 03:40:55,825 - INFO - __main__ - Epoch  63, Step:  137100, Batch Loss:     4.114178, Lr: 0.000054, Tokens per sec:   2896
2023-03-09 03:41:13,872 - INFO - __main__ - Epoch  63, Step:  137200, Batch Loss:     4.257711, Lr: 0.000054, Tokens per sec:   2984
2023-03-09 03:41:27,853 - INFO - __main__ - Epoch  63: total training loss 7069.48
2023-03-09 03:41:27,854 - INFO - __main__ - Epoch 64
2023-03-09 03:41:32,355 - INFO - __main__ - Epoch  64, Step:  137300, Batch Loss:     2.787580, Lr: 0.000053, Tokens per sec:   2702
2023-03-09 03:41:50,423 - INFO - __main__ - Epoch  64, Step:  137400, Batch Loss:     2.000365, Lr: 0.000053, Tokens per sec:   3000
2023-03-09 03:42:09,283 - INFO - __main__ - Epoch  64, Step:  137500, Batch Loss:     1.958136, Lr: 0.000053, Tokens per sec:   2861
2023-03-09 03:42:27,650 - INFO - __main__ - Epoch  64, Step:  137600, Batch Loss:     3.211714, Lr: 0.000053, Tokens per sec:   2936
2023-03-09 03:42:46,869 - INFO - __main__ - Epoch  64, Step:  137700, Batch Loss:     3.454655, Lr: 0.000053, Tokens per sec:   2773
2023-03-09 03:43:05,889 - INFO - __main__ - Epoch  64, Step:  137800, Batch Loss:     2.582272, Lr: 0.000053, Tokens per sec:   2819
2023-03-09 03:43:25,348 - INFO - __main__ - Epoch  64, Step:  137900, Batch Loss:     3.105091, Lr: 0.000053, Tokens per sec:   2847
2023-03-09 03:43:43,698 - INFO - __main__ - Epoch  64, Step:  138000, Batch Loss:     2.878289, Lr: 0.000053, Tokens per sec:   2926
2023-03-09 03:44:01,735 - INFO - __main__ - Epoch  64, Step:  138100, Batch Loss:     3.405323, Lr: 0.000053, Tokens per sec:   2954
2023-03-09 03:44:19,716 - INFO - __main__ - Epoch  64, Step:  138200, Batch Loss:     3.193005, Lr: 0.000053, Tokens per sec:   2988
2023-03-09 03:44:37,708 - INFO - __main__ - Epoch  64, Step:  138300, Batch Loss:     3.259051, Lr: 0.000053, Tokens per sec:   2971
2023-03-09 03:44:55,719 - INFO - __main__ - Epoch  64, Step:  138400, Batch Loss:     2.665630, Lr: 0.000053, Tokens per sec:   2996
2023-03-09 03:45:13,713 - INFO - __main__ - Epoch  64, Step:  138500, Batch Loss:     2.798425, Lr: 0.000053, Tokens per sec:   3030
2023-03-09 03:45:31,788 - INFO - __main__ - Epoch  64, Step:  138600, Batch Loss:     4.363991, Lr: 0.000053, Tokens per sec:   2989
2023-03-09 03:45:51,582 - INFO - __main__ - Epoch  64, Step:  138700, Batch Loss:     3.345114, Lr: 0.000053, Tokens per sec:   2644
2023-03-09 03:46:09,700 - INFO - __main__ - Epoch  64, Step:  138800, Batch Loss:     3.071073, Lr: 0.000053, Tokens per sec:   2956
2023-03-09 03:46:28,552 - INFO - __main__ - Epoch  64, Step:  138900, Batch Loss:     2.094287, Lr: 0.000053, Tokens per sec:   2904
2023-03-09 03:46:46,918 - INFO - __main__ - Epoch  64, Step:  139000, Batch Loss:     3.711772, Lr: 0.000053, Tokens per sec:   2952
2023-03-09 03:47:05,652 - INFO - __main__ - Epoch  64, Step:  139100, Batch Loss:     2.245452, Lr: 0.000053, Tokens per sec:   2895
2023-03-09 03:47:24,133 - INFO - __main__ - Epoch  64, Step:  139200, Batch Loss:     4.097347, Lr: 0.000053, Tokens per sec:   2925
2023-03-09 03:47:42,279 - INFO - __main__ - Epoch  64, Step:  139300, Batch Loss:     3.124593, Lr: 0.000053, Tokens per sec:   2923
2023-03-09 03:48:00,693 - INFO - __main__ - Epoch  64, Step:  139400, Batch Loss:     3.101394, Lr: 0.000053, Tokens per sec:   2956
2023-03-09 03:48:11,953 - INFO - __main__ - Epoch  64: total training loss 6892.81
2023-03-09 03:48:11,954 - INFO - __main__ - Epoch 65
2023-03-09 03:48:20,974 - INFO - __main__ - Epoch  65, Step:  139500, Batch Loss:     1.990959, Lr: 0.000053, Tokens per sec:   2642
2023-03-09 03:48:39,690 - INFO - __main__ - Epoch  65, Step:  139600, Batch Loss:     2.435745, Lr: 0.000053, Tokens per sec:   2855
2023-03-09 03:48:57,887 - INFO - __main__ - Epoch  65, Step:  139700, Batch Loss:     2.617393, Lr: 0.000053, Tokens per sec:   2939
2023-03-09 03:49:15,939 - INFO - __main__ - Epoch  65, Step:  139800, Batch Loss:     2.705848, Lr: 0.000053, Tokens per sec:   2987
2023-03-09 03:49:35,268 - INFO - __main__ - Epoch  65, Step:  139900, Batch Loss:     2.866955, Lr: 0.000053, Tokens per sec:   2769
2023-03-09 03:49:53,401 - INFO - __main__ - Epoch  65, Step:  140000, Batch Loss:     2.489680, Lr: 0.000053, Tokens per sec:   2950
2023-03-09 03:50:12,600 - INFO - __main__ - Epoch  65, Step:  140100, Batch Loss:     3.194822, Lr: 0.000053, Tokens per sec:   2829
2023-03-09 03:50:32,588 - INFO - __main__ - Epoch  65, Step:  140200, Batch Loss:     2.742831, Lr: 0.000053, Tokens per sec:   2666
2023-03-09 03:50:52,509 - INFO - __main__ - Epoch  65, Step:  140300, Batch Loss:     2.956573, Lr: 0.000053, Tokens per sec:   2687
2023-03-09 03:51:12,435 - INFO - __main__ - Epoch  65, Step:  140400, Batch Loss:     2.660268, Lr: 0.000053, Tokens per sec:   2685
2023-03-09 03:51:31,554 - INFO - __main__ - Epoch  65, Step:  140500, Batch Loss:     2.287060, Lr: 0.000053, Tokens per sec:   2810
2023-03-09 03:51:50,023 - INFO - __main__ - Epoch  65, Step:  140600, Batch Loss:     3.155318, Lr: 0.000053, Tokens per sec:   2924
2023-03-09 03:52:08,787 - INFO - __main__ - Epoch  65, Step:  140700, Batch Loss:     3.116350, Lr: 0.000053, Tokens per sec:   2911
2023-03-09 03:52:28,715 - INFO - __main__ - Epoch  65, Step:  140800, Batch Loss:     3.762597, Lr: 0.000053, Tokens per sec:   2638
2023-03-09 03:52:47,391 - INFO - __main__ - Epoch  65, Step:  140900, Batch Loss:     3.229726, Lr: 0.000053, Tokens per sec:   2907
2023-03-09 03:53:06,480 - INFO - __main__ - Epoch  65, Step:  141000, Batch Loss:     4.750574, Lr: 0.000053, Tokens per sec:   2864
2023-03-09 03:53:25,788 - INFO - __main__ - Epoch  65, Step:  141100, Batch Loss:     3.531962, Lr: 0.000053, Tokens per sec:   2802
2023-03-09 03:53:43,967 - INFO - __main__ - Epoch  65, Step:  141200, Batch Loss:     3.372306, Lr: 0.000053, Tokens per sec:   2941
2023-03-09 03:54:02,920 - INFO - __main__ - Epoch  65, Step:  141300, Batch Loss:     2.869542, Lr: 0.000053, Tokens per sec:   2895
2023-03-09 03:54:21,058 - INFO - __main__ - Epoch  65, Step:  141400, Batch Loss:     4.598052, Lr: 0.000053, Tokens per sec:   2964
2023-03-09 03:54:39,271 - INFO - __main__ - Epoch  65, Step:  141500, Batch Loss:     2.762858, Lr: 0.000053, Tokens per sec:   2963
2023-03-09 03:54:57,369 - INFO - __main__ - Epoch  65, Step:  141600, Batch Loss:     2.775545, Lr: 0.000053, Tokens per sec:   2981
2023-03-09 03:55:04,028 - INFO - __main__ - Epoch  65: total training loss 6740.83
2023-03-09 03:55:04,029 - INFO - __main__ - Epoch 66
2023-03-09 03:55:16,961 - INFO - __main__ - Epoch  66, Step:  141700, Batch Loss:     3.089440, Lr: 0.000052, Tokens per sec:   2691
2023-03-09 03:55:35,680 - INFO - __main__ - Epoch  66, Step:  141800, Batch Loss:     3.139311, Lr: 0.000052, Tokens per sec:   2883
2023-03-09 03:55:55,663 - INFO - __main__ - Epoch  66, Step:  141900, Batch Loss:     1.632257, Lr: 0.000052, Tokens per sec:   2732
2023-03-09 03:56:14,609 - INFO - __main__ - Epoch  66, Step:  142000, Batch Loss:     2.748977, Lr: 0.000052, Tokens per sec:   2890
2023-03-09 03:56:32,955 - INFO - __main__ - Epoch  66, Step:  142100, Batch Loss:     2.197741, Lr: 0.000052, Tokens per sec:   2934
2023-03-09 03:56:52,779 - INFO - __main__ - Epoch  66, Step:  142200, Batch Loss:     4.252187, Lr: 0.000052, Tokens per sec:   2741
2023-03-09 03:57:12,056 - INFO - __main__ - Epoch  66, Step:  142300, Batch Loss:     3.378859, Lr: 0.000052, Tokens per sec:   2815
2023-03-09 03:57:31,031 - INFO - __main__ - Epoch  66, Step:  142400, Batch Loss:     3.324821, Lr: 0.000052, Tokens per sec:   2803
2023-03-09 03:57:49,115 - INFO - __main__ - Epoch  66, Step:  142500, Batch Loss:     4.884210, Lr: 0.000052, Tokens per sec:   3016
2023-03-09 03:58:07,170 - INFO - __main__ - Epoch  66, Step:  142600, Batch Loss:     3.615637, Lr: 0.000052, Tokens per sec:   2989
2023-03-09 03:58:25,271 - INFO - __main__ - Epoch  66, Step:  142700, Batch Loss:     2.582045, Lr: 0.000052, Tokens per sec:   2962
2023-03-09 03:58:43,820 - INFO - __main__ - Epoch  66, Step:  142800, Batch Loss:     2.494977, Lr: 0.000052, Tokens per sec:   2925
2023-03-09 03:59:02,055 - INFO - __main__ - Epoch  66, Step:  142900, Batch Loss:     3.212949, Lr: 0.000052, Tokens per sec:   2934
2023-03-09 03:59:20,796 - INFO - __main__ - Epoch  66, Step:  143000, Batch Loss:     3.778985, Lr: 0.000052, Tokens per sec:   2854
2023-03-09 03:59:39,356 - INFO - __main__ - Epoch  66, Step:  143100, Batch Loss:     3.588722, Lr: 0.000052, Tokens per sec:   2874
2023-03-09 03:59:58,138 - INFO - __main__ - Epoch  66, Step:  143200, Batch Loss:     3.868653, Lr: 0.000052, Tokens per sec:   2919
2023-03-09 04:00:17,473 - INFO - __main__ - Epoch  66, Step:  143300, Batch Loss:     3.264323, Lr: 0.000052, Tokens per sec:   2729
2023-03-09 04:00:37,431 - INFO - __main__ - Epoch  66, Step:  143400, Batch Loss:     2.445733, Lr: 0.000052, Tokens per sec:   2714
2023-03-09 04:00:56,993 - INFO - __main__ - Epoch  66, Step:  143500, Batch Loss:     2.791530, Lr: 0.000052, Tokens per sec:   2715
2023-03-09 04:01:16,450 - INFO - __main__ - Epoch  66, Step:  143600, Batch Loss:     3.727894, Lr: 0.000052, Tokens per sec:   2702
2023-03-09 04:01:35,680 - INFO - __main__ - Epoch  66, Step:  143700, Batch Loss:     2.756118, Lr: 0.000052, Tokens per sec:   2838
2023-03-09 04:01:55,474 - INFO - __main__ - Epoch  66, Step:  143800, Batch Loss:     2.349267, Lr: 0.000052, Tokens per sec:   2683
2023-03-09 04:01:58,170 - INFO - __main__ - Epoch  66: total training loss 6598.91
2023-03-09 04:01:58,171 - INFO - __main__ - Epoch 67
2023-03-09 04:02:15,127 - INFO - __main__ - Epoch  67, Step:  143900, Batch Loss:     2.679077, Lr: 0.000052, Tokens per sec:   2742
2023-03-09 04:02:33,434 - INFO - __main__ - Epoch  67, Step:  144000, Batch Loss:     2.986616, Lr: 0.000052, Tokens per sec:   2995
2023-03-09 04:02:51,676 - INFO - __main__ - Epoch  67, Step:  144100, Batch Loss:     3.452515, Lr: 0.000052, Tokens per sec:   2956
2023-03-09 04:03:09,957 - INFO - __main__ - Epoch  67, Step:  144200, Batch Loss:     3.441456, Lr: 0.000052, Tokens per sec:   2909
2023-03-09 04:03:28,181 - INFO - __main__ - Epoch  67, Step:  144300, Batch Loss:     3.246452, Lr: 0.000052, Tokens per sec:   2954
2023-03-09 04:03:46,737 - INFO - __main__ - Epoch  67, Step:  144400, Batch Loss:     3.106899, Lr: 0.000052, Tokens per sec:   2859
2023-03-09 04:04:06,705 - INFO - __main__ - Epoch  67, Step:  144500, Batch Loss:     3.039375, Lr: 0.000052, Tokens per sec:   2658
2023-03-09 04:04:26,628 - INFO - __main__ - Epoch  67, Step:  144600, Batch Loss:     2.671780, Lr: 0.000052, Tokens per sec:   2684
2023-03-09 04:04:46,607 - INFO - __main__ - Epoch  67, Step:  144700, Batch Loss:     2.298708, Lr: 0.000052, Tokens per sec:   2727
2023-03-09 04:05:06,588 - INFO - __main__ - Epoch  67, Step:  144800, Batch Loss:     3.025679, Lr: 0.000052, Tokens per sec:   2694
2023-03-09 04:05:26,539 - INFO - __main__ - Epoch  67, Step:  144900, Batch Loss:     2.081568, Lr: 0.000052, Tokens per sec:   2703
2023-03-09 04:05:46,491 - INFO - __main__ - Epoch  67, Step:  145000, Batch Loss:     3.118593, Lr: 0.000052, Tokens per sec:   2730
2023-03-09 04:06:06,466 - INFO - __main__ - Epoch  67, Step:  145100, Batch Loss:     3.403007, Lr: 0.000052, Tokens per sec:   2682
2023-03-09 04:06:26,381 - INFO - __main__ - Epoch  67, Step:  145200, Batch Loss:     2.069217, Lr: 0.000052, Tokens per sec:   2663
2023-03-09 04:06:46,333 - INFO - __main__ - Epoch  67, Step:  145300, Batch Loss:     2.472791, Lr: 0.000052, Tokens per sec:   2683
2023-03-09 04:07:06,305 - INFO - __main__ - Epoch  67, Step:  145400, Batch Loss:     3.697064, Lr: 0.000052, Tokens per sec:   2717
2023-03-09 04:07:26,320 - INFO - __main__ - Epoch  67, Step:  145500, Batch Loss:     2.259224, Lr: 0.000052, Tokens per sec:   2693
2023-03-09 04:07:46,269 - INFO - __main__ - Epoch  67, Step:  145600, Batch Loss:     2.941673, Lr: 0.000052, Tokens per sec:   2719
2023-03-09 04:08:06,196 - INFO - __main__ - Epoch  67, Step:  145700, Batch Loss:     3.310602, Lr: 0.000052, Tokens per sec:   2707
2023-03-09 04:08:26,163 - INFO - __main__ - Epoch  67, Step:  145800, Batch Loss:     3.021178, Lr: 0.000052, Tokens per sec:   2691
2023-03-09 04:08:46,093 - INFO - __main__ - Epoch  67, Step:  145900, Batch Loss:     3.909044, Lr: 0.000052, Tokens per sec:   2721
2023-03-09 04:09:04,747 - INFO - __main__ - Epoch  67: total training loss 6461.51
2023-03-09 04:09:04,748 - INFO - __main__ - Epoch 68
2023-03-09 04:09:06,448 - INFO - __main__ - Epoch  68, Step:  146000, Batch Loss:     2.799567, Lr: 0.000051, Tokens per sec:   2419
2023-03-09 04:09:26,398 - INFO - __main__ - Epoch  68, Step:  146100, Batch Loss:     3.727125, Lr: 0.000051, Tokens per sec:   2702
2023-03-09 04:09:46,374 - INFO - __main__ - Epoch  68, Step:  146200, Batch Loss:     3.364537, Lr: 0.000051, Tokens per sec:   2710
2023-03-09 04:10:06,333 - INFO - __main__ - Epoch  68, Step:  146300, Batch Loss:     3.025101, Lr: 0.000051, Tokens per sec:   2687
2023-03-09 04:10:26,263 - INFO - __main__ - Epoch  68, Step:  146400, Batch Loss:     2.281500, Lr: 0.000051, Tokens per sec:   2675
2023-03-09 04:10:46,213 - INFO - __main__ - Epoch  68, Step:  146500, Batch Loss:     3.037378, Lr: 0.000051, Tokens per sec:   2698
2023-03-09 04:11:06,194 - INFO - __main__ - Epoch  68, Step:  146600, Batch Loss:     3.546543, Lr: 0.000051, Tokens per sec:   2710
2023-03-09 04:11:26,182 - INFO - __main__ - Epoch  68, Step:  146700, Batch Loss:     3.003549, Lr: 0.000051, Tokens per sec:   2724
2023-03-09 04:11:46,176 - INFO - __main__ - Epoch  68, Step:  146800, Batch Loss:     2.357937, Lr: 0.000051, Tokens per sec:   2665
2023-03-09 04:12:06,151 - INFO - __main__ - Epoch  68, Step:  146900, Batch Loss:     2.350696, Lr: 0.000051, Tokens per sec:   2699
2023-03-09 04:12:26,078 - INFO - __main__ - Epoch  68, Step:  147000, Batch Loss:     2.881311, Lr: 0.000051, Tokens per sec:   2685
2023-03-09 04:12:46,021 - INFO - __main__ - Epoch  68, Step:  147100, Batch Loss:     2.908344, Lr: 0.000051, Tokens per sec:   2695
2023-03-09 04:13:05,992 - INFO - __main__ - Epoch  68, Step:  147200, Batch Loss:     3.110918, Lr: 0.000051, Tokens per sec:   2720
2023-03-09 04:13:25,961 - INFO - __main__ - Epoch  68, Step:  147300, Batch Loss:     2.651926, Lr: 0.000051, Tokens per sec:   2707
2023-03-09 04:13:45,938 - INFO - __main__ - Epoch  68, Step:  147400, Batch Loss:     2.714919, Lr: 0.000051, Tokens per sec:   2627
2023-03-09 04:14:05,936 - INFO - __main__ - Epoch  68, Step:  147500, Batch Loss:     3.084248, Lr: 0.000051, Tokens per sec:   2723
2023-03-09 04:14:25,910 - INFO - __main__ - Epoch  68, Step:  147600, Batch Loss:     3.575040, Lr: 0.000051, Tokens per sec:   2746
2023-03-09 04:14:45,897 - INFO - __main__ - Epoch  68, Step:  147700, Batch Loss:     3.137739, Lr: 0.000051, Tokens per sec:   2674
2023-03-09 04:15:05,854 - INFO - __main__ - Epoch  68, Step:  147800, Batch Loss:     3.377445, Lr: 0.000051, Tokens per sec:   2703
2023-03-09 04:15:25,828 - INFO - __main__ - Epoch  68, Step:  147900, Batch Loss:     2.753207, Lr: 0.000051, Tokens per sec:   2661
2023-03-09 04:15:45,779 - INFO - __main__ - Epoch  68, Step:  148000, Batch Loss:     2.249376, Lr: 0.000051, Tokens per sec:   2700
2023-03-09 04:16:05,737 - INFO - __main__ - Epoch  68, Step:  148100, Batch Loss:     3.435090, Lr: 0.000051, Tokens per sec:   2698
2023-03-09 04:16:20,179 - INFO - __main__ - Epoch  68: total training loss 6289.76
2023-03-09 04:16:20,180 - INFO - __main__ - Epoch 69
2023-03-09 04:16:26,066 - INFO - __main__ - Epoch  69, Step:  148200, Batch Loss:     2.266832, Lr: 0.000050, Tokens per sec:   2580
2023-03-09 04:16:46,043 - INFO - __main__ - Epoch  69, Step:  148300, Batch Loss:     1.984110, Lr: 0.000050, Tokens per sec:   2679
2023-03-09 04:17:05,990 - INFO - __main__ - Epoch  69, Step:  148400, Batch Loss:     3.718229, Lr: 0.000050, Tokens per sec:   2688
2023-03-09 04:17:25,994 - INFO - __main__ - Epoch  69, Step:  148500, Batch Loss:     2.860345, Lr: 0.000050, Tokens per sec:   2672
2023-03-09 04:17:45,983 - INFO - __main__ - Epoch  69, Step:  148600, Batch Loss:     2.454304, Lr: 0.000050, Tokens per sec:   2712
2023-03-09 04:18:05,953 - INFO - __main__ - Epoch  69, Step:  148700, Batch Loss:     2.542552, Lr: 0.000050, Tokens per sec:   2669
2023-03-09 04:18:25,969 - INFO - __main__ - Epoch  69, Step:  148800, Batch Loss:     2.128883, Lr: 0.000050, Tokens per sec:   2693
2023-03-09 04:18:45,923 - INFO - __main__ - Epoch  69, Step:  148900, Batch Loss:     2.150476, Lr: 0.000050, Tokens per sec:   2662
2023-03-09 04:19:05,853 - INFO - __main__ - Epoch  69, Step:  149000, Batch Loss:     2.048320, Lr: 0.000050, Tokens per sec:   2742
2023-03-09 04:19:25,827 - INFO - __main__ - Epoch  69, Step:  149100, Batch Loss:     2.864765, Lr: 0.000050, Tokens per sec:   2691
2023-03-09 04:19:45,805 - INFO - __main__ - Epoch  69, Step:  149200, Batch Loss:     2.506117, Lr: 0.000050, Tokens per sec:   2713
2023-03-09 04:20:05,800 - INFO - __main__ - Epoch  69, Step:  149300, Batch Loss:     3.505464, Lr: 0.000050, Tokens per sec:   2727
2023-03-09 04:20:25,767 - INFO - __main__ - Epoch  69, Step:  149400, Batch Loss:     1.819928, Lr: 0.000050, Tokens per sec:   2689
2023-03-09 04:20:45,741 - INFO - __main__ - Epoch  69, Step:  149500, Batch Loss:     2.476959, Lr: 0.000050, Tokens per sec:   2699
2023-03-09 04:21:05,583 - INFO - __main__ - Epoch  69, Step:  149600, Batch Loss:     2.195160, Lr: 0.000050, Tokens per sec:   2731
2023-03-09 04:21:23,505 - INFO - __main__ - Epoch  69, Step:  149700, Batch Loss:     3.451264, Lr: 0.000050, Tokens per sec:   3034
2023-03-09 04:21:43,418 - INFO - __main__ - Epoch  69, Step:  149800, Batch Loss:     2.826952, Lr: 0.000050, Tokens per sec:   2718
2023-03-09 04:22:03,381 - INFO - __main__ - Epoch  69, Step:  149900, Batch Loss:     1.783367, Lr: 0.000050, Tokens per sec:   2692
2023-03-09 04:22:23,250 - INFO - __main__ - Epoch  69, Step:  150000, Batch Loss:     3.203891, Lr: 0.000050, Tokens per sec:   2713
2023-03-09 04:22:43,224 - INFO - __main__ - Epoch  69, Step:  150100, Batch Loss:     2.866315, Lr: 0.000050, Tokens per sec:   2717
2023-03-09 04:23:03,198 - INFO - __main__ - Epoch  69, Step:  150200, Batch Loss:     2.599241, Lr: 0.000050, Tokens per sec:   2677
2023-03-09 04:23:23,130 - INFO - __main__ - Epoch  69, Step:  150300, Batch Loss:     2.759481, Lr: 0.000050, Tokens per sec:   2645
2023-03-09 04:23:33,353 - INFO - __main__ - Epoch  69: total training loss 6137.84
2023-03-09 04:23:33,354 - INFO - __main__ - Epoch 70
2023-03-09 04:23:43,392 - INFO - __main__ - Epoch  70, Step:  150400, Batch Loss:     2.364232, Lr: 0.000050, Tokens per sec:   2599
2023-03-09 04:24:03,276 - INFO - __main__ - Epoch  70, Step:  150500, Batch Loss:     2.402883, Lr: 0.000050, Tokens per sec:   2724
2023-03-09 04:24:23,274 - INFO - __main__ - Epoch  70, Step:  150600, Batch Loss:     3.285188, Lr: 0.000050, Tokens per sec:   2719
2023-03-09 04:24:43,208 - INFO - __main__ - Epoch  70, Step:  150700, Batch Loss:     1.894098, Lr: 0.000050, Tokens per sec:   2690
2023-03-09 04:25:03,198 - INFO - __main__ - Epoch  70, Step:  150800, Batch Loss:     2.698272, Lr: 0.000050, Tokens per sec:   2646
2023-03-09 04:25:23,173 - INFO - __main__ - Epoch  70, Step:  150900, Batch Loss:     3.293300, Lr: 0.000050, Tokens per sec:   2724
2023-03-09 04:25:43,143 - INFO - __main__ - Epoch  70, Step:  151000, Batch Loss:     2.459731, Lr: 0.000050, Tokens per sec:   2717
2023-03-09 04:26:03,070 - INFO - __main__ - Epoch  70, Step:  151100, Batch Loss:     2.974217, Lr: 0.000050, Tokens per sec:   2749
2023-03-09 04:26:23,057 - INFO - __main__ - Epoch  70, Step:  151200, Batch Loss:     2.301880, Lr: 0.000050, Tokens per sec:   2619
2023-03-09 04:26:42,965 - INFO - __main__ - Epoch  70, Step:  151300, Batch Loss:     2.616119, Lr: 0.000050, Tokens per sec:   2692
2023-03-09 04:27:02,856 - INFO - __main__ - Epoch  70, Step:  151400, Batch Loss:     2.452777, Lr: 0.000050, Tokens per sec:   2684
2023-03-09 04:27:22,813 - INFO - __main__ - Epoch  70, Step:  151500, Batch Loss:     3.235693, Lr: 0.000050, Tokens per sec:   2719
2023-03-09 04:27:42,209 - INFO - __main__ - Epoch  70, Step:  151600, Batch Loss:     2.471671, Lr: 0.000050, Tokens per sec:   2784
2023-03-09 04:28:01,541 - INFO - __main__ - Epoch  70, Step:  151700, Batch Loss:     2.564768, Lr: 0.000050, Tokens per sec:   2776
2023-03-09 04:28:21,464 - INFO - __main__ - Epoch  70, Step:  151800, Batch Loss:     2.173209, Lr: 0.000050, Tokens per sec:   2637
2023-03-09 04:28:41,448 - INFO - __main__ - Epoch  70, Step:  151900, Batch Loss:     3.484355, Lr: 0.000050, Tokens per sec:   2720
2023-03-09 04:29:00,418 - INFO - __main__ - Epoch  70, Step:  152000, Batch Loss:     3.057674, Lr: 0.000050, Tokens per sec:   2820
2023-03-09 04:29:18,376 - INFO - __main__ - Epoch  70, Step:  152100, Batch Loss:     1.909368, Lr: 0.000050, Tokens per sec:   2978
2023-03-09 04:29:37,750 - INFO - __main__ - Epoch  70, Step:  152200, Batch Loss:     2.786046, Lr: 0.000050, Tokens per sec:   2786
2023-03-09 04:29:57,740 - INFO - __main__ - Epoch  70, Step:  152300, Batch Loss:     3.874320, Lr: 0.000050, Tokens per sec:   2716
2023-03-09 04:30:17,717 - INFO - __main__ - Epoch  70, Step:  152400, Batch Loss:     3.847069, Lr: 0.000050, Tokens per sec:   2742
2023-03-09 04:30:37,575 - INFO - __main__ - Epoch  70, Step:  152500, Batch Loss:     2.770534, Lr: 0.000050, Tokens per sec:   2733
2023-03-09 04:30:43,619 - INFO - __main__ - Epoch  70: total training loss 6027.91
2023-03-09 04:30:43,621 - INFO - __main__ - Epoch 71
2023-03-09 04:30:57,945 - INFO - __main__ - Epoch  71, Step:  152600, Batch Loss:     2.341896, Lr: 0.000049, Tokens per sec:   2571
2023-03-09 04:31:17,877 - INFO - __main__ - Epoch  71, Step:  152700, Batch Loss:     2.060750, Lr: 0.000049, Tokens per sec:   2725
2023-03-09 04:31:37,815 - INFO - __main__ - Epoch  71, Step:  152800, Batch Loss:     2.725655, Lr: 0.000049, Tokens per sec:   2661
2023-03-09 04:31:57,763 - INFO - __main__ - Epoch  71, Step:  152900, Batch Loss:     1.686207, Lr: 0.000049, Tokens per sec:   2693
2023-03-09 04:32:17,739 - INFO - __main__ - Epoch  71, Step:  153000, Batch Loss:     2.511244, Lr: 0.000049, Tokens per sec:   2670
2023-03-09 04:32:37,699 - INFO - __main__ - Epoch  71, Step:  153100, Batch Loss:     2.330225, Lr: 0.000049, Tokens per sec:   2685
2023-03-09 04:32:57,526 - INFO - __main__ - Epoch  71, Step:  153200, Batch Loss:     2.611546, Lr: 0.000049, Tokens per sec:   2710
2023-03-09 04:33:17,433 - INFO - __main__ - Epoch  71, Step:  153300, Batch Loss:     2.267056, Lr: 0.000049, Tokens per sec:   2691
2023-03-09 04:33:37,454 - INFO - __main__ - Epoch  71, Step:  153400, Batch Loss:     2.735781, Lr: 0.000049, Tokens per sec:   2710
2023-03-09 04:33:57,425 - INFO - __main__ - Epoch  71, Step:  153500, Batch Loss:     2.093306, Lr: 0.000049, Tokens per sec:   2724
2023-03-09 04:34:17,421 - INFO - __main__ - Epoch  71, Step:  153600, Batch Loss:     2.970311, Lr: 0.000049, Tokens per sec:   2718
2023-03-09 04:34:37,371 - INFO - __main__ - Epoch  71, Step:  153700, Batch Loss:     3.919756, Lr: 0.000049, Tokens per sec:   2709
2023-03-09 04:34:57,359 - INFO - __main__ - Epoch  71, Step:  153800, Batch Loss:     1.883803, Lr: 0.000049, Tokens per sec:   2678
2023-03-09 04:35:17,362 - INFO - __main__ - Epoch  71, Step:  153900, Batch Loss:     2.160662, Lr: 0.000049, Tokens per sec:   2679
2023-03-09 04:35:37,326 - INFO - __main__ - Epoch  71, Step:  154000, Batch Loss:     4.472728, Lr: 0.000049, Tokens per sec:   2674
2023-03-09 04:35:57,328 - INFO - __main__ - Epoch  71, Step:  154100, Batch Loss:     3.540979, Lr: 0.000049, Tokens per sec:   2688
2023-03-09 04:36:17,300 - INFO - __main__ - Epoch  71, Step:  154200, Batch Loss:     2.028903, Lr: 0.000049, Tokens per sec:   2668
2023-03-09 04:36:37,295 - INFO - __main__ - Epoch  71, Step:  154300, Batch Loss:     3.730478, Lr: 0.000049, Tokens per sec:   2708
2023-03-09 04:36:57,248 - INFO - __main__ - Epoch  71, Step:  154400, Batch Loss:     2.485516, Lr: 0.000049, Tokens per sec:   2716
2023-03-09 04:37:17,145 - INFO - __main__ - Epoch  71, Step:  154500, Batch Loss:     2.900847, Lr: 0.000049, Tokens per sec:   2732
2023-03-09 04:37:37,056 - INFO - __main__ - Epoch  71, Step:  154600, Batch Loss:     2.675373, Lr: 0.000049, Tokens per sec:   2727
2023-03-09 04:37:57,010 - INFO - __main__ - Epoch  71, Step:  154700, Batch Loss:     3.224745, Lr: 0.000049, Tokens per sec:   2719
2023-03-09 04:37:58,891 - INFO - __main__ - Epoch  71: total training loss 5898.72
2023-03-09 04:37:58,892 - INFO - __main__ - Epoch 72
2023-03-09 04:38:17,365 - INFO - __main__ - Epoch  72, Step:  154800, Batch Loss:     1.880965, Lr: 0.000049, Tokens per sec:   2650
2023-03-09 04:38:37,317 - INFO - __main__ - Epoch  72, Step:  154900, Batch Loss:     2.006909, Lr: 0.000049, Tokens per sec:   2707
2023-03-09 04:38:57,287 - INFO - __main__ - Epoch  72, Step:  155000, Batch Loss:     1.897708, Lr: 0.000049, Tokens per sec:   2679
2023-03-09 04:39:17,281 - INFO - __main__ - Epoch  72, Step:  155100, Batch Loss:     1.861192, Lr: 0.000049, Tokens per sec:   2684
2023-03-09 04:39:37,255 - INFO - __main__ - Epoch  72, Step:  155200, Batch Loss:     2.068268, Lr: 0.000049, Tokens per sec:   2719
2023-03-09 04:39:57,230 - INFO - __main__ - Epoch  72, Step:  155300, Batch Loss:     2.174901, Lr: 0.000049, Tokens per sec:   2736
2023-03-09 04:40:17,199 - INFO - __main__ - Epoch  72, Step:  155400, Batch Loss:     3.364709, Lr: 0.000049, Tokens per sec:   2697
2023-03-09 04:40:37,176 - INFO - __main__ - Epoch  72, Step:  155500, Batch Loss:     1.903953, Lr: 0.000049, Tokens per sec:   2694
2023-03-09 04:40:57,151 - INFO - __main__ - Epoch  72, Step:  155600, Batch Loss:     2.612888, Lr: 0.000049, Tokens per sec:   2740
2023-03-09 04:41:17,061 - INFO - __main__ - Epoch  72, Step:  155700, Batch Loss:     1.915312, Lr: 0.000049, Tokens per sec:   2703
2023-03-09 04:41:37,036 - INFO - __main__ - Epoch  72, Step:  155800, Batch Loss:     3.101214, Lr: 0.000049, Tokens per sec:   2699
2023-03-09 04:41:57,013 - INFO - __main__ - Epoch  72, Step:  155900, Batch Loss:     1.684167, Lr: 0.000049, Tokens per sec:   2705
2023-03-09 04:42:16,983 - INFO - __main__ - Epoch  72, Step:  156000, Batch Loss:     2.356090, Lr: 0.000049, Tokens per sec:   2676
2023-03-09 04:42:36,938 - INFO - __main__ - Epoch  72, Step:  156100, Batch Loss:     1.940925, Lr: 0.000049, Tokens per sec:   2663
2023-03-09 04:42:56,871 - INFO - __main__ - Epoch  72, Step:  156200, Batch Loss:     3.023372, Lr: 0.000049, Tokens per sec:   2756
2023-03-09 04:43:16,804 - INFO - __main__ - Epoch  72, Step:  156300, Batch Loss:     1.841979, Lr: 0.000049, Tokens per sec:   2696
2023-03-09 04:43:36,758 - INFO - __main__ - Epoch  72, Step:  156400, Batch Loss:     2.987361, Lr: 0.000049, Tokens per sec:   2698
2023-03-09 04:43:56,732 - INFO - __main__ - Epoch  72, Step:  156500, Batch Loss:     3.840680, Lr: 0.000049, Tokens per sec:   2683
2023-03-09 04:44:16,672 - INFO - __main__ - Epoch  72, Step:  156600, Batch Loss:     2.742357, Lr: 0.000049, Tokens per sec:   2672
2023-03-09 04:44:36,653 - INFO - __main__ - Epoch  72, Step:  156700, Batch Loss:     1.871485, Lr: 0.000049, Tokens per sec:   2666
2023-03-09 04:44:56,629 - INFO - __main__ - Epoch  72, Step:  156800, Batch Loss:     2.755605, Lr: 0.000049, Tokens per sec:   2708
2023-03-09 04:45:14,268 - INFO - __main__ - Epoch  72: total training loss 5752.74
2023-03-09 04:45:14,269 - INFO - __main__ - Epoch 73
2023-03-09 04:45:16,963 - INFO - __main__ - Epoch  73, Step:  156900, Batch Loss:     1.285499, Lr: 0.000048, Tokens per sec:   2399
2023-03-09 04:45:36,934 - INFO - __main__ - Epoch  73, Step:  157000, Batch Loss:     2.990978, Lr: 0.000048, Tokens per sec:   2702
2023-03-09 04:45:56,910 - INFO - __main__ - Epoch  73, Step:  157100, Batch Loss:     2.462106, Lr: 0.000048, Tokens per sec:   2677
2023-03-09 04:46:16,647 - INFO - __main__ - Epoch  73, Step:  157200, Batch Loss:     2.714228, Lr: 0.000048, Tokens per sec:   2779
2023-03-09 04:46:34,619 - INFO - __main__ - Epoch  73, Step:  157300, Batch Loss:     2.264633, Lr: 0.000048, Tokens per sec:   3000
2023-03-09 04:46:52,542 - INFO - __main__ - Epoch  73, Step:  157400, Batch Loss:     1.928319, Lr: 0.000048, Tokens per sec:   3009
2023-03-09 04:47:11,033 - INFO - __main__ - Epoch  73, Step:  157500, Batch Loss:     1.838527, Lr: 0.000048, Tokens per sec:   2892
2023-03-09 04:47:30,963 - INFO - __main__ - Epoch  73, Step:  157600, Batch Loss:     2.971790, Lr: 0.000048, Tokens per sec:   2699
2023-03-09 04:47:50,916 - INFO - __main__ - Epoch  73, Step:  157700, Batch Loss:     2.806768, Lr: 0.000048, Tokens per sec:   2690
2023-03-09 04:48:10,892 - INFO - __main__ - Epoch  73, Step:  157800, Batch Loss:     3.110927, Lr: 0.000048, Tokens per sec:   2667
2023-03-09 04:48:30,862 - INFO - __main__ - Epoch  73, Step:  157900, Batch Loss:     1.984476, Lr: 0.000048, Tokens per sec:   2697
2023-03-09 04:48:50,817 - INFO - __main__ - Epoch  73, Step:  158000, Batch Loss:     1.781862, Lr: 0.000048, Tokens per sec:   2714
2023-03-09 04:49:10,794 - INFO - __main__ - Epoch  73, Step:  158100, Batch Loss:     2.562688, Lr: 0.000048, Tokens per sec:   2747
2023-03-09 04:49:30,760 - INFO - __main__ - Epoch  73, Step:  158200, Batch Loss:     2.736516, Lr: 0.000048, Tokens per sec:   2705
2023-03-09 04:49:50,440 - INFO - __main__ - Epoch  73, Step:  158300, Batch Loss:     2.371016, Lr: 0.000048, Tokens per sec:   2729
2023-03-09 04:50:09,287 - INFO - __main__ - Epoch  73, Step:  158400, Batch Loss:     3.017851, Lr: 0.000048, Tokens per sec:   2871
2023-03-09 04:50:27,258 - INFO - __main__ - Epoch  73, Step:  158500, Batch Loss:     2.198821, Lr: 0.000048, Tokens per sec:   2985
2023-03-09 04:50:45,212 - INFO - __main__ - Epoch  73, Step:  158600, Batch Loss:     3.600282, Lr: 0.000048, Tokens per sec:   2979
2023-03-09 04:51:04,492 - INFO - __main__ - Epoch  73, Step:  158700, Batch Loss:     1.141075, Lr: 0.000048, Tokens per sec:   2759
2023-03-09 04:51:24,381 - INFO - __main__ - Epoch  73, Step:  158800, Batch Loss:     3.487880, Lr: 0.000048, Tokens per sec:   2733
2023-03-09 04:51:44,361 - INFO - __main__ - Epoch  73, Step:  158900, Batch Loss:     2.593472, Lr: 0.000048, Tokens per sec:   2663
2023-03-09 04:52:04,289 - INFO - __main__ - Epoch  73, Step:  159000, Batch Loss:     2.358097, Lr: 0.000048, Tokens per sec:   2697
2023-03-09 04:52:17,713 - INFO - __main__ - Epoch  73: total training loss 5646.43
2023-03-09 04:52:17,714 - INFO - __main__ - Epoch 74
2023-03-09 04:52:24,653 - INFO - __main__ - Epoch  74, Step:  159100, Batch Loss:     1.777604, Lr: 0.000048, Tokens per sec:   2659
2023-03-09 04:52:44,651 - INFO - __main__ - Epoch  74, Step:  159200, Batch Loss:     2.300126, Lr: 0.000048, Tokens per sec:   2710
2023-03-09 04:53:04,656 - INFO - __main__ - Epoch  74, Step:  159300, Batch Loss:     2.428155, Lr: 0.000048, Tokens per sec:   2693
2023-03-09 04:53:24,626 - INFO - __main__ - Epoch  74, Step:  159400, Batch Loss:     2.385480, Lr: 0.000048, Tokens per sec:   2735
2023-03-09 04:53:44,539 - INFO - __main__ - Epoch  74, Step:  159500, Batch Loss:     2.563323, Lr: 0.000048, Tokens per sec:   2721
2023-03-09 04:54:04,493 - INFO - __main__ - Epoch  74, Step:  159600, Batch Loss:     1.914428, Lr: 0.000048, Tokens per sec:   2716
2023-03-09 04:54:24,407 - INFO - __main__ - Epoch  74, Step:  159700, Batch Loss:     2.442279, Lr: 0.000048, Tokens per sec:   2681
2023-03-09 04:54:44,367 - INFO - __main__ - Epoch  74, Step:  159800, Batch Loss:     1.723843, Lr: 0.000048, Tokens per sec:   2725
2023-03-09 04:55:04,244 - INFO - __main__ - Epoch  74, Step:  159900, Batch Loss:     1.795049, Lr: 0.000048, Tokens per sec:   2726
2023-03-09 04:55:24,244 - INFO - __main__ - Epoch  74, Step:  160000, Batch Loss:     3.951441, Lr: 0.000048, Tokens per sec:   2689
2023-03-09 04:55:44,239 - INFO - __main__ - Epoch  74, Step:  160100, Batch Loss:     3.602562, Lr: 0.000048, Tokens per sec:   2694
2023-03-09 04:56:04,176 - INFO - __main__ - Epoch  74, Step:  160200, Batch Loss:     2.389118, Lr: 0.000048, Tokens per sec:   2677
2023-03-09 04:56:24,173 - INFO - __main__ - Epoch  74, Step:  160300, Batch Loss:     2.102626, Lr: 0.000048, Tokens per sec:   2708
2023-03-09 04:56:44,120 - INFO - __main__ - Epoch  74, Step:  160400, Batch Loss:     2.984565, Lr: 0.000048, Tokens per sec:   2685
2023-03-09 04:57:04,090 - INFO - __main__ - Epoch  74, Step:  160500, Batch Loss:     2.100200, Lr: 0.000048, Tokens per sec:   2634
2023-03-09 04:57:24,027 - INFO - __main__ - Epoch  74, Step:  160600, Batch Loss:     2.821834, Lr: 0.000048, Tokens per sec:   2747
2023-03-09 04:57:44,023 - INFO - __main__ - Epoch  74, Step:  160700, Batch Loss:     1.720997, Lr: 0.000048, Tokens per sec:   2656
2023-03-09 04:58:03,957 - INFO - __main__ - Epoch  74, Step:  160800, Batch Loss:     2.601180, Lr: 0.000048, Tokens per sec:   2694
2023-03-09 04:58:23,247 - INFO - __main__ - Epoch  74, Step:  160900, Batch Loss:     3.718117, Lr: 0.000048, Tokens per sec:   2798
2023-03-09 04:58:42,163 - INFO - __main__ - Epoch  74, Step:  161000, Batch Loss:     2.381857, Lr: 0.000048, Tokens per sec:   2789
2023-03-09 04:59:02,159 - INFO - __main__ - Epoch  74, Step:  161100, Batch Loss:     2.770165, Lr: 0.000048, Tokens per sec:   2675
2023-03-09 04:59:22,087 - INFO - __main__ - Epoch  74, Step:  161200, Batch Loss:     1.715782, Lr: 0.000048, Tokens per sec:   2683
2023-03-09 04:59:31,302 - INFO - __main__ - Epoch  74: total training loss 5532.41
2023-03-09 04:59:31,303 - INFO - __main__ - Epoch 75
2023-03-09 04:59:42,427 - INFO - __main__ - Epoch  75, Step:  161300, Batch Loss:     3.149596, Lr: 0.000048, Tokens per sec:   2581
2023-03-09 05:00:02,385 - INFO - __main__ - Epoch  75, Step:  161400, Batch Loss:     1.803081, Lr: 0.000048, Tokens per sec:   2696
2023-03-09 05:00:20,614 - INFO - __main__ - Epoch  75, Step:  161500, Batch Loss:     2.157430, Lr: 0.000048, Tokens per sec:   2917
2023-03-09 05:00:38,591 - INFO - __main__ - Epoch  75, Step:  161600, Batch Loss:     2.472158, Lr: 0.000048, Tokens per sec:   2981
2023-03-09 05:00:56,584 - INFO - __main__ - Epoch  75, Step:  161700, Batch Loss:     1.653921, Lr: 0.000048, Tokens per sec:   3040
2023-03-09 05:01:14,526 - INFO - __main__ - Epoch  75, Step:  161800, Batch Loss:     2.178756, Lr: 0.000048, Tokens per sec:   2997
2023-03-09 05:01:33,269 - INFO - __main__ - Epoch  75, Step:  161900, Batch Loss:     1.710396, Lr: 0.000048, Tokens per sec:   2851
2023-03-09 05:01:53,121 - INFO - __main__ - Epoch  75, Step:  162000, Batch Loss:     3.070776, Lr: 0.000048, Tokens per sec:   2699
2023-03-09 05:02:11,098 - INFO - __main__ - Epoch  75, Step:  162100, Batch Loss:     2.211655, Lr: 0.000048, Tokens per sec:   3030
2023-03-09 05:02:29,051 - INFO - __main__ - Epoch  75, Step:  162200, Batch Loss:     2.982248, Lr: 0.000048, Tokens per sec:   3014
2023-03-09 05:02:47,043 - INFO - __main__ - Epoch  75, Step:  162300, Batch Loss:     2.966984, Lr: 0.000048, Tokens per sec:   3035
2023-03-09 05:03:06,740 - INFO - __main__ - Epoch  75, Step:  162400, Batch Loss:     3.679533, Lr: 0.000048, Tokens per sec:   2742
2023-03-09 05:03:26,672 - INFO - __main__ - Epoch  75, Step:  162500, Batch Loss:     4.055549, Lr: 0.000048, Tokens per sec:   2712
2023-03-09 05:03:46,616 - INFO - __main__ - Epoch  75, Step:  162600, Batch Loss:     3.833148, Lr: 0.000048, Tokens per sec:   2743
2023-03-09 05:04:06,616 - INFO - __main__ - Epoch  75, Step:  162700, Batch Loss:     3.327049, Lr: 0.000048, Tokens per sec:   2700
2023-03-09 05:04:26,588 - INFO - __main__ - Epoch  75, Step:  162800, Batch Loss:     2.805861, Lr: 0.000048, Tokens per sec:   2728
2023-03-09 05:04:46,545 - INFO - __main__ - Epoch  75, Step:  162900, Batch Loss:     2.789730, Lr: 0.000048, Tokens per sec:   2671
2023-03-09 05:05:06,498 - INFO - __main__ - Epoch  75, Step:  163000, Batch Loss:     2.583833, Lr: 0.000048, Tokens per sec:   2690
2023-03-09 05:05:26,472 - INFO - __main__ - Epoch  75, Step:  163100, Batch Loss:     2.400656, Lr: 0.000048, Tokens per sec:   2695
2023-03-09 05:05:46,461 - INFO - __main__ - Epoch  75, Step:  163200, Batch Loss:     2.886570, Lr: 0.000048, Tokens per sec:   2648
2023-03-09 05:06:06,401 - INFO - __main__ - Epoch  75, Step:  163300, Batch Loss:     2.333589, Lr: 0.000048, Tokens per sec:   2681
2023-03-09 05:06:26,373 - INFO - __main__ - Epoch  75, Step:  163400, Batch Loss:     2.237165, Lr: 0.000048, Tokens per sec:   2668
2023-03-09 05:06:31,410 - INFO - __main__ - Epoch  75: total training loss 5409.14
2023-03-09 05:06:31,411 - INFO - __main__ - Epoch 76
2023-03-09 05:06:46,689 - INFO - __main__ - Epoch  76, Step:  163500, Batch Loss:     2.024471, Lr: 0.000047, Tokens per sec:   2673
2023-03-09 05:07:06,653 - INFO - __main__ - Epoch  76, Step:  163600, Batch Loss:     2.488595, Lr: 0.000047, Tokens per sec:   2710
2023-03-09 05:07:26,635 - INFO - __main__ - Epoch  76, Step:  163700, Batch Loss:     2.341109, Lr: 0.000047, Tokens per sec:   2682
2023-03-09 05:07:46,623 - INFO - __main__ - Epoch  76, Step:  163800, Batch Loss:     2.357721, Lr: 0.000047, Tokens per sec:   2676
2023-03-09 05:08:06,604 - INFO - __main__ - Epoch  76, Step:  163900, Batch Loss:     2.685506, Lr: 0.000047, Tokens per sec:   2689
2023-03-09 05:08:26,596 - INFO - __main__ - Epoch  76, Step:  164000, Batch Loss:     2.120951, Lr: 0.000047, Tokens per sec:   2648
2023-03-09 05:08:46,526 - INFO - __main__ - Epoch  76, Step:  164100, Batch Loss:     3.032729, Lr: 0.000047, Tokens per sec:   2614
2023-03-09 05:09:06,442 - INFO - __main__ - Epoch  76, Step:  164200, Batch Loss:     2.063700, Lr: 0.000047, Tokens per sec:   2669
2023-03-09 05:09:26,419 - INFO - __main__ - Epoch  76, Step:  164300, Batch Loss:     2.658535, Lr: 0.000047, Tokens per sec:   2728
2023-03-09 05:09:46,408 - INFO - __main__ - Epoch  76, Step:  164400, Batch Loss:     2.030247, Lr: 0.000047, Tokens per sec:   2698
2023-03-09 05:10:06,406 - INFO - __main__ - Epoch  76, Step:  164500, Batch Loss:     2.946343, Lr: 0.000047, Tokens per sec:   2684
2023-03-09 05:10:26,376 - INFO - __main__ - Epoch  76, Step:  164600, Batch Loss:     2.392468, Lr: 0.000047, Tokens per sec:   2746
2023-03-09 05:10:46,368 - INFO - __main__ - Epoch  76, Step:  164700, Batch Loss:     2.618748, Lr: 0.000047, Tokens per sec:   2679
2023-03-09 05:11:06,343 - INFO - __main__ - Epoch  76, Step:  164800, Batch Loss:     3.019056, Lr: 0.000047, Tokens per sec:   2673
2023-03-09 05:11:25,033 - INFO - __main__ - Epoch  76, Step:  164900, Batch Loss:     3.066911, Lr: 0.000047, Tokens per sec:   2897
2023-03-09 05:11:44,627 - INFO - __main__ - Epoch  76, Step:  165000, Batch Loss:     1.897072, Lr: 0.000047, Tokens per sec:   2733
2023-03-09 05:12:04,625 - INFO - __main__ - Epoch  76, Step:  165100, Batch Loss:     2.754894, Lr: 0.000047, Tokens per sec:   2742
2023-03-09 05:12:24,558 - INFO - __main__ - Epoch  76, Step:  165200, Batch Loss:     1.581751, Lr: 0.000047, Tokens per sec:   2729
2023-03-09 05:12:44,514 - INFO - __main__ - Epoch  76, Step:  165300, Batch Loss:     1.705348, Lr: 0.000047, Tokens per sec:   2757
2023-03-09 05:13:04,514 - INFO - __main__ - Epoch  76, Step:  165400, Batch Loss:     1.838228, Lr: 0.000047, Tokens per sec:   2681
2023-03-09 05:13:24,438 - INFO - __main__ - Epoch  76, Step:  165500, Batch Loss:     2.645998, Lr: 0.000047, Tokens per sec:   2670
2023-03-09 05:13:44,417 - INFO - __main__ - Epoch  76, Step:  165600, Batch Loss:     1.999286, Lr: 0.000047, Tokens per sec:   2715
2023-03-09 05:13:45,280 - INFO - __main__ - Epoch  76: total training loss 5325.94
2023-03-09 05:13:45,281 - INFO - __main__ - Epoch 77
2023-03-09 05:14:02,793 - INFO - __main__ - Epoch  77, Step:  165700, Batch Loss:     3.149564, Lr: 0.000047, Tokens per sec:   2992
2023-03-09 05:14:22,362 - INFO - __main__ - Epoch  77, Step:  165800, Batch Loss:     1.716876, Lr: 0.000047, Tokens per sec:   2799
2023-03-09 05:14:42,321 - INFO - __main__ - Epoch  77, Step:  165900, Batch Loss:     2.825166, Lr: 0.000047, Tokens per sec:   2671
2023-03-09 05:15:02,250 - INFO - __main__ - Epoch  77, Step:  166000, Batch Loss:     3.550958, Lr: 0.000047, Tokens per sec:   2691
2023-03-09 05:15:22,206 - INFO - __main__ - Epoch  77, Step:  166100, Batch Loss:     1.948673, Lr: 0.000047, Tokens per sec:   2680
2023-03-09 05:15:42,184 - INFO - __main__ - Epoch  77, Step:  166200, Batch Loss:     2.992852, Lr: 0.000047, Tokens per sec:   2689
2023-03-09 05:16:02,116 - INFO - __main__ - Epoch  77, Step:  166300, Batch Loss:     2.752746, Lr: 0.000047, Tokens per sec:   2730
2023-03-09 05:16:22,070 - INFO - __main__ - Epoch  77, Step:  166400, Batch Loss:     2.625938, Lr: 0.000047, Tokens per sec:   2739
2023-03-09 05:16:42,039 - INFO - __main__ - Epoch  77, Step:  166500, Batch Loss:     3.023902, Lr: 0.000047, Tokens per sec:   2701
2023-03-09 05:16:59,997 - INFO - __main__ - Epoch  77, Step:  166600, Batch Loss:     2.552818, Lr: 0.000047, Tokens per sec:   2963
2023-03-09 05:17:17,954 - INFO - __main__ - Epoch  77, Step:  166700, Batch Loss:     3.270393, Lr: 0.000047, Tokens per sec:   2885
2023-03-09 05:17:35,894 - INFO - __main__ - Epoch  77, Step:  166800, Batch Loss:     3.263252, Lr: 0.000047, Tokens per sec:   2973
2023-03-09 05:17:53,842 - INFO - __main__ - Epoch  77, Step:  166900, Batch Loss:     2.301514, Lr: 0.000047, Tokens per sec:   3017
2023-03-09 05:18:11,833 - INFO - __main__ - Epoch  77, Step:  167000, Batch Loss:     2.210060, Lr: 0.000047, Tokens per sec:   2983
2023-03-09 05:18:29,809 - INFO - __main__ - Epoch  77, Step:  167100, Batch Loss:     2.036807, Lr: 0.000047, Tokens per sec:   3006
2023-03-09 05:18:47,777 - INFO - __main__ - Epoch  77, Step:  167200, Batch Loss:     3.039434, Lr: 0.000047, Tokens per sec:   3039
2023-03-09 05:19:05,796 - INFO - __main__ - Epoch  77, Step:  167300, Batch Loss:     2.763780, Lr: 0.000047, Tokens per sec:   2931
2023-03-09 05:19:24,608 - INFO - __main__ - Epoch  77, Step:  167400, Batch Loss:     2.427306, Lr: 0.000047, Tokens per sec:   2859
2023-03-09 05:19:44,606 - INFO - __main__ - Epoch  77, Step:  167500, Batch Loss:     1.897804, Lr: 0.000047, Tokens per sec:   2678
2023-03-09 05:20:04,603 - INFO - __main__ - Epoch  77, Step:  167600, Batch Loss:     2.962337, Lr: 0.000047, Tokens per sec:   2722
2023-03-09 05:20:24,571 - INFO - __main__ - Epoch  77, Step:  167700, Batch Loss:     2.432040, Lr: 0.000047, Tokens per sec:   2731
2023-03-09 05:20:41,221 - INFO - __main__ - Epoch  77: total training loss 5191.51
2023-03-09 05:20:41,222 - INFO - __main__ - Epoch 78
2023-03-09 05:20:44,926 - INFO - __main__ - Epoch  78, Step:  167800, Batch Loss:     2.154092, Lr: 0.000046, Tokens per sec:   2530
2023-03-09 05:21:04,861 - INFO - __main__ - Epoch  78, Step:  167900, Batch Loss:     1.535411, Lr: 0.000046, Tokens per sec:   2713
2023-03-09 05:21:24,835 - INFO - __main__ - Epoch  78, Step:  168000, Batch Loss:     1.543257, Lr: 0.000046, Tokens per sec:   2680
2023-03-09 05:21:44,787 - INFO - __main__ - Epoch  78, Step:  168100, Batch Loss:     2.201792, Lr: 0.000046, Tokens per sec:   2699
2023-03-09 05:22:04,719 - INFO - __main__ - Epoch  78, Step:  168200, Batch Loss:     2.110380, Lr: 0.000046, Tokens per sec:   2706
2023-03-09 05:22:24,706 - INFO - __main__ - Epoch  78, Step:  168300, Batch Loss:     2.675832, Lr: 0.000046, Tokens per sec:   2683
2023-03-09 05:22:44,679 - INFO - __main__ - Epoch  78, Step:  168400, Batch Loss:     1.929375, Lr: 0.000046, Tokens per sec:   2721
2023-03-09 05:23:04,664 - INFO - __main__ - Epoch  78, Step:  168500, Batch Loss:     1.975665, Lr: 0.000046, Tokens per sec:   2720
2023-03-09 05:23:24,616 - INFO - __main__ - Epoch  78, Step:  168600, Batch Loss:     2.897614, Lr: 0.000046, Tokens per sec:   2637
2023-03-09 05:23:44,570 - INFO - __main__ - Epoch  78, Step:  168700, Batch Loss:     1.623423, Lr: 0.000046, Tokens per sec:   2746
2023-03-09 05:24:04,528 - INFO - __main__ - Epoch  78, Step:  168800, Batch Loss:     2.462870, Lr: 0.000046, Tokens per sec:   2718
2023-03-09 05:24:24,397 - INFO - __main__ - Epoch  78, Step:  168900, Batch Loss:     2.270350, Lr: 0.000046, Tokens per sec:   2734
2023-03-09 05:24:44,391 - INFO - __main__ - Epoch  78, Step:  169000, Batch Loss:     1.764221, Lr: 0.000046, Tokens per sec:   2722
2023-03-09 05:25:04,345 - INFO - __main__ - Epoch  78, Step:  169100, Batch Loss:     2.603349, Lr: 0.000046, Tokens per sec:   2663
2023-03-09 05:25:24,293 - INFO - __main__ - Epoch  78, Step:  169200, Batch Loss:     2.016325, Lr: 0.000046, Tokens per sec:   2711
2023-03-09 05:25:44,214 - INFO - __main__ - Epoch  78, Step:  169300, Batch Loss:     1.347541, Lr: 0.000046, Tokens per sec:   2627
2023-03-09 05:26:04,201 - INFO - __main__ - Epoch  78, Step:  169400, Batch Loss:     3.076269, Lr: 0.000046, Tokens per sec:   2714
2023-03-09 05:26:24,196 - INFO - __main__ - Epoch  78, Step:  169500, Batch Loss:     2.423278, Lr: 0.000046, Tokens per sec:   2692
2023-03-09 05:26:44,080 - INFO - __main__ - Epoch  78, Step:  169600, Batch Loss:     1.825183, Lr: 0.000046, Tokens per sec:   2693
2023-03-09 05:27:04,023 - INFO - __main__ - Epoch  78, Step:  169700, Batch Loss:     1.671431, Lr: 0.000046, Tokens per sec:   2709
2023-03-09 05:27:23,982 - INFO - __main__ - Epoch  78, Step:  169800, Batch Loss:     2.871595, Lr: 0.000046, Tokens per sec:   2693
2023-03-09 05:27:43,961 - INFO - __main__ - Epoch  78, Step:  169900, Batch Loss:     2.521742, Lr: 0.000046, Tokens per sec:   2695
2023-03-09 05:27:56,419 - INFO - __main__ - Epoch  78: total training loss 5097.76
2023-03-09 05:27:56,420 - INFO - __main__ - Epoch 79
2023-03-09 05:28:04,285 - INFO - __main__ - Epoch  79, Step:  170000, Batch Loss:     1.980133, Lr: 0.000046, Tokens per sec:   2611
2023-03-09 05:28:24,220 - INFO - __main__ - Epoch  79, Step:  170100, Batch Loss:     2.800169, Lr: 0.000046, Tokens per sec:   2713
2023-03-09 05:28:44,196 - INFO - __main__ - Epoch  79, Step:  170200, Batch Loss:     2.076873, Lr: 0.000046, Tokens per sec:   2734
2023-03-09 05:29:03,595 - INFO - __main__ - Epoch  79, Step:  170300, Batch Loss:     2.784437, Lr: 0.000046, Tokens per sec:   2758
2023-03-09 05:29:21,504 - INFO - __main__ - Epoch  79, Step:  170400, Batch Loss:     1.540530, Lr: 0.000046, Tokens per sec:   2970
2023-03-09 05:29:39,475 - INFO - __main__ - Epoch  79, Step:  170500, Batch Loss:     2.041454, Lr: 0.000046, Tokens per sec:   2963
2023-03-09 05:29:57,381 - INFO - __main__ - Epoch  79, Step:  170600, Batch Loss:     2.082734, Lr: 0.000046, Tokens per sec:   3023
2023-03-09 05:30:16,903 - INFO - __main__ - Epoch  79, Step:  170700, Batch Loss:     2.461599, Lr: 0.000046, Tokens per sec:   2771
2023-03-09 05:30:36,881 - INFO - __main__ - Epoch  79, Step:  170800, Batch Loss:     3.346269, Lr: 0.000046, Tokens per sec:   2744
2023-03-09 05:30:56,091 - INFO - __main__ - Epoch  79, Step:  170900, Batch Loss:     2.526861, Lr: 0.000046, Tokens per sec:   2786
2023-03-09 05:31:15,844 - INFO - __main__ - Epoch  79, Step:  171000, Batch Loss:     2.019297, Lr: 0.000046, Tokens per sec:   2720
2023-03-09 05:31:35,826 - INFO - __main__ - Epoch  79, Step:  171100, Batch Loss:     2.743978, Lr: 0.000046, Tokens per sec:   2721
2023-03-09 05:31:55,793 - INFO - __main__ - Epoch  79, Step:  171200, Batch Loss:     2.908116, Lr: 0.000046, Tokens per sec:   2713
2023-03-09 05:32:15,770 - INFO - __main__ - Epoch  79, Step:  171300, Batch Loss:     2.218771, Lr: 0.000046, Tokens per sec:   2675
2023-03-09 05:32:35,762 - INFO - __main__ - Epoch  79, Step:  171400, Batch Loss:     2.189960, Lr: 0.000046, Tokens per sec:   2688
2023-03-09 05:32:55,712 - INFO - __main__ - Epoch  79, Step:  171500, Batch Loss:     2.190392, Lr: 0.000046, Tokens per sec:   2657
2023-03-09 05:33:15,664 - INFO - __main__ - Epoch  79, Step:  171600, Batch Loss:     2.074559, Lr: 0.000046, Tokens per sec:   2693
2023-03-09 05:33:35,639 - INFO - __main__ - Epoch  79, Step:  171700, Batch Loss:     2.591460, Lr: 0.000046, Tokens per sec:   2674
2023-03-09 05:33:55,576 - INFO - __main__ - Epoch  79, Step:  171800, Batch Loss:     2.739893, Lr: 0.000046, Tokens per sec:   2689
2023-03-09 05:34:13,487 - INFO - __main__ - Epoch  79, Step:  171900, Batch Loss:     2.509037, Lr: 0.000046, Tokens per sec:   3006
2023-03-09 05:34:31,395 - INFO - __main__ - Epoch  79, Step:  172000, Batch Loss:     1.700075, Lr: 0.000046, Tokens per sec:   3041
2023-03-09 05:34:49,359 - INFO - __main__ - Epoch  79, Step:  172100, Batch Loss:     2.827280, Lr: 0.000046, Tokens per sec:   2990
2023-03-09 05:34:56,752 - INFO - __main__ - Epoch  79: total training loss 5016.36
2023-03-09 05:34:56,753 - INFO - __main__ - Epoch 80
2023-03-09 05:35:08,832 - INFO - __main__ - Epoch  80, Step:  172200, Batch Loss:     2.723952, Lr: 0.000045, Tokens per sec:   2606
2023-03-09 05:35:28,805 - INFO - __main__ - Epoch  80, Step:  172300, Batch Loss:     1.821895, Lr: 0.000045, Tokens per sec:   2701
2023-03-09 05:35:48,746 - INFO - __main__ - Epoch  80, Step:  172400, Batch Loss:     2.293308, Lr: 0.000045, Tokens per sec:   2665
2023-03-09 05:36:08,711 - INFO - __main__ - Epoch  80, Step:  172500, Batch Loss:     2.822765, Lr: 0.000045, Tokens per sec:   2676
2023-03-09 05:36:28,704 - INFO - __main__ - Epoch  80, Step:  172600, Batch Loss:     1.722284, Lr: 0.000045, Tokens per sec:   2703
2023-03-09 05:36:48,683 - INFO - __main__ - Epoch  80, Step:  172700, Batch Loss:     2.994076, Lr: 0.000045, Tokens per sec:   2705
2023-03-09 05:37:08,671 - INFO - __main__ - Epoch  80, Step:  172800, Batch Loss:     1.710464, Lr: 0.000045, Tokens per sec:   2692
2023-03-09 05:37:28,611 - INFO - __main__ - Epoch  80, Step:  172900, Batch Loss:     2.453443, Lr: 0.000045, Tokens per sec:   2638
2023-03-09 05:37:48,581 - INFO - __main__ - Epoch  80, Step:  173000, Batch Loss:     2.488004, Lr: 0.000045, Tokens per sec:   2738
2023-03-09 05:38:08,500 - INFO - __main__ - Epoch  80, Step:  173100, Batch Loss:     1.463225, Lr: 0.000045, Tokens per sec:   2716
2023-03-09 05:38:27,584 - INFO - __main__ - Epoch  80, Step:  173200, Batch Loss:     1.166099, Lr: 0.000045, Tokens per sec:   2863
2023-03-09 05:38:45,561 - INFO - __main__ - Epoch  80, Step:  173300, Batch Loss:     2.723387, Lr: 0.000045, Tokens per sec:   2972
2023-03-09 05:39:04,317 - INFO - __main__ - Epoch  80, Step:  173400, Batch Loss:     2.013117, Lr: 0.000045, Tokens per sec:   2855
2023-03-09 05:39:24,290 - INFO - __main__ - Epoch  80, Step:  173500, Batch Loss:     2.489286, Lr: 0.000045, Tokens per sec:   2722
2023-03-09 05:39:44,261 - INFO - __main__ - Epoch  80, Step:  173600, Batch Loss:     2.557026, Lr: 0.000045, Tokens per sec:   2678
2023-03-09 05:40:04,223 - INFO - __main__ - Epoch  80, Step:  173700, Batch Loss:     2.301135, Lr: 0.000045, Tokens per sec:   2674
2023-03-09 05:40:24,152 - INFO - __main__ - Epoch  80, Step:  173800, Batch Loss:     2.069306, Lr: 0.000045, Tokens per sec:   2732
2023-03-09 05:40:44,123 - INFO - __main__ - Epoch  80, Step:  173900, Batch Loss:     2.132119, Lr: 0.000045, Tokens per sec:   2673
2023-03-09 05:41:04,059 - INFO - __main__ - Epoch  80, Step:  174000, Batch Loss:     2.367194, Lr: 0.000045, Tokens per sec:   2768
2023-03-09 05:41:24,032 - INFO - __main__ - Epoch  80, Step:  174100, Batch Loss:     3.437887, Lr: 0.000045, Tokens per sec:   2720
2023-03-09 05:41:44,009 - INFO - __main__ - Epoch  80, Step:  174200, Batch Loss:     3.555702, Lr: 0.000045, Tokens per sec:   2711
2023-03-09 05:42:03,595 - INFO - __main__ - Epoch  80, Step:  174300, Batch Loss:     2.283891, Lr: 0.000045, Tokens per sec:   2705
2023-03-09 05:42:07,242 - INFO - __main__ - Epoch  80: total training loss 4903.62
2023-03-09 05:42:07,243 - INFO - __main__ - Epoch 81
2023-03-09 05:42:22,813 - INFO - __main__ - Epoch  81, Step:  174400, Batch Loss:     3.274942, Lr: 0.000045, Tokens per sec:   2772
2023-03-09 05:42:42,811 - INFO - __main__ - Epoch  81, Step:  174500, Batch Loss:     2.902963, Lr: 0.000045, Tokens per sec:   2712
2023-03-09 05:43:02,760 - INFO - __main__ - Epoch  81, Step:  174600, Batch Loss:     2.262820, Lr: 0.000045, Tokens per sec:   2684
2023-03-09 05:43:22,733 - INFO - __main__ - Epoch  81, Step:  174700, Batch Loss:     2.710230, Lr: 0.000045, Tokens per sec:   2700
2023-03-09 05:43:42,676 - INFO - __main__ - Epoch  81, Step:  174800, Batch Loss:     2.898859, Lr: 0.000045, Tokens per sec:   2646
2023-03-09 05:44:02,626 - INFO - __main__ - Epoch  81, Step:  174900, Batch Loss:     2.416527, Lr: 0.000045, Tokens per sec:   2687
2023-03-09 05:44:22,594 - INFO - __main__ - Epoch  81, Step:  175000, Batch Loss:     2.331259, Lr: 0.000045, Tokens per sec:   2701
2023-03-09 05:44:42,551 - INFO - __main__ - Epoch  81, Step:  175100, Batch Loss:     1.523003, Lr: 0.000045, Tokens per sec:   2683
2023-03-09 05:45:02,484 - INFO - __main__ - Epoch  81, Step:  175200, Batch Loss:     2.747146, Lr: 0.000045, Tokens per sec:   2637
2023-03-09 05:45:22,437 - INFO - __main__ - Epoch  81, Step:  175300, Batch Loss:     2.050869, Lr: 0.000045, Tokens per sec:   2722
2023-03-09 05:45:42,372 - INFO - __main__ - Epoch  81, Step:  175400, Batch Loss:     2.147457, Lr: 0.000045, Tokens per sec:   2722
2023-03-09 05:46:02,308 - INFO - __main__ - Epoch  81, Step:  175500, Batch Loss:     1.918594, Lr: 0.000045, Tokens per sec:   2722
2023-03-09 05:46:20,316 - INFO - __main__ - Epoch  81, Step:  175600, Batch Loss:     2.809272, Lr: 0.000045, Tokens per sec:   3061
2023-03-09 05:46:38,712 - INFO - __main__ - Epoch  81, Step:  175700, Batch Loss:     2.173151, Lr: 0.000045, Tokens per sec:   2913
2023-03-09 05:46:58,642 - INFO - __main__ - Epoch  81, Step:  175800, Batch Loss:     2.080928, Lr: 0.000045, Tokens per sec:   2702
2023-03-09 05:47:18,607 - INFO - __main__ - Epoch  81, Step:  175900, Batch Loss:     3.663499, Lr: 0.000045, Tokens per sec:   2714
2023-03-09 05:47:38,571 - INFO - __main__ - Epoch  81, Step:  176000, Batch Loss:     1.932402, Lr: 0.000045, Tokens per sec:   2709
2023-03-09 05:47:57,989 - INFO - __main__ - Epoch  81, Step:  176100, Batch Loss:     2.805176, Lr: 0.000045, Tokens per sec:   2750
2023-03-09 05:48:16,602 - INFO - __main__ - Epoch  81, Step:  176200, Batch Loss:     2.404763, Lr: 0.000045, Tokens per sec:   2895
2023-03-09 05:48:36,555 - INFO - __main__ - Epoch  81, Step:  176300, Batch Loss:     1.705840, Lr: 0.000045, Tokens per sec:   2693
2023-03-09 05:48:56,505 - INFO - __main__ - Epoch  81, Step:  176400, Batch Loss:     1.995147, Lr: 0.000045, Tokens per sec:   2735
2023-03-09 05:49:16,341 - INFO - __main__ - Epoch  81: total training loss 4802.78
2023-03-09 05:49:16,342 - INFO - __main__ - Epoch 82
2023-03-09 05:49:16,843 - INFO - __main__ - Epoch  82, Step:  176500, Batch Loss:     1.744735, Lr: 0.000044, Tokens per sec:   1202
2023-03-09 05:49:36,794 - INFO - __main__ - Epoch  82, Step:  176600, Batch Loss:     1.150805, Lr: 0.000044, Tokens per sec:   2644
2023-03-09 05:49:56,745 - INFO - __main__ - Epoch  82, Step:  176700, Batch Loss:     1.931082, Lr: 0.000044, Tokens per sec:   2695
2023-03-09 05:50:16,679 - INFO - __main__ - Epoch  82, Step:  176800, Batch Loss:     2.652474, Lr: 0.000044, Tokens per sec:   2710
2023-03-09 05:50:36,634 - INFO - __main__ - Epoch  82, Step:  176900, Batch Loss:     1.434404, Lr: 0.000044, Tokens per sec:   2670
2023-03-09 05:50:56,575 - INFO - __main__ - Epoch  82, Step:  177000, Batch Loss:     2.182213, Lr: 0.000044, Tokens per sec:   2659
2023-03-09 05:51:16,529 - INFO - __main__ - Epoch  82, Step:  177100, Batch Loss:     2.809705, Lr: 0.000044, Tokens per sec:   2672
2023-03-09 05:51:36,515 - INFO - __main__ - Epoch  82, Step:  177200, Batch Loss:     2.271988, Lr: 0.000044, Tokens per sec:   2753
2023-03-09 05:51:56,448 - INFO - __main__ - Epoch  82, Step:  177300, Batch Loss:     1.683274, Lr: 0.000044, Tokens per sec:   2700
2023-03-09 05:52:16,407 - INFO - __main__ - Epoch  82, Step:  177400, Batch Loss:     2.833233, Lr: 0.000044, Tokens per sec:   2735
2023-03-09 05:52:36,339 - INFO - __main__ - Epoch  82, Step:  177500, Batch Loss:     2.487077, Lr: 0.000044, Tokens per sec:   2695
2023-03-09 05:52:56,287 - INFO - __main__ - Epoch  82, Step:  177600, Batch Loss:     2.140638, Lr: 0.000044, Tokens per sec:   2724
2023-03-09 05:53:16,264 - INFO - __main__ - Epoch  82, Step:  177700, Batch Loss:     2.119320, Lr: 0.000044, Tokens per sec:   2695
2023-03-09 05:53:36,202 - INFO - __main__ - Epoch  82, Step:  177800, Batch Loss:     1.876387, Lr: 0.000044, Tokens per sec:   2700
2023-03-09 05:53:56,112 - INFO - __main__ - Epoch  82, Step:  177900, Batch Loss:     2.260548, Lr: 0.000044, Tokens per sec:   2683
2023-03-09 05:54:16,089 - INFO - __main__ - Epoch  82, Step:  178000, Batch Loss:     2.103700, Lr: 0.000044, Tokens per sec:   2701
2023-03-09 05:54:36,058 - INFO - __main__ - Epoch  82, Step:  178100, Batch Loss:     1.730999, Lr: 0.000044, Tokens per sec:   2718
2023-03-09 05:54:56,051 - INFO - __main__ - Epoch  82, Step:  178200, Batch Loss:     1.949679, Lr: 0.000044, Tokens per sec:   2721
2023-03-09 05:55:15,964 - INFO - __main__ - Epoch  82, Step:  178300, Batch Loss:     1.386623, Lr: 0.000044, Tokens per sec:   2711
2023-03-09 05:55:35,926 - INFO - __main__ - Epoch  82, Step:  178400, Batch Loss:     2.059433, Lr: 0.000044, Tokens per sec:   2704
2023-03-09 05:55:55,854 - INFO - __main__ - Epoch  82, Step:  178500, Batch Loss:     2.443711, Lr: 0.000044, Tokens per sec:   2645
2023-03-09 05:56:15,789 - INFO - __main__ - Epoch  82, Step:  178600, Batch Loss:     1.972679, Lr: 0.000044, Tokens per sec:   2742
2023-03-09 05:56:31,439 - INFO - __main__ - Epoch  82: total training loss 4717.12
2023-03-09 05:56:31,441 - INFO - __main__ - Epoch 83
2023-03-09 05:56:36,119 - INFO - __main__ - Epoch  83, Step:  178700, Batch Loss:     1.528848, Lr: 0.000044, Tokens per sec:   2622
2023-03-09 05:56:56,073 - INFO - __main__ - Epoch  83, Step:  178800, Batch Loss:     3.127224, Lr: 0.000044, Tokens per sec:   2683
2023-03-09 05:57:15,987 - INFO - __main__ - Epoch  83, Step:  178900, Batch Loss:     2.200691, Lr: 0.000044, Tokens per sec:   2734
2023-03-09 05:57:36,003 - INFO - __main__ - Epoch  83, Step:  179000, Batch Loss:     2.459751, Lr: 0.000044, Tokens per sec:   2667
2023-03-09 05:57:56,011 - INFO - __main__ - Epoch  83, Step:  179100, Batch Loss:     3.177222, Lr: 0.000044, Tokens per sec:   2718
2023-03-09 05:58:15,992 - INFO - __main__ - Epoch  83, Step:  179200, Batch Loss:     1.869313, Lr: 0.000044, Tokens per sec:   2698
2023-03-09 05:58:35,964 - INFO - __main__ - Epoch  83, Step:  179300, Batch Loss:     1.471200, Lr: 0.000044, Tokens per sec:   2676
2023-03-09 05:58:55,937 - INFO - __main__ - Epoch  83, Step:  179400, Batch Loss:     2.415344, Lr: 0.000044, Tokens per sec:   2673
2023-03-09 05:59:15,202 - INFO - __main__ - Epoch  83, Step:  179500, Batch Loss:     1.935156, Lr: 0.000044, Tokens per sec:   2731
2023-03-09 05:59:33,017 - INFO - __main__ - Epoch  83, Step:  179600, Batch Loss:     1.334550, Lr: 0.000044, Tokens per sec:   3005
2023-03-09 05:59:50,935 - INFO - __main__ - Epoch  83, Step:  179700, Batch Loss:     1.299047, Lr: 0.000044, Tokens per sec:   3005
2023-03-09 06:00:09,792 - INFO - __main__ - Epoch  83, Step:  179800, Batch Loss:     2.564204, Lr: 0.000044, Tokens per sec:   2884
2023-03-09 06:00:29,766 - INFO - __main__ - Epoch  83, Step:  179900, Batch Loss:     1.814372, Lr: 0.000044, Tokens per sec:   2694
2023-03-09 06:00:49,761 - INFO - __main__ - Epoch  83, Step:  180000, Batch Loss:     2.585561, Lr: 0.000044, Tokens per sec:   2653
2023-03-09 06:01:09,711 - INFO - __main__ - Epoch  83, Step:  180100, Batch Loss:     1.897623, Lr: 0.000044, Tokens per sec:   2721
2023-03-09 06:01:29,668 - INFO - __main__ - Epoch  83, Step:  180200, Batch Loss:     1.844748, Lr: 0.000044, Tokens per sec:   2700
2023-03-09 06:01:49,614 - INFO - __main__ - Epoch  83, Step:  180300, Batch Loss:     2.077616, Lr: 0.000044, Tokens per sec:   2710
2023-03-09 06:02:07,591 - INFO - __main__ - Epoch  83, Step:  180400, Batch Loss:     2.762311, Lr: 0.000044, Tokens per sec:   2975
2023-03-09 06:02:25,555 - INFO - __main__ - Epoch  83, Step:  180500, Batch Loss:     2.138690, Lr: 0.000044, Tokens per sec:   3057
2023-03-09 06:02:43,522 - INFO - __main__ - Epoch  83, Step:  180600, Batch Loss:     2.356073, Lr: 0.000044, Tokens per sec:   3012
2023-03-09 06:03:02,041 - INFO - __main__ - Epoch  83, Step:  180700, Batch Loss:     1.191710, Lr: 0.000044, Tokens per sec:   2892
2023-03-09 06:03:22,033 - INFO - __main__ - Epoch  83, Step:  180800, Batch Loss:     2.206604, Lr: 0.000044, Tokens per sec:   2696
2023-03-09 06:03:33,516 - INFO - __main__ - Epoch  83: total training loss 4649.94
2023-03-09 06:03:33,517 - INFO - __main__ - Epoch 84
2023-03-09 06:03:42,402 - INFO - __main__ - Epoch  84, Step:  180900, Batch Loss:     1.464790, Lr: 0.000043, Tokens per sec:   2622
2023-03-09 06:04:02,315 - INFO - __main__ - Epoch  84, Step:  181000, Batch Loss:     2.203565, Lr: 0.000043, Tokens per sec:   2702
2023-03-09 06:04:22,265 - INFO - __main__ - Epoch  84, Step:  181100, Batch Loss:     1.751217, Lr: 0.000043, Tokens per sec:   2692
2023-03-09 06:04:42,224 - INFO - __main__ - Epoch  84, Step:  181200, Batch Loss:     1.960701, Lr: 0.000043, Tokens per sec:   2688
2023-03-09 06:05:02,217 - INFO - __main__ - Epoch  84, Step:  181300, Batch Loss:     2.116211, Lr: 0.000043, Tokens per sec:   2715
2023-03-09 06:05:22,173 - INFO - __main__ - Epoch  84, Step:  181400, Batch Loss:     1.901197, Lr: 0.000043, Tokens per sec:   2668
2023-03-09 06:05:42,144 - INFO - __main__ - Epoch  84, Step:  181500, Batch Loss:     1.502403, Lr: 0.000043, Tokens per sec:   2721
2023-03-09 06:06:02,059 - INFO - __main__ - Epoch  84, Step:  181600, Batch Loss:     3.057348, Lr: 0.000043, Tokens per sec:   2693
2023-03-09 06:06:20,511 - INFO - __main__ - Epoch  84, Step:  181700, Batch Loss:     2.411762, Lr: 0.000043, Tokens per sec:   2945
2023-03-09 06:06:40,326 - INFO - __main__ - Epoch  84, Step:  181800, Batch Loss:     1.455982, Lr: 0.000043, Tokens per sec:   2699
2023-03-09 06:07:00,284 - INFO - __main__ - Epoch  84, Step:  181900, Batch Loss:     1.446185, Lr: 0.000043, Tokens per sec:   2692
2023-03-09 06:07:20,253 - INFO - __main__ - Epoch  84, Step:  182000, Batch Loss:     2.431690, Lr: 0.000043, Tokens per sec:   2727
2023-03-09 06:07:38,600 - INFO - __main__ - Epoch  84, Step:  182100, Batch Loss:     2.819980, Lr: 0.000043, Tokens per sec:   2949
2023-03-09 06:07:56,542 - INFO - __main__ - Epoch  84, Step:  182200, Batch Loss:     2.645296, Lr: 0.000043, Tokens per sec:   3002
2023-03-09 06:08:14,518 - INFO - __main__ - Epoch  84, Step:  182300, Batch Loss:     2.558523, Lr: 0.000043, Tokens per sec:   3009
2023-03-09 06:08:32,459 - INFO - __main__ - Epoch  84, Step:  182400, Batch Loss:     2.594678, Lr: 0.000043, Tokens per sec:   3022
2023-03-09 06:08:52,066 - INFO - __main__ - Epoch  84, Step:  182500, Batch Loss:     3.101842, Lr: 0.000043, Tokens per sec:   2754
2023-03-09 06:09:12,037 - INFO - __main__ - Epoch  84, Step:  182600, Batch Loss:     1.410781, Lr: 0.000043, Tokens per sec:   2670
2023-03-09 06:09:32,014 - INFO - __main__ - Epoch  84, Step:  182700, Batch Loss:     3.117447, Lr: 0.000043, Tokens per sec:   2649
2023-03-09 06:09:51,928 - INFO - __main__ - Epoch  84, Step:  182800, Batch Loss:     2.182406, Lr: 0.000043, Tokens per sec:   2642
2023-03-09 06:10:11,803 - INFO - __main__ - Epoch  84, Step:  182900, Batch Loss:     2.108198, Lr: 0.000043, Tokens per sec:   2682
2023-03-09 06:10:31,754 - INFO - __main__ - Epoch  84, Step:  183000, Batch Loss:     1.678229, Lr: 0.000043, Tokens per sec:   2733
2023-03-09 06:10:38,992 - INFO - __main__ - Epoch  84: total training loss 4518.36
2023-03-09 06:10:38,993 - INFO - __main__ - Epoch 85
2023-03-09 06:10:52,030 - INFO - __main__ - Epoch  85, Step:  183100, Batch Loss:     2.746556, Lr: 0.000043, Tokens per sec:   2618
2023-03-09 06:11:12,040 - INFO - __main__ - Epoch  85, Step:  183200, Batch Loss:     1.874772, Lr: 0.000043, Tokens per sec:   2683
2023-03-09 06:11:32,033 - INFO - __main__ - Epoch  85, Step:  183300, Batch Loss:     1.903899, Lr: 0.000043, Tokens per sec:   2714
2023-03-09 06:11:51,985 - INFO - __main__ - Epoch  85, Step:  183400, Batch Loss:     1.743157, Lr: 0.000043, Tokens per sec:   2674
2023-03-09 06:12:11,928 - INFO - __main__ - Epoch  85, Step:  183500, Batch Loss:     2.120909, Lr: 0.000043, Tokens per sec:   2708
2023-03-09 06:12:31,837 - INFO - __main__ - Epoch  85, Step:  183600, Batch Loss:     2.858200, Lr: 0.000043, Tokens per sec:   2732
2023-03-09 06:12:51,806 - INFO - __main__ - Epoch  85, Step:  183700, Batch Loss:     2.120155, Lr: 0.000043, Tokens per sec:   2696
2023-03-09 06:13:11,744 - INFO - __main__ - Epoch  85, Step:  183800, Batch Loss:     2.274724, Lr: 0.000043, Tokens per sec:   2695
2023-03-09 06:13:31,694 - INFO - __main__ - Epoch  85, Step:  183900, Batch Loss:     2.286528, Lr: 0.000043, Tokens per sec:   2678
2023-03-09 06:13:51,653 - INFO - __main__ - Epoch  85, Step:  184000, Batch Loss:     2.201555, Lr: 0.000043, Tokens per sec:   2748
2023-03-09 06:14:11,622 - INFO - __main__ - Epoch  85, Step:  184100, Batch Loss:     2.683418, Lr: 0.000043, Tokens per sec:   2738
2023-03-09 06:14:31,527 - INFO - __main__ - Epoch  85, Step:  184200, Batch Loss:     1.949668, Lr: 0.000043, Tokens per sec:   2707
2023-03-09 06:14:49,474 - INFO - __main__ - Epoch  85, Step:  184300, Batch Loss:     2.569427, Lr: 0.000043, Tokens per sec:   2961
2023-03-09 06:15:07,404 - INFO - __main__ - Epoch  85, Step:  184400, Batch Loss:     2.272154, Lr: 0.000043, Tokens per sec:   2961
2023-03-09 06:15:25,324 - INFO - __main__ - Epoch  85, Step:  184500, Batch Loss:     1.733924, Lr: 0.000043, Tokens per sec:   2997
2023-03-09 06:15:43,261 - INFO - __main__ - Epoch  85, Step:  184600, Batch Loss:     1.681442, Lr: 0.000043, Tokens per sec:   2971
2023-03-09 06:16:01,163 - INFO - __main__ - Epoch  85, Step:  184700, Batch Loss:     2.643417, Lr: 0.000043, Tokens per sec:   2973
2023-03-09 06:16:20,844 - INFO - __main__ - Epoch  85, Step:  184800, Batch Loss:     2.539866, Lr: 0.000043, Tokens per sec:   2753
2023-03-09 06:16:39,059 - INFO - __main__ - Epoch  85, Step:  184900, Batch Loss:     2.163189, Lr: 0.000043, Tokens per sec:   2907
2023-03-09 06:16:57,014 - INFO - __main__ - Epoch  85, Step:  185000, Batch Loss:     2.196740, Lr: 0.000043, Tokens per sec:   3048
2023-03-09 06:17:14,983 - INFO - __main__ - Epoch  85, Step:  185100, Batch Loss:     1.930325, Lr: 0.000043, Tokens per sec:   2959
2023-03-09 06:17:34,840 - INFO - __main__ - Epoch  85, Step:  185200, Batch Loss:     1.920738, Lr: 0.000043, Tokens per sec:   2764
2023-03-09 06:17:37,879 - INFO - __main__ - Epoch  85: total training loss 4476.14
2023-03-09 06:17:37,880 - INFO - __main__ - Epoch 86
2023-03-09 06:17:55,151 - INFO - __main__ - Epoch  86, Step:  185300, Batch Loss:     2.388011, Lr: 0.000043, Tokens per sec:   2669
2023-03-09 06:18:15,129 - INFO - __main__ - Epoch  86, Step:  185400, Batch Loss:     1.496806, Lr: 0.000043, Tokens per sec:   2709
2023-03-09 06:18:35,039 - INFO - __main__ - Epoch  86, Step:  185500, Batch Loss:     2.305008, Lr: 0.000043, Tokens per sec:   2676
2023-03-09 06:18:54,994 - INFO - __main__ - Epoch  86, Step:  185600, Batch Loss:     3.241621, Lr: 0.000043, Tokens per sec:   2690
2023-03-09 06:19:14,949 - INFO - __main__ - Epoch  86, Step:  185700, Batch Loss:     1.867827, Lr: 0.000043, Tokens per sec:   2670
2023-03-09 06:19:34,891 - INFO - __main__ - Epoch  86, Step:  185800, Batch Loss:     1.696206, Lr: 0.000043, Tokens per sec:   2719
2023-03-09 06:19:54,851 - INFO - __main__ - Epoch  86, Step:  185900, Batch Loss:     2.260036, Lr: 0.000043, Tokens per sec:   2703
2023-03-09 06:20:13,346 - INFO - __main__ - Epoch  86, Step:  186000, Batch Loss:     1.919932, Lr: 0.000043, Tokens per sec:   2940
2023-03-09 06:20:31,288 - INFO - __main__ - Epoch  86, Step:  186100, Batch Loss:     2.985258, Lr: 0.000043, Tokens per sec:   2959
2023-03-09 06:20:49,251 - INFO - __main__ - Epoch  86, Step:  186200, Batch Loss:     1.833297, Lr: 0.000043, Tokens per sec:   3001
2023-03-09 06:21:07,206 - INFO - __main__ - Epoch  86, Step:  186300, Batch Loss:     2.004107, Lr: 0.000043, Tokens per sec:   2941
2023-03-09 06:21:25,582 - INFO - __main__ - Epoch  86, Step:  186400, Batch Loss:     2.599410, Lr: 0.000043, Tokens per sec:   2960
2023-03-09 06:21:45,577 - INFO - __main__ - Epoch  86, Step:  186500, Batch Loss:     2.418419, Lr: 0.000043, Tokens per sec:   2732
2023-03-09 06:22:05,531 - INFO - __main__ - Epoch  86, Step:  186600, Batch Loss:     2.445777, Lr: 0.000043, Tokens per sec:   2709
2023-03-09 06:22:25,503 - INFO - __main__ - Epoch  86, Step:  186700, Batch Loss:     2.267868, Lr: 0.000043, Tokens per sec:   2729
2023-03-09 06:22:45,476 - INFO - __main__ - Epoch  86, Step:  186800, Batch Loss:     2.314171, Lr: 0.000043, Tokens per sec:   2676
2023-03-09 06:23:05,409 - INFO - __main__ - Epoch  86, Step:  186900, Batch Loss:     2.108722, Lr: 0.000043, Tokens per sec:   2711
2023-03-09 06:23:25,401 - INFO - __main__ - Epoch  86, Step:  187000, Batch Loss:     2.105527, Lr: 0.000043, Tokens per sec:   2702
2023-03-09 06:23:45,379 - INFO - __main__ - Epoch  86, Step:  187100, Batch Loss:     3.077822, Lr: 0.000043, Tokens per sec:   2689
2023-03-09 06:24:05,312 - INFO - __main__ - Epoch  86, Step:  187200, Batch Loss:     3.508750, Lr: 0.000043, Tokens per sec:   2702
2023-03-09 06:24:25,285 - INFO - __main__ - Epoch  86, Step:  187300, Batch Loss:     1.157688, Lr: 0.000043, Tokens per sec:   2684
2023-03-09 06:24:44,094 - INFO - __main__ - Epoch  86: total training loss 4392.90
2023-03-09 06:24:44,095 - INFO - __main__ - Epoch 87
2023-03-09 06:24:45,613 - INFO - __main__ - Epoch  87, Step:  187400, Batch Loss:     1.422189, Lr: 0.000042, Tokens per sec:   1959
2023-03-09 06:25:05,576 - INFO - __main__ - Epoch  87, Step:  187500, Batch Loss:     1.421568, Lr: 0.000042, Tokens per sec:   2691
2023-03-09 06:25:25,505 - INFO - __main__ - Epoch  87, Step:  187600, Batch Loss:     2.118337, Lr: 0.000042, Tokens per sec:   2728
2023-03-09 06:25:45,489 - INFO - __main__ - Epoch  87, Step:  187700, Batch Loss:     1.135631, Lr: 0.000042, Tokens per sec:   2707
2023-03-09 06:26:05,471 - INFO - __main__ - Epoch  87, Step:  187800, Batch Loss:     1.668775, Lr: 0.000042, Tokens per sec:   2730
2023-03-09 06:26:25,445 - INFO - __main__ - Epoch  87, Step:  187900, Batch Loss:     1.809713, Lr: 0.000042, Tokens per sec:   2718
2023-03-09 06:26:45,417 - INFO - __main__ - Epoch  87, Step:  188000, Batch Loss:     2.385262, Lr: 0.000042, Tokens per sec:   2688
2023-03-09 06:27:05,396 - INFO - __main__ - Epoch  87, Step:  188100, Batch Loss:     2.712721, Lr: 0.000042, Tokens per sec:   2721
2023-03-09 06:27:25,316 - INFO - __main__ - Epoch  87, Step:  188200, Batch Loss:     2.303729, Lr: 0.000042, Tokens per sec:   2711
2023-03-09 06:27:45,317 - INFO - __main__ - Epoch  87, Step:  188300, Batch Loss:     1.759781, Lr: 0.000042, Tokens per sec:   2707
2023-03-09 06:28:05,267 - INFO - __main__ - Epoch  87, Step:  188400, Batch Loss:     1.667608, Lr: 0.000042, Tokens per sec:   2705
2023-03-09 06:28:25,261 - INFO - __main__ - Epoch  87, Step:  188500, Batch Loss:     2.104086, Lr: 0.000042, Tokens per sec:   2682
2023-03-09 06:28:45,216 - INFO - __main__ - Epoch  87, Step:  188600, Batch Loss:     1.777547, Lr: 0.000042, Tokens per sec:   2619
2023-03-09 06:29:05,191 - INFO - __main__ - Epoch  87, Step:  188700, Batch Loss:     1.778220, Lr: 0.000042, Tokens per sec:   2692
2023-03-09 06:29:25,127 - INFO - __main__ - Epoch  87, Step:  188800, Batch Loss:     1.542694, Lr: 0.000042, Tokens per sec:   2737
2023-03-09 06:29:45,112 - INFO - __main__ - Epoch  87, Step:  188900, Batch Loss:     1.952668, Lr: 0.000042, Tokens per sec:   2675
2023-03-09 06:30:05,096 - INFO - __main__ - Epoch  87, Step:  189000, Batch Loss:     1.601179, Lr: 0.000042, Tokens per sec:   2723
2023-03-09 06:30:25,064 - INFO - __main__ - Epoch  87, Step:  189100, Batch Loss:     1.775997, Lr: 0.000042, Tokens per sec:   2645
2023-03-09 06:30:44,959 - INFO - __main__ - Epoch  87, Step:  189200, Batch Loss:     2.761015, Lr: 0.000042, Tokens per sec:   2701
2023-03-09 06:31:04,950 - INFO - __main__ - Epoch  87, Step:  189300, Batch Loss:     2.439310, Lr: 0.000042, Tokens per sec:   2692
2023-03-09 06:31:23,591 - INFO - __main__ - Epoch  87, Step:  189400, Batch Loss:     1.977205, Lr: 0.000042, Tokens per sec:   2902
2023-03-09 06:31:41,556 - INFO - __main__ - Epoch  87, Step:  189500, Batch Loss:     2.150490, Lr: 0.000042, Tokens per sec:   2976
2023-03-09 06:31:54,734 - INFO - __main__ - Epoch  87: total training loss 4291.89
2023-03-09 06:31:54,735 - INFO - __main__ - Epoch 88
2023-03-09 06:32:00,437 - INFO - __main__ - Epoch  88, Step:  189600, Batch Loss:     1.701168, Lr: 0.000042, Tokens per sec:   2487
2023-03-09 06:32:20,400 - INFO - __main__ - Epoch  88, Step:  189700, Batch Loss:     2.030598, Lr: 0.000042, Tokens per sec:   2709
2023-03-09 06:32:40,363 - INFO - __main__ - Epoch  88, Step:  189800, Batch Loss:     1.363928, Lr: 0.000042, Tokens per sec:   2721
2023-03-09 06:33:00,329 - INFO - __main__ - Epoch  88, Step:  189900, Batch Loss:     1.585960, Lr: 0.000042, Tokens per sec:   2708
2023-03-09 06:33:20,225 - INFO - __main__ - Epoch  88, Step:  190000, Batch Loss:     1.612046, Lr: 0.000042, Tokens per sec:   2656
2023-03-09 06:33:40,217 - INFO - __main__ - Epoch  88, Step:  190100, Batch Loss:     1.886951, Lr: 0.000042, Tokens per sec:   2705
2023-03-09 06:34:00,168 - INFO - __main__ - Epoch  88, Step:  190200, Batch Loss:     1.593663, Lr: 0.000042, Tokens per sec:   2689
2023-03-09 06:34:20,168 - INFO - __main__ - Epoch  88, Step:  190300, Batch Loss:     1.836600, Lr: 0.000042, Tokens per sec:   2713
2023-03-09 06:34:40,163 - INFO - __main__ - Epoch  88, Step:  190400, Batch Loss:     1.413348, Lr: 0.000042, Tokens per sec:   2722
2023-03-09 06:35:00,174 - INFO - __main__ - Epoch  88, Step:  190500, Batch Loss:     1.289113, Lr: 0.000042, Tokens per sec:   2679
2023-03-09 06:35:20,151 - INFO - __main__ - Epoch  88, Step:  190600, Batch Loss:     2.640859, Lr: 0.000042, Tokens per sec:   2733
2023-03-09 06:35:39,280 - INFO - __main__ - Epoch  88, Step:  190700, Batch Loss:     2.537583, Lr: 0.000042, Tokens per sec:   2810
2023-03-09 06:35:57,214 - INFO - __main__ - Epoch  88, Step:  190800, Batch Loss:     2.520058, Lr: 0.000042, Tokens per sec:   2948
2023-03-09 06:36:15,170 - INFO - __main__ - Epoch  88, Step:  190900, Batch Loss:     3.607960, Lr: 0.000042, Tokens per sec:   2972
2023-03-09 06:36:33,105 - INFO - __main__ - Epoch  88, Step:  191000, Batch Loss:     2.365743, Lr: 0.000042, Tokens per sec:   3004
2023-03-09 06:36:51,048 - INFO - __main__ - Epoch  88, Step:  191100, Batch Loss:     1.542905, Lr: 0.000042, Tokens per sec:   2966
2023-03-09 06:37:08,973 - INFO - __main__ - Epoch  88, Step:  191200, Batch Loss:     1.934290, Lr: 0.000042, Tokens per sec:   3048
2023-03-09 06:37:26,903 - INFO - __main__ - Epoch  88, Step:  191300, Batch Loss:     2.156354, Lr: 0.000042, Tokens per sec:   3004
2023-03-09 06:37:44,881 - INFO - __main__ - Epoch  88, Step:  191400, Batch Loss:     1.952257, Lr: 0.000042, Tokens per sec:   3052
2023-03-09 06:38:02,814 - INFO - __main__ - Epoch  88, Step:  191500, Batch Loss:     1.488138, Lr: 0.000042, Tokens per sec:   2994
2023-03-09 06:38:20,809 - INFO - __main__ - Epoch  88, Step:  191600, Batch Loss:     1.605134, Lr: 0.000042, Tokens per sec:   3009
2023-03-09 06:38:39,239 - INFO - __main__ - Epoch  88, Step:  191700, Batch Loss:     2.118528, Lr: 0.000042, Tokens per sec:   2898
2023-03-09 06:38:49,669 - INFO - __main__ - Epoch  88: total training loss 4196.76
2023-03-09 06:38:49,671 - INFO - __main__ - Epoch 89
2023-03-09 06:38:59,558 - INFO - __main__ - Epoch  89, Step:  191800, Batch Loss:     1.464648, Lr: 0.000041, Tokens per sec:   2673
2023-03-09 06:39:19,530 - INFO - __main__ - Epoch  89, Step:  191900, Batch Loss:     1.463560, Lr: 0.000041, Tokens per sec:   2681
2023-03-09 06:39:39,463 - INFO - __main__ - Epoch  89, Step:  192000, Batch Loss:     1.424113, Lr: 0.000041, Tokens per sec:   2710
2023-03-09 06:39:58,071 - INFO - __main__ - Epoch  89, Step:  192100, Batch Loss:     1.180215, Lr: 0.000041, Tokens per sec:   2938
2023-03-09 06:40:16,034 - INFO - __main__ - Epoch  89, Step:  192200, Batch Loss:     2.362389, Lr: 0.000041, Tokens per sec:   3022
2023-03-09 06:40:35,984 - INFO - __main__ - Epoch  89, Step:  192300, Batch Loss:     1.531961, Lr: 0.000041, Tokens per sec:   2640
2023-03-09 06:40:55,954 - INFO - __main__ - Epoch  89, Step:  192400, Batch Loss:     2.709335, Lr: 0.000041, Tokens per sec:   2735
2023-03-09 06:41:15,895 - INFO - __main__ - Epoch  89, Step:  192500, Batch Loss:     1.664692, Lr: 0.000041, Tokens per sec:   2646
2023-03-09 06:41:35,827 - INFO - __main__ - Epoch  89, Step:  192600, Batch Loss:     2.827246, Lr: 0.000041, Tokens per sec:   2734
2023-03-09 06:41:55,734 - INFO - __main__ - Epoch  89, Step:  192700, Batch Loss:     1.686417, Lr: 0.000041, Tokens per sec:   2679
2023-03-09 06:42:14,491 - INFO - __main__ - Epoch  89, Step:  192800, Batch Loss:     1.516775, Lr: 0.000041, Tokens per sec:   2880
2023-03-09 06:42:32,696 - INFO - __main__ - Epoch  89, Step:  192900, Batch Loss:     1.514465, Lr: 0.000041, Tokens per sec:   2935
2023-03-09 06:42:52,603 - INFO - __main__ - Epoch  89, Step:  193000, Batch Loss:     1.683544, Lr: 0.000041, Tokens per sec:   2676
2023-03-09 06:43:12,550 - INFO - __main__ - Epoch  89, Step:  193100, Batch Loss:     1.483717, Lr: 0.000041, Tokens per sec:   2706
2023-03-09 06:43:32,781 - INFO - __main__ - Epoch  89, Step:  193200, Batch Loss:     1.521242, Lr: 0.000041, Tokens per sec:   2644
2023-03-09 06:43:52,778 - INFO - __main__ - Epoch  89, Step:  193300, Batch Loss:     2.555694, Lr: 0.000041, Tokens per sec:   2729
2023-03-09 06:44:12,697 - INFO - __main__ - Epoch  89, Step:  193400, Batch Loss:     1.565769, Lr: 0.000041, Tokens per sec:   2678
2023-03-09 06:44:32,660 - INFO - __main__ - Epoch  89, Step:  193500, Batch Loss:     2.585707, Lr: 0.000041, Tokens per sec:   2717
2023-03-09 06:44:52,622 - INFO - __main__ - Epoch  89, Step:  193600, Batch Loss:     1.608715, Lr: 0.000041, Tokens per sec:   2653
2023-03-09 06:45:12,594 - INFO - __main__ - Epoch  89, Step:  193700, Batch Loss:     2.658989, Lr: 0.000041, Tokens per sec:   2693
2023-03-09 06:45:32,571 - INFO - __main__ - Epoch  89, Step:  193800, Batch Loss:     1.896308, Lr: 0.000041, Tokens per sec:   2688
2023-03-09 06:45:52,558 - INFO - __main__ - Epoch  89, Step:  193900, Batch Loss:     1.561975, Lr: 0.000041, Tokens per sec:   2761
2023-03-09 06:45:58,752 - INFO - __main__ - Epoch  89: total training loss 4129.52
2023-03-09 06:45:58,753 - INFO - __main__ - Epoch 90
2023-03-09 06:46:12,796 - INFO - __main__ - Epoch  90, Step:  194000, Batch Loss:     2.181098, Lr: 0.000041, Tokens per sec:   2576
2023-03-09 06:46:32,748 - INFO - __main__ - Epoch  90, Step:  194100, Batch Loss:     2.132156, Lr: 0.000041, Tokens per sec:   2705
2023-03-09 06:46:52,723 - INFO - __main__ - Epoch  90, Step:  194200, Batch Loss:     2.356492, Lr: 0.000041, Tokens per sec:   2709
2023-03-09 06:47:12,716 - INFO - __main__ - Epoch  90, Step:  194300, Batch Loss:     2.274999, Lr: 0.000041, Tokens per sec:   2677
2023-03-09 06:47:32,707 - INFO - __main__ - Epoch  90, Step:  194400, Batch Loss:     1.094140, Lr: 0.000041, Tokens per sec:   2690
2023-03-09 06:47:52,665 - INFO - __main__ - Epoch  90, Step:  194500, Batch Loss:     1.997696, Lr: 0.000041, Tokens per sec:   2687
2023-03-09 06:48:12,586 - INFO - __main__ - Epoch  90, Step:  194600, Batch Loss:     1.783812, Lr: 0.000041, Tokens per sec:   2658
2023-03-09 06:48:32,593 - INFO - __main__ - Epoch  90, Step:  194700, Batch Loss:     2.621862, Lr: 0.000041, Tokens per sec:   2669
2023-03-09 06:48:52,526 - INFO - __main__ - Epoch  90, Step:  194800, Batch Loss:     1.852474, Lr: 0.000041, Tokens per sec:   2703
2023-03-09 06:49:12,495 - INFO - __main__ - Epoch  90, Step:  194900, Batch Loss:     1.147417, Lr: 0.000041, Tokens per sec:   2702
2023-03-09 06:49:32,492 - INFO - __main__ - Epoch  90, Step:  195000, Batch Loss:     1.887298, Lr: 0.000041, Tokens per sec:   2698
2023-03-09 06:49:52,446 - INFO - __main__ - Epoch  90, Step:  195100, Batch Loss:     1.959915, Lr: 0.000041, Tokens per sec:   2697
2023-03-09 06:50:12,422 - INFO - __main__ - Epoch  90, Step:  195200, Batch Loss:     1.732567, Lr: 0.000041, Tokens per sec:   2714
2023-03-09 06:50:32,364 - INFO - __main__ - Epoch  90, Step:  195300, Batch Loss:     1.455654, Lr: 0.000041, Tokens per sec:   2719
2023-03-09 06:50:52,387 - INFO - __main__ - Epoch  90, Step:  195400, Batch Loss:     2.093776, Lr: 0.000041, Tokens per sec:   2702
2023-03-09 06:51:12,358 - INFO - __main__ - Epoch  90, Step:  195500, Batch Loss:     1.745652, Lr: 0.000041, Tokens per sec:   2720
2023-03-09 06:51:32,307 - INFO - __main__ - Epoch  90, Step:  195600, Batch Loss:     1.714076, Lr: 0.000041, Tokens per sec:   2721
2023-03-09 06:51:52,302 - INFO - __main__ - Epoch  90, Step:  195700, Batch Loss:     2.791889, Lr: 0.000041, Tokens per sec:   2704
2023-03-09 06:52:11,241 - INFO - __main__ - Epoch  90, Step:  195800, Batch Loss:     1.720086, Lr: 0.000041, Tokens per sec:   2833
2023-03-09 06:52:30,513 - INFO - __main__ - Epoch  90, Step:  195900, Batch Loss:     1.645850, Lr: 0.000041, Tokens per sec:   2845
2023-03-09 06:52:50,366 - INFO - __main__ - Epoch  90, Step:  196000, Batch Loss:     2.599791, Lr: 0.000041, Tokens per sec:   2687
2023-03-09 06:53:10,302 - INFO - __main__ - Epoch  90, Step:  196100, Batch Loss:     1.959125, Lr: 0.000041, Tokens per sec:   2688
2023-03-09 06:53:12,364 - INFO - __main__ - Epoch  90: total training loss 4083.73
2023-03-09 06:53:12,365 - INFO - __main__ - Epoch 91
2023-03-09 06:53:30,653 - INFO - __main__ - Epoch  91, Step:  196200, Batch Loss:     1.946106, Lr: 0.000040, Tokens per sec:   2661
2023-03-09 06:53:50,589 - INFO - __main__ - Epoch  91, Step:  196300, Batch Loss:     1.509477, Lr: 0.000040, Tokens per sec:   2706
2023-03-09 06:54:10,558 - INFO - __main__ - Epoch  91, Step:  196400, Batch Loss:     2.385988, Lr: 0.000040, Tokens per sec:   2708
2023-03-09 06:54:30,050 - INFO - __main__ - Epoch  91, Step:  196500, Batch Loss:     1.296110, Lr: 0.000040, Tokens per sec:   2813
2023-03-09 06:54:49,791 - INFO - __main__ - Epoch  91, Step:  196600, Batch Loss:     1.635680, Lr: 0.000040, Tokens per sec:   2733
2023-03-09 06:55:09,522 - INFO - __main__ - Epoch  91, Step:  196700, Batch Loss:     2.063342, Lr: 0.000040, Tokens per sec:   2755
2023-03-09 06:55:29,228 - INFO - __main__ - Epoch  91, Step:  196800, Batch Loss:     1.435660, Lr: 0.000040, Tokens per sec:   2737
2023-03-09 06:55:49,208 - INFO - __main__ - Epoch  91, Step:  196900, Batch Loss:     1.776276, Lr: 0.000040, Tokens per sec:   2771
2023-03-09 06:56:09,158 - INFO - __main__ - Epoch  91, Step:  197000, Batch Loss:     2.259148, Lr: 0.000040, Tokens per sec:   2680
2023-03-09 06:56:29,154 - INFO - __main__ - Epoch  91, Step:  197100, Batch Loss:     1.686548, Lr: 0.000040, Tokens per sec:   2663
2023-03-09 06:56:49,108 - INFO - __main__ - Epoch  91, Step:  197200, Batch Loss:     1.395255, Lr: 0.000040, Tokens per sec:   2684
2023-03-09 06:57:09,099 - INFO - __main__ - Epoch  91, Step:  197300, Batch Loss:     2.035130, Lr: 0.000040, Tokens per sec:   2698
2023-03-09 06:57:27,574 - INFO - __main__ - Epoch  91, Step:  197400, Batch Loss:     1.595429, Lr: 0.000040, Tokens per sec:   2908
2023-03-09 06:57:46,291 - INFO - __main__ - Epoch  91, Step:  197500, Batch Loss:     2.231726, Lr: 0.000040, Tokens per sec:   2808
2023-03-09 06:58:06,260 - INFO - __main__ - Epoch  91, Step:  197600, Batch Loss:     1.449272, Lr: 0.000040, Tokens per sec:   2725
2023-03-09 06:58:26,254 - INFO - __main__ - Epoch  91, Step:  197700, Batch Loss:     2.477243, Lr: 0.000040, Tokens per sec:   2733
2023-03-09 06:58:46,185 - INFO - __main__ - Epoch  91, Step:  197800, Batch Loss:     0.954971, Lr: 0.000040, Tokens per sec:   2697
2023-03-09 06:59:06,137 - INFO - __main__ - Epoch  91, Step:  197900, Batch Loss:     1.333758, Lr: 0.000040, Tokens per sec:   2693
2023-03-09 06:59:26,079 - INFO - __main__ - Epoch  91, Step:  198000, Batch Loss:     1.538701, Lr: 0.000040, Tokens per sec:   2680
2023-03-09 06:59:46,025 - INFO - __main__ - Epoch  91, Step:  198100, Batch Loss:     2.032472, Lr: 0.000040, Tokens per sec:   2697
2023-03-09 07:00:05,961 - INFO - __main__ - Epoch  91, Step:  198200, Batch Loss:     1.082029, Lr: 0.000040, Tokens per sec:   2663
2023-03-09 07:00:23,813 - INFO - __main__ - Epoch  91: total training loss 4000.40
2023-03-09 07:00:23,814 - INFO - __main__ - Epoch 92
2023-03-09 07:00:26,333 - INFO - __main__ - Epoch  92, Step:  198300, Batch Loss:     2.434019, Lr: 0.000040, Tokens per sec:   2518
2023-03-09 07:00:46,270 - INFO - __main__ - Epoch  92, Step:  198400, Batch Loss:     1.475761, Lr: 0.000040, Tokens per sec:   2684
2023-03-09 07:01:06,243 - INFO - __main__ - Epoch  92, Step:  198500, Batch Loss:     1.298409, Lr: 0.000040, Tokens per sec:   2694
2023-03-09 07:01:26,223 - INFO - __main__ - Epoch  92, Step:  198600, Batch Loss:     1.165253, Lr: 0.000040, Tokens per sec:   2714
2023-03-09 07:01:46,207 - INFO - __main__ - Epoch  92, Step:  198700, Batch Loss:     0.884945, Lr: 0.000040, Tokens per sec:   2739
2023-03-09 07:02:06,182 - INFO - __main__ - Epoch  92, Step:  198800, Batch Loss:     2.139032, Lr: 0.000040, Tokens per sec:   2652
2023-03-09 07:02:26,152 - INFO - __main__ - Epoch  92, Step:  198900, Batch Loss:     0.739879, Lr: 0.000040, Tokens per sec:   2701
2023-03-09 07:02:46,122 - INFO - __main__ - Epoch  92, Step:  199000, Batch Loss:     2.553919, Lr: 0.000040, Tokens per sec:   2673
2023-03-09 07:03:06,121 - INFO - __main__ - Epoch  92, Step:  199100, Batch Loss:     2.317188, Lr: 0.000040, Tokens per sec:   2690
2023-03-09 07:03:26,093 - INFO - __main__ - Epoch  92, Step:  199200, Batch Loss:     2.288572, Lr: 0.000040, Tokens per sec:   2663
2023-03-09 07:03:46,065 - INFO - __main__ - Epoch  92, Step:  199300, Batch Loss:     1.839487, Lr: 0.000040, Tokens per sec:   2724
2023-03-09 07:04:05,997 - INFO - __main__ - Epoch  92, Step:  199400, Batch Loss:     1.725934, Lr: 0.000040, Tokens per sec:   2739
2023-03-09 07:04:25,994 - INFO - __main__ - Epoch  92, Step:  199500, Batch Loss:     1.702554, Lr: 0.000040, Tokens per sec:   2696
2023-03-09 07:04:45,946 - INFO - __main__ - Epoch  92, Step:  199600, Batch Loss:     2.204622, Lr: 0.000040, Tokens per sec:   2677
2023-03-09 07:05:05,925 - INFO - __main__ - Epoch  92, Step:  199700, Batch Loss:     2.190143, Lr: 0.000040, Tokens per sec:   2674
2023-03-09 07:05:25,876 - INFO - __main__ - Epoch  92, Step:  199800, Batch Loss:     2.001264, Lr: 0.000040, Tokens per sec:   2692
2023-03-09 07:05:45,842 - INFO - __main__ - Epoch  92, Step:  199900, Batch Loss:     3.047747, Lr: 0.000040, Tokens per sec:   2748
2023-03-09 07:06:05,823 - INFO - __main__ - Epoch  92, Step:  200000, Batch Loss:     1.339863, Lr: 0.000040, Tokens per sec:   2695
2023-03-09 07:06:25,780 - INFO - __main__ - Epoch  92, Step:  200100, Batch Loss:     1.220866, Lr: 0.000040, Tokens per sec:   2727
2023-03-09 07:06:45,745 - INFO - __main__ - Epoch  92, Step:  200200, Batch Loss:     1.824151, Lr: 0.000040, Tokens per sec:   2655
2023-03-09 07:07:05,704 - INFO - __main__ - Epoch  92, Step:  200300, Batch Loss:     2.474596, Lr: 0.000040, Tokens per sec:   2677
2023-03-09 07:07:25,633 - INFO - __main__ - Epoch  92, Step:  200400, Batch Loss:     1.527816, Lr: 0.000040, Tokens per sec:   2649
2023-03-09 07:07:39,267 - INFO - __main__ - Epoch  92: total training loss 3929.92
2023-03-09 07:07:39,268 - INFO - __main__ - Epoch 93
2023-03-09 07:07:45,932 - INFO - __main__ - Epoch  93, Step:  200500, Batch Loss:     1.633263, Lr: 0.000040, Tokens per sec:   2585
2023-03-09 07:08:05,845 - INFO - __main__ - Epoch  93, Step:  200600, Batch Loss:     1.734391, Lr: 0.000040, Tokens per sec:   2682
2023-03-09 07:08:25,833 - INFO - __main__ - Epoch  93, Step:  200700, Batch Loss:     1.664004, Lr: 0.000040, Tokens per sec:   2675
2023-03-09 07:08:45,842 - INFO - __main__ - Epoch  93, Step:  200800, Batch Loss:     1.831530, Lr: 0.000040, Tokens per sec:   2716
2023-03-09 07:09:05,825 - INFO - __main__ - Epoch  93, Step:  200900, Batch Loss:     2.029356, Lr: 0.000040, Tokens per sec:   2684
2023-03-09 07:09:25,760 - INFO - __main__ - Epoch  93, Step:  201000, Batch Loss:     1.898123, Lr: 0.000040, Tokens per sec:   2736
2023-03-09 07:09:45,731 - INFO - __main__ - Epoch  93, Step:  201100, Batch Loss:     1.599555, Lr: 0.000040, Tokens per sec:   2680
2023-03-09 07:10:05,723 - INFO - __main__ - Epoch  93, Step:  201200, Batch Loss:     2.075048, Lr: 0.000040, Tokens per sec:   2721
2023-03-09 07:10:25,713 - INFO - __main__ - Epoch  93, Step:  201300, Batch Loss:     2.033875, Lr: 0.000040, Tokens per sec:   2700
2023-03-09 07:10:45,690 - INFO - __main__ - Epoch  93, Step:  201400, Batch Loss:     2.136200, Lr: 0.000040, Tokens per sec:   2685
2023-03-09 07:11:05,610 - INFO - __main__ - Epoch  93, Step:  201500, Batch Loss:     1.566887, Lr: 0.000040, Tokens per sec:   2696
2023-03-09 07:11:25,573 - INFO - __main__ - Epoch  93, Step:  201600, Batch Loss:     1.325851, Lr: 0.000040, Tokens per sec:   2685
2023-03-09 07:11:45,512 - INFO - __main__ - Epoch  93, Step:  201700, Batch Loss:     1.314238, Lr: 0.000040, Tokens per sec:   2657
2023-03-09 07:12:03,525 - INFO - __main__ - Epoch  93, Step:  201800, Batch Loss:     2.179740, Lr: 0.000040, Tokens per sec:   3002
2023-03-09 07:12:21,437 - INFO - __main__ - Epoch  93, Step:  201900, Batch Loss:     1.706772, Lr: 0.000040, Tokens per sec:   3013
2023-03-09 07:12:39,350 - INFO - __main__ - Epoch  93, Step:  202000, Batch Loss:     1.449067, Lr: 0.000040, Tokens per sec:   3002
2023-03-09 07:12:58,369 - INFO - __main__ - Epoch  93, Step:  202100, Batch Loss:     1.872575, Lr: 0.000040, Tokens per sec:   2885
2023-03-09 07:13:18,309 - INFO - __main__ - Epoch  93, Step:  202200, Batch Loss:     1.670549, Lr: 0.000040, Tokens per sec:   2635
2023-03-09 07:13:38,234 - INFO - __main__ - Epoch  93, Step:  202300, Batch Loss:     2.003052, Lr: 0.000040, Tokens per sec:   2731
2023-03-09 07:13:58,226 - INFO - __main__ - Epoch  93, Step:  202400, Batch Loss:     1.811680, Lr: 0.000040, Tokens per sec:   2744
2023-03-09 07:14:18,201 - INFO - __main__ - Epoch  93, Step:  202500, Batch Loss:     2.810190, Lr: 0.000040, Tokens per sec:   2655
2023-03-09 07:14:38,151 - INFO - __main__ - Epoch  93, Step:  202600, Batch Loss:     2.039918, Lr: 0.000040, Tokens per sec:   2709
2023-03-09 07:14:47,568 - INFO - __main__ - Epoch  93: total training loss 3871.92
2023-03-09 07:14:47,569 - INFO - __main__ - Epoch 94
2023-03-09 07:14:58,446 - INFO - __main__ - Epoch  94, Step:  202700, Batch Loss:     1.525085, Lr: 0.000039, Tokens per sec:   2672
2023-03-09 07:15:18,425 - INFO - __main__ - Epoch  94, Step:  202800, Batch Loss:     1.507985, Lr: 0.000039, Tokens per sec:   2745
2023-03-09 07:15:37,479 - INFO - __main__ - Epoch  94, Step:  202900, Batch Loss:     1.486566, Lr: 0.000039, Tokens per sec:   2841
2023-03-09 07:15:55,388 - INFO - __main__ - Epoch  94, Step:  203000, Batch Loss:     1.295884, Lr: 0.000039, Tokens per sec:   3003
2023-03-09 07:16:13,265 - INFO - __main__ - Epoch  94, Step:  203100, Batch Loss:     1.107441, Lr: 0.000039, Tokens per sec:   3017
2023-03-09 07:16:31,219 - INFO - __main__ - Epoch  94, Step:  203200, Batch Loss:     1.951683, Lr: 0.000039, Tokens per sec:   3062
2023-03-09 07:16:49,130 - INFO - __main__ - Epoch  94, Step:  203300, Batch Loss:     2.076046, Lr: 0.000039, Tokens per sec:   3037
2023-03-09 07:17:08,907 - INFO - __main__ - Epoch  94, Step:  203400, Batch Loss:     2.097759, Lr: 0.000039, Tokens per sec:   2690
2023-03-09 07:17:28,837 - INFO - __main__ - Epoch  94, Step:  203500, Batch Loss:     1.981771, Lr: 0.000039, Tokens per sec:   2681
2023-03-09 07:17:48,811 - INFO - __main__ - Epoch  94, Step:  203600, Batch Loss:     1.340192, Lr: 0.000039, Tokens per sec:   2733
2023-03-09 07:18:08,735 - INFO - __main__ - Epoch  94, Step:  203700, Batch Loss:     1.653828, Lr: 0.000039, Tokens per sec:   2663
2023-03-09 07:18:28,699 - INFO - __main__ - Epoch  94, Step:  203800, Batch Loss:     2.193350, Lr: 0.000039, Tokens per sec:   2683
2023-03-09 07:18:48,633 - INFO - __main__ - Epoch  94, Step:  203900, Batch Loss:     2.167918, Lr: 0.000039, Tokens per sec:   2689
2023-03-09 07:19:08,544 - INFO - __main__ - Epoch  94, Step:  204000, Batch Loss:     1.338462, Lr: 0.000039, Tokens per sec:   2765
2023-03-09 07:19:28,419 - INFO - __main__ - Epoch  94, Step:  204100, Batch Loss:     1.521574, Lr: 0.000039, Tokens per sec:   2691
2023-03-09 07:19:48,412 - INFO - __main__ - Epoch  94, Step:  204200, Batch Loss:     1.263575, Lr: 0.000039, Tokens per sec:   2720
2023-03-09 07:20:08,373 - INFO - __main__ - Epoch  94, Step:  204300, Batch Loss:     1.801752, Lr: 0.000039, Tokens per sec:   2652
2023-03-09 07:20:28,301 - INFO - __main__ - Epoch  94, Step:  204400, Batch Loss:     1.957700, Lr: 0.000039, Tokens per sec:   2719
2023-03-09 07:20:48,233 - INFO - __main__ - Epoch  94, Step:  204500, Batch Loss:     1.448099, Lr: 0.000039, Tokens per sec:   2662
2023-03-09 07:21:08,207 - INFO - __main__ - Epoch  94, Step:  204600, Batch Loss:     1.972977, Lr: 0.000039, Tokens per sec:   2667
2023-03-09 07:21:28,144 - INFO - __main__ - Epoch  94, Step:  204700, Batch Loss:     1.803590, Lr: 0.000039, Tokens per sec:   2690
2023-03-09 07:21:48,116 - INFO - __main__ - Epoch  94, Step:  204800, Batch Loss:     1.461228, Lr: 0.000039, Tokens per sec:   2630
2023-03-09 07:21:53,377 - INFO - __main__ - Epoch  94: total training loss 3804.50
2023-03-09 07:21:53,378 - INFO - __main__ - Epoch 95
2023-03-09 07:22:08,450 - INFO - __main__ - Epoch  95, Step:  204900, Batch Loss:     1.708117, Lr: 0.000039, Tokens per sec:   2687
2023-03-09 07:22:28,425 - INFO - __main__ - Epoch  95, Step:  205000, Batch Loss:     1.853716, Lr: 0.000039, Tokens per sec:   2699
2023-03-09 07:22:48,386 - INFO - __main__ - Epoch  95, Step:  205100, Batch Loss:     1.093216, Lr: 0.000039, Tokens per sec:   2747
2023-03-09 07:23:08,225 - INFO - __main__ - Epoch  95, Step:  205200, Batch Loss:     1.469651, Lr: 0.000039, Tokens per sec:   2716
2023-03-09 07:23:26,152 - INFO - __main__ - Epoch  95, Step:  205300, Batch Loss:     1.944972, Lr: 0.000039, Tokens per sec:   3037
2023-03-09 07:23:44,052 - INFO - __main__ - Epoch  95, Step:  205400, Batch Loss:     1.439866, Lr: 0.000039, Tokens per sec:   3037
2023-03-09 07:24:01,887 - INFO - __main__ - Epoch  95, Step:  205500, Batch Loss:     2.345297, Lr: 0.000039, Tokens per sec:   3062
2023-03-09 07:24:21,527 - INFO - __main__ - Epoch  95, Step:  205600, Batch Loss:     1.582267, Lr: 0.000039, Tokens per sec:   2706
2023-03-09 07:24:41,439 - INFO - __main__ - Epoch  95, Step:  205700, Batch Loss:     1.917941, Lr: 0.000039, Tokens per sec:   2683
2023-03-09 07:24:59,632 - INFO - __main__ - Epoch  95, Step:  205800, Batch Loss:     1.919413, Lr: 0.000039, Tokens per sec:   2908
2023-03-09 07:25:18,006 - INFO - __main__ - Epoch  95, Step:  205900, Batch Loss:     1.209064, Lr: 0.000039, Tokens per sec:   2932
2023-03-09 07:25:37,502 - INFO - __main__ - Epoch  95, Step:  206000, Batch Loss:     1.560550, Lr: 0.000039, Tokens per sec:   2737
2023-03-09 07:25:55,401 - INFO - __main__ - Epoch  95, Step:  206100, Batch Loss:     1.512130, Lr: 0.000039, Tokens per sec:   3004
2023-03-09 07:26:13,326 - INFO - __main__ - Epoch  95, Step:  206200, Batch Loss:     1.487798, Lr: 0.000039, Tokens per sec:   3007
2023-03-09 07:26:31,282 - INFO - __main__ - Epoch  95, Step:  206300, Batch Loss:     1.576059, Lr: 0.000039, Tokens per sec:   2994
2023-03-09 07:26:50,601 - INFO - __main__ - Epoch  95, Step:  206400, Batch Loss:     1.763540, Lr: 0.000039, Tokens per sec:   2793
2023-03-09 07:27:10,569 - INFO - __main__ - Epoch  95, Step:  206500, Batch Loss:     0.875959, Lr: 0.000039, Tokens per sec:   2644
2023-03-09 07:27:30,523 - INFO - __main__ - Epoch  95, Step:  206600, Batch Loss:     1.473688, Lr: 0.000039, Tokens per sec:   2729
2023-03-09 07:27:50,460 - INFO - __main__ - Epoch  95, Step:  206700, Batch Loss:     1.950143, Lr: 0.000039, Tokens per sec:   2635
2023-03-09 07:28:10,396 - INFO - __main__ - Epoch  95, Step:  206800, Batch Loss:     1.538156, Lr: 0.000039, Tokens per sec:   2711
2023-03-09 07:28:30,347 - INFO - __main__ - Epoch  95, Step:  206900, Batch Loss:     2.174598, Lr: 0.000039, Tokens per sec:   2675
2023-03-09 07:28:50,337 - INFO - __main__ - Epoch  95, Step:  207000, Batch Loss:     2.030371, Lr: 0.000039, Tokens per sec:   2727
2023-03-09 07:28:51,405 - INFO - __main__ - Epoch  95: total training loss 3753.57
2023-03-09 07:28:51,406 - INFO - __main__ - Epoch 96
2023-03-09 07:29:10,675 - INFO - __main__ - Epoch  96, Step:  207100, Batch Loss:     1.527318, Lr: 0.000038, Tokens per sec:   2665
2023-03-09 07:29:30,667 - INFO - __main__ - Epoch  96, Step:  207200, Batch Loss:     1.813144, Lr: 0.000038, Tokens per sec:   2713
2023-03-09 07:29:50,638 - INFO - __main__ - Epoch  96, Step:  207300, Batch Loss:     0.962905, Lr: 0.000038, Tokens per sec:   2640
2023-03-09 07:30:10,596 - INFO - __main__ - Epoch  96, Step:  207400, Batch Loss:     1.865117, Lr: 0.000038, Tokens per sec:   2643
2023-03-09 07:30:30,528 - INFO - __main__ - Epoch  96, Step:  207500, Batch Loss:     1.587476, Lr: 0.000038, Tokens per sec:   2694
2023-03-09 07:30:50,469 - INFO - __main__ - Epoch  96, Step:  207600, Batch Loss:     1.971482, Lr: 0.000038, Tokens per sec:   2666
2023-03-09 07:31:10,412 - INFO - __main__ - Epoch  96, Step:  207700, Batch Loss:     1.249240, Lr: 0.000038, Tokens per sec:   2684
2023-03-09 07:31:30,366 - INFO - __main__ - Epoch  96, Step:  207800, Batch Loss:     1.949359, Lr: 0.000038, Tokens per sec:   2736
2023-03-09 07:31:50,342 - INFO - __main__ - Epoch  96, Step:  207900, Batch Loss:     1.472691, Lr: 0.000038, Tokens per sec:   2679
2023-03-09 07:32:10,314 - INFO - __main__ - Epoch  96, Step:  208000, Batch Loss:     1.778510, Lr: 0.000038, Tokens per sec:   2717
2023-03-09 07:32:30,290 - INFO - __main__ - Epoch  96, Step:  208100, Batch Loss:     1.329713, Lr: 0.000038, Tokens per sec:   2690
2023-03-09 07:32:50,282 - INFO - __main__ - Epoch  96, Step:  208200, Batch Loss:     2.470193, Lr: 0.000038, Tokens per sec:   2683
2023-03-09 07:33:10,235 - INFO - __main__ - Epoch  96, Step:  208300, Batch Loss:     1.047170, Lr: 0.000038, Tokens per sec:   2713
2023-03-09 07:33:30,206 - INFO - __main__ - Epoch  96, Step:  208400, Batch Loss:     1.755163, Lr: 0.000038, Tokens per sec:   2731
2023-03-09 07:33:50,122 - INFO - __main__ - Epoch  96, Step:  208500, Batch Loss:     1.049668, Lr: 0.000038, Tokens per sec:   2717
2023-03-09 07:34:10,114 - INFO - __main__ - Epoch  96, Step:  208600, Batch Loss:     2.121411, Lr: 0.000038, Tokens per sec:   2630
2023-03-09 07:34:30,112 - INFO - __main__ - Epoch  96, Step:  208700, Batch Loss:     2.168665, Lr: 0.000038, Tokens per sec:   2680
2023-03-09 07:34:50,101 - INFO - __main__ - Epoch  96, Step:  208800, Batch Loss:     0.948371, Lr: 0.000038, Tokens per sec:   2713
2023-03-09 07:35:10,097 - INFO - __main__ - Epoch  96, Step:  208900, Batch Loss:     1.332331, Lr: 0.000038, Tokens per sec:   2783
2023-03-09 07:35:30,066 - INFO - __main__ - Epoch  96, Step:  209000, Batch Loss:     2.303155, Lr: 0.000038, Tokens per sec:   2680
2023-03-09 07:35:50,007 - INFO - __main__ - Epoch  96, Step:  209100, Batch Loss:     2.045533, Lr: 0.000038, Tokens per sec:   2702
2023-03-09 07:36:06,838 - INFO - __main__ - Epoch  96: total training loss 3700.26
2023-03-09 07:36:06,839 - INFO - __main__ - Epoch 97
2023-03-09 07:36:10,318 - INFO - __main__ - Epoch  97, Step:  209200, Batch Loss:     1.395296, Lr: 0.000038, Tokens per sec:   2431
2023-03-09 07:36:30,252 - INFO - __main__ - Epoch  97, Step:  209300, Batch Loss:     1.266581, Lr: 0.000038, Tokens per sec:   2722
2023-03-09 07:36:50,207 - INFO - __main__ - Epoch  97, Step:  209400, Batch Loss:     1.371815, Lr: 0.000038, Tokens per sec:   2678
2023-03-09 07:37:10,144 - INFO - __main__ - Epoch  97, Step:  209500, Batch Loss:     1.874735, Lr: 0.000038, Tokens per sec:   2722
2023-03-09 07:37:30,151 - INFO - __main__ - Epoch  97, Step:  209600, Batch Loss:     1.503293, Lr: 0.000038, Tokens per sec:   2693
2023-03-09 07:37:50,145 - INFO - __main__ - Epoch  97, Step:  209700, Batch Loss:     1.600127, Lr: 0.000038, Tokens per sec:   2664
2023-03-09 07:38:10,098 - INFO - __main__ - Epoch  97, Step:  209800, Batch Loss:     2.105114, Lr: 0.000038, Tokens per sec:   2650
2023-03-09 07:38:30,097 - INFO - __main__ - Epoch  97, Step:  209900, Batch Loss:     1.469153, Lr: 0.000038, Tokens per sec:   2731
2023-03-09 07:38:50,065 - INFO - __main__ - Epoch  97, Step:  210000, Batch Loss:     1.730520, Lr: 0.000038, Tokens per sec:   2677
2023-03-09 07:39:10,064 - INFO - __main__ - Epoch  97, Step:  210100, Batch Loss:     1.287635, Lr: 0.000038, Tokens per sec:   2642
2023-03-09 07:39:30,005 - INFO - __main__ - Epoch  97, Step:  210200, Batch Loss:     1.516697, Lr: 0.000038, Tokens per sec:   2706
2023-03-09 07:39:49,969 - INFO - __main__ - Epoch  97, Step:  210300, Batch Loss:     1.328266, Lr: 0.000038, Tokens per sec:   2663
2023-03-09 07:40:09,943 - INFO - __main__ - Epoch  97, Step:  210400, Batch Loss:     1.485383, Lr: 0.000038, Tokens per sec:   2718
2023-03-09 07:40:29,935 - INFO - __main__ - Epoch  97, Step:  210500, Batch Loss:     1.578963, Lr: 0.000038, Tokens per sec:   2711
2023-03-09 07:40:49,907 - INFO - __main__ - Epoch  97, Step:  210600, Batch Loss:     1.208380, Lr: 0.000038, Tokens per sec:   2719
2023-03-09 07:41:09,882 - INFO - __main__ - Epoch  97, Step:  210700, Batch Loss:     1.843168, Lr: 0.000038, Tokens per sec:   2666
2023-03-09 07:41:29,855 - INFO - __main__ - Epoch  97, Step:  210800, Batch Loss:     1.928616, Lr: 0.000038, Tokens per sec:   2737
2023-03-09 07:41:49,809 - INFO - __main__ - Epoch  97, Step:  210900, Batch Loss:     1.664640, Lr: 0.000038, Tokens per sec:   2700
2023-03-09 07:42:09,739 - INFO - __main__ - Epoch  97, Step:  211000, Batch Loss:     1.719734, Lr: 0.000038, Tokens per sec:   2669
2023-03-09 07:42:28,523 - INFO - __main__ - Epoch  97, Step:  211100, Batch Loss:     1.381984, Lr: 0.000038, Tokens per sec:   2846
2023-03-09 07:42:48,507 - INFO - __main__ - Epoch  97, Step:  211200, Batch Loss:     1.592273, Lr: 0.000038, Tokens per sec:   2713
2023-03-09 07:43:08,460 - INFO - __main__ - Epoch  97, Step:  211300, Batch Loss:     2.067568, Lr: 0.000038, Tokens per sec:   2769
2023-03-09 07:43:21,131 - INFO - __main__ - Epoch  97: total training loss 3625.76
2023-03-09 07:43:21,132 - INFO - __main__ - Epoch 98
2023-03-09 07:43:28,814 - INFO - __main__ - Epoch  98, Step:  211400, Batch Loss:     1.451122, Lr: 0.000038, Tokens per sec:   2655
2023-03-09 07:43:48,762 - INFO - __main__ - Epoch  98, Step:  211500, Batch Loss:     1.583666, Lr: 0.000038, Tokens per sec:   2709
2023-03-09 07:44:08,724 - INFO - __main__ - Epoch  98, Step:  211600, Batch Loss:     1.604455, Lr: 0.000038, Tokens per sec:   2689
2023-03-09 07:44:28,699 - INFO - __main__ - Epoch  98, Step:  211700, Batch Loss:     2.238377, Lr: 0.000038, Tokens per sec:   2707
2023-03-09 07:44:48,674 - INFO - __main__ - Epoch  98, Step:  211800, Batch Loss:     1.903031, Lr: 0.000038, Tokens per sec:   2700
2023-03-09 07:45:08,646 - INFO - __main__ - Epoch  98, Step:  211900, Batch Loss:     1.781205, Lr: 0.000038, Tokens per sec:   2711
2023-03-09 07:45:28,599 - INFO - __main__ - Epoch  98, Step:  212000, Batch Loss:     1.859276, Lr: 0.000038, Tokens per sec:   2724
2023-03-09 07:45:48,569 - INFO - __main__ - Epoch  98, Step:  212100, Batch Loss:     2.190003, Lr: 0.000038, Tokens per sec:   2644
2023-03-09 07:46:08,527 - INFO - __main__ - Epoch  98, Step:  212200, Batch Loss:     1.919778, Lr: 0.000038, Tokens per sec:   2698
2023-03-09 07:46:27,533 - INFO - __main__ - Epoch  98, Step:  212300, Batch Loss:     1.566279, Lr: 0.000038, Tokens per sec:   2841
2023-03-09 07:46:47,056 - INFO - __main__ - Epoch  98, Step:  212400, Batch Loss:     1.430221, Lr: 0.000038, Tokens per sec:   2757
2023-03-09 07:47:06,967 - INFO - __main__ - Epoch  98, Step:  212500, Batch Loss:     1.619391, Lr: 0.000038, Tokens per sec:   2687
2023-03-09 07:47:26,957 - INFO - __main__ - Epoch  98, Step:  212600, Batch Loss:     2.085920, Lr: 0.000038, Tokens per sec:   2728
2023-03-09 07:47:46,948 - INFO - __main__ - Epoch  98, Step:  212700, Batch Loss:     1.292655, Lr: 0.000038, Tokens per sec:   2673
2023-03-09 07:48:06,947 - INFO - __main__ - Epoch  98, Step:  212800, Batch Loss:     1.389467, Lr: 0.000038, Tokens per sec:   2721
2023-03-09 07:48:26,882 - INFO - __main__ - Epoch  98, Step:  212900, Batch Loss:     2.150386, Lr: 0.000038, Tokens per sec:   2698
2023-03-09 07:48:46,855 - INFO - __main__ - Epoch  98, Step:  213000, Batch Loss:     1.113631, Lr: 0.000038, Tokens per sec:   2677
2023-03-09 07:49:06,826 - INFO - __main__ - Epoch  98, Step:  213100, Batch Loss:     1.194460, Lr: 0.000038, Tokens per sec:   2641
2023-03-09 07:49:26,801 - INFO - __main__ - Epoch  98, Step:  213200, Batch Loss:     1.407826, Lr: 0.000038, Tokens per sec:   2680
2023-03-09 07:49:46,713 - INFO - __main__ - Epoch  98, Step:  213300, Batch Loss:     0.853531, Lr: 0.000038, Tokens per sec:   2711
2023-03-09 07:50:05,546 - INFO - __main__ - Epoch  98, Step:  213400, Batch Loss:     1.994956, Lr: 0.000038, Tokens per sec:   2897
2023-03-09 07:50:25,362 - INFO - __main__ - Epoch  98, Step:  213500, Batch Loss:     1.976970, Lr: 0.000038, Tokens per sec:   2682
2023-03-09 07:50:33,810 - INFO - __main__ - Epoch  98: total training loss 3542.93
2023-03-09 07:50:33,811 - INFO - __main__ - Epoch 99
2023-03-09 07:50:45,335 - INFO - __main__ - Epoch  99, Step:  213600, Batch Loss:     1.027007, Lr: 0.000037, Tokens per sec:   2683
2023-03-09 07:51:03,264 - INFO - __main__ - Epoch  99, Step:  213700, Batch Loss:     1.820549, Lr: 0.000037, Tokens per sec:   2969
2023-03-09 07:51:21,147 - INFO - __main__ - Epoch  99, Step:  213800, Batch Loss:     1.351706, Lr: 0.000037, Tokens per sec:   3013
2023-03-09 07:51:39,068 - INFO - __main__ - Epoch  99, Step:  213900, Batch Loss:     1.282545, Lr: 0.000037, Tokens per sec:   3041
2023-03-09 07:51:57,045 - INFO - __main__ - Epoch  99, Step:  214000, Batch Loss:     2.047291, Lr: 0.000037, Tokens per sec:   2985
2023-03-09 07:52:14,941 - INFO - __main__ - Epoch  99, Step:  214100, Batch Loss:     1.672902, Lr: 0.000037, Tokens per sec:   2979
2023-03-09 07:52:32,902 - INFO - __main__ - Epoch  99, Step:  214200, Batch Loss:     1.644208, Lr: 0.000037, Tokens per sec:   3009
2023-03-09 07:52:50,825 - INFO - __main__ - Epoch  99, Step:  214300, Batch Loss:     1.175301, Lr: 0.000037, Tokens per sec:   3022
2023-03-09 07:53:08,730 - INFO - __main__ - Epoch  99, Step:  214400, Batch Loss:     1.000553, Lr: 0.000037, Tokens per sec:   3041
2023-03-09 07:53:26,658 - INFO - __main__ - Epoch  99, Step:  214500, Batch Loss:     2.093436, Lr: 0.000037, Tokens per sec:   3019
2023-03-09 07:53:44,622 - INFO - __main__ - Epoch  99, Step:  214600, Batch Loss:     1.872288, Lr: 0.000037, Tokens per sec:   2977
2023-03-09 07:54:02,547 - INFO - __main__ - Epoch  99, Step:  214700, Batch Loss:     0.858324, Lr: 0.000037, Tokens per sec:   3016
2023-03-09 07:54:20,523 - INFO - __main__ - Epoch  99, Step:  214800, Batch Loss:     1.481657, Lr: 0.000037, Tokens per sec:   3018
2023-03-09 07:54:38,433 - INFO - __main__ - Epoch  99, Step:  214900, Batch Loss:     2.632783, Lr: 0.000037, Tokens per sec:   3013
2023-03-09 07:54:56,331 - INFO - __main__ - Epoch  99, Step:  215000, Batch Loss:     1.348437, Lr: 0.000037, Tokens per sec:   2985
2023-03-09 07:55:14,391 - INFO - __main__ - Epoch  99, Step:  215100, Batch Loss:     2.737105, Lr: 0.000037, Tokens per sec:   3008
2023-03-09 07:55:33,881 - INFO - __main__ - Epoch  99, Step:  215200, Batch Loss:     1.824460, Lr: 0.000037, Tokens per sec:   2733
2023-03-09 07:55:51,813 - INFO - __main__ - Epoch  99, Step:  215300, Batch Loss:     1.717670, Lr: 0.000037, Tokens per sec:   3044
2023-03-09 07:56:09,789 - INFO - __main__ - Epoch  99, Step:  215400, Batch Loss:     1.993098, Lr: 0.000037, Tokens per sec:   2990
2023-03-09 07:56:27,666 - INFO - __main__ - Epoch  99, Step:  215500, Batch Loss:     1.562608, Lr: 0.000037, Tokens per sec:   3008
2023-03-09 07:56:45,580 - INFO - __main__ - Epoch  99, Step:  215600, Batch Loss:     2.620613, Lr: 0.000037, Tokens per sec:   2995
2023-03-09 07:57:04,797 - INFO - __main__ - Epoch  99, Step:  215700, Batch Loss:     1.256471, Lr: 0.000037, Tokens per sec:   2755
2023-03-09 07:57:09,031 - INFO - __main__ - Epoch  99: total training loss 3516.99
2023-03-09 07:57:09,032 - INFO - __main__ - Epoch 100
2023-03-09 07:57:25,108 - INFO - __main__ - Epoch 100, Step:  215800, Batch Loss:     1.558477, Lr: 0.000037, Tokens per sec:   2604
2023-03-09 07:57:45,084 - INFO - __main__ - Epoch 100, Step:  215900, Batch Loss:     1.170268, Lr: 0.000037, Tokens per sec:   2693
2023-03-09 07:58:04,976 - INFO - __main__ - Epoch 100, Step:  216000, Batch Loss:     1.824615, Lr: 0.000037, Tokens per sec:   2674
2023-03-09 07:58:24,947 - INFO - __main__ - Epoch 100, Step:  216100, Batch Loss:     1.339879, Lr: 0.000037, Tokens per sec:   2672
2023-03-09 07:58:44,877 - INFO - __main__ - Epoch 100, Step:  216200, Batch Loss:     2.245555, Lr: 0.000037, Tokens per sec:   2729
2023-03-09 07:59:04,820 - INFO - __main__ - Epoch 100, Step:  216300, Batch Loss:     1.251965, Lr: 0.000037, Tokens per sec:   2734
2023-03-09 07:59:24,766 - INFO - __main__ - Epoch 100, Step:  216400, Batch Loss:     1.588350, Lr: 0.000037, Tokens per sec:   2685
2023-03-09 07:59:44,721 - INFO - __main__ - Epoch 100, Step:  216500, Batch Loss:     1.125471, Lr: 0.000037, Tokens per sec:   2706
2023-03-09 08:00:04,696 - INFO - __main__ - Epoch 100, Step:  216600, Batch Loss:     2.112933, Lr: 0.000037, Tokens per sec:   2724
2023-03-09 08:00:24,614 - INFO - __main__ - Epoch 100, Step:  216700, Batch Loss:     1.346287, Lr: 0.000037, Tokens per sec:   2642
2023-03-09 08:00:44,583 - INFO - __main__ - Epoch 100, Step:  216800, Batch Loss:     1.546697, Lr: 0.000037, Tokens per sec:   2725
2023-03-09 08:01:04,554 - INFO - __main__ - Epoch 100, Step:  216900, Batch Loss:     2.518069, Lr: 0.000037, Tokens per sec:   2640
2023-03-09 08:01:24,468 - INFO - __main__ - Epoch 100, Step:  217000, Batch Loss:     1.635631, Lr: 0.000037, Tokens per sec:   2691
2023-03-09 08:01:44,405 - INFO - __main__ - Epoch 100, Step:  217100, Batch Loss:     1.639334, Lr: 0.000037, Tokens per sec:   2670
2023-03-09 08:02:04,335 - INFO - __main__ - Epoch 100, Step:  217200, Batch Loss:     1.442396, Lr: 0.000037, Tokens per sec:   2710
2023-03-09 08:02:24,331 - INFO - __main__ - Epoch 100, Step:  217300, Batch Loss:     1.605055, Lr: 0.000037, Tokens per sec:   2732
2023-03-09 08:02:44,284 - INFO - __main__ - Epoch 100, Step:  217400, Batch Loss:     1.801112, Lr: 0.000037, Tokens per sec:   2720
2023-03-09 08:03:04,264 - INFO - __main__ - Epoch 100, Step:  217500, Batch Loss:     1.689053, Lr: 0.000037, Tokens per sec:   2718
2023-03-09 08:03:24,134 - INFO - __main__ - Epoch 100, Step:  217600, Batch Loss:     1.875669, Lr: 0.000037, Tokens per sec:   2718
2023-03-09 08:03:44,107 - INFO - __main__ - Epoch 100, Step:  217700, Batch Loss:     2.761645, Lr: 0.000037, Tokens per sec:   2721
2023-03-09 08:04:04,086 - INFO - __main__ - Epoch 100, Step:  217800, Batch Loss:     1.682945, Lr: 0.000037, Tokens per sec:   2708
2023-03-09 08:04:23,901 - INFO - __main__ - Epoch 100, Step:  217900, Batch Loss:     0.362977, Lr: 0.000037, Tokens per sec:   2698
2023-03-09 08:04:24,044 - INFO - __main__ - Epoch 100: total training loss 3432.97
2023-03-09 08:04:24,045 - INFO - __main__ - Training ended after 100 epoches!
2023-03-09 08:04:24,045 - INFO - __main__ - Best Validation result (greedy) at step        0:   -inf bleu.
